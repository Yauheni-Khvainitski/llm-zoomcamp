{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ba3282a-6099-44b3-81c6-ba4b73d80e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon’t forget to register in DataTalks.Club's Slack and join the channel.\", 'section': 'General course-related questions', 'question': 'Course - When will the course start?', 'course': 'data-engineering-zoomcamp'}, {'text': 'GitHub - DataTalksClub data-engineering-zoomcamp#prerequisites', 'section': 'General course-related questions', 'question': 'Course - What are the prerequisites for this course?', 'course': 'data-engineering-zoomcamp'}, {'text': \"Yes, even if you don't register, you're still eligible to submit the homeworks.\\nBe aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\", 'section': 'General course-related questions', 'question': 'Course - Can I still join the course after the start date?', 'course': 'data-engineering-zoomcamp'}, {'text': \"You don't need it. You're accepted. You can also just start learning and submitting homework without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.\", 'section': 'General course-related questions', 'question': 'Course - I have registered for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?', 'course': 'data-engineering-zoomcamp'}, {'text': 'You can start by installing and setting up all the dependencies and requirements:\\nGoogle cloud account\\nGoogle Cloud SDK\\nPython 3 (installed with Anaconda)\\nTerraform\\nGit\\nLook over the prerequisites and syllabus to see if you are comfortable with these subjects.', 'section': 'General course-related questions', 'question': 'Course - What can I do before the course starts?', 'course': 'data-engineering-zoomcamp'}, {'text': \"There are 3 Zoom Camps in a year, as of 2024. However, they are for separate courses:\\nData-Engineering (Jan - Apr)\\nMLOps (May - Aug)\\nMachine Learning (Sep - Jan)\\nThere's only one Data-Engineering Zoomcamp “live” cohort per year, for the certification. Same as for the other Zoomcamps.\\nThey follow pretty much the same schedule for each cohort per zoomcamp. For Data-Engineering it is (generally) from Jan-Apr of the year. If you’re not interested in the Certificate, you can take any zoom camps at any time, at your own pace, out of sync with any “live” cohort.\", 'section': 'General course-related questions', 'question': 'Course - how many Zoomcamps in a year?', 'course': 'data-engineering-zoomcamp'}, {'text': 'Yes. For the 2024 edition we are using Mage AI instead of Prefect and re-recorded the terraform videos, For 2023, we used Prefect instead of Airflow..', 'section': 'General course-related questions', 'question': 'Course - Is the current cohort going to be different from the previous cohort?', 'course': 'data-engineering-zoomcamp'}, {'text': 'Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\\nYou can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.', 'section': 'General course-related questions', 'question': 'Course - Can I follow the course after it finishes?', 'course': 'data-engineering-zoomcamp'}, {'text': 'Yes, the slack channel remains open and you can ask questions there. But always sDocker containers exit code w search the channel first and second, check the FAQ (this document), most likely all your questions are already answered here.\\nYou can also tag the bot @ZoomcampQABot to help you conduct the search, but don’t rely on its answers 100%, it is pretty good though.', 'section': 'General course-related questions', 'question': 'Course - Can I get support if I take the course in the self-paced mode?', 'course': 'data-engineering-zoomcamp'}, {'text': 'All the main videos are stored in the Main “DATA ENGINEERING” playlist (no year specified). The Github repository has also been updated to show each video with a thumbnail, that would bring you directly to the same playlist below.\\nBelow is the MAIN PLAYLIST’. And then you refer to the year specific playlist for additional videos for that year like for office hours videos etc. Also find this playlist pinned to the slack channel.\\nh\\nttps://youtube.com/playlist?list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&si=NspQhtZhZQs1B9F-', 'section': 'General course-related questions', 'question': 'Course - Which playlist on YouTube should I refer to?', 'course': 'data-engineering-zoomcamp'}, {'text': 'It depends on your background and previous experience with modules. It is expected to require about 5 - 15 hours per week. [source1] [source2]\\nYou can also calculate it yourself using this data and then update this answer.', 'section': 'General course-related questions', 'question': 'Course - \\u200b\\u200bHow many hours per week am I expected to spend on this  course?', 'course': 'data-engineering-zoomcamp'}, {'text': \"No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\", 'section': 'General course-related questions', 'question': 'Certificate - Can I follow the course in a self-paced mode and get a certificate?', 'course': 'data-engineering-zoomcamp'}, {'text': 'The zoom link is only published to instructors/presenters/TAs.\\nStudents participate via Youtube Live and submit questions to Slido (link would be pinned in the chat when Alexey goes Live). The video URL should be posted in the announcements channel on Telegram & Slack before it begins. Also, you will see it live on the DataTalksClub YouTube Channel.\\nDon’t post your questions in chat as it would be off-screen before the instructors/moderators have a chance to answer it if the room is very active.', 'section': 'General course-related questions', 'question': 'Office Hours - What is the video/zoom link to the stream for the “Office Hour” or workshop sessions?', 'course': 'data-engineering-zoomcamp'}, {'text': 'Yes! Every “Office Hours” will be recorded and available a few minutes after the live session is over; so you can view (or rewatch) whenever you want.', 'section': 'General course-related questions', 'question': 'Office Hours - I can’t attend the “Office hours” / workshop, will it be recorded?', 'course': 'data-engineering-zoomcamp'}, {'text': 'You can find the latest and up-to-date deadlines here: https://docs.google.com/spreadsheets/d/e/2PACX-1vQACMLuutV5rvXg5qICuJGL-yZqIV0FBD84CxPdC5eZHf8TfzB-CJT_3Mo7U7oGVTXmSihPgQxuuoku/pubhtml\\nAlso, take note of Announcements from @Au-Tomator for any extensions or other news. Or, the form may also show the updated deadline, if Instructor(s) has updated it.', 'section': 'General course-related questions', 'question': 'Homework - What are homework and project deadlines?', 'course': 'data-engineering-zoomcamp'}, {'text': 'No, late submissions are not allowed. But if the form is still not closed and it’s after the due date, you can still submit the homework. confirm your submission by the date-timestamp on the Course page.y\\nOlder news:[source1] [source2]', 'section': 'General course-related questions', 'question': 'Homework - Are late submissions of homework allowed?', 'course': 'data-engineering-zoomcamp'}, {'text': 'Answer: In short, it’s your repository on github, gitlab, bitbucket, etc\\nIn long, your repository or any other location you have your code where a reasonable person would look at it and think yes, you went through the week and exercises.', 'section': 'General course-related questions', 'question': 'Homework - What is the homework URL in the homework link?', 'course': 'data-engineering-zoomcamp'}, {'text': 'After you submit your homework it will be graded based on the amount of questions in a particular homework. You can see how many points you have right on the page of the homework up top. Additionally in the leaderboard you will find the sum of all points you’ve earned - points for Homeworks, FAQs and Learning in Public. If homework is clear, others work as follows: if you submit something to FAQ, you get one point, for each learning in a public link you get one point.\\n(https://datatalks-club.slack.com/archives/C01FABYF2RG/p1706846846359379?thread_ts=1706825019.546229&cid=C01FABYF2RG)', 'section': 'General course-related questions', 'question': 'Homework and Leaderboard - what is the system for points in the course management platform?', 'course': 'data-engineering-zoomcamp'}, {'text': 'When you set up your account you are automatically assigned a random name such as “Lucid Elbakyan” for example. If you want to see what your Display name is.\\nGo to the Homework submission link →  https://courses.datatalks.club/de-zoomcamp-2024/homework/hw2 - Log in > Click on ‘Data Engineering Zoom Camp 2024’ > click on ‘Edit Course Profile’ - your display name is here, you can also change it should you wish:', 'section': 'General course-related questions', 'question': 'Leaderboard - I am not on the leaderboard / how do I know which one I am on the leaderboard?', 'course': 'data-engineering-zoomcamp'}, {'text': 'Yes, for simplicity (of troubleshooting against the recorded videos) and stability. [source]\\nBut Python 3.10 and 3.11 should work fine.', 'section': 'General course-related questions', 'question': 'Environment - Is Python 3.9 still the recommended version to use in 2024?', 'course': 'data-engineering-zoomcamp'}, {'text': 'You can set it up on your laptop or PC if you prefer to work locally from your laptop or PC.\\nYou might face some challenges, especially for Windows users. If you face cnd2\\nIf you prefer to work on the local machine, you may start with the week 1 Introduction to Docker and follow through.\\nHowever, if you prefer to set up a virtual machine, you may start with these first:\\nUsing GitHub Codespaces\\nSetting up the environment on a cloudV Mcodespace\\nI decided to work on a virtual machine because I have different laptops & PCs for my home & office, so I can work on this boot camp virtually anywhere.', 'section': 'General course-related questions', 'question': 'Environment - Should I use my local machine, GCP, or GitHub Codespaces for my environment?', 'course': 'data-engineering-zoomcamp'}, {'text': 'GitHub Codespaces offers you computing Linux resources with many pre-installed tools (Docker, Docker Compose, Python).\\nYou can also open any GitHub repository in a GitHub Codespace.', 'section': 'General course-related questions', 'question': 'Environment - Is GitHub codespaces an alternative to using cli/git bash to ingest the data and create a docker file?', 'course': 'data-engineering-zoomcamp'}, {'text': \"It's up to you which platform and environment you use for the course.\\nGithub codespaces or GCP VM are just possible options, but you can do the entire course from your laptop.\", 'section': 'General course-related questions', 'question': 'Environment - Do we really have to use GitHub codespaces? I already have PostgreSQL & Docker installed.', 'course': 'data-engineering-zoomcamp'}, {'text': 'Choose the approach that aligns the most with your idea for the end project\\nOne of those should suffice. However, BigQuery, which is part of GCP, will be used, so learning that is probably a better option. Or you can set up a local environment for most of this course.', 'section': 'General course-related questions', 'question': 'Environment - Do I need both GitHub Codespaces and GCP?', 'course': 'data-engineering-zoomcamp'}, {'text': '1. To open Run command window, you can either:\\n(1-1) Use the shortcut keys: \\'Windows + R\\', or\\n(1-2) Right Click \"Start\", and click \"Run\" to open.\\n2. Registry Values Located in Registry Editor, to open it: Type \\'regedit\\' in the Run command window, and then press Enter.\\' 3. Now you can change the registry values \"Autorun\" in \"HKEY_CURRENT_USER\\\\Software\\\\Microsoft\\\\Command Processor\" from \"if exists\" to a blank.\\nAlternatively, You can simplify the solution by deleting the fingerprint saved within the known_hosts file. In Windows, this file is placed at  C:\\\\Users\\\\<your_user_name>\\\\.ssh\\\\known_host', 'section': 'General course-related questions', 'question': 'This happens when attempting to connect to a GCP VM using VSCode on a Windows machine. Changing registry value in registry editor', 'course': 'data-engineering-zoomcamp'}, {'text': 'For uniformity at least, but you’re not restricted to GCP, you can use other cloud platforms like AWS if you’re comfortable with other cloud platforms, since you get every service that’s been provided by GCP in Azure and AWS or others..\\nBecause everyone has a google account, GCP has a free trial period and gives $300 in credits  to new users. Also, we are working with BigQuery, which is a part of GCP.\\nNote that to sign up for a free GCP account, you must have a valid credit card.', 'section': 'General course-related questions', 'question': 'Environment - Why are we using GCP and not other cloud providers?', 'course': 'data-engineering-zoomcamp'}, {'text': 'No, if you use GCP and take advantage of their free trial.', 'section': 'General course-related questions', 'question': 'Should I pay for cloud services?', 'course': 'data-engineering-zoomcamp'}, {'text': 'You can do most of the course without a cloud. Almost everything we use (excluding BigQuery) can be run locally. We won’t be able to provide guidelines for some things, but most of the materials are runnable without GCP.\\nFor everything in the course, there’s a local alternative. You could even do the whole course locally.', 'section': 'General course-related questions', 'question': 'Environment - The GCP and other cloud providers are unavailable in some countries. Is it possible to provide a guide to installing a home lab?', 'course': 'data-engineering-zoomcamp'}, {'text': 'Yes, you can. Just remember to adapt all the information on the videos to AWS. Besides, the final capstone will be evaluated based on the task: Create a data pipeline! Develop a visualisation!\\nThe problem would be when you need help. You’d need to rely on  fellow coursemates who also use AWS (or have experience using it before), which might be in smaller numbers than those learning the course with GCP.\\nAlso see Is it possible to use x tool instead of the one tool you use?', 'section': 'General course-related questions', 'question': 'Environment - I want to use AWS. May I do that?', 'course': 'data-engineering-zoomcamp'}, {'text': 'We will probably have some calls during the Capstone period to clear some questions but it will be announced in advance if that happens.', 'section': 'General course-related questions', 'question': 'Besides the “Office Hour” which are the live zoom calls?', 'course': 'data-engineering-zoomcamp'}, {'text': 'We will use the same data, as the project will essentially remain the same as last year’s. The data is available here', 'section': 'General course-related questions', 'question': 'Are we still using the NYC Trip data for January 2021? Or are we using the 2022 data?', 'course': 'data-engineering-zoomcamp'}, {'text': 'No, but we moved the 2022 stuff here', 'section': 'General course-related questions', 'question': 'Is the 2022 repo deleted?', 'course': 'data-engineering-zoomcamp'}, {'text': 'Yes, you can use any tool you want for your project.', 'section': 'General course-related questions', 'question': 'Can I use Airflow instead for my final project?', 'course': 'data-engineering-zoomcamp'}, {'text': 'Yes, this applies if you want to use Airflow or Prefect instead of Mage, AWS or Snowflake instead of GCP products or Tableau instead of Metabase or Google data studio.\\nThe course covers 2 alternative data stacks, one using GCP and one using local installation of everything. You can use one of them or use your tool of choice.\\nShould you consider it instead of the one tool you use? That we can’t support you if you choose to use a different stack, also you would need to explain the different choices of tool for the peer review of your capstone project.', 'section': 'General course-related questions', 'question': 'Is it possible to use tool “X” instead of the one tool you use in the course?', 'course': 'data-engineering-zoomcamp'}, {'text': 'Star the repo! Share it with friends if you find it useful ❣️\\nCreate a PR if you see you can improve the text or the structure of the repository.', 'section': 'General course-related questions', 'question': 'How can we contribute to the course?', 'course': 'data-engineering-zoomcamp'}, {'text': 'Yes! Linux is ideal but technically it should not matter. Students last year used all 3 OSes successfully', 'section': 'General course-related questions', 'question': 'Environment - Is the course [Windows/mac/Linux/...] friendly?', 'course': 'data-engineering-zoomcamp'}, {'text': \"Have no idea how past cohorts got past this as I haven't read old slack messages, and no FAQ entries that I can find.\\nLater modules (module-05 & RisingWave workshop) use shell scripts in *.sh files and most Windows users not using WSL would hit a wall and cannot continue, even in git bash or MINGW64. This is why WSL environment setup is recommended from the start.\", 'section': 'General course-related questions', 'question': 'Environment - Roadblock for Windows users in modules with *.sh (shell scripts).', 'course': 'data-engineering-zoomcamp'}, {'text': 'Yes to both! check out this document: https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/awesome-data-engineering.md', 'section': 'General course-related questions', 'question': 'Any books or additional resources you recommend?', 'course': 'data-engineering-zoomcamp'}, {'text': 'You will have two attempts for a project. If the first project deadline is over and you’re late or you submit the project and fail the first attempt, you have another chance to submit the project with the second attempt.', 'section': 'General course-related questions', 'question': 'Project - What is Project Attemp #1 and Project Attempt #2 exactly?', 'course': 'data-engineering-zoomcamp'}, {'text': \"The first step is to try to solve the issue on your own. Get used to solving problems and reading documentation. This will be a real life skill you need when employed. [ctrl+f] is your friend, use it! It is a universal shortcut and works in all apps/browsers.\\nWhat does the error say? There will often be a description of the error or instructions on what is needed or even how to fix it. I have even seen a link to the solution. Does it reference a specific line of your code?\\nRestart app or server/pc.\\nGoogle it, use ChatGPT, Bing AI etc.\\nIt is going to be rare that you are the first to have the problem, someone out there has posted the fly issue and likely the solution.\\nSearch using: <technology> <problem statement>. Example: pgcli error column c.relhasoids does not exist.\\nThere are often different solutions for the same problem due to variation in environments.\\nCheck the tech’s documentation. Use its search if available or use the browsers search function.\\nTry uninstall (this may remove the bad actor) and reinstall of application or reimplementation of action. Remember to restart the server/pc for reinstalls.\\nSometimes reinstalling fails to resolve the issue but works if you uninstall first.\\nPost your question to Stackoverflow. Read the Stackoverflow guide on posting good questions.\\nhttps://stackoverflow.com/help/how-to-ask\\nThis will be your real life. Ask an expert in the future (in addition to coworkers).\\nAsk in Slack\\nBefore asking a question,\\nCheck Pins (where the shortcut to the repo and this FAQ is located)\\nUse the slack app’s search function\\nUse the bot @ZoomcampQABot to do the search for you\\ncheck the FAQ (this document), use search [ctrl+f]\\nWhen asking a question, include as much information as possible:\\nWhat are you coding on? What OS?\\nWhat command did you run, which video did you follow? Etc etc\\nWhat error did you get? Does it have a line number to the “offending” code and have you check it for typos?\\nWhat have you tried that did not work? This answer is crucial as without it, helpers would ask you to do the suggestions in the error log first. Or just read this FAQ document.\\nDO NOT use screenshots, especially don’t take pictures from a phone.\\nDO NOT tag instructors, it may discourage others from helping you. Copy and paste errors; if it’s long, just post it in a reply to your thread.\\nUse ``` for formatting your code.\\nUse the same thread for the conversation (that means reply to your own thread).\\nDO NOT create multiple posts to discuss the issue.\\nlearYou may create a new post if the issue reemerges down the road. Describe what has changed in the environment.\\nProvide additional information in the same thread of the steps you have taken for resolution.\\nTake a break and come back later. You will be amazed at how often you figure out the solution after letting your brain rest. Get some fresh air, workout, play a video game, watch a tv show, whatever allows your brain to not think about it for a little while or even until the next day.\\nRemember technology issues in real life sometimes take days or even weeks to resolve.\\nIf somebody helped you with your problem and it's not in the FAQ, please add it there. It will help other students.\", 'section': 'General course-related questions', 'question': 'How to troubleshoot issues', 'course': 'data-engineering-zoomcamp'}, {'text': 'When the troubleshooting guide above does not help resolve it and you need another pair of eyeballs to spot mistakes. When asking a question, include as much information as possible:\\nWhat are you coding on? What OS?\\nWhat command did you run, which video did you follow? Etc etc\\nWhat error did you get? Does it have a line number to the “offending” code and have you check it for typos?\\nWhat have you tried that did not work? This answer is crucial as without it, helpers would ask you to do the suggestions in the error log first. Or just read this FAQ document.', 'section': 'General course-related questions', 'question': 'How to ask questions', 'course': 'data-engineering-zoomcamp'}, {'text': 'After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\\nHaving this local repository on your computer will make it easy for you to access the instructors’ code and make pull requests (if you want to add your own notes or make changes to the course content).\\nYou will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\\nRemember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\\nThis is also a great resource: https://dangitgit.com/', 'section': 'General course-related questions', 'question': 'How do I use Git / GitHub for this course?', 'course': 'data-engineering-zoomcamp'}, {'text': 'Error: Makefile:2: *** missing separator.  Stop.\\nSolution: Tabs in document should be converted to Tab instead of spaces. Follow this stack.', 'section': 'General course-related questions', 'question': 'VS Code: Tab using spaces', 'course': 'data-engineering-zoomcamp'}, {'text': \"If you’re running Linux on Windows Subsystem for Linux (WSL) 2, you can open HTML files from the guest (Linux) with whatever Internet Browser you have installed on the host (Windows). Just install wslu and open the page with wslview <file>, for example:\\nwslview index.html\\nYou can customise which browser to use by setting the BROWSER environment variable first. For example:\\nexport BROWSER='/mnt/c/Program Files/Firefox/firefox.exe'\", 'section': 'General course-related questions', 'question': 'Opening an HTML file with a Windows browser from Linux running on WSL', 'course': 'data-engineering-zoomcamp'}, {'text': 'This tutorial shows you how to set up the Chrome Remote Desktop service on a Debian Linux virtual machine (VM) instance on Compute Engine. Chrome Remote Desktop allows you to remotely access applications with a graphical user interface.\\nTaxi Data - Yellow Taxi Trip Records downloading error, Error no or XML error webpage\\nWhen you try to download the 2021 data from TLC website, you get this error:\\nIf you click on the link, and ERROR 403: Forbidden on the terminal.\\nWe have a backup, so use it instead: https://github.com/DataTalksClub/nyc-tlc-data\\nSo the link should be https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz\\nNote: Make sure to unzip the “gz” file (no, the “unzip” command won’t work for this.)\\n“gzip -d file.gz”g', 'section': 'Module 1: Docker and Terraform', 'question': 'Set up Chrome Remote Desktop for Linux on Compute Engine', 'course': 'data-engineering-zoomcamp'}, {'text': 'In this video, we store the data file as “output.csv”. The data file won’t store correctly if the file extension is csv.gz instead of csv. One alternative is to replace csv_name = “output.cs -v” with the file name given at the end of the URL. Notice that the URL for the yellow taxi data is: https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz where the highlighted part is the name of the file. We can parse this file name from the URL and use it as csv_name. That is, we can replace csv_name = “output.csv” with\\ncsv_name = url.split(“/”)[-1] . Then when we use csv_name to using pd.read_csv, there won’t be an issue even though the file name really has the extension csv.gz instead of csv since the pandas read_csv function can read csv.gz files directly.', 'section': 'Module 1: Docker and Terraform', 'question': 'Taxi Data - How to handle taxi data files, now that the files are available as *.csv.gz?', 'course': 'data-engineering-zoomcamp'}, {'text': 'Yellow Trips: https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf\\nGreen Trips: https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_green.pdf', 'section': 'Module 1: Docker and Terraform', 'question': 'Taxi Data - Data Dictionary for NY Taxi data?', 'course': 'data-engineering-zoomcamp'}, {'text': 'You can unzip this downloaded parquet file, in the command line. The result is a csv file which can be imported with pandas using the pd.read_csv() shown in the videos.\\n‘’’gunzip green_tripdata_2019-09.csv.gz’’’\\nSOLUTION TO USING PARQUET FILES DIRECTLY IN PYTHON SCRIPT ingest_data.py\\nIn the def main(params) add this line\\nparquet_name= \\'output.parquet\\'\\nThen edit the code which downloads the files\\nos.system(f\"wget {url} -O {parquet_name}\")\\nConvert the download .parquet file to csv and rename as csv_name to keep it relevant to the rest of the code\\ndf = pd.read_parquet(parquet_name)\\ndf.to_csv(csv_name, index=False)', 'section': 'Module 1: Docker and Terraform', 'question': 'Taxi Data - Unzip Parquet file', 'course': 'data-engineering-zoomcamp'}, {'text': '“wget is not recognized as an internal or external command”, you need to install it.\\nOn Ubuntu, run:\\n$ sudo apt-get install wget\\nOn MacOS, the easiest way to install wget is to use Brew:\\n$ brew install wget\\nOn Windows, the easiest way to install wget is to use Chocolatey:\\n$ choco install wget\\nOr you can download a binary (https://gnuwin32.sourceforge.net/packages/wget.htm) and put it to any location in your PATH (e.g. C:/tools/)\\nAlso, you can following this step to install Wget on MS Windows\\n* Download the latest wget binary for windows from [eternallybored] (https://eternallybored.org/misc/wget/) (they are available as a zip with documentation, or just an exe)\\n* If you downloaded the zip, extract all (if windows built in zip utility gives an error, use [7-zip] (https://7-zip.org/)).\\n* Rename the file `wget64.exe` to `wget.exe` if necessary.\\n* Move wget.exe to your `Git\\\\mingw64\\\\bin\\\\`.\\nAlternatively, you can use a Python wget library, but instead of simply using “wget” you’ll need to use\\npython -m wget\\nYou need to install it with pip first:\\npip install wget\\nAlternatively, you can just paste the file URL into your web browser and download the file normally that way. You’ll want to move the resulting file into your working directory.\\nAlso recommended a look at the python library requests for the loading gz file  https://pypi.org/project/requests', 'section': 'Module 1: Docker and Terraform', 'question': 'lwget is not recognized as an internal or external command', 'course': 'data-engineering-zoomcamp'}, {'text': 'Firstly, make sure that you add “!” before wget if you’re running your command in a Jupyter Notebook or CLI. Then, you can check one of this 2 things (from CLI):\\nUsing the Python library wget you installed with pip, try python -m wget <url>\\nWrite the usual command and add --no-check-certificate at the end. So it should be:\\n!wget <website_url> --no-check-certificate', 'section': 'Module 1: Docker and Terraform', 'question': 'wget - ERROR: cannot verify <website> certificate  (MacOS)', 'course': 'data-engineering-zoomcamp'}, {'text': 'For those who wish to use the backslash as an escape character in Git Bash for Windows (as Alexey normally does), type in the terminal: bash.escapeChar=\\\\ (no need to include in .bashrc)', 'section': 'Module 1: Docker and Terraform', 'question': 'Git Bash - Backslash as an escape character in Git Bash for Windows', 'course': 'data-engineering-zoomcamp'}, {'text': 'Instruction on how to store secrets that will be avialable in GitHub  Codespaces.\\nManaging your account-specific secrets for GitHub Codespaces - GitHub Docs', 'section': 'Module 1: Docker and Terraform', 'question': 'GitHub Codespaces - How to store secrets', 'course': 'data-engineering-zoomcamp'}, {'text': \"Make sure you're able to start the Docker daemon, and check the issue immediately down below:\\nAnd don’t forget to update the wsl in powershell the  command is wsl –update\", 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - Cannot connect to Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?', 'course': 'data-engineering-zoomcamp'}, {'text': \"As the official Docker for Windows documentation says, the Docker engine can either use the\\nHyper-V or WSL2 as its backend. However, a few constraints might apply\\nWindows 10 Pro / 11 Pro Users: \\nIn order to use Hyper-V as its back-end, you MUST have it enabled first, which you can do by following the tutorial: Enable Hyper-V Option on Windows 10 / 11\\nWindows 10 Home / 11 Home Users: \\nOn the other hand, Users of the 'Home' version do NOT have the option Hyper-V option enabled, which means, you can only get Docker up and running using the WSL2 credentials(Windows Subsystem for Linux). Url\\nYou can find the detailed instructions to do so here: rt ghttps://pureinfotech.com/install-wsl-windows-11/\\nIn case, you run into another issue while trying to install WSL2 (WslRegisterDistribution failed with error: 0x800701bc), Make sure you update the WSL2 Linux Kernel, following the guidelines here: \\n\\nhttps://github.com/microsoft/WSL/issues/5393\", 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - Error during connect: In the default daemon configuration on Windows, the docker client must be run with elevated privileges to connect.: Post: \"http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.24/containers/create\" : open //./pipe/docker_engine: The system cannot find the file specified', 'course': 'data-engineering-zoomcamp'}, {'text': 'Whenever a `docker pull is performed (either manually or by `docker-compose up`), it attempts to fetch the given image name (pgadmin4, for the example above) from a repository (dbpage).\\nIF the repository is public, the fetch and download happens without any issue whatsoever.\\nFor instance:\\ndocker pull postgres:13\\ndocker pull dpage/pgadmin4\\nBE ADVISED:\\n\\nThe Docker Images we\\'ll be using throughout the Data Engineering Zoomcamp are all public (except when or if explicitly said otherwise by the instructors or co-instructors).\\n\\nMeaning: you are NOT required to perform a docker login to fetch them. \\n\\nSo if you get the message above saying \"docker login\\': denied: requested access to the resource is denied. That is most likely due to a typo in your image name:\\n\\nFor instance:\\n$ docker pull dbpage/pgadmin4\\nWill throw that exception telling you \"repository does not exist or may require \\'docker login\\'\\nError response from daemon: pull access denied for dbpage/pgadmin4, repository does not exist or \\nmay require \\'docker login\\': denied: requested access to the resource is denied\\nBut that actually happened because the actual image is dpage/pgadmin4 and NOT dbpage/pgadmin4\\nHow to fix it:\\n$ docker pull dpage/pgadmin4\\nEXTRA NOTES:\\nIn the real world, occasionally, when you\\'re working for a company or closed organisation, the Docker image you\\'re trying to fetch might be under a private repo that your DockerHub Username was granted access to.\\nFor which cases, you must first execute:\\n$ docker login\\nFill in the details of your username and password.\\nAnd only then perform the `docker pull` against that private repository\\nWhy am I encountering a \"permission denied\" error when creating a PostgreSQL Docker container for the New York Taxi Database with a mounted volume on macOS M1?\\nIssue Description:\\nWhen attempting to run a Docker command similar to the one below:\\ndocker run -it \\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"root\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n-p 5432:5432 \\\\mount\\npostgres:13\\nYou encounter the error message:\\ndocker: Error response from daemon: error while creating mount source path \\'/path/to/ny_taxi_postgres_data\\': chown /path/to/ny_taxi_postgres_data: permission denied.\\nSolution:\\n1- Stop Rancher Desktop:\\nIf you are using Rancher Desktop and face this issue, stop Rancher Desktop to resolve compatibility problems.\\n2- Install Docker Desktop:\\nInstall Docker Desktop, ensuring that it is properly configured and has the required permissions.\\n2-Retry Docker Command:\\nRun the Docker command again after switching to Docker Desktop. This step resolves compatibility issues on some systems.\\nNote: The issue occurred because Rancher Desktop was in use. Switching to Docker Desktop resolves compatibility problems and allows for the successful creation of PostgreSQL containers with mounted volumes for the New York Taxi Database on macOS M1.', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - docker pull dbpage', 'course': 'data-engineering-zoomcamp'}, {'text': 'When I runned command to create postgre in docker container it created folder on my local machine to mount it to volume inside container. It has write and read protection and owned by user 999, so I could not delete it by simply drag to trash.  My obsidian could not started due to access error, so I had to change placement of this folder and delete old folder by this command:\\nsudo rm -r -f docker_test/\\n- where `rm` - remove, `-r` - recursively, `-f` - force, `docker_test/` - folder.', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - can’t delete local folder that mounted to docker volume', 'course': 'data-engineering-zoomcamp'}, {'text': 'First off, make sure you\\'re running the latest version of Docker for Windows, which you can download from here. Sometimes using the menu to \"Upgrade\" doesn\\'t work (which is another clear indicator for you to uninstall, and reinstall with the latest version)\\nIf docker is stuck on starting, first try to switch containers by right clicking the docker symbol from the running programs and switch the containers from windows to linux or vice versa\\n[Windows 10 / 11 Pro Edition] The Pro Edition of Windows can run Docker either by using Hyper-V or WSL2 as its backend (Docker Engine)\\nIn order to use Hyper-V as its back-end, you MUST have it enabled first, which you can do by following the tutorial: Enable Hyper-V Option on Windows 10 / 11\\nIf you opt-in for WSL2, you can follow the same steps as detailed in the tutorial here', 'section': 'Module 1: Docker and Terraform', 'question': \"Docker - Docker won't start or is stuck in settings (Windows 10 / 11)\", 'course': 'data-engineering-zoomcamp'}, {'text': \"It is recommended by the Docker do\\n[Windows 10 / 11 Home Edition] If you're running a Home Edition, you can still make it work with WSL2 (Windows Subsystem for Linux) by following the tutorial here\\nIf even after making sure your WSL2 (or Hyper-V) is set up accordingly, Docker remains stuck, you can try the option to Reset to Factory Defaults or do a fresh install.\", 'section': 'Module 1: Docker and Terraform', 'question': 'Should I run docker commands from the windows file system or a file system of a Linux distribution in WSL?', 'course': 'data-engineering-zoomcamp'}, {'text': 'More info in the Docker Docs on Best Practises', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - cs to store all code in your default Linux distro to get the best out of file system performance (since Docker runs on WSL2 backend by default for Windows 10 Home / Windows 11 Home users).', 'course': 'data-engineering-zoomcamp'}, {'text': 'You may have this error:\\n$ docker run -it ubuntu bash\\nthe input device is not a TTY. If you are using mintty, try prefixing the command with \\'winpty\\'\\nerror:\\nSolution:\\nUse winpty before docker command (source)\\n$ winpty docker run -it ubuntu bash\\nYou also can make an alias:\\necho \"alias docker=\\'winpty docker\\'\" >> ~/.bashrc\\nOR\\necho \"alias docker=\\'winpty docker\\'\" >> ~/.bash_profile', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - The input device is not a TTY (Docker run for Windows)', 'course': 'data-engineering-zoomcamp'}, {'text': \"You may have this error:\\nRetrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.u\\nrllib3.connection.HTTPSConnection object at 0x7efe331cf790>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')':\\n/simple/pandas/\\nPossible solution might be:\\n$ winpty docker run -it --dns=8.8.8.8 --entrypoint=bash python:3.9\", 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - Cannot pip install on Docker container (Windows)', 'course': 'data-engineering-zoomcamp'}, {'text': 'Even after properly running the docker script the folder is empty in the vs code  then try this (For Windows)\\nwinpty docker run -it \\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"root\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-v \"C:\\\\Users\\\\abhin\\\\dataengg\\\\DE_Project_git_connected\\\\DE_OLD\\\\week1_set_up\\\\docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data\" \\\\\\n-p 5432:5432 \\\\\\npostgres:13\\nHere quoting the absolute path in  the -v parameter is solving the issue and all the files are visible in the Vs-code ny_taxi folder as shown in the video', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - ny_taxi_postgres_data is empty', 'course': 'data-engineering-zoomcamp'}, {'text': 'Check this article for details - Setting up docker in macOS\\nFrom researching it seems this method might be out of date, it seems that since docker changed their licensing model, the above is a bit hit and miss. What worked for me was to just go to the docker website and download their dmg. Haven’t had an issue with that method.', 'section': 'Module 1: Docker and Terraform', 'question': 'dasDocker - Setting up Docker on Mac', 'course': 'data-engineering-zoomcamp'}, {'text': '$ docker run -it\\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"admin\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-v \"/mnt/path/to/ny_taxi_postgres_data\":\"/var/lib/postgresql/data\" \\\\\\n-p 5432:5432 \\\\\\npostgres:13\\nCCW\\nThe files belonging to this database system will be owned by user \"postgres\".\\nThis use The database cluster will be initialized with locale \"en_US.utf8\".\\nThe default databerrorase encoding has accordingly been set to \"UTF8\".\\nxt search configuration will be set to \"english\".\\nData page checksums are disabled.\\nfixing permissions on existing directory /var/lib/postgresql/data ... initdb: f\\nerror: could not change permissions of directory \"/var/lib/postgresql/data\": Operation not permitted  volume\\nOne way to solve this issue is to create a local docker volume and map it to postgres data directory /var/lib/postgresql/data\\nThe input dtc_postgres_volume_local must match in both commands below\\n$ docker volume create --name dtc_postgres_volume_local -d local\\n$ docker run -it\\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"root\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-v dtc_postgres_volume_local:/var/lib/postgresql/data \\\\\\n-p 5432:5432\\\\\\npostgres:13\\nTo verify the above command works in (WSL2 Ubuntu 22.04, verified 2024-Jan), go to the Docker Desktop app and look under Volumes - dtc_postgres_volume_local would be listed there. The folder ny_taxi_postgres_data would however be empty, since we used an alternative config.\\nAn alternate error could be:\\ninitdb: error: directory \"/var/lib/postgresql/data\" exists but is not empty\\nIf you want to create a new database system, either remove or empthe directory \"/var/lib/postgresql/data\" or run initdb\\nwitls', 'section': 'Module 1: Docker and Terraform', 'question': '1Docker - Could not change permissions of directory \"/var/lib/postgresql/data\": Operation not permitted', 'course': 'data-engineering-zoomcamp'}, {'text': 'Mapping volumes on Windows could be tricky. The way it was done in the course video doesn’t work for everyone.\\nFirst, if yo\\nmove your data to some folder without spaces. E.g. if your code is in “C:/Users/Alexey Grigorev/git/…”, move it to “C:/git/…”\\nTry replacing the “-v” part with one of the following options:\\n-v /c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n-v //c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n-v /c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n-v //c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n--volume //driveletter/path/ny_taxi_postgres_data/:/var/lib/postgresql/data\\nwinpty docker run -it\\n-e POSTGRES_USER=\"root\"\\n-e POSTGRES_PASSWORD=\"root\"\\n-e POSTGRES_DB=\"ny_taxi\"\\n-v /c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n-p 5432:5432\\npostgres:1\\nTry adding winpty before the whole command\\n3\\nwin\\nTry adding quotes:\\n-v \"/c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\"\\n-v \"//c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\"\\n-v “/c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\"\\n-v \"//c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\"\\n-v \"c:\\\\some\\\\path\\\\ny_taxi_postgres_data\":/var/lib/postgresql/data\\nNote:  (Window) if it automatically creates a folder called “ny_taxi_postgres_data;C” suggests you have problems with volume mapping, try deleting both folders and replacing “-v” part with other options. For me “//c/” works instead of “/c/”. And it will work by automatically creating a correct folder called “ny_taxi_postgres_data”.\\nA possible solution to this error would be to use /”$(pwd)”/ny_taxi_postgres_data:/var/lib/postgresql/data (with quotes’ position varying as in the above list).\\nYes for windows use the command it works perfectly fine\\n-v /”$(pwd)”/ny_taxi_postgres_data:/var/lib/postgresql/data\\nImportant: note how the quotes are placed.\\nIf none of these options work, you can use a volume name instead of the path:\\n-v ny_taxi_postgres_data:/var/lib/postgresql/data\\nFor Mac: You can wrap $(pwd) with quotes like the highlighted.\\ndocker run -it \\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"root\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-v \"$(pwd)\"/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n-p 5432:5432 \\\\\\nPostgres:13\\ndocker run -it \\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"root\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-v \"$(pwd)\"/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n-p 5432:5432 \\\\\\npostgres:13\\nSource:https://stackoverflow.com/questions/48522615/docker-error-invalid-reference-format-repository-name-must-be-lowercase', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - invalid reference format: repository name must be lowercase (Mounting volumes with Docker on Windows)', 'course': 'data-engineering-zoomcamp'}, {'text': 'Change the mounting path. Replace it with one of following:\\n-v /e/zoomcamp/...:/var/lib/postgresql/data\\n-v /c:/.../ny_taxi_postgres_data:/var/lib/postgresql/data\\\\ (leading slash in front of c:)', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - Error response from daemon: invalid mode: \\\\Program Files\\\\Git\\\\var\\\\lib\\\\postgresql\\\\data.', 'course': 'data-engineering-zoomcamp'}, {'text': 'When you run this command second time\\ndocker run -it \\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"root\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-v <your path>:/var/lib/postgresql/data \\\\\\n-p 5432:5432 \\\\\\npostgres:13\\nThe error message above could happen. That means you should not mount on the second run. This command helped me:\\nWhen you run this command second time\\ndocker run -it \\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"root\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-p 5432:5432 \\\\\\npostgres:13', 'section': 'Module 1: Docker and Terraform', 'question': \"Docker - Error response from daemon: error while creating buildmount source path '/run/desktop/mnt/host/c/<your path>': mkdir /run/desktop/mnt/host/c: file exists\", 'course': 'data-engineering-zoomcamp'}, {'text': 'This error appeared when running the command: docker build -t taxi_ingest:v001 .\\nWhen feeding the database with the data the user id of the directory ny_taxi_postgres_data was changed to 999, so my user couldn’t access it when running the above command. Even though this is not the problem here it helped to raise the error due to the permission issue.\\nSince at this point we only need the files Dockerfile and ingest_data.py, to fix this error one can run the docker build command on a different directory (having only these two files).\\nA more complete explanation can be found here: https://stackoverflow.com/questions/41286028/docker-build-error-checking-context-cant-stat-c-users-username-appdata\\nYou can fix the problem by changing the permission of the directory on ubuntu with following command:\\nsudo chown -R $USER dir_path\\nOn windows follow the link: https://thegeekpage.com/take-ownership-of-a-file-folder-through-command-prompt-in-windows-10/ \\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tAdded by\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tKenan Arslanbay', 'section': 'Module 1: Docker and Terraform', 'question': \"Docker - build error: error checking context: 'can't stat '/home/user/repos/data-engineering/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data''.\", 'course': 'data-engineering-zoomcamp'}, {'text': 'You might have installed docker via snap. Run “sudo snap status docker” to verify.\\nIf you have “error: unknown command \"status\", see \\'snap help\\'.” as a response than deinstall docker and install via the official website\\nBind for 0.0.0.0:5432 failed: port is a', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - ERRO[0000] error waiting for container: context canceled', 'course': 'data-engineering-zoomcamp'}, {'text': 'Found the issue in the PopOS linux. It happened because our user didn’t have authorization rights to the host folder ( which also caused folder seems empty, but it didn’t!).\\n✅Solution:\\nJust add permission for everyone to the corresponding folder\\nsudo chmod -R 777 <path_to_folder>\\nExample:\\nsudo chmod -R 777 ny_taxi_postgres_data/', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - build error checking context: can’t stat ‘/home/fhrzn/Projects/…./ny_taxi_postgres_data’', 'course': 'data-engineering-zoomcamp'}, {'text': 'This happens on Ubuntu/Linux systems when trying to run the command to build the Docker container again.\\n$ docker build -t taxi_ingest:v001 .\\nA folder is created to host the Docker files. When the build command is executed again to rebuild the pipeline or create a new one the error is raised as there are no permissions on this new folder. Grant permissions by running this comtionmand;\\n$ sudo chmod -R 755 ny_taxi_postgres_data\\nOr use 777 if you still see problems. 755 grants write access to only the owner.', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - failed to solve with frontend dockerfile.v0: failed to read dockerfile: error from sender: open ny_taxi_postgres_data: permission denied.', 'course': 'data-engineering-zoomcamp'}, {'text': 'Get the network name via: $ docker network ls.', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - Docker network name', 'course': 'data-engineering-zoomcamp'}, {'text': 'Sometimes, when you try to restart a docker image configured with a network name, the above message appears. In this case, use the following command with the appropriate container name:\\n>>> If the container is running state, use docker stop <container_name>\\n>>> then, docker rm pg-database\\nOr use docker start instead of docker run in order to restart the docker image without removing it.', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - Error response from daemon: Conflict. The container name \"pg-database\" is already in use by container “xxx”.  You have to remove (or rename) that container to be able to reuse that name.', 'course': 'data-engineering-zoomcamp'}, {'text': 'Typical error: sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name \"pgdatabase\" to address: Name or service not known\\nWhen running docker-compose up -d see which network is created and use this for the ingestions script instead of pg-network and see the name of the database to use instead of pgdatabase\\nE.g.:\\npg-network becomes 2docker_default\\nPgdatabase becomes 2docker-pgdatabase-1', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - ingestion when using docker-compose could not translate host name', 'course': 'data-engineering-zoomcamp'}, {'text': 'terraformRun this command before starting your VM:\\nOn Intel CPU:\\nmodprobe -r kvm_intel\\nmodprobe kvm_intel nested=1\\nOn AMD CPU:\\nmodprobe -r kvm_amd\\nmodprobe kvm_amd nested=1', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - Cannot install docker on MacOS/Windows 11 VM running on top of Linux (due to Nested virtualization).', 'course': 'data-engineering-zoomcamp'}, {'text': 'It’s very easy to manage your docker container, images, network and compose projects from VS Code.\\nJust install the official extension and launch it from the left side icon.\\nIt will work even if your Docker runs on WSL2, as VS Code can easily connect with your Linux.\\nDocker - How to stop a container?\\nUse the following command:\\n$ docker stop <container_id>', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - Connecting from VS Code', 'course': 'data-engineering-zoomcamp'}, {'text': \"When you see this in logs, your container with postgres is not accepting any requests, so if you attempt to connect, you'll get this error:\\nconnection failed: server closed the connection unexpectedly\\nThis probably means the server terminated abnormally before or while processing the request.\\nIn this case, you need to delete the directory with data (the one you map to the container with the -v flag) and restart the container.\", 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - PostgreSQL Database directory appears to contain a database. Database system is shut down', 'course': 'data-engineering-zoomcamp'}, {'text': 'On few versions of Ubuntu, snap command can be used to install Docker.\\nsudo snap install docker', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker not installable on Ubuntu', 'course': 'data-engineering-zoomcamp'}, {'text': 'error: could not change permissions of directory \"/var/lib/postgresql/data\": Operation not permitted  volume\\nif you have used the prev answer (just before this) and have created a local docker volume, then you need to tell the compose file about the named volume:\\nvolumes:\\ndtc_postgres_volume_local:  # Define the named volume here\\n# services mentioned in the compose file auto become part of the same network!\\nservices:\\nyour remaining code here . . .\\nnow use docker volume inspect dtc_postgres_volume_local to see the location by checking the value of Mountpoint\\nIn my case, after i ran docker compose up the mounting dir created was named ‘docker_sql_dtc_postgres_volume_local’ whereas it should have used the already existing ‘dtc_postgres_volume_local’\\nAll i did to fix this is that I renamed the existing ‘dtc_postgres_volume_local’ to ‘docker_sql_dtc_postgres_volume_local’ and removed the newly created one (just be careful when doing this)\\nrun docker compose up again and check if the table is there or not!', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker-Compose - mounting error', 'course': 'data-engineering-zoomcamp'}, {'text': 'Couldn’t translate host name to address\\nMake sure postgres database is running.\\n\\n\\u200b\\u200bUse the command to start containers in detached mode: docker-compose up -d\\n(data-engineering-zoomcamp) hw % docker compose up -d\\n[+] Running 2/2\\n⠿ Container pg-admin     Started                                                                                                                                                                      0.6s\\n⠿ Container pg-database  Started\\nTo view the containers use: docker ps.\\n(data-engineering-zoomcamp) hw % docker ps\\nCONTAINER ID   IMAGE            COMMAND                  CREATED          STATUS          PORTS                           NAMES\\nfaf05090972e   postgres:13      \"docker-entrypoint.s…\"   39 seconds ago   Up 37 seconds   0.0.0.0:5432->5432/tcp          pg-database\\n6344dcecd58f   dpage/pgadmin4   \"/entrypoint.sh\"         39 seconds ago   Up 37 seconds   443/tcp, 0.0.0.0:8080->80/tcp   pg-admin\\nhw\\nTo view logs for a container: docker logs <containerid>\\n(data-engineering-zoomcamp) hw % docker logs faf05090972e\\nPostgreSQL Database directory appears to contain a database; Skipping initialization\\n2022-01-25 05:58:45.948 UTC [1] LOG:  starting PostgreSQL 13.5 (Debian 13.5-1.pgdg110+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\\n2022-01-25 05:58:45.948 UTC [1] LOG:  listening on IPv4 address \"0.0.0.0\", port 5432\\n2022-01-25 05:58:45.948 UTC [1] LOG:  listening on IPv6 address \"::\", port 5432\\n2022-01-25 05:58:45.954 UTC [1] LOG:  listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5432\"\\n2022-01-25 05:58:45.984 UTC [28] LOG:  database system was interrupted; last known up at 2022-01-24 17:48:35 UTC\\n2022-01-25 05:58:48.581 UTC [28] LOG:  database system was not properly shut down; automatic recovery in\\nprogress\\n2022-01-25 05:58:48.602 UTC [28] LOG:  redo starts at 0/872A5910\\n2022-01-25 05:59:33.726 UTC [28] LOG:  invalid record length at 0/98A3C160: wanted 24, got 0\\n2022-01-25 05:59:33.726 UTC [28\\n] LOG:  redo done at 0/98A3C128\\n2022-01-25 05:59:48.051 UTC [1] LOG:  database system is ready to accept connections\\nIf docker ps doesn’t show pgdatabase running, run: docker ps -a\\nThis should show all containers, either running or stopped.\\nGet the container id for pgdatabase-1, and run', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker-Compose - Error translating host name to address', 'course': 'data-engineering-zoomcamp'}, {'text': 'After executing `docker-compose up` - if you lose database data and are unable to successfully execute your Ingestion script (to re-populate your database) but receive the following error:\\nsqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name /data_pgadmin:/var/lib/pgadmin\"pg-database\" to address: Name or service not known\\nDocker compose is creating its own default network since it is no longer specified in a docker execution command or file. Docker Compose will emit to logs the new network name. See the logs after executing `docker compose up` to find the network name and change the network name argument in your Ingestion script.\\nIf problems persist with pgcli, we can use HeidiSQL,usql\\nKrishna Anand', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker-Compose -  Data retention (could not translate host name \"pg-database\" to address: Name or service not known)', 'course': 'data-engineering-zoomcamp'}, {'text': 'It returns --> Error response from daemon: network 66ae65944d643fdebbc89bd0329f1409dec2c9e12248052f5f4c4be7d1bdc6a3 not found\\nTry:\\ndocker ps -a to see all the stopped & running containers\\nd to nuke all the containers\\nTry: docker-compose up -d again ports\\nOn localhost:8080 server → Unable to connect to server: could not translate host name \\'pg-database\\' to address: Name does not resolve\\nTry: new host name, best without “ - ” e.g. pgdatabase\\nAnd on docker-compose.yml, should specify docker network & specify the same network in both  containers\\nservices:\\npgdatabase:\\nimage: postgres:13\\nenvironment:\\n- POSTGRES_USER=root\\n- POSTGRES_PASSWORD=root\\n- POSTGRES_DB=ny_taxi\\nvolumes:\\n- \"./ny_taxi_postgres_data:/var/lib/postgresql/data:rw\"\\nports:\\n- \"5431:5432\"\\nnetworks:\\n- pg-network\\npgadmin:\\nimage: dpage/pgadmin4\\nenvironment:\\n- PGADMIN_DEFAULT_EMAIL=admin@admin.com\\n- PGADMIN_DEFAULT_PASSWORD=root\\nports:\\n- \"8080:80\"\\nnetworks:\\n- pg-network\\nnetworks:\\npg-network:\\nname: pg-network', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker-Compose - Hostname does not resolve', 'course': 'data-engineering-zoomcamp'}, {'text': 'So one common issue is when you run docker-compose on GCP, postgres won’t persist it’s data to mentioned path for example:\\nservices:\\n…\\n…\\npgadmin:\\n…\\n…\\nVolumes:\\n“./pgadmin”:/var/lib/pgadmin:wr”\\nMight not work so in this use you can use Docker Volume to make it persist, by simply changing\\nservices:\\n…\\n….\\npgadmin:\\n…\\n…\\nVolumes:\\npgadmin:/var/lib/pgadmin\\nvolumes:\\nPgadmin:', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker-Compose - Persist PGAdmin docker contents on GCP', 'course': 'data-engineering-zoomcamp'}, {'text': 'The docker will keep on crashing continuously\\nNot working after restart\\ndocker engine stopped\\nAnd failed to fetch extensions pop ups will on screen non-stop\\nSolution :\\nTry checking if latest version of docker is installed / Try updating the docker\\nIf Problem still persist then final solution is to reinstall docker\\n(Just have to fetch images again else no issues)', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker engine stopped_failed to fetch extensions', 'course': 'data-engineering-zoomcamp'}, {'text': 'As per the lessons,\\nPersisting pgAdmin configuration (i.e. server name) is done by adding a “volumes” section:\\nservices:\\npgdatabase:\\n[...]\\npgadmin:\\nimage: dpage/pgadmin4\\nenvironment:\\n- PGADMIN_DEFAULT_EMAIL=admin@admin.com\\n- PGADMIN_DEFAULT_PASSWORD=root\\nvolumes:\\n- \"./pgAdmin_data:/var/lib/pgadmin/sessions:rw\"\\nports:\\n- \"8080:80\"\\nIn the example above, ”pgAdmin_data” is a folder on the host machine, and “/var/lib/pgadmin/sessions” is the session settings folder in the pgAdmin container.\\nBefore running docker-compose up on the YAML file, we also need to give the pgAdmin container access to write to the “pgAdmin_data” folder. The container runs with a username called “5050” and user group “5050”. The bash command to give access over the mounted volume is:\\nsudo chown -R 5050:5050 pgAdmin_data', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker-Compose - Persist PGAdmin configuration', 'course': 'data-engineering-zoomcamp'}, {'text': 'This happens if you did not create the docker group and added your user. Follow these steps from the link:\\nguides/docker-without-sudo.md at main · sindresorhus/guides · GitHub\\nAnd then press ctrl+D to log-out and log-in again. pgAdmin: Maintain state so that it remembers your previous connection\\nIf you are tired of having to setup your database connection each time that you fire up the containers, all you have to do is create a volume for pgAdmin:\\nIn your docker-compose.yaml file, enter the following into your pgAdmin declaration:\\nvolumes:\\n- type: volume\\nsource: pgadmin_data\\ntarget: /var/lib/pgadmin\\nAlso add the following to the end of the file:ls\\nvolumes:\\nPgadmin_data:', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker-Compose - dial unix /var/run/docker.sock: connect: permission denied', 'course': 'data-engineering-zoomcamp'}, {'text': 'This is happen to me after following 1.4.1 video where we are installing docker compose in our Google Cloud VM. In my case, the docker-compose file downloaded from github named docker-compose-linux-x86_64 while it is more convenient to use docker-compose command instead. So just change the docker-compose-linux-x86_64 into docker-compose.', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker-Compose - docker-compose still not available after changing .bashrc', 'course': 'data-engineering-zoomcamp'}, {'text': 'Installing pass via ‘sudo apt install pass’ helped to solve the issue. More about this can be found here: https://github.com/moby/buildkit/issues/1078', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker-Compose - Error getting credentials after running docker-compose up -d', 'course': 'data-engineering-zoomcamp'}, {'text': \"For everyone who's having problem with Docker compose, getting the data in postgres and similar issues, please take care of the following:\\ncreate a new volume on docker (either using the command line or docker desktop app)\\nmake the following changes to your docker-compose.yml file (see attachment)\\nset low_memory=false when importing the csv file (df = pd.read_csv('yellow_tripdata_2021-01.csv', nrows=1000, low_memory=False))\\nuse the below function (in the upload-data.ipynb) for better tracking of your ingestion process (see attachment)\\nOrder of execution:\\n(1) open terminal in 2_docker_sql folder and run docker compose up\\n(2) ensure no other containers are running except the one you just executed (pgadmin and pgdatabase)\\n(3) open jupyter notebook and begin the data ingestion\\n(4) open pgadmin and set up a server (make sure you use the same configurations as your docker-compose.yml file like the same name (pgdatabase), port, databasename (ny_taxi) etc.\", 'section': 'Module 1: Docker and Terraform', 'question': 'Docker-Compose - Errors pertaining to docker-compose.yml and pgadmin setup', 'course': 'data-engineering-zoomcamp'}, {'text': 'Locate config.json file for docker (check your home directory; Users/username/.docker).\\nModify credsStore to credStore\\nSave and re-run', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker Compose up -d error getting credentials - err: exec: \"docker-credential-desktop\": executable file not found in %PATH%, out: ``', 'course': 'data-engineering-zoomcamp'}, {'text': 'To figure out which docker-compose you need to download from https://github.com/docker/compose/releases you can check your system with these commands:\\nuname -s  -> return Linux most likely\\nuname -m -> return \"flavor\"\\nOr try this command -\\nsudo curl -L \"https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker-Compose - Which docker-compose binary to use for WSL?', 'course': 'data-engineering-zoomcamp'}, {'text': 'If you wrote the docker-compose.yaml file exactly like the video, you might run into an error like this:dev\\nservice \"pgdatabase\" refers to undefined volume dtc_postgres_volume_local: invalid compose project\\nIn order to make it work, you need to include the volume in your docker-compose file. Just add the following:\\nvolumes:\\ndtc_postgres_volume_local:\\n(Make sure volumes are at the same level as services.)', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker-Compose - Error undefined volume in Windows/WSL', 'course': 'data-engineering-zoomcamp'}, {'text': 'Error:  initdb: error: could not change permissions of directory\\nIssue: WSL and Windows do not manage permissions in the same way causing conflict if using the Windows file system rather than the WSL file system.\\nSolution: Use Docker volumes.\\nWhy: Volume is used for storage of persistent data and not for use of transferring files. A local volume is unnecessary.\\nBenefit: This resolves permission issues and allows for better management of volumes.\\nNOTE: the ‘user:’ is not necessary if using docker volumes, but is if using local drive.\\n</>  docker-compose.yaml\\nservices:\\npostgres:\\nimage: postgres:15-alpine\\ncontainer_name: postgres\\nuser: \"0:0\"\\nenvironment:\\n- POSTGRES_USER=postgres\\n- POSTGRES_PASSWORD=postgres\\n- POSTGRES_DB=ny_taxi\\nvolumes:\\n- \"pg-data:/var/lib/postgresql/data\"\\nports:\\n- \"5432:5432\"\\nnetworks:\\n- pg-network\\npgadmin:\\nimage: dpage/pgadmin4\\ncontainer_name: pgadmin\\nuser: \"${UID}:${GID}\"\\nenvironment:\\n- PGADMIN_DEFAULT_EMAIL=email@some-site.com\\n- PGADMIN_DEFAULT_PASSWORD=pgadmin\\nvolumes:\\n- \"pg-admin:/var/lib/pgadmin\"\\nports:\\n- \"8080:80\"\\nnetworks:\\n- pg-network\\nnetworks:\\npg-network:\\nname: pg-network\\nvolumes:\\npg-data:\\nname: ingest_pgdata\\npg-admin:\\nname: ingest_pgadmin', 'section': 'Module 1: Docker and Terraform', 'question': 'WSL Docker directory permissions error', 'course': 'data-engineering-zoomcamp'}, {'text': 'Cause : If Running on git bash or vm in windows pgadmin doesnt work easily LIbraries like psycopg2 and libpq ar required still the error persists.\\nSolution- I use psql instead of pgadmin totally same\\nPip install psycopg2\\ndock', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - If pgadmin is not working for Querying in Postgres Use PSQL', 'course': 'data-engineering-zoomcamp'}, {'text': 'Cause:\\nIt happens because the apps are not updated. To be specific, search for any pending updates for Windows Terminal, WSL and Windows Security updates.\\nSolution\\nfor updating Windows terminal which worked for me:\\nGo to Microsoft Store.\\nGo to the library of apps installed in your system.\\nSearch for Windows terminal.\\nUpdate the app and restart your system to  see the changes.\\nFor updating the Windows security updates:\\nGo to Windows updates and check if there are any pending updates from Windows, especially security updates.\\nDo restart your system once the updates are downloaded and installed successfully.', 'section': 'Module 1: Docker and Terraform', 'question': 'WSL - Insufficient system resources exist to complete the requested service.', 'course': 'data-engineering-zoomcamp'}, {'text': 'Up restardoting the same issue appears. Happens out of the blue on windows.\\nSolution 1: Fixing DNS Issue (credit: reddit) this worked for me personally\\nreg add \"HKLM\\\\System\\\\CurrentControlSet\\\\Services\\\\Dnscache\" /v \"Start\" /t REG_DWORD /d \"4\" /f\\nRestart your computer and then enable it with the following\\nreg add \"HKLM\\\\System\\\\CurrentControlSet\\\\Services\\\\Dnscache\" /v \"Start\" /t REG_DWORD /d \"2\" /f\\nRestart your OS again. It should work.\\nSolution 2: right click on running Docker icon (next to clock) and chose \"Switch to Linux containers\"\\nbash: conda: command not found\\nDatabase is uninitialized and superuser password is not specified.\\nDatabase is uninitialized and superuser password is not specified.', 'section': 'Module 1: Docker and Terraform', 'question': 'WSL - WSL integration with distro Ubuntu unexpectedly stopped with exit code 1.', 'course': 'data-engineering-zoomcamp'}, {'text': 'Issue when trying to run the GPC VM through SSH through WSL2,  probably because WSL2 isn’t looking for .ssh keys in the correct folder. My case I was trying to run this command in the terminal and getting an error\\nPC:/mnt/c/Users/User/.ssh$ ssh -i gpc [username]@[my external IP]\\nYou can try to use sudo before the command\\nSudo .ssh$ ssh -i gpc [username]@[my external IP]\\nYou can also try to cd to your folder and change the permissions for the private key SSH file.\\nchmod 600 gpc\\nIf that doesn’t work, create a .ssh folder in the home diretory of WSL2 and copy the content of windows .ssh folder to that new folder.\\ncd ~\\nmkdir .ssh\\ncp -r /mnt/c/Users/YourUsername/.ssh/* ~/.ssh/\\nYou might need to adjust the permissions of the files and folders in the .ssh directory.', 'section': 'Module 1: Docker and Terraform', 'question': 'WSL - Permissions too open at Windows', 'course': 'data-engineering-zoomcamp'}, {'text': 'Such as the issue above, WSL2 may not be referencing the correct .ssh/config path from Windows. You can create a config file at the home directory of WSL2.\\ncd ~\\nmkdir .ssh\\nCreate a config file in this new .ssh/ folder referencing this folder:\\nHostName [GPC VM external IP]\\nUser [username]\\nIdentityFile ~/.ssh/[private key]', 'section': 'Module 1: Docker and Terraform', 'question': 'WSL - Could not resolve host name', 'course': 'data-engineering-zoomcamp'}, {'text': 'Change TO Socket\\npgcli -h 127.0.0.1 -p 5432 -u root -d ny_taxi\\npgcli -h 127.0.0.1 -p 5432 -u root -d ny_taxi', 'section': 'Module 1: Docker and Terraform', 'question': 'PGCLI - connection failed: :1), port 5432 failed: could not receive data from server: Connection refused could not send SSL negotiation packet: Connection refused', 'course': 'data-engineering-zoomcamp'}, {'text': 'probably some installation error, check out sy', 'section': 'Module 1: Docker and Terraform', 'question': 'PGCLI --help error', 'course': 'data-engineering-zoomcamp'}, {'text': 'In this section of the course, the 5432 port of pgsql is mapped to your computer’s 5432 port. Which means you can access the postgres database via pgcli directly from your computer.\\nSo No, you don’t need to run it inside another container. Your local system will do.', 'section': 'Module 1: Docker and Terraform', 'question': 'PGCLI - INKhould we run pgcli inside another docker container?', 'course': 'data-engineering-zoomcamp'}, {'text': 'FATAL:  password authentication failed for user \"root\"\\nobservations: Below in bold do not forget the folder that was created ny_taxi_postgres_data\\nThis happens if you have a local Postgres installation in your computer. To mitigate this, use a different port, like 5431, when creating the docker container, as in: -p 5431: 5432\\nThen, we need to use this port when connecting to pgcli, as shown below:\\npgcli -h localhost -p 5431 -u root -d ny_taxi\\nThis will connect you to your postgres docker container, which is mapped to your host’s 5431 port (though you might choose any port of your liking as long as it is not occupied).\\nFor a more visual and detailed explanation, feel free to check the video 1.4.2 - Port Mapping and Networks in Docker\\nIf you want to debug: the following can help (on a MacOS)\\nTo find out if something is blocking your port (on a MacOS):\\nYou can use the lsof command to find out which application is using a specific port on your local machine. `lsof -i :5432`wi\\nOr list the running postgres services on your local machine with launchctl\\nTo unload the running service on your local machine (on a MacOS):\\nunload the launch agent for the PostgreSQL service, which will stop the service and free up the port  \\n`launchctl unload -w ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist`\\nthis one to start it again\\n`launchctl load -w ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist`\\nChanging port from 5432:5432 to 5431:5432 helped me to avoid this error.', 'section': 'Module 1: Docker and Terraform', 'question': 'PGCLI - FATAL: password authentication failed for user \"root\" (You already have Postgres)', 'course': 'data-engineering-zoomcamp'}, {'text': 'I get this error\\npgcli -h localhost -p 5432 -U root -d ny_taxi\\nTraceback (most recent call last):\\nFile \"/opt/anaconda3/bin/pgcli\", line 8, in <module>\\nsys.exit(cli())\\nFile \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 1128, in __call__\\nreturn self.main(*args, **kwargs)\\nFile \"/opt/anaconda3/lib/python3.9/sitYe-packages/click/core.py\", line\\n1053, in main\\nrv = self.invoke(ctx)\\nFile \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 1395, in invoke\\nreturn ctx.invoke(self.callback, **ctx.params)\\nFile \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 754, in invoke\\nreturn __callback(*args, **kwargs)\\nFile \"/opt/anaconda3/lib/python3.9/site-packages/pgcli/main.py\", line 880, in cli\\nos.makedirs(config_dir)\\nFile \"/opt/anaconda3/lib/python3.9/os.py\", line 225, in makedirspython\\nmkdir(name, mode)PermissionError: [Errno 13] Permission denied: \\'/Users/vray/.config/pgcli\\'\\nMake sure you install pgcli without sudo.\\nThe recommended approach is to use conda/anaconda to make sure your system python is not affected.\\nIf conda install gets stuck at \"Solving environment\" try these alternatives: https://stackoverflow.com/questions/63734508/stuck-at-solving-environment-on-anaconda', 'section': 'Module 1: Docker and Terraform', 'question': \"PGCLI - PermissionError: [Errno 13] Permission denied: '/some/path/.config/pgcli'\", 'course': 'data-engineering-zoomcamp'}, {'text': 'ImportError: no pq wrapper available.\\nAttempts made:\\n- couldn\\'t import \\\\dt\\nopg \\'c\\' implementation: No module named \\'psycopg_c\\'\\n- couldn\\'t import psycopg \\'binary\\' implementation: No module named \\'psycopg_binary\\'\\n- couldn\\'t import psycopg \\'python\\' implementation: libpq library not found\\nSolution:\\nFirst, make sure your Python is set to 3.9, at least.\\nAnd the reason for that is we have had cases of \\'psycopg2-binary\\' failing to install because of an old version of Python (3.7.3). \\n\\n0. You can check your current python version with: \\n$ python -V(the V must be capital)\\n1. Based on the previous output, if you\\'ve got a 3.9, skip to Step #2\\n   Otherwispye better off with a new environment with 3.9\\n$ conda create –name de-zoomcamp python=3.9\\n$ conda activate de-zoomcamp\\n2. Next, you should be able to install the lib for postgres like this:\\n```\\n$ e\\n$ pip install psycopg2_binary\\n```\\n3. Finally, make sure you\\'re also installing pgcli, but use conda for that:\\n```\\n$ pgcli -h localhost -U root -d ny_taxisudo\\n```\\nThere, you should be good to go now!\\nAnother solution:\\nRun this\\npip install \"psycopg[binary,pool]\"', 'section': 'Module 1: Docker and Terraform', 'question': 'PGCLI - no pq wrapper available.', 'course': 'data-engineering-zoomcamp'}, {'text': 'If your Bash prompt is stuck on the password command for postgres\\nUse winpty:\\nwinpty pgcli -h localhost -p 5432 -u root -d ny_taxi\\nAlternatively, try using Windows terminal or terminal in VS code.\\nEditPGCLI -connection failed: FATAL:  password authentication failed for user \"root\"\\nThe error above was faced continually despite inputting the correct password\\nSolution\\nOption 1: Stop the PostgreSQL service on Windows\\nOption 2 (using WSL): Completely uninstall Protgres 12 from Windows and install postgresql-client on WSL (sudo apt install postgresql-client-common postgresql-client libpq-dev)\\nOption 3: Change the port of the docker container\\nNEW SOLUTION: 27/01/2024\\nPGCLI -connection failed: FATAL:  password authentication failed for user \"root\"\\nIf you’ve got the error above, it’s probably because you were just like me, closed the connection to the Postgres:13 image in the previous step of the tutorial, which is\\n\\ndocker run -it \\\\\\n-e POSTGRES_USER=root \\\\\\n-e POSTGRES_PASSWORD=root \\\\\\n-e POSTGRES_DB=ny_taxi \\\\\\n-v d:/git/data-engineering-zoomcamp/week_1/docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n-p 5432:5432 \\\\\\npostgres:13\\nSo keep the database connected and you will be able to implement all the next steps of the tutorial.', 'section': 'Module 1: Docker and Terraform', 'question': 'PGCLI -  stuck on password prompt', 'course': 'data-engineering-zoomcamp'}, {'text': 'Problem: If you have already installed pgcli but bash doesn\\'t recognize pgcli\\nOn Git bash: bash: pgcli: command not found\\nOn Windows Terminal: pgcli: The term \\'pgcli\\' is not recognized…\\nSolution: Try adding a Python path C:\\\\Users\\\\...\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\Scripts to Windows PATH\\nFor details:\\nGet the location: pip list -v\\nCopy C:\\\\Users\\\\...\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\site-packages\\n3. Replace site-packages with Scripts: C:\\\\Users\\\\...\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\Scripts\\nIt can also be that you have Python installed elsewhere.\\nFor me it was under c:\\\\python310\\\\lib\\\\site-packages\\nSo I had to add c:\\\\python310\\\\lib\\\\Scripts to PATH, as shown below.\\nPut the above path in \"Path\" (or \"PATH\") in System Variables\\nReference: https://stackoverflow.com/a/68233660', 'section': 'Module 1: Docker and Terraform', 'question': 'PGCLI - pgcli: command not found', 'course': 'data-engineering-zoomcamp'}, {'text': 'In case running pgcli  locally causes issues or you do not want to install it locally you can use it running in a Docker container instead.\\nBelow the usage with values used in the videos of the course for:\\nnetwork name (docker network)\\npostgres related variables for pgcli\\nHostname\\nUsername\\nPort\\nDatabase name\\n$ docker run -it --rm --network pg-network ai2ys/dockerized-pgcli:4.0.1\\n175dd47cda07:/# pgcli -h pg-database -U root -p 5432 -d ny_taxi\\nPassword for root:\\nServer: PostgreSQL 16.1 (Debian 16.1-1.pgdg120+1)\\nVersion: 4.0.1\\nHome: http://pgcli.com\\nroot@pg-database:ny_taxi> \\\\dt\\n+--------+------------------+-------+-------+\\n| Schema | Name             | Type  | Owner |\\n|--------+------------------+-------+-------|\\n| public | yellow_taxi_data | table | root  |\\n+--------+------------------+-------+-------+\\nSELECT 1\\nTime: 0.009s\\nroot@pg-database:ny_taxi>', 'section': 'Module 1: Docker and Terraform', 'question': 'PGCLI - running in a Docker container', 'course': 'data-engineering-zoomcamp'}, {'text': 'PULocationID will not be recognized but “PULocationID” will be. This is because unquoted \"Localidentifiers are case insensitive. See docs.', 'section': 'Module 1: Docker and Terraform', 'question': 'PGCLI - case sensitive use “Quotations” around columns with capital letters', 'course': 'data-engineering-zoomcamp'}, {'text': 'When using the command `\\\\d <database name>` you get the error column `c.relhasoids does not exist`.\\nResolution:\\nUninstall pgcli\\nReinstall pgclidatabase \"ny_taxi\" does not exist\\nRestart pc', 'section': 'Module 1: Docker and Terraform', 'question': 'PGCLI - error column c.relhasoids does not exist', 'course': 'data-engineering-zoomcamp'}, {'text': \"This happens while uploading data via the connection in jupyter notebook\\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\\nThe port 5432 was taken by another postgres. We are not connecting to the port in docker, but to the port on our machine. Substitute 5431 or whatever port you mapped to for port 5432.\\nAlso if this error is still persistent , kindly check if you have a service in windows running postgres , Stopping that service will resolve the issue\", 'section': 'Module 1: Docker and Terraform', 'question': 'Postgres - OperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  password authentication failed for user \"root\"', 'course': 'data-engineering-zoomcamp'}, {'text': 'Can happen when connecting via pgcli\\npgcli -h localhost -p 5432 -U root -d ny_taxi\\nOr while uploading data via the connection in jupyter notebook\\nengine = create_engine(\\'postgresql://root:root@localhost:5432/ny_taxi\\')\\nThis can happen when Postgres is already installed on your computer. Changing the port can resolve that (e.g. from 5432 to 5431).\\nTo check whether there even is a root user with the ability to login:\\nTry: docker exec -it <your_container_name> /bin/bash\\nAnd then run\\n???\\nAlso, you could change port from 5432:5432 to 5431:5432\\nOther solution that worked:\\nChanging `POSTGRES_USER=juroot` to `PGUSER=postgres`\\nBased on this: postgres with docker compose gives FATAL: role \"root\" does not exist error - Stack Overflow\\nAlso `docker compose down`, removing folder that had postgres volume, running `docker compose up` again.', 'section': 'Module 1: Docker and Terraform', 'question': 'Postgres - OperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  role \"root\" does not exist', 'course': 'data-engineering-zoomcamp'}, {'text': '~\\\\anaconda3\\\\lib\\\\site-packages\\\\psycopg2\\\\__init__.py in connect(dsn, connection_factory, cursor_factory, **kwargs)\\n120\\n121     dsn = _ext.make_dsn(dsn, **kwargs)\\n--> 122     conn = _connect(dsn, connection_factory=connection_factory, **kwasync)\\n123     if cursor_factory is not None:\\n124         conn.cursor_factory = cursor_factory\\nOperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  database \"ny_taxi\" does not exist\\nMake sure postgres is running. You can check that by running `docker ps`\\n✅Solution: If you have postgres software installed on your computer before now, build your instance on a different port like 8080 instead of 5432', 'section': 'Module 1: Docker and Terraform', 'question': 'Postgres - OperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  dodatabase \"ny_taxi\" does not exist', 'course': 'data-engineering-zoomcamp'}, {'text': \"Issue:\\ne…\\nSolution:\\npip install psycopg2-binary\\nIf you already have it, you might need to update it:\\npip install psycopg2-binary --upgrade\\nOther methods, if the above fails:\\nif you are getting the “ ModuleNotFoundError: No module named 'psycopg2' “ error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\\nFirst uninstall the psycopg package\\nThen update conda or pip\\nThen install psycopg again using pip.\\nif you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\", 'section': 'Module 1: Docker and Terraform', 'question': \"Postgres - ModuleNotFoundError: No module named 'psycopg2'\", 'course': 'data-engineering-zoomcamp'}, {'text': 'In the join queries, if we mention the column name directly or enclosed in single quotes it’ll throw an error says “column does not exist”.\\n✅Solution: But if we enclose the column names in double quotes then it will work', 'section': 'Module 1: Docker and Terraform', 'question': 'Postgres - \"Column does not exist\" but it actually does (Pyscopg2 error in MacBook Pro M2)', 'course': 'data-engineering-zoomcamp'}, {'text': 'pgAdmin has a new version. Create server dialog may not appear. Try using register-> server instead.', 'section': 'Module 1: Docker and Terraform', 'question': 'pgAdmin - Create server dialog does not appear', 'course': 'data-engineering-zoomcamp'}, {'text': 'Using GitHub Codespaces in the browser resulted in a blank screen after the login to pgAdmin (running in a Docker container). The terminal of the pgAdmin container was showing the following error message:\\nCSRFError: 400 Bad Request: The referrer does not match the host.\\nSolution #1:\\nAs recommended in the following issue  https://github.com/pgadmin-org/pgadmin4/issues/5432 setting the following environment variable solved it.\\nPGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\"\\nModified “docker run” command\\ndocker run --rm -it \\\\\\n-e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\\\\n-e PGADMIN_DEFAULT_PASSWORD=\"root\" \\\\\\n-e PGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\" \\\\\\n-p \"8080:80\" \\\\\\n--name pgadmin \\\\\\n--network=pg-network \\\\\\ndpage/pgadmin4:8.2\\nSolution #2:\\nUsing the local installed VSCode to display GitHub Codespaces.\\nWhen using GitHub Codespaces in the locally installed VSCode (opening a Codespace or creating/starting one) this issue did not occur.', 'section': 'Module 1: Docker and Terraform', 'question': 'pgAdmin - Blank/white screen after login (browser)', 'course': 'data-engineering-zoomcamp'}, {'text': 'I am using a Mac Pro device and connect to the GCP Compute Engine via Remote SSH - VSCode. But when I trying to run the PgAdmin container via docker run or docker compose command, I am failed to access the pgAdmin address via my browser. I have switched to another browser, but still can not access the pgAdmin address. So I modified a little bit the configuration from the previous DE Zoomcamp repository like below and can access the pgAdmin address:\\nSolution #1:\\nModified “docker run” command\\ndocker run --rm -it \\\\\\n-e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\\\\n-e PGADMIN_DEFAULT_PASSWORD=\"pgadmin\" \\\\\\n-e PGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\" \\\\\\n-e PGADMIN_LISTEN_ADDRESS=0.0.0.0 \\\\\\n-e PGADMIN_LISTEN_PORT=5050 \\\\\\n-p 5050:5050 \\\\\\n--network=de-zoomcamp-network \\\\\\n--name pgadmin-container \\\\\\n--link postgres-container \\\\\\n-t dpage/pgadmin4\\nSolution #2:\\nModified docker-compose.yaml configuration (via “docker compose up” command)\\npgadmin:\\nimage: dpage/pgadmin4\\ncontainer_name: pgadmin-conntainer\\nenvironment:\\n- PGADMIN_DEFAULT_EMAIL=admin@admin.com\\n- PGADMIN_DEFAULT_PASSWORD=pgadmin\\n- PGADMIN_CONFIG_WTF_CSRF_ENABLED=False\\n- PGADMIN_LISTEN_ADDRESS=0.0.0.0\\n- PGADMIN_LISTEN_PORT=5050\\nvolumes:\\n- \"./pgadmin_data:/var/lib/pgadmin/data\"\\nports:\\n- \"5050:5050\"\\nnetworks:\\n- de-zoomcamp-network\\ndepends_on:\\n- postgres-conntainer\\nPython - ModuleNotFoundError: No module named \\'pysqlite2\\'\\nImportError: DLL load failed while importing _sqlite3: The specified module could not be found. ModuleNotFoundError: No module named \\'pysqlite2\\'\\nThe issue seems to arise from the missing of sqlite3.dll in path \".\\\\Anaconda\\\\Dlls\\\\\".\\n✅I solved it by simply copying that .dll file from \\\\Anaconda3\\\\Library\\\\bin and put it under the path mentioned above. (if you are using anaconda)', 'section': 'Module 1: Docker and Terraform', 'question': 'pgAdmin - Can not access/open the PgAdmin address via browser', 'course': 'data-engineering-zoomcamp'}, {'text': 'If you follow the video 1.2.2 - Ingesting NY Taxi Data to Postgres and you execute all the same\\nsteps as Alexey does, you will ingest all the data (~1.3 million rows) into the table yellow_taxi_data as expected.\\nHowever, if you try to run the whole script in the Jupyter notebook for a second time from top to bottom, you will be missing the first chunk of 100000 records. This is because there is a call to the iterator before the while loop that puts the data in the table. The while loop therefore starts by ingesting the second chunk, not the first.\\n✅Solution: remove the cell “df=next(df_iter)” that appears higher up in the notebook than the while loop. The first time w(df_iter) is called should be within the while loop.\\n📔Note: As this notebook is just used as a way to test the code, it was not intended to be run top to bottom, and the logic is tidied up in a later step when it is instead inserted into a .py file for the pipeline', 'section': 'Module 1: Docker and Terraform', 'question': 'Python - Ingestion with Jupyter notebook - missing 100000 records', 'course': 'data-engineering-zoomcamp'}, {'text': '{t_end - t_start} seconds\")\\nimport pandas as pd\\ndf = pd.read_csv(\\'path/to/file.csv.gz\\', /app/ingest_data.py:1: DeprecationWarning:)\\nIf you prefer to keep the uncompressed csv (easier preview in vscode and similar), gzip files can be unzipped using gunzip (but not unzip). On a Ubuntu local or virtual machine, you may need to apt-get install gunzip first.', 'section': 'Module 1: Docker and Terraform', 'question': 'Python - Iteration csv without error', 'course': 'data-engineering-zoomcamp'}, {'text': \"Pandas can interpret “string” column values as “datetime” directly when reading the CSV file using “pd.read_csv” using the parameter “parse_dates”, which for example can contain a list of column names or column indices. Then the conversion afterwards is not required anymore.\\npandas.read_csv — pandas 2.1.4 documentation (pydata.org)\\nExample from week 1\\nimport pandas as pd\\ndf = pd.read_csv(\\n'yellow_tripdata_2021-01.csv',\\nnrows=100,\\nparse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\\ndf.info()\\nwhich will output\\n<class 'pandas.core.frame.DataFrame'>\\nRangeIndex: 100 entries, 0 to 99\\nData columns (total 18 columns):\\n#   Column                 Non-Null Count  Dtype\\n---  ------                 --------------  -----\\n0   VendorID               100 non-null    int64\\n1   tpep_pickup_datetime   100 non-null    datetime64[ns]\\n2   tpep_dropoff_datetime  100 non-null    datetime64[ns]\\n3   passenger_count        100 non-null    int64\\n4   trip_distance          100 non-null    float64\\n5   RatecodeID             100 non-null    int64\\n6   store_and_fwd_flag     100 non-null    object\\n7   PULocationID           100 non-null    int64\\n8   DOLocationID           100 non-null    int64\\n9   payment_type           100 non-null    int64\\n10  fare_amount            100 non-null    float64\\n11  extra                  100 non-null    float64\\n12  mta_tax                100 non-null    float64\\n13  tip_amount             100 non-null    float64\\n14  tolls_amount           100 non-null    float64\\n15  improvement_surcharge  100 non-null    float64\\n16  total_amount           100 non-null    float64\\n17  congestion_surcharge   100 non-null    float64\\ndtypes: datetime64[ns](2), float64(9), int64(6), object(1)\\nmemory usage: 14.2+ KB\", 'section': 'Module 1: Docker and Terraform', 'question': 'iPython - Pandas parsing dates with ‘read_csv’', 'course': 'data-engineering-zoomcamp'}, {'text': 'os.system(f\"curl -LO {url} -o {csv_name}\")', 'section': 'Module 1: Docker and Terraform', 'question': 'Python - Python cant ingest data from the github link provided using curl', 'course': 'data-engineering-zoomcamp'}, {'text': 'When a CSV file is compressed using Gzip, it is saved with a \".csv.gz\" file extension. This file type is also known as a Gzip compressed CSV file. When you want to read a Gzip compressed CSV file using Pandas, you can use the read_csv() function, which is specifically designed to read CSV files. The read_csv() function accepts several parameters, including a file path or a file-like object. To read a Gzip compressed CSV file, you can pass the file path of the \".csv.gz\" file as an argument to the read_csv() function.\\nHere is an example of how to read a Gzip compressed CSV file using Pandas:\\ndf = pd.read_csv(\\'file.csv.gz\\'\\n, compression=\\'gzip\\'\\n, low_memory=False\\n)', 'section': 'Module 1: Docker and Terraform', 'question': 'Python - Pandas can read *.csv.gzip', 'course': 'data-engineering-zoomcamp'}, {'text': \"Contrary to panda’s read_csv method there’s no such easy way to iterate through and set chunksize for parquet files. We can use PyArrow (Apache Arrow Python bindings) to resolve that.\\nimport pyarrow.parquet as pq\\noutput_name = “https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet”\\nparquet_file = pq.ParquetFile(output_name)\\nparquet_size = parquet_file.metadata.num_rows\\nengine = create_engine(f'postgresql://{user}:{password}@{host}:{port}/{db}')\\ntable_name=”yellow_taxi_schema”\\n# Clear table if exists\\npq.read_table(output_name).to_pandas().head(n=0).to_sql(name=table_name, con=engine, if_exists='replace')\\n# default (and max) batch size\\nindex = 65536\\nfor i in parquet_file.iter_batches(use_threads=True):\\nt_start = time()\\nprint(f'Ingesting {index} out of {parquet_size} rows ({index / parquet_size:.0%})')\\ni.to_pandas().to_sql(name=table_name, con=engine, if_exists='append')\\nindex += 65536\\nt_end = time()\\nprint(f'\\\\t- it took %.1f seconds' % (t_end - t_start))\", 'section': 'Module 1: Docker and Terraform', 'question': 'Python - How to iterate through and ingest parquet file', 'course': 'data-engineering-zoomcamp'}, {'text': 'Error raised during the jupyter notebook’s cell execution:\\nfrom sqlalchemy import create_engine.\\nSolution: Version of Python module “typing_extensions” >= 4.6.0. Can be updated by Conda or pip.', 'section': 'Module 1: Docker and Terraform', 'question': \"Python - SQLAlchemy - ImportError: cannot import name 'TypeAliasType' from 'typing_extensions'.\", 'course': 'data-engineering-zoomcamp'}, {'text': 'create_engine(\\'postgresql://root:root@localhost:5432/ny_taxi\\')  I get the error \"TypeError: \\'module\\' object is not callable\"\\nSolution:\\nconn_string = \"postgresql+psycopg://root:root@localhost:5432/ny_taxi\"\\nengine = create_engine(conn_string)', 'section': 'Module 1: Docker and Terraform', 'question': \"Python - SQLALchemy - TypeError 'module' object is not callable\", 'course': 'data-engineering-zoomcamp'}, {'text': \"Error raised during the jupyter notebook’s cell execution:\\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi').\\nSolution: Need to install Python module “psycopg2”. Can be installed by Conda or pip.\", 'section': 'Module 1: Docker and Terraform', 'question': \"Python - SQLAlchemy - ModuleNotFoundError: No module named 'psycopg2'.\", 'course': 'data-engineering-zoomcamp'}, {'text': 'Unable to add Google Cloud SDK PATH to Windows\\nWindows error: The installer is unable to automatically update your system PATH. Please add  C:\\\\tools\\\\google-cloud-sdk\\\\bin\\nif you are constantly getting this feedback. Might be that you needed to add Gitbash to your Windows path:\\nOne way of doing that is to use conda: ‘If you are not already using it\\nDownload the Anaconda Navigator\\nMake sure to check the box (add conda to the path when installing navigator: although not recommended do it anyway)\\nYou might also need to install git bash if you are not already using it(or you might need to uninstall it to reinstall it properly)\\nMake sure to check the following boxes while you install Gitbash\\nAdd a GitBash to Windows Terminal\\nUse Git and optional Unix tools from the command prompt\\nNow open up git bash and type conda init bash This should modify your bash profile\\nAdditionally, you might want to use Gitbash as your default terminal.\\nOpen your Windows terminal and go to settings, on the default profile change Windows power shell to git bash', 'section': 'Module 1: Docker and Terraform', 'question': 'GCP - Unable to add Google Cloud SDK PATH to Windows', 'course': 'data-engineering-zoomcamp'}, {'text': 'It asked me to create a project. This should be done from the cloud console. So maybe we don’t need this FAQ.\\nWARNING: Project creation failed: HttpError accessing <https://cloudresourcemanager.googleapis.com/v1/projects?alt=json>: response: <{\\'vtpep_pickup_datetimeary\\': \\'Origin, X-Origin, Referer\\', \\'content-type\\': \\'application/json; charset=UTF-8\\', \\'content-encoding\\': \\'gzip\\', \\'date\\': \\'Mon, 24 Jan 2022 19:29:12 GMT\\', \\'server\\': \\'ESF\\', \\'cache-control\\': \\'private\\', \\'x-xss-protection\\': \\'0\\', \\'x-frame-options\\': \\'SAMEORIGIN\\', \\'x-content-type-options\\': \\'nosniff\\', \\'server-timing\\': \\'gfet4t7; dur=189\\', \\'alt-svc\\': \\'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000,h3-Q050=\":443\"; ma=2592000,h3-Q046=\":443\"; ma=2592000,h3-Q043=\":443\"; ma=2592000,quic=\":443\"; ma=2592000; v=\"46,43\"\\', \\'transfer-encoding\\': \\'chunked\\', \\'status\\': 409}>, content <{\\n\"error\": {\\n\"code\": 409,\\n\"message\": \"Requested entity alreadytpep_pickup_datetime exists\",\\n\"status\": \"ALREADY_EXISTS\"\\n}\\n}\\nFrom Stackoverflow: https://stackoverflow.com/questions/52561383/gcloud-cli-cannot-create-project-the-project-id-you-specified-is-already-in-us?rq=1\\nProject IDs are unique across all projects. That means if any user ever had a project with that ID, you cannot use it. testproject is pretty common, so it\\'s not surprising it\\'s already taken.', 'section': 'Module 1: Docker and Terraform', 'question': 'GCP - Project creation failed: HttpError accessing … Requested entity alreadytpep_pickup_datetime exists', 'course': 'data-engineering-zoomcamp'}, {'text': 'If you receive the error: “Error 403: The project to be billed is associated with an absent billing account., accountDisabled” It is most likely because you did not enter YOUR project ID. The snip below is from video 1.3.2\\nThe value you enter here will be unique to each student. You can find this value on your GCP Dashboard when you login.\\nAshish Agrawal\\nAnother possibility is that you have not linked your billing account to your current project', 'section': 'Module 1: Docker and Terraform', 'question': 'GCP - The project to be billed is associated with an absent billing account', 'course': 'data-engineering-zoomcamp'}, {'text': 'GCP Account Suspension Inquiry\\nIf Google refuses your credit/debit card, try another - I’ve got an issue with Kaspi (Kazakhstan) but it worked with TBC (Georgia).\\nUnfortunately, there’s small hope that support will help.\\nIt seems that Pyypl web-card should work too.', 'section': 'Module 1: Docker and Terraform', 'question': 'GCP - OR-CBAT-15 ERROR Google cloud free trial account', 'course': 'data-engineering-zoomcamp'}, {'text': 'The ny-rides.json is your private file in Google Cloud Platform (GCP). \\n\\nAnd here’s the way to find it:\\nGCP -> Select project with your  instance -> IAM & Admin -> Service Accounts Keys tab -> add key, JSON as key type, then click create\\nNote: Once you go into Service Accounts Keys tab, click the email, then you can see the “KEYS” tab where you can add key as a JSON as its key type', 'section': 'Module 1: Docker and Terraform', 'question': 'GCP - Where can I find the “ny-rides.json” file?', 'course': 'data-engineering-zoomcamp'}, {'text': 'In this lecture, Alexey deleted his instance in Google Cloud. Do I have to do it?\\nNope. Do not delete your instance in Google Cloud platform. Otherwise, you have to do this twice for the week 1 readings.', 'section': 'Module 1: Docker and Terraform', 'question': 'GCP - Do I need to delete my instance in Google Cloud?', 'course': 'data-engineering-zoomcamp'}, {'text': 'System Resource Usage:\\ntop or htop: Shows real-time information about system resource usage, including CPU, memory, and processes.\\nfree -h: Displays information about system memory usage and availability.\\ndf -h: Shows disk space usage of file systems.\\ndu -h <directory>: Displays disk usage of a specific directory.\\nRunning Processes:\\nps aux: Lists all running processes along with detailed information.\\nNetwork:\\nifconfig or ip addr show: Shows network interface configuration.\\nnetstat -tuln: Displays active network connections and listening ports.\\nHardware Information:\\nlscpu: Displays CPU information.\\nlsblk: Lists block devices (disks and partitions).\\nlshw: Lists hardware configuration.\\nUser and Permissions:\\nwho: Shows who is logged on and their activities.\\nw: Displays information about currently logged-in users and their processes.\\nPackage Management:\\napt list --installed: Lists installed packages (for Ubuntu and Debian-based systems)', 'section': 'Module 1: Docker and Terraform', 'question': 'Commands to inspect the health of your VM:', 'course': 'data-engineering-zoomcamp'}, {'text': 'if you’ve got the error\\n│ Error: Error updating Dataset \"projects/<your-project-id>/datasets/demo_dataset\": googleapi: Error 403: Billing has not been enabled for this project. Enable billing at https://console.cloud.google.com/billing. The default table expiration time must be less than 60 days, billingNotEnabled\\nbut you’ve set your billing account indeed, then try to disable billing for the project and enable it again. It worked for ME!', 'section': 'Module 1: Docker and Terraform', 'question': 'Billing account has not been enabled for this project. But you’ve done it indeed!', 'course': 'data-engineering-zoomcamp'}, {'text': 'for windows if you having trouble install SDK try follow these steps on the link, if you getting this error:\\nThese credentials will be used by any library that requests Application Default Credentials (ADC).\\nWARNING:\\nCannot find a quota project to add to ADC. You might receive a \"quota exceeded\" or \"API not enabled\" error. Run $ gcloud auth application-default set-quota-project to add a quota project.\\nFor me:\\nI reinstalled the sdk using unzip file “install.bat”,\\nafter successfully checking gcloud version,\\nrun gcloud init to set up project before\\nyou run gcloud auth application-default login\\nhttps://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/1_terraform_gcp/windows.md\\nGCP VM - I cannot get my Virtual Machine to start because GCP has no resources.\\nClick on your VM\\nCreate an image of your VM\\nOn the page of the image, tell GCP to create a new VM instance via the image\\nOn the settings page, change the location', 'section': 'Module 1: Docker and Terraform', 'question': 'GCP - Windows Google Cloud SDK install issue:gcp', 'course': 'data-engineering-zoomcamp'}, {'text': 'The reason this video about the GCP VM exists is that many students had problems configuring their env. You can use your own env if it works for you.\\nAnd the advantage of using your own environment is that if you are working in a Github repo where you can commit, you will be able to commit the changes that you do. In the VM the repo is cloned via HTTPS so it is not possible to directly commit, even if you are the owner of the repo.', 'section': 'Module 1: Docker and Terraform', 'question': 'GCP VM - Is it necessary to use a GCP VM? When is it useful?', 'course': 'data-engineering-zoomcamp'}, {'text': \"I am trying to create a directory but it won't let me do it\\nUser1@DESKTOP-PD6UM8A MINGW64 /\\n$ mkdir .ssh\\nmkdir: cannot create directory ‘.ssh’: Permission denied\\nYou should do it in your home directory. Should be your home (~)\\nLocal. But it seems you're trying to do it in the root folder (/). Should be your home (~)\\nLink to Video 1.4.1\", 'section': 'Module 1: Docker and Terraform', 'question': 'GCP VM - mkdir: cannot create directory ‘.ssh’: Permission denied', 'course': 'data-engineering-zoomcamp'}, {'text': \"Failed to save '<file>': Unable to write file 'vscode-remote://ssh-remote+de-zoomcamp/home/<user>/data_engineering_course/week_2/airflow/dags/<file>' (NoPermissions (FileSystemError): Error: EACCES: permission denied, open '/home/<user>/data_engineering_course/week_2/airflow/dags/<file>')\\nYou need to change the owner of the files you are trying to edit via VS Code. You can run the following command to change the ownership.\\nssh\\nsudo chown -R <user> <path to your directory>\", 'section': 'Module 1: Docker and Terraform', 'question': 'GCP VM - Error while saving the file in VM via VS Code', 'course': 'data-engineering-zoomcamp'}, {'text': 'Question: I connected to my VM perfectly fine last week (ssh) but when I tried again this week, the connection request keeps timing out.\\n✅Answer: Start your VM. Once the VM is running, copy its External IP and paste that into your config file within the ~/.ssh folder.\\ncd ~/.ssh\\ncode config ← this opens the config file in VSCode', 'section': 'Module 1: Docker and Terraform', 'question': '. GCP VM - VM connection request timeout', 'course': 'data-engineering-zoomcamp'}, {'text': '(reference: https://serverfault.com/questions/953290/google-compute-engine-ssh-connect-to-host-ip-port-22-operation-timed-out)Go to edit your VM.\\nGo to section Automation\\nAdd Startup script\\n```\\n#!/bin/bash\\nsudo ufw allow ssh\\n```\\nStop and Start VM.', 'section': 'Module 1: Docker and Terraform', 'question': 'GCP VM -  connect to host port 22 no route to host', 'course': 'data-engineering-zoomcamp'}, {'text': 'You can easily forward the ports of pgAdmin, postgres and Jupyter Notebook using the built-in tools in Ubuntu and without any additional client:\\nFirst, in the VM machine, launch docker-compose up -d and jupyter notebook in the correct folder.\\nFrom the local machine, execute: ssh -i ~/.ssh/gcp -L 5432:localhost:5432 username@external_ip_of_vm\\nExecute the same command but with ports 8080 and 8888.\\nNow you can access pgAdmin on local machine in browser typing localhost:8080\\nFor Jupyter Notebook, type localhost:8888 in the browser of your local machine. If you have problems with the credentials, it is possible that you have to copy the link with the access token provided in the logs of the terminal of the VM machine when you launched the jupyter notebook command.\\nTo forward both pgAdmin and postgres use, ssh -i ~/.ssh/gcp -L 5432:localhost:5432 -L 8080:localhost:8080 modito@35.197.218.128', 'section': 'Module 1: Docker and Terraform', 'question': 'GCP VM - Port forwarding from GCP without using VS Code', 'course': 'data-engineering-zoomcamp'}, {'text': 'If you are using MS VS Code and running gcloud in WSL2, when you first try to login to gcp via the gcloud cli gcloud auth application-default login, you will see a message like this, and nothing will happen\\nAnd there might be a prompt to ask if you want to open it via browser, if you click on it, it will open up a page with error message\\nSolution : you should instead hover on the long link, and ctrl + click the long link\\n\\nClick configure Trusted Domains here\\n\\nPopup will appear, pick first or second entry\\nNext time you gcloud auth, the login page should popup via default browser without issues', 'section': 'Module 1: Docker and Terraform', 'question': 'GCP gcloud + MS VS Code - gcloud auth hangs', 'course': 'data-engineering-zoomcamp'}, {'text': 'It is an internet connectivity error, terraform is somehow not able to access the online registry. Check your VPN/Firewall settings (or just clear cookies or restart your network). Try terraform init again after this, it should work.', 'section': 'Module 1: Docker and Terraform', 'question': 'Terraform - Error: Failed to query available provider packages │ Could not retrieve the list of available versions for provider hashicorp/google: could not query │ provider registry for registry.terrafogorm.io/hashicorp/google: the request failed after 2 attempts, │ please try again later', 'course': 'data-engineering-zoomcamp'}, {'text': \"The issue was with the network. Google is not accessible in my country, I am using a VPN. And The terminal program does not automatically follow the system proxy and requires separate proxy configuration settings.I opened a Enhanced Mode in Clash, which is a VPN app, and 'terraform apply' works! So if you encounter the same issue, you can ask help for your vpn provider.\", 'section': 'Module 1: Docker and Terraform', 'question': 'Terraform - Error:Post \"https://storage.googleapis.com/storage/v1/b?alt=json&prettyPrint=false&project=coherent-ascent-379901\": oauth2: cannot fetch token: Post \"https://oauth2.googleapis.com/token\": dial tcp 172.217.163.42:443: i/o timeout', 'course': 'data-engineering-zoomcamp'}, {'text': 'https://techcommunity.microsoft.com/t5/azure-developer-community-blog/configuring-terraform-on-windows-10-linux-sub-system/ba-p/393845', 'section': 'Module 1: Docker and Terraform', 'question': 'Terraform - Install for WSL', 'course': 'data-engineering-zoomcamp'}, {'text': 'https://github.com/hashicorp/terraform/issues/14513', 'section': 'Module 1: Docker and Terraform', 'question': 'Terraform - Error acquiring the state lock', 'course': 'data-engineering-zoomcamp'}, {'text': 'When running\\nterraform apply\\non wsl2 I\\'ve got this error:\\n│ Error: Post \"https://storage.googleapis.com/storage/v1/b?alt=json&prettyPrint=false&project=<your-project-id>\": oauth2: cannot fetch token: 400 Bad Request\\n│ Response: {\"error\":\"invalid_grant\",\"error_description\":\"Invalid JWT: Token must be a short-lived token (60 minutes) and in a reasonable timeframe. Check your iat and exp values in the JWT claim.\"}\\nIT happens because there may be time desync on your machine which affects computing JWT\\nTo fix this, run the command\\nsudo hwclock -s\\nwhich fixes your system time.\\nReference', 'section': 'Module 1: Docker and Terraform', 'question': 'Terraform - Error 400 Bad Request.  Invalid JWT Token  on WSL.', 'course': 'data-engineering-zoomcamp'}, {'text': '│ Error: googleapi: Error 403: Access denied., forbidden\\nYour $GOOGLE_APPLICATION_CREDENTIALS might not be pointing to the correct file \\nrun = export GOOGLE_APPLICATION_CREDENTIALS=~/.gc/YOUR_JSON.json\\nAnd then = gcloud auth activate-service-account --key-file $GOOGLE_APPLICATION_CREDENTIALS', 'section': 'Module 1: Docker and Terraform', 'question': 'Terraform - Error 403 : Access denied', 'course': 'data-engineering-zoomcamp'}, {'text': \"One service account is enough for all the services/resources you'll use in this course. After you get the file with your credentials and set your environment variable, you should be good to go.\", 'section': 'Module 1: Docker and Terraform', 'question': 'Terraform - Do I need to make another service account for terraform before I get the keys (.json file)?', 'course': 'data-engineering-zoomcamp'}, {'text': 'Here: https://releases.hashicorp.com/terraform/1.1.3/terraform_1.1.3_linux_amd64.zip', 'section': 'Module 1: Docker and Terraform', 'question': 'Terraform - Where can I find the Terraform 1.1.3 Linux (AMD 64)?', 'course': 'data-engineering-zoomcamp'}, {'text': 'You get this error because I run the command terraform init outside the working directory, and this is wrong.You need first to navigate to the working directory that contains terraform configuration files, and and then run the command.', 'section': 'Module 1: Docker and Terraform', 'question': 'Terraform - Terraform initialized in an empty directory! The directory has no Terraform configuration files. You may begin working with Terraform immediately by creating Terraform configuration files.g', 'course': 'data-engineering-zoomcamp'}, {'text': 'The error:\\nError: googleapi: Error 403: Access denied., forbidden\\n│\\nand\\n│ Error: Error creating Dataset: googleapi: Error 403: Request had insufficient authentication scopes.\\nFor this solution make sure to run:\\necho $GOOGLE_APPLICATION_CREDENTIALS\\necho $?\\nSolution:\\nYou have to set again the GOOGLE_APPLICATION_CREDENTIALS as Alexey did in the environment set-up video in week1:\\nexport GOOGLE_APPLICATION_CREDENTIALS=\"<path/to/your/service-account-authkeys>.json', 'section': 'Module 1: Docker and Terraform', 'question': 'Terraform - Error creating Dataset: googleapi: Error 403: Request had insufficient authentication scopes', 'course': 'data-engineering-zoomcamp'}, {'text': \"The error:\\nError: googleapi: Error 403: terraform-trans-campus@trans-campus-410115.iam.gserviceaccount.com does not have storage.buckets.create access to the Google Cloud project. Permission 'storage.buckets.create' denied on resource (or it may not exist)., forbidden\\nThe solution:\\nYou have to declare the project name as your Project ID, and not your Project name, available on GCP console Dashboard.\", 'section': 'Module 1: Docker and Terraform', 'question': 'Terraform - Error creating Bucket: googleapi: Error 403: Permission denied to access ‘storage.buckets.create’', 'course': 'data-engineering-zoomcamp'}, {'text': 'provider \"google\" {\\nproject     = var.projectId\\ncredentials = file(\"${var.gcpkey}\")\\n#region      = var.region\\nzone = var.zone\\n}', 'section': 'Module 1: Docker and Terraform', 'question': 'To ensure the sensitivity of the credentials file, I had to spend lot of time to input that as a file.', 'course': 'data-engineering-zoomcamp'}, {'text': 'For the HW1 I encountered this issue. The solution is\\nSELECT * FROM zones AS z WHERE z.\"Zone\" = \\'Astoria Zone\\';\\nI think columns which start with uppercase need to go between “Column”. I ran into a lot of issues like this and “ ” made it work out.\\nAddition to the above point, for me, there is no ‘Astoria Zone’, only ‘Astoria’ is existing in the dataset.\\nSELECT * FROM zones AS z WHERE z.\"Zone\" = \\'Astoria’;', 'section': 'Module 1: Docker and Terraform', 'question': \"SQL - SELECT * FROM zones_taxi WHERE Zone='Astoria Zone'; Error Column Zone doesn't exist\", 'course': 'data-engineering-zoomcamp'}, {'text': 'It is inconvenient to use quotation marks all the time, so it is better to put the data to the database all in lowercase, so in Pandas after\\ndf = pd.read_csv(‘taxi+_zone_lookup.csv’)\\nAdd the row:\\ndf.columns = df.columns.str.lower()', 'section': 'Module 1: Docker and Terraform', 'question': \"SQL - SELECT Zone FROM taxi_zones Error Column Zone doesn't exist\", 'course': 'data-engineering-zoomcamp'}, {'text': 'Solution (for mac users): os.system(f\"curl {url} --output {csv_name}\")', 'section': 'Module 1: Docker and Terraform', 'question': 'CURL - curl: (6) Could not resolve host: output.csv', 'course': 'data-engineering-zoomcamp'}, {'text': 'To resolve this, ensure that your config file is in C/User/Username/.ssh/config', 'section': 'Module 1: Docker and Terraform', 'question': 'SSH Error: ssh: Could not resolve hostname linux: Name or service not known', 'course': 'data-engineering-zoomcamp'}, {'text': 'If you use Anaconda (recommended for the course), it comes with pip, so the issues is probably that the anaconda’s Python is not on the PATH.\\nAdding it to the PATH is different for each operation system.\\nFor Linux and MacOS:\\nOpen a terminal.\\nFind the path to your Anaconda installation. This is typically `~/anaconda3` or `~/opt/anaconda3`.\\nAdd Anaconda to your PATH with the command: `export PATH=\"/path/to/anaconda3/bin:$PATH\"`.\\nTo make this change permanent, add the command to your `.bashrc` (Linux) or `.bash_profile` (MacOS) file.\\nOn Windows, python and pip are in different locations (python is in the anaconda root, and pip is in Scripts). With GitBash:\\nLocate your Anaconda installation. The default path is usually `C:\\\\Users\\\\[YourUsername]\\\\Anaconda3`.\\nDetermine the correct path format for Git Bash. Paths in Git Bash follow the Unix-style, so convert the Windows path to a Unix-style path. For example, `C:\\\\Users\\\\[YourUsername]\\\\Anaconda3` becomes `/c/Users/[YourUsername]/Anaconda3`.\\nAdd Anaconda to your PATH with the command: `export PATH=\"/c/Users/[YourUsername]/Anaconda3/:/c/Users/[YourUsername]/Anaconda3/Scripts/$PATH\"`.\\nTo make this change permanent, add the command to your `.bashrc` file in your home directory.\\nRefresh your environment with the command: `source ~/.bashrc`.\\nFor Windows (without Git Bash):\\nRight-click on \\'This PC\\' or \\'My Computer\\' and select \\'Properties\\'.\\nClick on \\'Advanced system settings\\'.\\nIn the System Properties window, click on \\'Environment Variables\\'.\\nIn the Environment Variables window, select the \\'Path\\' variable in the \\'System variables\\' section and click \\'Edit\\'.\\nIn the Edit Environment Variable window, click \\'New\\' and add the path to your Anaconda installation (typically `C:\\\\Users\\\\[YourUsername]\\\\Anaconda3` and C:\\\\Users\\\\[YourUsername]\\\\Anaconda3\\\\Scripts`).\\nClick \\'OK\\' in all windows to apply the changes.\\nAfter adding Anaconda to the PATH, you should be able to use `pip` from the command line. Remember to restart your terminal (or command prompt in Windows) to apply these changes.', 'section': 'Module 1: Docker and Terraform', 'question': \"'pip' is not recognized as an internal or external command, operable program or batch file.\", 'course': 'data-engineering-zoomcamp'}, {'text': \"Resolution: You need to stop the services which is using the port.\\nRun the following:\\n```\\nsudo kill -9 `sudo lsof -t -i:<port>`\\n```\\n<port> being 8080 in this case. This will free up the port for use.\\n~ Abhijit Chakraborty\\nError: error response from daemon: cannot stop container: 1afaf8f7d52277318b71eef8f7a7f238c777045e769dd832426219d6c4b8dfb4: permission denied\\nResolution: In my case, I had to stop docker and restart the service to get it running properly\\nUse the following command:\\n```\\nsudo systemctl restart docker.socket docker.service\\n```\\n~ Abhijit Chakraborty\\nError: cannot import module psycopg2\\nResolution: Run the following command in linux:\\n```\\nsudo apt-get install libpq-dev\\npip install psycopg2\\n```\\n~ Abhijit Chakraborty\\nError: docker build Error checking context: 'can't stat '<path-to-file>'\\nResolution: This happens due to insufficient permission for docker to access a certain file within the directory which hosts the Dockerfile.\\n1. You can create a .dockerignore file and add the directory/file which you want Dockerfile to ignore while build.\\n2. If the above does not work, then put the dockerfile and corresponding script, `\\t1.py` in our case to a subfolder. and run `docker build ...`\\nfrom inside the new folder.\\n~ Abhijit Chakraborty\", 'section': 'Module 1: Docker and Terraform', 'question': 'Error: error starting userland proxy: listen tcp4 0.0.0.0:8080: bind: address already in use', 'course': 'data-engineering-zoomcamp'}, {'text': 'To get a pip-friendly requirements.txt file file from Anaconda use\\nconda install pip then `pip list –format=freeze > requirements.txt`.\\n`conda list -d > requirements.txt` will not work and `pip freeze > requirements.txt` may give odd pathing.', 'section': 'Module 2: Workflow Orchestration', 'question': 'Anaconda to PIP', 'course': 'data-engineering-zoomcamp'}, {'text': 'Prefect: https://docs.google.com/document/d/1K_LJ9RhAORQk3z4Qf_tfGQCDbu8wUWzru62IUscgiGU/edit?usp=sharing\\nAirflow: https://docs.google.com/document/d/1-BwPAsyDH_mAsn8HH5z_eNYVyBMAtawJRjHHsjEKHyY/edit?usp=sharing', 'section': 'Module 2: Workflow Orchestration', 'question': 'Where are the FAQ questions from the previous cohorts for the orchestration module?', 'course': 'data-engineering-zoomcamp'}, {'text': 'Issue : Docker containers exit instantly with code 132, upon docker compose up\\nMage documentation has it listing the cause as \"older architecture\" .\\nThis might be a hardware issue, so unless you have another computer, you can\\'t solve it without purchasing a new one, so the next best solution is a VM.\\nThis is from a student running on a VirtualBox VM, Ubuntu 22.04.3 LTS, Docker version 25.0.2. So not having the context on how the vbox was spin up with (CPU, RAM, network, etc), it’s really inconclusive at this time.', 'section': 'Module 2: Workflow Orchestration', 'question': 'Docker - 2.2.2 Configure Mage', 'course': 'data-engineering-zoomcamp'}, {'text': 'This issue was occurring with Windows WSL 2\\nFor me this was because WSL 2 was not dedicating enough cpu cores to Docker.The load seems to take up at least one cpu core so I recommend dedicating at least two.\\nOpen Bash and run the following code:\\n$ cd ~\\n$ ls -la\\nLook for the .wsl config file:\\n-rw-r--r-- 1 ~1049089       31 Jan 25 12:54  .wslconfig\\nUsing a text editing tool of your choice edit or create your .wslconfig file:\\n$ nano .wslconfig\\nPaste the following into the new file/ edit the existing file in this format and save:\\n*** Note - for memory– this is the RAM on your machine you can dedicate to Docker, your situation may be different than mine ***\\n[wsl2]\\nprocessors=<Number of Processors - at least 2!> example: 4\\nmemory=<memory> example:4GB\\nExample:\\nOnce you do that run:\\n$ wsl --shutdown\\nThis shuts down WSL\\nThen Restart Docker Desktop - You should now be able to load the .csv.gz file without the error into a pandas dataframe', 'section': 'Module 2: Workflow Orchestration', 'question': 'WSL - 2.2.3 Mage - Unexpected Kernel Restarts; Kernel Running out of memory:', 'course': 'data-engineering-zoomcamp'}, {'text': 'The issue and solution on the link:\\nhttps://datatalks-club.slack.com/archives/C01FABYF2RG/p1706817366764269?thread_ts=1706815324.993529&cid=C01FABYF2RG', 'section': 'Module 2: Workflow Orchestration', 'question': '2.2.3 Configuring Postgres', 'course': 'data-engineering-zoomcamp'}, {'text': 'Check that the POSTGRES_PORT variable in the io_config.yml  file is set to port 5432, which is the default postgres port. The POSTGRES_PORT variable is the mage container port, not the host port. Hence, there’s no need to set the POSTGRES_PORT to 5431 just because you already have a conflicting postgres installation in your host machine.', 'section': 'Module 2: Workflow Orchestration', 'question': 'MAGE - 2.2.3 OperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5431 failed: Connection refused', 'course': 'data-engineering-zoomcamp'}, {'text': 'You forgot to select ‘dev’ profile in the dropdown menu next to where you select ‘PostgreSQL’ in the connection drop down.', 'section': 'Module 2: Workflow Orchestration', 'question': 'MAGE - 2.2.4 executing SELECT 1; results in KeyError', 'course': 'data-engineering-zoomcamp'}, {'text': 'If you are getting this error. Update your mage io_config.yaml file, and specify a timeout value set to 600 like this.\\nMake sure to save your changes.\\nMAGE - 2.2.4 Testing BigQuery connection using SQL 404 error:\\nNotFound: 404 Not found: Dataset ny-rides-diegogutierrez:None was not found in location northamerica-northeast1\\nIf you get this error even with all roles/permissions given to the service account check if you have ticked the box where it says “Use raw SQL”, just like the image below.', 'section': 'Module 2: Workflow Orchestration', 'question': \"MAGE -2.2.4 ConnectionError: ('Connection aborted.', TimeoutError('The write operation timed out'))\", 'course': 'data-engineering-zoomcamp'}, {'text': 'Solution: https://stackoverflow.com/questions/48056381/google-client-invalid-jwt-token-must-be-a-short-lived-token', 'section': 'Module 2: Workflow Orchestration', 'question': \"Problem: RefreshError: ('invalid_grant: Invalid JWT: Token must be a short-lived token (60 minutes) and in a reasonable timeframe. Check your iat and exp values in the JWT claim.', {'error': 'invalid_grant', 'error_description': 'Invalid JWT: Token must be a short-lived token (60 minutes) and in a reasonable timeframe. Check your iat and exp values in the JWT claim.'})\", 'course': 'data-engineering-zoomcamp'}, {'text': \"Origin of Solution (Mage Slack-Channel): https://mageai.slack.com/archives/C03HTTWFEKE/p1706543947795599\\nProblem: This error can often be seen after solving the error mentioned in 2.2.4. The error can be found in Mage version 0.9.61 and is a side-effect of the update of the code for data-loader blocks.\\nNote: Mage 0.9.62 has been released, as of Feb 5 2024. Please recheck. Solution below may be obsolete\\nSolution: Using a “fixed” version of the docker container\\nPull updated docker image from docker-hub\\nmageai/mageaidocker pull:alpha\\nUpdate docker-compose.yaml\\nversion: '3'\\nservices:\\nmagic:\\nimage: mageai/mageai:alpha  <--- instead of “latest”-tag\\ndocker-compose up\\nThe original Error is still present, but the SQL-query will return the desired result:\\n--------------------------------------------------------------------------------------\", 'section': 'Module 2: Workflow Orchestration', 'question': 'Mage - 2.2.4 IndexError: list index out of range', 'course': 'data-engineering-zoomcamp'}, {'text': 'Add\\nif not path.parent.is_dir():\\npath.parent.mkdir(parents=True)\\npath = Path(path).as_posix()\\nsee:\\nhttps://datatalks-club.slack.com/archives/C01FABYF2RG/p1675774214591809?thread_ts=1675768839.028879&cid=C01FABYF2RG', 'section': 'Module 2: Workflow Orchestration', 'question': '2.2.6 OSError: Cannot save file into a non-existent directory: \\'..\\\\\\\\..\\\\\\\\data\\\\\\\\yellow\\'\\\\n\")', 'course': 'data-engineering-zoomcamp'}, {'text': 'The video DE Zoomcamp 2.2.7 is missing  the actual deployment of Mage using Terraform to GCP. The steps for the deployment were not covered in the video.\\nI successfully deployed it and wanted to share some key points:\\nIn variables.tf, set the project_id default value to your GCP project ID.\\nEnable the Cloud Filestore API:\\nVisit the Google Cloud Console.to\\nNavigate to \"APIs & Services\" > \"Library.\"\\nSearch for \"Cloud Filestore API.\"\\nClick on the API and enable it.\\nTo perform the deployment:\\nterraform init\\nterraform apply\\nPlease note that during the terraform apply step, Terraform will prompt you to enter the PostgreSQL password. After that, it will ask for confirmation to proceed with the deployment. Review the changes, type \\'yes\\' when prompted, and press Enter.', 'section': 'Module 2: Workflow Orchestration', 'question': 'GCP - 2.2.7d Deploying Mage to GCP', 'course': 'data-engineering-zoomcamp'}, {'text': 'If you want to rune multiple docker containers from different directories. Then make sure to change the port mappings in the docker-compose.yml file.\\nports:\\n- 8088:6789\\nThe 8088 port in above case is hostport, where mage will run on your local machine. You can customize this as long as the port is available. If you are running on VM, make sure to forward the port too. You need to keep the container port to 6789 as this is the port where mage is running.\\nGCP - 2.2.7d Deploying Mage to Google Cloud\\nWhile terraforming all the resources inside a VM created in GCS the following error is shown.\\nError log:\\nmodule.lb-http.google_compute_backend_service.default[\"default\"]: Creating...\\n╷\\n│ Error: Error creating GlobalAddress: googleapi: Error 403: Request had insufficient authentication scopes.\\n│ Details:\\n│ [\\n│   {\\n│     \"@type\": \"type.googleapis.com/google.rpc.ErrorInfo\",\\n│     \"domain\": \"googleapis.com\",\\n│     \"metadatas\": {\\n│       \"method\": \"compute.beta.GlobalAddressesService.Insert\",\\n│       \"service\": \"compute.googleapis.com\"\\n│     },\\n│     \"reason\": \"ACCESS_TOKEN_SCOPE_INSUFFICIENT\"\\n│   }\\n│ ]\\n│\\n│ More details:\\n│ Reason: insufficientPermissions, Message: Insufficient Permission\\nThis error might happen when you are using a VM inside GCS. To use the Google APIs from a GCP virtual machine you need to add the cloud platform scope (\"https://www.googleapis.com/auth/cloud-platform\") to your VM when it is created.\\nSince ours is already created you can just stop it and change the permissions. You can do it in the console, just go to \"EDIT\", g99o all the way down until you find \"Cloud API access scopes\". There you can \"Allow full access to all Cloud APIs\". I did this and all went smoothly generating all the resources needed. Hope it helps if you encounter this same error.\\nResources: https://stackoverflow.com/questions/35928534/403-request-had-insufficient-authentication-scopes-during-gcloud-container-clu', 'section': 'Module 2: Workflow Orchestration', 'question': 'Ruuning Multiple Mage instances in Docker from different directories', 'course': 'data-engineering-zoomcamp'}, {'text': 'If you are on the free trial account on GCP you will face this issue when trying to deploy the infrastructures with terraform. This service is not available for this kind of account.\\nThe solution I found was to delete the load_balancer.tf file and to comment or delete the rows that differentiate it on the main.tf file. After this just do terraform destroy to delete any infrastructure created on the fail attempts and re-run the terraform apply.\\nCode on main.tf to comment/delete:\\nLine 166, 167, 168', 'section': 'Module 2: Workflow Orchestration', 'question': 'GCP - 2.2.7d Load Balancer Problem (Security Policies quota)', 'course': 'data-engineering-zoomcamp'}, {'text': \"If you get the following error\\nYou have to edit variables.tf on the gcp folder, set your project-id and region and zones properly. Then, run terraform apply again.\\nYou can find correct regions/zones here: https://cloud.google.com/compute/docs/regions-zones\\nDeploying MAGE to GCP  with Terraform via the VM (2.2.7)\\nFYI - It can take up to 20 minutes to deploy the MAGE Terraform files if you are using a GCP Virtual Machine. It is normal, so don’t interrupt the process or think it’s taking too long. If you have, make sure you run a terraform destroy before trying again as you will have likely partially created resources which will cause errors next time you run `terraform apply`.\\n`terraform destroy` may not completely delete partial resources - go to Google Cloud Console and use the search bar at the top to search for the ‘app.name’ you declared in your variables.tf file; this will list all resources with that name - make sure you delete them all before running `terraform apply` again.\\nWhy are my GCP free credits going so fast? MAGE .tf files - Terraform Destroy not destroying all Resources\\nI checked my GCP billing last night & the MAGE Terraform IaC didn't destroy a GCP Resource called Filestore as ‘mage-data-prep- it has been costing £5.01 of my free credits each day  I now have £151 left - Alexey has assured me that This amount WILL BE SUFFICIENT funds to finish the course. Note to anyone who had issues deploying the MAGE terraform code: check your billing account to see what you're being charged for (main menu - billing) (even if it's your free credits) and run a search for 'mage-data-prep' in the top bar just to be sure that your resources have been destroyed - if any come up delete them.\", 'section': 'Module 2: Workflow Orchestration', 'question': 'GCP - 2.2.7d Part 2 - Getting error when you run terraform apply', 'course': 'data-engineering-zoomcamp'}, {'text': '```\\n│ Error: Error creating Connector: googleapi: Error 403: Permission \\'vpcaccess.connectors.create\\' denied on resource \\'//vpcaccess.googleapis.com/projects/<ommit>/locations/us-west1\\' (or it may not exist).\\n│ Details:\\n│ [\\n│   {\\n│     \"@type\": \"type.googleapis.com/google.rpc.ErrorInfo\",\\n│     \"domain\": \"vpcaccess.googleapis.com\",\\n│     \"metadata\": {\\n│       \"permission\": \"vpcaccess.connectors.create\",\\n│       \"resource\": \"projects/<ommit>/locations/us-west1\"\\n│     },\\n│     \"reason\": \"IAM_PERMISSION_DENIED\"\\n│   }\\n│ ]\\n│\\n│   with google_vpc_access_connector.connector,\\n│   on fs.tf line 19, in resource \"google_vpc_access_connector\" \"connector\":\\n│   19: resource \"google_vpc_access_connector\" \"connector\" {\\n│\\n```\\nSolution: Add Serverless VPC Access Admin to Service Account.\\nLine 148', 'section': 'Module 2: Workflow Orchestration', 'question': \"Question: Permission 'vpcaccess.connectors.create'\", 'course': 'data-engineering-zoomcamp'}, {'text': 'Git won’t push an empty folder to GitHub, so if you put a file in that folder and then push, then you should be good to go.\\nOr - in your code- make the folder if it doesn’t exist using Pathlib as shown here: https://stackoverflow.com/a/273227/4590385.\\nFor some reason, when using github storage, the relative path for writing locally no longer works. Try using two separate paths, one full path for the local write, and the original relative path for GCS bucket upload.', 'section': 'Module 2: Workflow Orchestration', 'question': \"File Path: Cannot save file into a non-existent directory: 'data/green'\", 'course': 'data-engineering-zoomcamp'}, {'text': 'The green dataset contains lpep_pickup_datetime while the yellow contains tpep_pickup_datetime. Modify the script(s) depending on  the dataset as required.', 'section': 'Module 2: Workflow Orchestration', 'question': 'No column name lpep_pickup_datetime / tpep_pickup_datetime', 'course': 'data-engineering-zoomcamp'}, {'text': 'pd.read_csv\\ndf_iter = pd.read_csv(dataset_url, iterator=True, chunksize=100000)\\nThe data needs to be appended to the parquet file using the fastparquet engine\\ndf.to_parquet(path, compression=\"gzip\", engine=\\'fastparquet\\', append=True)', 'section': 'Module 2: Workflow Orchestration', 'question': 'Process to download the VSC using Pandas is killed right away', 'course': 'data-engineering-zoomcamp'}, {'text': 'denied: requested access to the resource is denied\\nThis can happen when you\\nHaven\\'t logged in properly to Docker Desktop (use docker login -u \"myusername\")\\nHave used the wrong username when pushing to docker images. Use the same one as your username and as the one you build on\\ndocker image build -t <myusername>/<imagename>:<tag>\\ndocker image push <myusername>/<imagename>:<tag>', 'section': 'Module 2: Workflow Orchestration', 'question': 'Push to docker image failure', 'course': 'data-engineering-zoomcamp'}, {'text': \"16:21:35.607 | INFO    | Flow run 'singing-malkoha' - Executing 'write_bq-b366772c-0' immediately...\\nKilled\\nSolution:  You probably are running out of memory on your VM and need to add more.  For example, if you have 8 gigs of RAM on your VM, you may want to expand that to 16 gigs.\", 'section': 'Module 2: Workflow Orchestration', 'question': 'Flow script fails with “killed” message:', 'course': 'data-engineering-zoomcamp'}, {'text': 'After playing around with prefect for a while this can happen.\\nSsh to your VM and run sudo du -h --block-size=G | sort -n -r | head -n 30 to see which directory needs the most space.\\nMost likely it will be …/.prefect/storage, where your cached flows are stored. You can delete older flows from there. You also have to delete the corresponding flow in the UI, otherwise it will throw you an error, when you try to run your next flow.\\nSSL Certificate Verify: (I got it when trying to run flows on MAC): urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED]\\npip install certifi\\n/Applications/Python\\\\ {ver}/Install\\\\ Certificates.command\\nor\\nrunning the “Install Certificate.command” inside of the python{ver} folder', 'section': 'Module 2: Workflow Orchestration', 'question': 'GCP VM: Disk Space is full', 'course': 'data-engineering-zoomcamp'}, {'text': 'It means your container consumed all available RAM allocated to it. It can happen in particular when working on Question#3 in the homework as the dataset is relatively large and containers eat a lot of memory in general.\\nI would recommend restarting your computer and only starting the necessary processes to run the container. If that doesn’t work, allocate more resources to docker. If also that doesn’t work because your workstation is a potato, you can use an online compute environment service like GitPod, which is free under under 50 hours / month of use.', 'section': 'Module 2: Workflow Orchestration', 'question': 'Docker: container crashed with status code 137.', 'course': 'data-engineering-zoomcamp'}, {'text': 'In Q3 there was a task to run the etl script from web to GCS. The problem was, it wasn’t really an ETL straight from web to GCS, but it was actually a web to local storage to local memory to GCS over network ETL. Yellow data is about 100 MB each per month compressed and ~700 MB after uncompressed on memory\\nThis leads to a problem where i either got a network type error because my not so good 3rd world internet or i got my WSL2 crashed/hanged because out of memory error and/or 100% resource usage hang.\\nSolution:\\nif you have a lot of time at hand, try compressing it to parquet and writing it to GCS with the timeout argument set to a really high number (the default os 60 seconds)\\nthe yellow taxi data for feb 2019 is about 100MB as parquet file\\ngcp_cloud_storage_bucket_block.upload_from_path(\\nfrom_path=f\"{path}\",\\nto_path=path,\\ntimeout=600\\n)', 'section': 'Module 2: Workflow Orchestration', 'question': 'Timeout due to slow upload internet', 'course': 'data-engineering-zoomcamp'}, {'text': 'This error occurs when you try to re-run the export block, of the transformed green_taxi data to PostgreSQL.\\nWhat you’ll need to do is to drop the table using SQL in Mage (screenshot below).\\nYou should be able to re-run the block successfully after dropping the table.', 'section': 'Module 2: Workflow Orchestration', 'question': 'UndefinedColumn: column \"ratecode_id\", \"rate_code_id\" “vendor_id”, “pu_location_id”, “do_location_id” of relation \"green_taxi\" does not exist - Export transformed green_taxi data to PostgreSQL', 'course': 'data-engineering-zoomcamp'}, {'text': 'SettingWithCopyWarning:\\nA value is trying to be set on a copy of a slice from a DataFrame.\\nUse the data.loc[] = value syntax instead of df[] = value to ensure that the new column is being assigned to the original dataframe instead of a copy of a dataframe or a series.', 'section': 'Module 2: Workflow Orchestration', 'question': 'Homework - Q3 SettingWithCopyWarning Error:', 'course': 'data-engineering-zoomcamp'}, {'text': 'CSV Files are very big in nyc data, so we instead of using Pandas/Python kernel , we can try Pyspark Kernel\\nDocumentation of Mage for using pyspark kernel: https://docs.mage.ai/integrations/spark-pyspark\\n?', 'section': 'Module 2: Workflow Orchestration', 'question': 'Since I was using slow laptop, and we have so big csv files, I used pyspark kernel in mage instead of python, How to do it?', 'course': 'data-engineering-zoomcamp'}, {'text': 'So we will first delete the connection between blocks then we can remove the connection.', 'section': 'Module 2: Workflow Orchestration', 'question': 'I got an error when I was deleting  BLOCK IN A PIPELINE', 'course': 'data-engineering-zoomcamp'}, {'text': 'While Editing the Pipeline Name It throws permission denied error.\\n(Work around)In that case proceed with the work and save later on revisit it will let you edit.', 'section': 'Module 2: Workflow Orchestration', 'question': 'Mage UI won’t let you edit the Pipeline name?', 'course': 'data-engineering-zoomcamp'}, {'text': 'Solution n°1 if you want to download everything :\\n```\\nimport pyarrow as pa\\nimport pyarrow.parquet as pq\\nfrom pyarrow.fs import GcsFileSystem\\n…\\n@data_loader\\ndef load_data(*args, **kwargs):\\n    bucket_name = YOUR_BUCKET_NAME_HERE\\'\\n    blob_prefix = \\'PATH / TO / WHERE / THE / PARTITIONS / ARE\\'\\n    root_path = f\"{bucket_name}/{blob_prefix}\"\\npa_table = pq.read_table(\\n        source=root_path,\\n        filesystem=GcsFileSystem(),        \\n    )\\n\\n    return pa_table.to_pandas()\\nSolution n°2 if you want to download only some dates :\\n@data_loader\\ndef load_data(*args, **kwargs):\\ngcs = pa.fs.GcsFileSystem()\\nbucket_name = \\'YOUR_BUCKET_NAME_HERE\\'\\nblob_prefix = \\'\\'PATH / TO / WHERE / THE / PARTITIONS / ARE\\'\\'\\nroot_path = f\"{bucket_name}/{blob_prefix}\"\\npa_dataset = pq.ParquetDataset(\\npath_or_paths=root_path,\\nfilesystem=gcs,\\nfilters=[(\\'lpep_pickup_date\\', \\'>=\\', \\'2020-10-01\\'), (\\'lpep_pickup_date\\', \\'<=\\', \\'2020-10-31\\')]\\n)\\nreturn pa_dataset.read().to_pandas()\\n# More information about the pq.Parquet.Dataset : Encapsulates details of reading a complete Parquet dataset possibly consisting of multiple files and partitions in subdirectories. Documentation here :\\nhttps://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetDataset.html#pyarrow.parquet.ParquetDataset\\nERROR: UndefinedColumn: column \"vendor_id\" of relation \"green_taxi\" does not exist\\nTwo possible solutions both of them work in the same way.\\nOpen up a Data Loader connect using SQL - RUN the command \\n`DROP TABLE mage.green_taxi`\\nElse, Open up a Data Extractor of SQL  - increase the rows to above the number of rows in the dataframe (you can find that in the bottom of the transformer block) change the Write Policy to `Replace` and run the SELECT statement', 'section': 'Module 2: Workflow Orchestration', 'question': 'How do I make Mage load the partitioned files that we created on 2.2.4, to load them into BigQuery ?', 'course': 'data-engineering-zoomcamp'}, {'text': \"All mage files are in your /home/src/folder where you saved your credentials.json so you should be able to access them locally. You will see a folder for ‘Pipelines’,  'data loaders', 'data transformers' & 'data exporters' - inside these will be the .py or .sql files for the blocks you created in your pipeline.\\nRight click & ‘download’ the pipeline itself to your local machine (which gives you metadata, pycache and other files)\\nAs above, download each .py/.sql file that corresponds to each block you created for the pipeline. You'll find these under 'data loaders', 'data transformers' 'data exporters'\\nMove the downloaded files to your GitHub repo folder & commit your changes.\", 'section': 'Module 2: Workflow Orchestration', 'question': 'Git - What Files Should I Submit for Homework 2 & How do I get them out of MAGE:', 'course': 'data-engineering-zoomcamp'}, {'text': 'Assuming you downloaded the Mage repo in the week 2 folder of the Data Engineering Zoomcamp, you might want to include your mage copy, demo pipelines and homework within your personal copy of the Data Engineering Zoomcamp repo. This will not work by default, because GitHub sees them as two separate repositories, and one does not track the other. To add the Mage files to your main DE Zoomcamp repo, you will need to:\\nMove the contents of the .gitignore file in your main .gitignore.\\nUse the terminal to cd into the Mage folder and:\\nrun “git remote remove origin” to de-couple the Mage repo,\\nrun “rm -rf .git” to delete local git files,\\nrun “git add .” to add the current folder as changes to stage, commit and push.', 'section': 'Module 2: Workflow Orchestration', 'question': 'Git - How do I include the files in the Mage repo (including exercise files and homework) in a personal copy of the Data Engineering Zoomcamp repo?', 'course': 'data-engineering-zoomcamp'}, {'text': \"When try to add three assertions:\\nvendor_id is one of the existing values in the column (currently)\\npassenger_count is greater than 0\\ntrip_distance is greater than 0\\nto test_output, I got ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). Below is my code:\\ndata_filter = (data['passenger_count'] > 0) and (data['trip_distance'] > 0)\\nAfter looking for solutions at Stackoverflow, I found great discussion about it. So I changed my code into:\\ndata_filter = (data['passenger_count'] > 0) & (data['trip_distance'] > 0)\", 'section': 'Module 2: Workflow Orchestration', 'question': 'Got ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()', 'course': 'data-engineering-zoomcamp'}, {'text': 'This happened when I just booted up my PC, continuing from the progress I was doing from yesterday.\\nAfter cd-ing into your directory, and running docker compose up , the web interface for the Mage shows, but the files that I had yesterday was gone.\\nIf your files are gone, go ahead and close the web interface, and properly shutting down the mage docker compose by doing Ctrl + C once. Try running it again. This worked for me more than once (yes the issue persisted with my PC twice)\\nAlso, you should check if you’re in the correct repository before doing docker compose up . This was discussed in the Slack #course-data-engineering channel', 'section': 'Module 2: Workflow Orchestration', 'question': 'Mage AI Files are Gone/disappearing', 'course': 'data-engineering-zoomcamp'}, {'text': 'The above errors due to “ at the trailing side and it need to be modified with ‘ quotes at both ends\\nKrishna Anand', 'section': 'Module 2: Workflow Orchestration', 'question': 'Mage - Errors in io.config.yaml file', 'course': 'data-engineering-zoomcamp'}, {'text': \"Problem: The following error occurs when attempting to export data from Mage to a GCS bucket using pyarrow suggesting Mage doesn’t have the necessary permissions to access the specified GCP credentials .json file.\\nArrowException: Unknown error: google::cloud::Status(UNKNOWN: Permanent error GetBucketMetadata: Could not create a OAuth2 access token to authenticate the request. The request was not sent, as such an access token is required to complete the request successfully. Learn more about Google Cloud authentication at https://cloud.google.com/docs/authentication. The underlying error message was: Cannot open credentials file /home/src/...\\nSolution: Inside the Mage app:\\nCreate a credentials folder (e.g. gcp-creds) within the magic-zoomcamp folder\\nIn the credentials folder create a .json key file (e.g. mage-gcp-creds.json)\\nCopy/paste GCP service account credentials into the .json key file and save\\nUpdate code to point to this file. E.g.\\nenviron['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/src/magic-zoomcamp/gcp-creds/mage-gcp-creds.json'\", 'section': 'Module 2: Workflow Orchestration', 'question': 'Mage - ArrowException Cannot open credentials file', 'course': 'data-engineering-zoomcamp'}, {'text': \"Oserror: google::cloud::status(unavailable: retry policy exhausted getbucketmetadata: could not create a OAuth2 access token to authenticate the request. the request was not sent, as such an access token is required to complete the request successfully. learn more about google cloud authentication at https://cloud.google.com/docs/authentication. the underlying error message was: performwork() - curl error [6]=couldn't resolve host name)\", 'section': 'Module 2: Workflow Orchestration', 'question': 'Mage - OSError', 'course': 'data-engineering-zoomcamp'}, {'text': \"Problem: The following error occurs when attempting to export data from Mage to a GCS bucket. Assigned service account doesn’t have the necessary permissions access Google Cloud Storage Bucket\\nPermissionError: [Errno 13] google::cloud::Status(PERMISSION_DENIED: Permanent error GetBucketMetadata:... .iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket. Permission 'storage.buckets.get' denied on resource (or it may not exist). error_info={reason=forbidden, domain=global, metadata={http_status_code=403}}). Detail: [errno 13] Permission denied\\nSolution: Add Cloud Storage Admin role to the service account:\\nGo to project in Google Cloud Console>IAM & Admin>IAM\\nClick Edit principal (pencil symbol) to the right of the service account you are using\\nClick + ADD ANOTHER ROLE\\nSelect Cloud Storage>Storage Admin\\nClick Save\", 'section': 'Module 2: Workflow Orchestration', 'question': 'Mage - PermissionError service account does not have storage.buckets.get access to the Google Cloud Storage bucket', 'course': 'data-engineering-zoomcamp'}, {'text': '1. Make sure your pyspark script is ready to be send to Dataproc cluster\\n2. Create a Dataproc Cluster in GCP Console\\n3. Make sure to edit the service account and add new role - Dataproc Editor\\n4. Copy the python script ./notebooks/pyspark_script.py and place it under GCS bucket path\\n5. Make sure gcloud cli is installed either in Mage manually or  via your Dockerfile and docker-compose files. This is needed to let Mage access google Dataproc and the script it needs to execute. Refer - Installing the latest gcloud CLI\\n6. Use the Bigquery/Dataproc script mentioned here - https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/05-batch/code/cloud.md . Use Mage to trigger the query', 'section': 'Module 3: Data Warehousing', 'question': 'Trigger Dataproc from Mage', 'course': 'data-engineering-zoomcamp'}, {'text': 'A:\\n1 solution) Add -Y flag, so that apt-get automatically agrees to install additional packages\\n2) Use python ZipFile package, which is included in all modern python distributions', 'section': 'Module 3: Data Warehousing', 'question': 'Docker-compose takes infinitely long to install zip unzip packages for linux, which are required to unpack datasets', 'course': 'data-engineering-zoomcamp'}, {'text': 'Make sure to use Nullable dataTypes, such as Int64 when appliable.', 'section': 'Module 3: Data Warehousing', 'question': 'GCS Bucket - error when writing data from web to GCS:', 'course': 'data-engineering-zoomcamp'}, {'text': 'Ultimately, when trying to ingest data into a BigQuery table, all files within a given directory must have the same schema.\\nWhen dealing for example with the FHV Datasets from 2019, however (see image below), one can see that the files for \\'2019-05\\', and 2019-06, have the columns \"PUlocationID\" and \"DOlocationID\" as Integers, while for the period of \\'2019-01\\' through \\'2019-04\\', the same column is defined as FLOAT.\\nSo while importing these files as parquet to BigQuery, the first one will be used to define the schema of the table, while all files following that will be used to append data on the existing table. Which means, they must all follow the very same schema of the file that created the table.\\nSo, in order to prevent errors like that, make sure to enforce the data types for the columns on the DataFrame before you serialize/upload them to BigQuery. Like this:\\npd.read_csv(\"path_or_url\").astype({\\n\\t\"col1_name\": \"datatype\",\\t\\n\\t\"col2_name\": \"datatype\",\\t\\n\\t...\\t\\t\\t\\t\\t\\n\\t\"colN_name\": \"datatype\" \\t\\n})', 'section': 'Module 3: Data Warehousing', 'question': \"GCS Bucket - Failed to create table: Error while reading data, error message: Parquet column 'XYZ' has type INT which does not match the target cpp_type DOUBLE. File: gs://path/to/some/blob.parquet\", 'course': 'data-engineering-zoomcamp'}, {'text': \"If you receive the error gzip.BadGzipFile: Not a gzipped file (b'\\\\n\\\\n'), this is because you have specified the wrong URL to the FHV dataset. Make sure to use https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/{dataset_file}.csv.gz\\nEmphasising the ‘/releases/download’ part of the URL.\", 'section': 'Module 3: Data Warehousing', 'question': 'GCS Bucket - Fix Error when importing FHV data to GCS', 'course': 'data-engineering-zoomcamp'}, {'text': 'Krishna Anand', 'section': 'Module 3: Data Warehousing', 'question': 'GCS Bucket - Load Data From URL list in to GCP Bucket', 'course': 'data-engineering-zoomcamp'}, {'text': 'Check the Schema\\nYou might have a wrong formatting\\nTry to upload the CSV.GZ files without formatting or going through pandas via wget\\nSee this Slack conversation for helpful tips', 'section': 'Module 3: Data Warehousing', 'question': 'GCS Bucket - I query my dataset and get a Bad character (ASCII 0) error?', 'course': 'data-engineering-zoomcamp'}, {'text': 'Run the following command to check if “BigQuery Command Line Tool” is installed or not: gcloud components list\\nYou can also use bq.cmd instead of bq to make it work.', 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - “bq: command not found”', 'course': 'data-engineering-zoomcamp'}, {'text': 'Use big queries carefully,\\nI created by bigquery dataset on an account where my free trial was exhausted, and got a bill of $80.\\nUse big query in free credits and destroy all the datasets after creation.\\nCheck your Billing daily! Especially if you’ve spinned up a VM.', 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - Caution in using bigquery:no', 'course': 'data-engineering-zoomcamp'}, {'text': 'Be careful when you create your resources on GCP, all of them have to share the same Region in order to allow load data from GCS Bucket to BigQuery. If you forgot it when you created them, you can create a new dataset on BigQuery using the same Region which you used on your GCS Bucket.\\nThis means that your GCS Bucket and the BigQuery dataset are placed in different regions. You have to create a new dataset inside BigQuery in the same region with your GCS bucket and store the data in the newly created dataset.', 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - Cannot read and write in different locations: source: EU, destination: US - Loading data from GCS into BigQuery (different Region):', 'course': 'data-engineering-zoomcamp'}, {'text': \"Make sure to create the BigQuery dataset in the very same location that you've created the GCS Bucket. For instance, if your GCS Bucket was created in `us-central1`, then BigQuery dataset must be created in the same region (us-central1, in this example)\", 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - Cannot read and write in different locations: source: <REGION_HERE>, destination: <ANOTHER_REGION_HERE>', 'course': 'data-engineering-zoomcamp'}, {'text': 'By the way, this isn’t a problem/solution, but a useful hint:\\nPlease, remember to save your progress in BigQuery SQL Editor.\\nI was almost finishing the homework, when my Chrome Tab froze and I had to reload it. Then I lost my entire SQL script.\\nSave your script from time to time. Just click on the button at the top bar. Your saved file will be available on the left panel.\\nAlternatively, you can copy paste your queries into an .sql file in your preferred editor (Notepad++, VS Code, etc.). Using the .sql extension will provide convenient color formatting.', 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - Remember to save your queries', 'course': 'data-engineering-zoomcamp'}, {'text': 'Ans :  While real-time analytics might not be explicitly mentioned, BigQuery has real-time data streaming capabilities, allowing for potential integration in future project iterations.', 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - Can I use BigQuery for real-time analytics in this project?', 'course': 'data-engineering-zoomcamp'}, {'text': \"could not parse 'pickup_datetime' as timestamp for field pickup_datetime (position 2)\\nThis error is caused by invalid data in the timestamp column. A way to identify the problem is to define the schema from the external table using string datatype. This enables the queries to work at which point we can filter out the invalid rows from the import to the materialised table and insert the fields with the timestamp data type.\", 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - Unable to load data from external tables into a materialized table in BigQuery due to an invalid timestamp error that are added while appending data to the file in Google Cloud Storage', 'course': 'data-engineering-zoomcamp'}, {'text': 'Background:\\n`pd.read_parquet`\\n`pd.to_datetime`\\n`pq.write_to_dataset`\\nReference:\\nhttps://stackoverflow.com/questions/48314880/are-parquet-file-created-with-pyarrow-vs-pyspark-compatible\\nhttps://stackoverflow.com/questions/57798479/editing-parquet-files-with-python-causes-errors-to-datetime-format\\nhttps://www.reddit.com/r/bigquery/comments/16aoq0u/parquet_timestamp_to_bq_coming_across_as_int/?share_id=YXqCs5Jl6hQcw-kg6-VgF&utm_content=1&utm_medium=ios_app&utm_name=ioscss&utm_source=share&utm_term=1\\nSolution:\\nAdd `use_deprecated_int96_timestamps=True` to `pq.write_to_dataset` function, like below\\npq.write_to_dataset(\\ntable,\\nroot_path=root_path,\\nfilesystem=gcs,\\nuse_deprecated_int96_timestamps=True\\n# Write timestamps to INT96 Parquet format\\n)', 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - Error Message in BigQuery: annotated as a valid Timestamp, please annotate it as TimestampType(MICROS) or TimestampType(MILLIS)', 'course': 'data-engineering-zoomcamp'}, {'text': 'Solution:\\nIf you’re using Mage, in the last Data Exporter that writes to Google Cloud Storage use PyArrow to generate the Parquet file with the correct logical type for the datetime columns, otherwise they won\\'t be converted to timestamp when loaded by BigQuery later on.\\nimport pyarrow as pa\\nimport pyarrow.parquet as pq\\nimport os\\nif \\'data_exporter\\' not in globals():\\nfrom mage_ai.data_preparation.decorators import data_exporter\\n# Replace with the location of your service account key JSON file.\\nos.environ[\\'GOOGLE_APPLICATION_CREDENTIALS\\'] = \\'/home/src/personal-gcp.json\\'\\nbucket_name = \"<YOUR_BUCKET_NAME>\"\\nobject_key = \\'nyc_taxi_data_2022.parquet\\'\\nwhere = f\\'{bucket_name}/{object_key}\\'\\n@data_exporter\\ndef export_data(data, *args, **kwargs):\\ntable = pa.Table.from_pandas(data, preserve_index=False)\\ngcs = pa.fs.GcsFileSystem()\\npq.write_table(\\ntable,\\nwhere,\\n# Convert integer columns in Epoch milliseconds\\n# to Timestamp columns in microseconds (\\'us\\') so\\n# they can be loaded into BigQuery with the right\\n# data type\\ncoerce_timestamps=\\'us\\',\\nfilesystem=gcs\\n)\\nSolution 2:\\nIf you’re using Mage, in the last Data Exporter that writes to Google Cloud Storage, provide PyArrow with explicit schema to generate the Parquet file with the correct logical type for the datetime columns, otherwise they won\\'t be converted to timestamp when loaded by BigQuery later on.\\nschema = pa.schema([\\n(\\'vendor_id\\', pa.int64()),\\n(\\'lpep_pickup_datetime\\', pa.timestamp(\\'ns\\')),\\n(\\'lpep_dropoff_datetime\\', pa.timestamp(\\'ns\\')),\\n(\\'store_and_fwd_flag\\', pa.string()),\\n(\\'ratecode_id\\', pa.int64()),\\n(\\'pu_location_id\\', pa.int64()),\\n(\\'do_location_id\\', pa.int64()),\\n(\\'passenger_count\\', pa.int64()),\\n(\\'trip_distance\\', pa.float64()),\\n(\\'fare_amount\\', pa.float64()),\\n(\\'extra\\', pa.float64()),\\n(\\'mta_tax\\', pa.float64()),\\n(\\'tip_amount\\', pa.float64()),\\n(\\'tolls_amount\\', pa.float64()),\\n(\\'improvement_surcharge\\', pa.float64()),\\n(\\'total_amount\\', pa.float64()),\\n(\\'payment_type\\', pa.int64()),\\n(\\'trip_type\\', pa.int64()),\\n(\\'congestion_surcharge\\', pa.float64()),\\n(\\'lpep_pickup_month\\', pa.int64())\\n])\\ntable = pa.Table.from_pandas(data, schema=schema)', 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - Datetime columns in Parquet files created from Pandas show up as integer columns in BigQuery', 'course': 'data-engineering-zoomcamp'}, {'text': 'Reference:\\nhttps://cloud.google.com/bigquery/docs/external-data-cloud-storage\\nSolution:\\nfrom google.cloud import bigquery\\n# Set table_id to the ID of the table to create\\ntable_id = f\"{project_id}.{dataset_name}.{table_name}\"\\n# Construct a BigQuery client object\\nclient = bigquery.Client()\\n# Set the external source format of your table\\nexternal_source_format = \"PARQUET\"\\n# Set the source_uris to point to your data in Google Cloud\\nsource_uris = [ f\\'gs://{bucket_name}/{object_key}/*\\']\\n# Create ExternalConfig object with external source format\\nexternal_config = bigquery.ExternalConfig(external_source_format)\\n# Set source_uris that point to your data in Google Cloud\\nexternal_config.source_uris = source_uris\\nexternal_config.autodetect = True\\ntable = bigquery.Table(table_id)\\n# Set the external data configuration of the table\\ntable.external_data_configuration = external_config\\ntable = client.create_table(table)  # Make an API request.\\nprint(f\\'Created table with external source: {table_id}\\')\\nprint(f\\'Format: {table.external_data_configuration.source_format}\\')', 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - Create External Table using Python', 'course': 'data-engineering-zoomcamp'}, {'text': 'Reference:\\nhttps://stackoverflow.com/questions/60941726/can-bigquery-api-overwrite-existing-table-view-with-create-table-tables-inser\\nSolution:\\nCombine with “Create External Table using Python”, use it before “client.create_table” function.\\ndef tableExists(tableID, client):\\n\"\"\"\\nCheck if a table already exists using the tableID.\\nreturn : (Boolean)\\n\"\"\"\\ntry:\\ntable = client.get_table(tableID)\\nreturn True\\nexcept Exception as e: # NotFound:\\nreturn False', 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - Check BigQuery Table Exist And Delete', 'course': 'data-engineering-zoomcamp'}, {'text': 'To avoid this error you can upload data from Google Cloud Storage to BigQuery through BigQuery Cloud Shell using the command:\\n$ bq load  --autodetect --allow_quoted_newlines --source_format=CSV dataset_name.table_name \"gs://dtc-data-lake-bucketname/fhv/fhv_tripdata_2019-*.csv.gz\"', 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - Error: Missing close double quote (\") character', 'course': 'data-engineering-zoomcamp'}, {'text': 'Solution: This problem arises if your gcs and bigquery storage is in different regions.\\nOne potential way to solve it:\\nGo to your google cloud bucket and check the region in field named “Location”\\nNow in bigquery, click on three dot icon near your project name and select create dataset.\\nIn region filed choose the same regions as you saw in your google cloud bucket', 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - Cannot read and write in different locations: source: asia-south2, destination: US', 'course': 'data-engineering-zoomcamp'}, {'text': 'There are multiple benefits of using Cloud Functions to automate tasks in Google Cloud.\\nUse below Cloud Function python script to load files directly to BigQuery. Use your project id, dataset id & table id as defined by you.\\nimport tempfile\\nimport requests\\nimport logging\\nfrom google.cloud import bigquery\\ndef hello_world(request):\\n# table_id = <project_id.dataset_id.table_id>\\ntable_id = \\'de-zoomcap-project.dezoomcamp.fhv-2019\\'\\n# Create a new BigQuery client\\nclient = bigquery.Client()\\nfor month in range(4, 13):\\n# Define the schema for the data in the CSV.gz files\\nurl = \\'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-{:02d}.csv.gz\\'.format(month)\\n# Download the CSV.gz file from Github\\nresponse = requests.get(url)\\n# Create new table if loading first month data else append\\nwrite_disposition_string = \"WRITE_APPEND\" if month > 1 else \"WRITE_TRUNCATE\"\\n# Defining LoadJobConfig with schema of table to prevent it from changing with every table\\njob_config = bigquery.LoadJobConfig(\\nschema=[\\nbigquery.SchemaField(\"dispatching_base_num\", \"STRING\"),\\nbigquery.SchemaField(\"pickup_datetime\", \"TIMESTAMP\"),\\nbigquery.SchemaField(\"dropOff_datetime\", \"TIMESTAMP\"),\\nbigquery.SchemaField(\"PUlocationID\", \"STRING\"),\\nbigquery.SchemaField(\"DOlocationID\", \"STRING\"),\\nbigquery.SchemaField(\"SR_Flag\", \"STRING\"),\\nbigquery.SchemaField(\"Affiliated_base_number\", \"STRING\"),\\n],\\nskip_leading_rows=1,\\nwrite_disposition=write_disposition_string,\\nautodetect=True,\\nsource_format=\"CSV\",\\n)\\n# Load the data into BigQuery\\n# Create a temporary file to prevent the exception- AttributeError: \\'bytes\\' object has no attribute \\'tell\\'\"\\nwith tempfile.NamedTemporaryFile() as f:\\nf.write(response.content)\\nf.seek(0)\\njob = client.load_table_from_file(\\nf,\\ntable_id,\\nlocation=\"US\",\\njob_config=job_config,\\n)\\njob.result()\\nlogging.info(\"Data for month %d successfully loaded into table %s.\", month, table_id)\\nreturn \\'Data loaded into table {}.\\'.format(table_id)', 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - Tip: Using Cloud Function to read csv.gz files from github directly to BigQuery in Google Cloud:', 'course': 'data-engineering-zoomcamp'}, {'text': 'You need to uncheck cache preferences in query settings', 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - When querying two different tables external and materialized you get the same result when count(distinct(*))', 'course': 'data-engineering-zoomcamp'}, {'text': 'Problem: When you inject data into GCS using Pandas, there is a chance that some dataset has missing values on  DOlocationID and PUlocationID. Pandas by default will cast these columns as float data type, causing inconsistent data type between parquet in GCS and schema defined in big query. You will see something like this:\\nSolution:\\nFix the data type issue in data pipeline\\nBefore injecting data into GCS, use astype and Int64 (which is different from int64 and accept both missing value and integer exist in the column) to cast the columns.\\nSomething like:\\ndf[\"PUlocationID\"] = df.PUlocationID.astype(\"Int64\")\\ndf[\"DOlocationID\"] = df.DOlocationID.astype(\"Int64\")\\nNOTE: It is best to define the data type of all the columns in the Transformation section of the ETL pipeline before loading to BigQuery', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'GCP BQ - How to handle type error from big query and parquet data?', 'course': 'data-engineering-zoomcamp'}, {'text': 'Problem occurs when misplacing content after fro``m clause in BigQuery SQLs.\\nCheck to remove any extra apaces or any other symbols, keep in lowercases, digits and dashes only', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'GCP BQ - Invalid project ID . Project IDs must contain 6-63 lowercase letters, digits, or dashes. Some project', 'course': 'data-engineering-zoomcamp'}, {'text': 'No. Based on the documentation for Bigquery, it does not support more than 1 column to be partitioned.\\n[source]', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'GCP BQ - Does BigQuery support multiple columns partition?', 'course': 'data-engineering-zoomcamp'}, {'text': 'Error Message:\\nPARTITION BY expression must be DATE(<timestamp_column>), DATE(<datetime_column>), DATETIME_TRUNC(<datetime_column>, DAY/HOUR/MONTH/YEAR), a DATE column, TIMESTAMP_TRUNC(<timestamp_column>, DAY/HOUR/MONTH/YEAR), DATE_TRUNC(<date_column>, MONTH/YEAR), or RANGE_BUCKET(<int64_column>, GENERATE_ARRAY(<int64_value>, <int64_value>[, <int64_value>]))\\nSolution:\\nConvert the column to datetime first.\\ndf[\"pickup_datetime\"] = pd.to_datetime(df[\"pickup_datetime\"])\\ndf[\"dropOff_datetime\"] = pd.to_datetime(df[\"dropOff_datetime\"])', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'GCP BQ - DATE() Error in BigQuery', 'course': 'data-engineering-zoomcamp'}, {'text': 'Native tables are tables where the data is stored in BigQuery.  External tables store the data outside BigQuery, with BigQuery storing metadata about that external table.\\nResources:\\nhttps://cloud.google.com/bigquery/docs/external-tables\\nhttps://cloud.google.com/bigquery/docs/tables-intro', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'GCP BQ - Native tables vs External tables in BigQuery?', 'course': 'data-engineering-zoomcamp'}, {'text': 'Issue: Tried running command to export ML model from BQ to GCS from Week 3\\nbq --project_id taxi-rides-ny extract -m nytaxi.tip_model gs://taxi_ml_model/tip_model\\nIt is failing on following error:\\nBigQuery error in extract operation: Error processing job Not found: Dataset was not found in location US\\nI verified the BQ data set and gcs bucket are in the same region- us-west1. Not sure how it gets location US. I couldn’t find the solution yet.\\nSolution:  Please enter correct project_id and gcs_bucket folder address. My gcs_bucket folder address is\\ngs://dtc_data_lake_optimum-airfoil-376815/tip_model', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'GCP BQ ML - Unable to run command (shown in video) to export ML model from BQ to GCS', 'course': 'data-engineering-zoomcamp'}, {'text': \"To solve this error mention the location = US when creating the dim_zones table\\n{{ config(\\nmaterialized='table',\\nlocation='US'\\n) }}\\nJust Update this part to solve the issue and run the dim_zones again and then run the fact_trips\", 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'Dim_zones.sql Dataset was not found in location US When Running fact_trips.sql', 'course': 'data-engineering-zoomcamp'}, {'text': 'Solution: proceed with setting up serving_dir on your computer as in the extract_model.md file. Then instead of\\ndocker pull tensorflow/serving\\nuse\\ndocker pull emacski/tensorflow-serving\\nThen\\ndocker run -p 8500:8500 -p 8501:8501 --mount type=bind,source=`pwd`/serving_dir/tip_model,target=/models/tip_model -e MODEL_NAME=tip_model -t emacski/tensorflow-serving\\nThen run the curl command as written, and you should get a prediction.', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'GCP BQ ML - Export ML model to make predictions does not work for MacBook with Apple M1 chip (arm architecture).', 'course': 'data-engineering-zoomcamp'}, {'text': 'Try deleting data you’ve saved to your VM locally during ETLs\\nKill processes related to deleted files\\nDownload ncdu and look for large files (pay particular attention to files related to Prefect)\\nIf you delete any files related to Prefect, eliminate caching from your flow code', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'VMs - What do I do if my VM runs out of space?', 'course': 'data-engineering-zoomcamp'}, {'text': \"Ans: What they mean is that they don't want you to do anything more than that. You should load the files into the bucket and create an external table based on those files (but nothing like cleaning the data and putting it in parquet format)\", 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': \"Homework - What does it mean “Stop with loading the files into a bucket.' Stop with loading the files into a bucket?”\", 'course': 'data-engineering-zoomcamp'}, {'text': 'If for whatever reason you try to read parquets directly from nyc.gov’s cloudfront into pandas, you might run into this error:\\npyarrow.lib.ArrowInvalid: Casting from timestamp[us] to timestamp[ns] would result in out of bounds\\nCause:\\nthere is one errant data record where the dropOff_datetime was set to year 3019 instead of 2019.\\npandas uses “timestamp[ns]” (as noted above), and int64 only allows a ~580 year range, centered on 2000. See `pd.Timestamp.max` and `pd.Timestamp.min`\\nThis becomes out of bounds when pandas tries to read it because 3019 > 2300 (approx value of pd.Timestamp.Max\\nFix:\\nUse pyarrow to read it:\\nimport pyarrow.parquet as pq df = pq.read_table(\\'fhv_tripdata_2019-02.parquet\\').to_pandas(safe=False)\\nHowever this results in weird timestamps for the offending record\\nRead the datetime columns separately using pq.read_table\\n\\ntable = pq.read_table(‘taxi.parquet’)\\ndatetimes = [‘list of datetime column names’]\\ndf_dts = pd.DataFrame()\\nfor col in datetimes:\\ndf_dts[col] = pd.to_datetime(table .column(col), errors=\\'coerce\\')\\n\\nThe `errors=’coerce’` parameter will convert the out of bounds timestamps into either the max or the min\\nUse parquet.compute.filter to remove the offending rows\\n\\nimport pyarrow.compute as pc\\ntable = pq.read_table(\"‘taxi.parquet\")\\ndf = table.filter(\\npc.less_equal(table[\"dropOff_datetime\"], pa.scalar(pd.Timestamp.max))\\n).to_pandas()', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'Homework - Reading parquets from nyc.gov directly into pandas returns Out of bounds error', 'course': 'data-engineering-zoomcamp'}, {'text': 'Answer: The 2022 NYC taxi data parquet files are available for each month separately. Therefore, you need to add all 12 files to your GCS bucket and then refer to them using the URIs option when creating an external table in BigQuery. You can use the wildcard \"*\" to refer to all 12 files using a single string.', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'Question: for homework 3 , we need all 12 parquet files for green taxi 2022 right ?', 'course': 'data-engineering-zoomcamp'}, {'text': 'This can help avoid schema issues in the homework. \\nDownload files locally and use the ‘upload files’ button in GCS at the desired path. You can upload many files at once. You can also choose to upload a folder.', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'Homework - Uploading files to GCS via GUI', 'course': 'data-engineering-zoomcamp'}, {'text': 'Ans: Take a careful look at the format of the dates in the question.', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'Homework - Qn 5: The partitioned/clustered table isn’t giving me the prediction I expected', 'course': 'data-engineering-zoomcamp'}, {'text': 'Many people aren’t getting an exact match, but are very close to one of the options. As per Alexey said to choose the closest option.', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'Homework - Qn 6: Did anyone get an exact match for one of the options given in Module 3 homework Q6?', 'course': 'data-engineering-zoomcamp'}, {'text': 'UnicodeDecodeError: \\'utf-8\\' codec can\\'t decode byte 0xa0 in position 41721: invalid start byte\\nSolution:\\nStep 1: When reading the data from the web into the pandas dataframe mention the encoding as follows:\\npd.read_csv(dataset_url, low_memory=False, encoding=\\'latin1\\')\\nStep 2: When writing the dataframe from the local system to GCS as a csv mention the encoding as follows:\\ndf.to_csv(path_on_gsc, compression=\"gzip\", encoding=\\'utf-8\\')\\nAlternative: use pd.read_parquet(url)', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'Python - invalid start byte Error Message', 'course': 'data-engineering-zoomcamp'}, {'text': 'A generator is a function in python that returns an iterator using the yield keyword.\\nA generator is a special type of iterable, similar to a list or a tuple, but with a crucial difference. Instead of creating and storing all the values in memory at once, a generator generates values on-the-fly as you iterate over it. This makes generators memory-efficient, particularly when dealing with large datasets.', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'Python - Generators in python', 'course': 'data-engineering-zoomcamp'}, {'text': 'The read_parquet function supports a list of files as an argument. The list of files will be merged into a single result table.', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'Python - Easiest way to read multiple files at the same time?', 'course': 'data-engineering-zoomcamp'}, {'text': \"Incorrect:\\ndf['DOlocationID'] = pd.to_numeric(df['DOlocationID'], downcast=integer) or\\ndf['DOlocationID'] = df['DOlocationID'].astype(int)\\nCorrect:\\ndf['DOlocationID'] = df['DOlocationID'].astype('Int64')\", 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': \"Python - These won't work. You need to make sure you use Int64:\", 'course': 'data-engineering-zoomcamp'}, {'text': \"ValueError: Path /Users/kt/.prefect/storage/44ccce0813ed4f24ab2d3783de7a9c3a does not exist.\\nRemove ```cache_key_fn=task_input_hash ``` as it’s in argument in your function & run your flow again.\\nNote: catche key is beneficial if you happen to run the code multiple times, it won't repeat the process which you have finished running in the previous run.  That means, if you have this ```cache_key``` in your initial run, this might cause the error.\", 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'Prefect - Error on Running Prefect Flow to Load data to GCS', 'course': 'data-engineering-zoomcamp'}, {'text': '@task\\ndef download_file(url: str, file_path: str):\\nresponse = requests.get(url)\\nopen(file_path, \"wb\").write(response.content)\\nreturn file_path\\n@flow\\ndef extract_from_web() -> None:\\nfile_path = download_file(url=f\\'{url-filename}.csv.gz\\',file_path=f\\'{filename}.csv.gz\\')', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Prefect - Tip: Downloading csv.gz from a url in a prefect environment (sample snippet).', 'course': 'data-engineering-zoomcamp'}, {'text': 'Update the seed column types in the dbt_project.yaml file\\nfor using double : float\\nfor using int : numeric\\nDBT Cloud production error: prod dataset not available in location EU\\nProblem: I am trying to deploy my DBT  models to production, using DBT Cloud. The data should live in BigQuery.  The dataset location is EU.  However, when I am running the model in production, a prod dataset is being create in BigQuery with a location US and the dbt invoke build is failing giving me \"ERROR 404: porject.dataset:prod not available in location EU\". I tried different ways to fix this. I am not sure if there is a more simple solution then creating my project or buckets in location US. Hope anyone can help here.\\nNote: Everything is working fine in development mode, the issue is just happening when scheduling and running job in production\\nSolution: I created the prod dataset manually in BQ and specified EU, then I ran the job.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'If you are getting not found in location us error.', 'course': 'data-engineering-zoomcamp'}, {'text': 'Error: This project does not have a development environment configured. Please create a development environment and configure your development credentials to use the dbt IDE.\\nThe error itself tells us how to solve this issue, the guide is here. And from videos @1:42 and also slack chat', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Setup - No development environment', 'course': 'data-engineering-zoomcamp'}, {'text': \"Runtime Error\\ndbt was unable to connect to the specified database.\\nThe database returned the following error:\\n>Database Error\\nAccess Denied: Project <project_name>: User does not have bigquery.jobs.create permission in project <project_name>.\\nCheck your database credentials and try again. For more information, visit:\\nhttps://docs.getdbt.com/docs/configure-your-profile\\nSteps to resolve error in Google Cloud:\\n1. Navigate to IAM & Admin and select IAM\\n2. Click Grant Access if your newly created dbt service account isn't listed\\n3. In New principals field, add your service account\\n4. Select a Role and search for BigQuery Job User to add\\n5. Go back to dbt cloud project setup and Test your connection\\n6. Note: Also add BigQuery Data Owner, Storage Object Admin, & Storage Admin to prevent permission issues later in the course\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'Setup - Connecting dbt Cloud with BigQuery Error', 'course': 'data-engineering-zoomcamp'}, {'text': 'error: This dbt Cloud run was cancelled because a valid dbt project was not found. Please check that the repository contains a proper dbt_project.yml config file. If your dbt project is located in a subdirectory of the connected repository, be sure to specify its location on the Project settings page in dbt Cloud', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Dbt build error', 'course': 'data-engineering-zoomcamp'}, {'text': \"Error: Failed to clone repository.\\ngit clone git@github.com:DataTalksClub/data-engineering-zoomcamp.git /usr/src/develop/…\\nCloning into '/usr/src/develop/...\\nWarning: Permanently added 'github.com,140.82.114.4' (ECDSA) to the list of known hosts.\\ngit@github.com: Permission denied (publickey).\\nfatal: Could not read from remote repository.\\nIssue: You don’t have permissions to write to DataTalksClub/data-engineering-zoomcamp.git\\nSolution 1: Clone the repository and use this forked repo, which contains your github username. Then, proceed to specify the path, as in:\\n[your github username]/data-engineering-zoomcamp.git\\nSolution 2: create a fresh repo for dbt-lessons. We’d need to do branching and PRs in this lesson, so it might be a good idea to also not mess up your whole other repo. Then you don’t have to create a subfolder for the dbt project files\\nSolution 3: Use https link\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'Setup - Failed to clone repository.', 'course': 'data-engineering-zoomcamp'}, {'text': \"Solution:\\nCheck if you’re on the Developer Plan. As per the prerequisites, you'll need to be enrolled in the Team Plan or Enterprise Plan to set up a CI Job in dbt Cloud.\\nSo If you're on the Developer Plan, you'll need to upgrade to utilise CI Jobs.\\nNote from another user: I’m in the Team Plan (trial period) but the option is still disabled. What worked for me instead was this. It works for the Developer (free) plan.\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'dbt job - Triggered by pull requests is disabled when I try to create a new Continuous Integration job in dbt cloud.', 'course': 'data-engineering-zoomcamp'}, {'text': 'Issue: If the DBT cloud IDE loading indefinitely then giving you this error\\nSolution: check the dbt_cloud_setup.md  file and make a SSH Key and use gitclone to import repo into dbt project, copy and paste deploy key back in your repo setting.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Setup - Your IDE session was unable to start. Please contact support.', 'course': 'data-engineering-zoomcamp'}, {'text': 'Issue: If you don’t define the column format while converting from csv to parquet Python will “choose” based on the first rows.\\n✅Solution: Defined the schema while running web_to_gcp.py pipeline.\\nSebastian adapted the script:\\nhttps://github.com/sebastian2296/data-engineering-zoomcamp/blob/main/week_4_analytics_engineering/web_to_gcs.py\\nNeed a quick change to make the file work with gz files, added the following lines (and don’t forget to delete the file at the end of each iteration of the loop to avoid any problem of disk space)\\nfile_name_gz = f\"{service}_tripdata_{year}-{month}.csv.gz\"\\nopen(file_name_gz, \\'wb\\').write(r.content)\\nos.system(f\"gzip -d {file_name_gz}\")\\nos.system(f\"rm {file_name_init}.*\")\\nSame ERROR - When running dbt run for fact_trips.sql, the task failed with error:\\n“Parquet column \\'ehail_fee\\' has type DOUBLE which does not match the target cpp_type INT64”\\n开启屏幕阅读器支持\\n要启用屏幕阅读器支持，请按Ctrl+Alt+Z。要了解键盘快捷键，请按Ctrl+斜杠。\\n查找和替换\\nReason: Parquet files have their own schema. Some parquet files for green data have records with decimals in ehail_fee column.\\nThere are some possible fixes:\\nDrop ehail_feel column since it is not really used. For instance when creating a partitioned table from the external table in BigQuery\\nSELECT * EXCEPT (ehail_fee) FROM…\\nModify stg_green_tripdata.sql model using this line cast(0 as numeric) as ehail_fee.\\nModify Airflow dag to make the conversion and avoid the error.\\npv.read_csv(src_file, convert_options=pv.ConvertOptions(column_types = {\\'ehail_fee\\': \\'float64\\'}))\\nSame type of ERROR - parquet files with different data types - Fix it with pandas\\nHere is another possibility that could be interesting:\\nYou can specify the dtypes when importing the file from csv to a dataframe with pandas\\npd.from_csv(..., dtype=type_dict)\\nOne obstacle is that the regular int64 pandas use (I think this is from the numpy library) does not accept null values (NaN, not a number). But you can use the pandas Int64 instead, notice capital ‘I’. The type_dict is a python dictionary mapping the column names to the dtypes.\\nSources:\\nhttps://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\\nNullable integer data type — pandas 1.5.3 documentation', 'section': 'Module 4: analytics engineering with dbt', 'question': 'DBT - I am having problems with columns datatype while running DBT/BigQuery', 'course': 'data-engineering-zoomcamp'}, {'text': 'If the provided URL isn’t working for you (https://nyc-tlc.s3.amazonaws.com/trip+data/):\\nWe can use the GitHub CLI to easily download the needed trip data from https://github.com/DataTalksClub/nyc-tlc-data, and manually upload to a GCS bucket.\\nInstructions on how to download the CLI here: https://github.com/cli/cli\\nCommands to use:\\ngh auth login\\ngh release list -R DataTalksClub/nyc-tlc-data\\ngh release download yellow -R DataTalksClub/nyc-tlc-data\\ngh release download green -R DataTalksClub/nyc-tlc-data\\netc.\\nNow you can upload the files to a GCS bucket using the GUI.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Ingestion: When attempting to use the provided quick script to load trip data into GCS, you receive error Access Denied from the S3 bucket', 'course': 'data-engineering-zoomcamp'}, {'text': \"R: This conversion is needed for the question 3 of homework, in order to process files for fhv data. The error is:\\npyarrow.lib.ArrowInvalid: CSV parse error: Expected 7 columns, got 1: B02765\\nCause: Some random line breaks in this particular file.\\nFixed by opening a bash in the container executing the dag and manually running the following command that deletes all \\\\n not preceded by \\\\r.\\nperl -i -pe 's/(?<!\\\\r)\\\\n/\\\\1/g' fhv_tripdata_2020-01.csv\\nAfter that, clear the failed task in Airflow to force re-execution.\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'Ingestion - Error thrown by format_to_parquet_task when converting fhv_tripdata_2020-01.csv using Airflow', 'course': 'data-engineering-zoomcamp'}, {'text': 'I initially followed data-engineering-zoomcamp/03-data-warehouse/extras/web_to_gcs.py at main · DataTalksClub/data-engineering-bootcamp (github.com)\\nBut it was taking forever for the yellow trip data and when I tried to download and upload the parquet files directly to GCS, that works fine but when creating the Bigquery table, there was a schema inconsistency issue\\nThen I found another hack shared in the slack which was suggested by Victoria.\\n[Optional] Hack for loading data to BigQuery for Week 4 - YouTube\\nPlease watch until the end as there is few schema changes required to be done', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Hack to load yellow and green trip data for 2019 and 2020', 'course': 'data-engineering-zoomcamp'}, {'text': '“gs\\\\storage_link\\\\*.parquet” need to be added in destination folder', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Move many files (more than one) from Google cloud storage bucket to Big query', 'course': 'data-engineering-zoomcamp'}, {'text': 'One common cause experienced is lack of space after running prefect several times. When running prefect, check the folder ‘.prefect/storage’ and delete the logs now and then to avoid the problem.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'GCP VM - All of sudden ssh stopped working for my VM after my last restart', 'course': 'data-engineering-zoomcamp'}, {'text': 'You can try to do this steps:', 'section': 'Module 4: analytics engineering with dbt', 'question': 'GCP VM - If you have lost SSH access to your machine due to lack of space. Permission denied (publickey)', 'course': 'data-engineering-zoomcamp'}, {'text': 'R: Go to BigQuery, and check the location of BOTH\\nThe source dataset (trips_data_all), and\\nThe schema you’re trying to write to (name should be \\tdbt_<first initial><last name> (if you didn’t change the default settings at the end when setting up your project))\\nLikely, your source data will be in your region, but the write location will be a multi-regional location (US in this example). Delete these datasets, and recreate them with your specified region and the correct naming format.\\nAlternatively, instead of removing datasets, you can specify the single-region location you are using. E.g. instead of ‘location: US’, specify the region, so ‘location: US-east1’. See this Github comment for more detail. Additionally please see this post of Sandy\\nIn DBT cloud you can actually specify the location using the following steps:\\nGPo to your profile page (top right drop-down --> profile)\\nThen go to under Credentials --> Analytics (you may have customised this name)\\nClick on Bigquery >\\nHit Edit\\nUpdate your location, you may need to re-upload your service account JSON to re-fetch your private key, and save. (NOTE: be sure to exactly copy the region BigQuery specifies your dataset is in.)', 'section': 'Module 4: analytics engineering with dbt', 'question': '404 Not found: Dataset eighth-zenith-372015:trip_data_all was not found in location us-west1', 'course': 'data-engineering-zoomcamp'}, {'text': 'Error: `dbt_utils.surrogate_key` has been replaced by `dbt_utils.generate_surrogate_key`\\nFix:\\nReplace dbt_utils.surrogate_key  with dbt_utils.generate_surrogate_key in stg_green_tripdata.sql\\nWhen executing dbt run after fact_trips.sql has been created, the task failed with error:\\nR: “Access Denied: BigQuery BigQuery: Permission denied while globbing file pattern.”\\n1. Fixed by adding the Storage Object Viewer role to the service account in use in BigQuery.\\n2. Add the related roles to the service account in use in GCS.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'When executing dbt run after installing dbt-utils latest version i.e., 1.0.0 warning has generated', 'course': 'data-engineering-zoomcamp'}, {'text': 'You need to create packages.yml file in main project directory and add packages’ meta data:\\npackages:\\n- package: dbt-labs/dbt_utils\\nversion: 0.8.0\\nAfter creating file run:\\nAnd hit enter.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'When You are getting error dbt_utils not found', 'course': 'data-engineering-zoomcamp'}, {'text': \"Ensure you properly format your yml file. Check the build logs if the run was completed successfully. You can expand the command history console (where you type the --vars '{'is_test_run': 'false'}')  and click on any stage’s logs to expand and read errors messages or warnings.\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'Lineage is currently unavailable. Check that your project does not contain compilation errors or contact support if this error persists.', 'course': 'data-engineering-zoomcamp'}, {'text': \"Make sure you use:\\ndbt run --var ‘is_test_run: false’ or\\ndbt build --var ‘is_test_run: false’\\n(watch out for formatted text from this document: re-type the single quotes). If that does not work, use --vars '{'is_test_run': 'false'}' with each phrase separately quoted.\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'Build - Why do my Fact_trips only contain a few days of data?', 'course': 'data-engineering-zoomcamp'}, {'text': 'Check if you specified if_exists argument correctly when writing data from GCS to BigQuery. When I wrote my automated flow for each month of the years 2019 and 2020 for green and yellow data I had specified if_exists=\"replace\" while I was experimenting with the flow setup. Once you want to run the flow for all months in 2019 and 2020 make sure to set if_exists=\"append\"\\nif_exists=\"replace\" will replace the whole table with only the month data that you are writing into BigQuery in that one iteration -> you end up with only one month in BigQuery (the last one you inserted)\\nif_exists=\"append\" will append the new monthly data -> you end up with data from all months', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Build - Why do my fact_trips only contain one month of data?', 'course': 'data-engineering-zoomcamp'}, {'text': \"R: After the second SELECT, change this line:\\ndate_trunc('month', pickup_datetime) as revenue_month,\\nTo this line:\\ndate_trunc(pickup_datetime, month) as revenue_month,\\nMake sure that “month” isn’t surrounded by quotes!\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'BigQuery returns an error when I try to run the dm_monthly_zone_revenue.sql model.', 'course': 'data-engineering-zoomcamp'}, {'text': 'For this instead:\\n{{ dbt_utils.generate_surrogate_key([ \\n     field_a, \\n     field_b, \\n     field_c,\\n     …,\\n     field_z\\n]) }}', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Replace: \\n{{ dbt_utils.surrogate_key([ \\n     field_a, \\n     field_b, \\n     field_c,\\n     …,\\n     field_z     \\n]) }}', 'course': 'data-engineering-zoomcamp'}, {'text': 'Remove the dataset from BigQuery which was created by dbt and run dbt run again so that it will recreate the dataset in BigQuery with the correct location', 'section': 'Module 4: analytics engineering with dbt', 'question': 'I changed location in dbt, but dbt run still gives me an error', 'course': 'data-engineering-zoomcamp'}, {'text': 'Remove the dataset from BigQuery created by dbt and run again (with test disabled) to ensure the dataset created has all the rows.\\nDBT - Why am I getting a new dataset after running my CI/CD Job? / What is this new dbt dataset in BigQuery?\\nAnswer: when you create the CI/CD job, under ‘Compare Changes against an environment (Deferral) make sure that you select ‘ No; do not defer to another environment’ - otherwise dbt won’t merge your dev models into production models; it will create a new environment called ‘dbt_cloud_pr_number of pull request’', 'section': 'Module 4: analytics engineering with dbt', 'question': 'I ran dbt run without specifying variable which gave me a table of 100 rows. I ran again with the variable value specified but my table still has 100 rows in BQ.', 'course': 'data-engineering-zoomcamp'}, {'text': \"Vic created three different datasets in the videos.. dbt_<name> was used for development and you used a production dataset for the production environment. What was the use for the staging dataset?\\nR: Staging, as the name suggests, is like an intermediate between the raw datasets and the fact and dim tables, which are the finished product, so to speak. You'll notice that the datasets in staging are materialised as views and not tables.\\nVic didn't use it for the project, you just need to create production and dbt_name + trips_data_all that you had already.\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'Why do we need the Staging dataset?', 'course': 'data-engineering-zoomcamp'}, {'text': 'Try removing the “network: host” line in docker-compose.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'DBT Docs Served but Not Accessible via Browser', 'course': 'data-engineering-zoomcamp'}, {'text': 'Go to Account settings >> Project >> Analytics >> Click on your connection >> go all the way down to Location and type in the GCP location just as displayed in GCP (e.g. europe-west6). You might need to reupload your GCP key.\\nDelete your dataset in GBQ\\nRebuild project: dbt build\\nNewly built dataset should be in the correct location', 'section': 'Module 4: analytics engineering with dbt', 'question': 'BigQuery adapter: 404 Not found: Dataset was not found in location europe-west6', 'course': 'data-engineering-zoomcamp'}, {'text': 'Create a new branch to edit. More on this can be found here in the dbt docs.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Dbt+git - Main branch is “read-only”', 'course': 'data-engineering-zoomcamp'}, {'text': 'Create a new branch for development, then you can merge it to the main branch\\nCreate a new branch and switch to this branch. It allows you to make changes. Then you can commit and push the changes to the “main” branch.', 'section': 'Module 4: analytics engineering with dbt', 'question': \"Dbt+git - It appears that I can't edit the files because I'm in read-only mode. Does anyone know how I can change that?\", 'course': 'data-engineering-zoomcamp'}, {'text': \"Error:\\nTriggered by pull requests\\nThis feature is only available for dbt repositories connected through dbt Cloud's native integration with Github, Gitlab, or Azure DevOps\\nSolution: Contrary to the guide on DTC repo, don’t use the Git Clone option. Use the Github one instead. Step-by-step guide to UN-LINK Git Clone and RE-LINK with Github in the next entry below\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'Dbt deploy + Git CI - cannot create CI checks job for deployment to Production. See more discussion in slack chat', 'course': 'data-engineering-zoomcamp'}, {'text': 'If you’re trying to configure CI with Github and on the job’s options you can’t see Run on Pull Requests? on triggers, you have to reconnect with Github using native connection instead clone by SSH. Follow these steps:\\nOn Profile Settings > Linked Accounts connect your Github account with dbt project allowing the permissions asked. More info at https://docs.getdbt.com/docs/collaborate/git/connect-gith\\nDisconnect your current Github’s configuration from Account Settings > Projects (analytics) > Github connection. At the bottom left appears the button Disconnect, press it.\\nOnce we have confirmed the change, we can configure it again. This time, choose Github and it will appear in all repositories which you have allowed to work with dbt. Select your repository and it’s ready.\\nGo to the Deploy > job configuration’s page and go down until Triggers and now you can see the option Run on Pull Requests:', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Dbt deploy + Git CI - Unable to configure Continuous Integration (CI) with Github', 'course': 'data-engineering-zoomcamp'}, {'text': \"If you're following video DE Zoomcamp 4.3.1 - Building the First DBT Models, you may have encountered an issue at 14:25 where the Lineage graph isn't displayed and a Compilation Error occurs, as shown in the attached image. Don't worry - a quick fix for this is to simply save your schema.yml file. Once you've done this, you should be able to view your Lineage graph without any further issues.\", 'section': 'Module 4: analytics engineering with dbt', 'question': \"Compilation Error (Model 'model.my_new_project.stg_green_tripdata' (models/staging/stg_green_tripdata.sql) depends on a source named 'staging.green_trip_external' which was not found)\", 'course': 'data-engineering-zoomcamp'}, {'text': '> in macro test_accepted_values (tests/generic/builtin.sql)\\n> called by test accepted_values_stg_green_tripdata_Payment_type__False___var_payment_type_values_ (models/staging/schema.yml)\\nRemember that you have to add to dbt_project.yml the vars:\\nvars:\\npayment_type_values: [1, 2, 3, 4, 5, 6]', 'section': 'Module 4: analytics engineering with dbt', 'question': \"'NoneType' object is not iterable\", 'course': 'data-engineering-zoomcamp'}, {'text': \"You will face this issue if you copied and pasted the exact macro directly from data-engineering-zoomcamp repo.\\nBigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for operator CASE for argument types: STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, NULL at [35:5]; reason: invalidQuery, location: query, message: No matching signature for operator CASE for argument types: STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, NULL at [35:5]')\\nWhat you’d have to do is to change the data type of the numbers (1, 2, 3 etc.) to text by inserting ‘’, as the initial ‘payment_type’ data type should be string (Note: I extracted and loaded the green trips data using Google BQ Marketplace)\\n{#\\nThis macro returns the description of the payment_type\\n#}\\n{% macro get_payment_type_description(payment_type) -%}\\ncase {{ payment_type }}\\nwhen '1' then 'Credit card'\\nwhen '2' then 'Cash'\\nwhen '3' then 'No charge'\\nwhen '4' then 'Dispute'\\nwhen '5' then 'Unknown'\\nwhen '6' then 'Voided trip'\\nend\\n{%- endmacro %}\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'dbt macro errors with get_payment_type_description(payment_type)', 'course': 'data-engineering-zoomcamp'}, {'text': 'The dbt error  log contains a link to BigQuery. When you follow it you will see your query and the problematic line will be highlighted.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Troubleshooting in dbt:', 'course': 'data-engineering-zoomcamp'}, {'text': 'It is a default behaviour of dbt to append custom schema to initial schema. To override this behaviour simply create a macro named “generate_schema_name.sql”:\\n{% macro generate_schema_name(custom_schema_name, node) -%}\\n{%- set default_schema = target.schema -%}\\n{%- if custom_schema_name is none -%}\\n{{ default_schema }}\\n{%- else -%}\\n{{ custom_schema_name | trim }}\\n{%- endif -%}\\n{%- endmacro %}\\nNow you can override default custom schema in “dbt_project.yml”:', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Why changing the target schema to “marts” actually creates a schema named “dbt_marts” instead?', 'course': 'data-engineering-zoomcamp'}, {'text': 'There is a project setting which allows you to set `Project subdirectory` in dbt cloud:', 'section': 'Module 4: analytics engineering with dbt', 'question': 'How to set subdirectory of the github repository as the dbt project root', 'course': 'data-engineering-zoomcamp'}, {'text': \"Remember that you should modify accordingly your .sql models, to read from existing table names in BigQuery/postgres db\\nExample: select * from {{ source('staging',<your table name in the database>') }}\", 'section': 'Module 4: analytics engineering with dbt', 'question': \"Compilation Error : Model 'model.XXX' (models/<model_path>/XXX.sql) depends on a source named '<a table name>' which was not found\", 'course': 'data-engineering-zoomcamp'}, {'text': 'Make sure that you create a pull request from your Development branch to the Production branch (main by default). After that, check in your ‘seeds’ folder if the seed file is inside it.\\nAnother thing to check is your .gitignore file. Make sure that the .csv extension is not included.', 'section': 'Module 4: analytics engineering with dbt', 'question': \"Compilation Error : Model '<model_name>' (<model_path>) depends on a node named '<seed_name>' which was not found   (Production Environment)\", 'course': 'data-engineering-zoomcamp'}, {'text': '1. Go to your dbt cloud service account\\n1. Adding the  [Storage Object Admin,Storage Admin] role in addition tco BigQuery Admin.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'When executing dbt run after using fhv_tripdata as an external table: you get “Access Denied: BigQuery BigQuery: Permission denied”', 'course': 'data-engineering-zoomcamp'}, {'text': 'Problem: when injecting data to bigquery, you may face the type error. This is because pandas by default will parse integer columns with missing value as float type.\\nSolution:\\nOne way to solve this problem is to specify/ cast data type Int64 during the data transformation stage.\\nHowever, you may be lazy to type all the int columns. If that is the case, you can simply use convert_dtypes to infer the data type\\n# Make pandas to infer correct data type (as pandas parse int with missing as float)\\ndf.fillna(-999999, inplace=True)\\ndf = df.convert_dtypes()\\ndf = df.replace(-999999, None)', 'section': 'Module 4: analytics engineering with dbt', 'question': 'How to automatically infer the column data type (pandas missing value issues)?', 'course': 'data-engineering-zoomcamp'}, {'text': 'Seed files loaded from directory with name ‘seed’, that’s why you should rename dir with name ‘data’ to ‘seed’', 'section': 'Module 4: analytics engineering with dbt', 'question': 'When loading github repo raise exception that ‘taxi_zone_lookup’ not found', 'course': 'data-engineering-zoomcamp'}, {'text': 'Check the .gitignore file and make sure you don’t have *.csv in it\\n\\nDbt error 404 was not found in location\\nMy specific error:\\nRuntime Error in rpc request (from remote system.sql) 404 Not found: Table dtc-de-0315:trips_data_all.green_tripdata_partitioned was not found in location europe-west6 Location: europe-west6 Job ID: 168ee9bd-07cd-4ca4-9ee0-4f6b0f33897c\\nMake sure all of your datasets have the correct region and not a generalised region:\\nEurope-west6 as opposed to EU\\n\\nMatch this in dbt settings:\\ndbt -> projects -> optional settings -> manually set location to match', 'section': 'Module 4: analytics engineering with dbt', 'question': '‘taxi_zone_lookup’ not found', 'course': 'data-engineering-zoomcamp'}, {'text': \"The easiest way to avoid these errors is by ingesting the relevant data in a .csv.gz file type. Then, do:\\nCREATE OR REPLACE EXTERNAL TABLE `dtc-de.trips_data_all.fhv_tripdata`\\nOPTIONS (\\nformat = 'CSV',\\nuris = ['gs://dtc_data_lake_dtc-de-updated/data/fhv_all/fhv_tripdata_2019-*.csv.gz']\\n);\\nAs an example. You should no longer have any data type issues for week 4.\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'Data type errors when ingesting with parquet files', 'course': 'data-engineering-zoomcamp'}, {'text': 'This is due to the way the deduplication is done in the two staging files.\\nSolution: add order by in the partition by part of both staging files. Keep adding columns to order by until the number of rows in the fact_trips table is consistent when re-running the fact_trips model.\\nExplanation (a bit convoluted, feel free to clarify, correct etc.)\\nWe partition by vendor id and pickup_datetime and choose the first row (rn=1) from all these partitions. These partitions are not ordered, so every time we run this, the first row might be a different one. Since the first row is different between runs, it might or might not contain an unknown borough. Then, in the fact_trips model we will discard a different number of rows when we discard all values with an unknown borough.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Inconsistent number of rows when re-running fact_trips model', 'course': 'data-engineering-zoomcamp'}, {'text': 'If you encounter data type error on trip_type column, it may due to some nan values that isn’t null in bigquery.\\nSolution: try casting it to FLOAT datatype instead of NUMERIC', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Data Type Error when running fact table', 'course': 'data-engineering-zoomcamp'}, {'text': \"This error could result if you are using some select * query without mentioning the name of table for ex:\\nwith dim_zones as (\\nselect * from `engaged-cosine-374921`.`dbt_victoria_mola`.`dim_zones`\\nwhere borough != 'Unknown'\\n),\\nfhv as (\\nselect * from `engaged-cosine-374921`.`dbt_victoria_mola`.`stg_fhv_tripdata`\\n)\\nselect * from fhv\\ninner join dim_zones as pickup_zone\\non fhv.PUlocationID = pickup_zone.locationid\\ninner join dim_zones as dropoff_zone\\non fhv.DOlocationID = dropoff_zone.locationid\\n);\\nTo resolve just replace use : select fhv.* from fhv\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'CREATE TABLE has columns with duplicate name locationid.', 'course': 'data-engineering-zoomcamp'}, {'text': 'Some ehail fees are null and casting them to integer gives Bad int64 value: 0.0 error,\\nSolution:\\nUsing safe_cast returns NULL instead of throwing an error. So use safe_cast from dbt_utils function in the jinja code for casting into integer as follows:\\n{{ dbt_utils.safe_cast(\\'ehail_fee\\',  api.Column.translate_type(\"integer\"))}} as ehail_fee,\\nCan also just use safe_cast(ehail_fee as integer) without relying on dbt_utils.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Bad int64 value: 0.0 error', 'course': 'data-engineering-zoomcamp'}, {'text': \"You might encounter this when building the fact_trips.sql model. The issue may be with the payment_type_description field.\\nUsing safe_cast as above, would cause the entire field to become null. A better approach is to drop the offending decimal place, then cast to integer.\\ncast(replace({{ payment_type }},'.0','') as integer)\\nBad int64 value: 1.0 error (again)\\n\\nI found that there are more columns causing the bad INT64: ratecodeid and trip_type on Green_tripdata table.\\nYou can use the queries below to address them:\\nCAST(\\nREGEXP_REPLACE(CAST(rate_code AS STRING), r'\\\\.0', '') AS INT64\\n) AS ratecodeid,\\nCAST(\\nCASE\\nWHEN REGEXP_CONTAINS(CAST(trip_type AS STRING), r'\\\\.\\\\d+') THEN NULL\\nELSE CAST(trip_type AS INT64)\\nEND AS INT64\\n) AS trip_type,\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'Bad int64 value: 2.0/1.0 error', 'course': 'data-engineering-zoomcamp'}, {'text': 'The two solution above don’t work for me - I used the line below in `stg_green_trips.sql` to replace the original ehail_fee line:\\n`{{ dbt.safe_cast(\\'ehail_fee\\',  api.Column.translate_type(\"numeric\"))}} as ehail_fee,`', 'section': 'Module 4: analytics engineering with dbt', 'question': 'DBT - Error on building fact_trips.sql: Parquet column \\'ehail_fee\\' has type DOUBLE which does not match the target cpp_type INT64. File: gs://<gcs bucket>/<table>/green_taxi_2019-01.parquet\")', 'course': 'data-engineering-zoomcamp'}, {'text': \"Remember to add a space between the variable and the value. Otherwise, it won't be interpreted as a dictionary.\\nIt should be:\\ndbt run --var 'is_test_run: false'\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'The - vars argument must be a YAML dictionary, but was of type str', 'course': 'data-engineering-zoomcamp'}, {'text': \"You don't need to change the environment type. If you are following the videos, you are creating a Production Deployment, so the only available option is the correct one.'\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'Not able to change Environment Type as it is greyed out and inaccessible', 'course': 'data-engineering-zoomcamp'}, {'text': 'Database Error in model stg_yellow_tripdata (models/staging/stg_yellow_tripdata.sql)\\nAccess Denied: Table taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata: User does not have permission to query table taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata, or perhaps it does not exist in location US.\\ncompiled Code at target/run/taxi_rides_ny/models/staging/stg_yellow_tripdata.sql\\nIn my case, I was set up in a different branch, so always check the branch you are working on. Change the 04-analytics-engineering/taxi_rides_ny/models/staging/schema.yml file in the\\nsources:\\n- name: staging\\ndatabase: your_database_name\\nIf this error will continue when running dbt job, As for changing the branch for your job, you can use the ‘Custom Branch’ settings in your dbt Cloud environment. This allows you to run your job on a different branch than the default one (usually main). To do this, you need to:\\nGo to an environment and select Settings to edit it\\nSelect Only run on a custom branch in General settings\\nEnter the name of your custom branch (e.g. HW)\\nClick Save\\nCould not parse the dbt project. please check that the repository contains a valid dbt project\\nRunning the Environment on the master branch causes this error, you must activate “Only run on a custom branch” checkbox and specify the branch you are  working when Environment is setup.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Access Denied: Table taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata: User does not have permission to query table taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata, or perhaps it does not exist in location US.', 'course': 'data-engineering-zoomcamp'}, {'text': 'Change to main branch, make a pull request from the development branch.\\nNote: this will take you to github.\\nApprove the merging and rerun you job, it would work as planned now', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Made change to your modelling files and commit the your development branch, but Job still runs on old file?', 'course': 'data-engineering-zoomcamp'}, {'text': 'Before you can develop some data model on dbt, you should create development environment and set some parameter on it. After the model being developed, we should also create deployment environment to create and run some jobs.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Setup - I’ve set Github and Bigquery to dbt successfully. Why nothing showed in my Develop tab?', 'course': 'data-engineering-zoomcamp'}, {'text': 'Error Message:\\nInvestigate Sentry error: ProtocolError \"Invalid input ConnectionInputs.SEND_HEADERS in state ConnectionState.CLOSED\"\\nSolution:\\nreference\\nRun it again because it happens sometimes. Or wait a few minutes, it will continue.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Prefect Agent retrieving runs from queue sometimes fails with httpx.LocalProtocolError', 'course': 'data-engineering-zoomcamp'}, {'text': \"My taxi data was loaded into gcs with etl_web_to_gcs.py script that converts csv data into parquet. Then I placed raw data trips into external tables and when I executed dbt run I got an error message: Parquet column 'passenger_count' has type INT64 which does not match the target cpp_type DOUBLE. It is because several columns in files have different formats of data.\\nWhen I added df[col] = df[col].astype('Int64') transformation to the columns: passenger_count, payment_type, RatecodeID, VendorID, trip_type it went ok. Several people also faced this error and more about it you can read on the slack channel.\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'BigQuery returns an error when i try to run ‘dbt run’:', 'course': 'data-engineering-zoomcamp'}, {'text': 'Use the syntax below instead if the code in the tutorial is not working.\\ndbt run --select stg_green_tripdata --vars \\'{\"is_test_run\": false}\\'', 'section': 'Module 4: analytics engineering with dbt', 'question': \"Running dbt run --models stg_green_tripdata --var 'is_test_run: false' is not returning anything:\", 'course': 'data-engineering-zoomcamp'}, {'text': \"Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\\nSolution:\\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\", 'section': 'Module 4: analytics engineering with dbt', 'question': \"DBT - Error: No module named 'pytz' while setting up dbt with docker\", 'course': 'data-engineering-zoomcamp'}, {'text': \"If you have problems editing dbt_project.yml when using Docker after ‘docker-compose run dbt-bq-dtc init’, to change profile ‘taxi_rides_ny’ to 'bq-dbt-workshop’, just run:\\nsudo chown -R username path\\nDBT - Internal Error: Profile should not be None if loading is completed\\nWhen  running dbt debug, change the directory to the newly created subdirectory (e.g: the newly created `taxi_rides_ny` directory, which contains the dbt project).\", 'section': 'Module 4: analytics engineering with dbt', 'question': '\\u200b\\u200bVS Code: NoPermissions (FileSystemError): Error: EACCES: permission denied (linux)', 'course': 'data-engineering-zoomcamp'}, {'text': 'When running a query on BigQuery sometimes could appear a this table is not on the specified location error.\\nFor this problem there is not a straightforward solution, you need to dig a little, but the problem could be one of these:\\nCheck the locations of your bucket, datasets and tables. Make sure they are all on the same one.\\nChange the query settings to the location you are in: on the query window select more -> query settings -> select the location\\nCheck if all the paths you are using in your query to your tables are correct: you can click on the table -> details -> and copy the path.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Google Cloud BigQuery Location Problems', 'course': 'data-engineering-zoomcamp'}, {'text': 'This happens because we have moved the dbt project to another directory on our repo.\\nOr might be that you’re on a different branch than is expected to be merged from / to.\\nSolution:\\nGo to the projects window on dbt cloud -> settings -> edit -> and add directory (the extra path to the dbt project)\\nFor example:\\n/week5/taxi_rides_ny\\nMake sure your file explorer path and this Project settings path matches and there’s no files waiting to be committed to github if you’re running the job to deploy to PROD.\\nAnd that you had setup the PROD environment to check in the main branch, or whichever you specified.\\nIn the picture below, I had set it to ella2024 to be checked as “production-ready” by the “freshness” check mark at the PROD environment settings. So each time I merge a branch from something else into ella2024 and then trigger the PR, the CI check job would kick-in. But we still do need to Merge and close the PR manually, I believe, that part is not automated.\\nYou set up the PROD custom branch (if not default main) in the Environment setup screen.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'DBT Deploy - This dbt Cloud run was cancelled because a valid dbt project was not found.', 'course': 'data-engineering-zoomcamp'}, {'text': 'When you are creating the pull request and running the CI, dbt is creating a new schema on BIgQuery. By default that new schema will be created on ‘US’ location, if you have your dataset, schemas and tables on ‘EU’ that will generate an error and the pull request will not be accepted. To change that location to ‘EU’ on the connection to BigQuery from dbt we need to add the location ‘EU’ on the connection optional settings:\\nDbt -> project -> settings -> connection BIgQuery -> OPtional Settings -> Location -> EU', 'section': 'Module 4: analytics engineering with dbt', 'question': 'DBT Deploy + CI - Location Problems on BigQuery', 'course': 'data-engineering-zoomcamp'}, {'text': 'When running trying to run the dbt project on prod there is some things you need to do and check on your own:\\nFirst Make the pull request and Merge the branch into the main.\\nMake sure you have the latest version, if you made changes to the repo in another place.\\nCheck if the dbt_project.yml file is accessible to the project, if not check this solution (Dbt: This dbt Cloud run was cancelled because a valid dbt project was not found.).\\nCheck if the name you gave to the dataset on BigQuery is the same you put on the dataset spot on the production environment created on dbt cloud.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'DBT Deploy - Error When trying to run the dbt project on Prod', 'course': 'data-engineering-zoomcamp'}, {'text': 'In the step in this video (DE Zoomcamp 4.3.1 - Build the First dbt Models), after creating `stg_green_tripdata.sql` and clicking `build`, I encountered an error saying dataset not found in location EU. The default location for dbt Bigquery is the US, so when generating the new Bigquery schema for dbt, unless specified, the schema locates in the US.\\nSolution:\\nTurns out I forgot to specify Location to be `EU` when adding connection details.\\nDevelop -> Configure Cloud CLI -> Projects -> taxi_rides_ny -> (connection) Bigquery -> Edit -> Location (Optional) -> type `EU` -> Save', 'section': 'Module 4: analytics engineering with dbt', 'question': 'DBT - Error: “404 Not found: Dataset <dataset_name>:<dbt_schema_name> was not found in location EU” after building from stg_green_tripdata.sql', 'course': 'data-engineering-zoomcamp'}, {'text': 'Issue: If you’re having problems loading the FHV_20?? data from the github repo into GCS and then into BQ (input file not of type parquet), you need to do two things. First, append the URL Template link with ‘?raw=true’ like so:\\nURL_TEMPLATE = URL_PREFIX + \"/fhv_tripdata_{{ execution_date.strftime(\\\\\\'%Y-%m\\\\\\') }}.parquet?raw=true\"\\nSecond, update make sure the URL_PREFIX is set to the following value:\\n\\nURL_PREFIX = \"https://github.com/alexeygrigorev/datasets/blob/master/nyc-tlc/fhv\"\\nIt is critical that you use this link with the keyword blob. If your link has ‘tree’ here, replace it. Everything else can stay the same, including the curl -sSLf command. ‘', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Homework - Ingesting FHV_20?? data', 'course': 'data-engineering-zoomcamp'}, {'text': 'I found out that the easies way to upload datasets form github for the homework is utilising this script git_csv_to_gcs.py. Thank you Lidia!!\\nIt is similar to a script that Alexey provided us in 03-data-warehouse/extras/web_to_gcs.py', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Homework - Ingesting NYC TLC Data', 'course': 'data-engineering-zoomcamp'}, {'text': 'If you have to securely put your credentials for a project and, probably, push it to a git repository then the best option is to use an environment variable\\nFor example for web_to_gcs.py or git_csv_to_gcs.py we have to set these variables:\\nGOOGLE_APPLICATION_CREDENTIALS\\nGCP_GCS_BUCKET\\nThe easises option to do it  is to use .env  (dotenv).\\nInstall it and add a few lines of code that inject these variables for your project\\npip install python-dotenv\\nfrom dotenv import load_dotenv\\nimport os\\n# Load environment variables from .env file\\nload_dotenv()\\n# Now you can access environment variables like GCP_GCS_BUCKET and GOOGLE_APPLICATION_CREDENTIALS\\ncredentials_path = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\\nBUCKET = os.environ.get(\"GCP_GCS_BUCKET\")', 'section': 'Module 4: analytics engineering with dbt', 'question': 'How to set environment variable easily for any credentials', 'course': 'data-engineering-zoomcamp'}, {'text': \"If you uploaded manually the fvh 2019 csv files, you may face errors regarding date types. Try to create an the external table in bigquery but define the pickup_datetime and dropoff_datetime to be strings\\nCREATE OR REPLACE EXTERNAL TABLE `gcp_project.trips_data_all.fhv_tripdata`  (\\ndispatching_base_num STRING,\\npickup_datetime STRING,\\ndropoff_datetime STRING,\\nPUlocationID STRING,\\nDOlocationID STRING,\\nSR_Flag STRING,\\nAffiliated_base_number STRING\\n)\\nOPTIONS (\\nformat = 'csv',\\nuris = ['gs://bucket/*.csv']\\n);\\nThen when creating the fhv core model in dbt, use TIMESTAMP(CAST(()) to ensure it first parses as a string and then convert it to timestamp.\\nwith fhv_tripdata as (\\nselect * from {{ ref('stg_fhv_tripdata') }}\\n),\\ndim_zones as (\\nselect * from {{ ref('dim_zones') }}\\nwhere borough != 'Unknown'\\n)\\nselect fhv_tripdata.dispatching_base_num,\\nTIMESTAMP(CAST(fhv_tripdata.pickup_datetime AS STRING)) AS pickup_datetime,\\nTIMESTAMP(CAST(fhv_tripdata.dropoff_datetime AS STRING)) AS dropoff_datetime,\", 'section': 'Module 4: analytics engineering with dbt', 'question': \"Invalid date types after Ingesting FHV data through CSV files: Could not parse 'pickup_datetime' as a timestamp\", 'course': 'data-engineering-zoomcamp'}, {'text': \"If you uploaded manually the fvh 2019 parquet files manually after downloading from https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-*.parquet you may face errors regarding date types while loading the data in a landing table (say fhv_tripdata). Try to create an the external table with the schema defines as following and load each month in a loop.\\n-----Correct load with schema defination----will not throw error----------------------\\nCREATE OR REPLACE EXTERNAL TABLE `dw-bigquery-week-3.trips_data_all.external_tlc_fhv_trips_2019` (\\ndispatching_base_num STRING,\\npickup_datetime TIMESTAMP,\\ndropoff_datetime TIMESTAMP,\\nPUlocationID FLOAT64,\\nDOlocationID FLOAT64,\\nSR_Flag FLOAT64,\\nAffiliated_base_number STRING\\n)\\nOPTIONS (\\nformat = 'PARQUET',\\nuris = ['gs://project id/fhv_2019_8.parquet']\\n);\\nCan Also USE  uris = ['gs://project id/fhv_2019_*.parquet'] (THIS WILL remove the need for the loop and can be done for all month in single RUN )\\n– THANKYOU FOR THIS –\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'Invalid data types after Ingesting FHV data through parquet files: Could not parse SR_Flag as Float64,Couldn’t parse datetime column as timestamp,couldn’t handle NULL values in PULocationID,DOLocationID', 'course': 'data-engineering-zoomcamp'}, {'text': 'When accessing Looker Studio through the Google Cloud Project console, you may be prompted to subscribe to the Pro version and receive the following errors:\\nInstead, navigate to https://lookerstudio.google.com/navigation/reporting which will take you to the free version.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Google Looker Studio - you have used up your 30-day trial', 'course': 'data-engineering-zoomcamp'}, {'text': 'Ans: Dbt provides a mechanism called \"ref\" to manage dependencies between models. By referencing other models using the \"ref\" keyword in SQL, dbt automatically understands the dependencies and ensures the correct execution order.\\nLoading FHV Data goes into slumber using Mage?\\nTry loading the data using jupyter notebooks in a local environment. There might be bandwidth issues with Mage.\\nLoad the data into a pandas dataframe using the urls, make necessary transformations, upload the gcp bucket / alternatively download the parquet/csv files locally and then upload to GCP manually.\\nRegion Mismatch in DBT and BigQuery\\nIf you are using the datasets copied into BigQuery from BigQuery public datasets, the region will be set as US by default and hence it is much easier to set your dbt profile location as US while transforming the tables and views. \\nYou can change the location as follows:', 'section': 'Module 4: analytics engineering with dbt', 'question': 'How does dbt handle dependencies between models?', 'course': 'data-engineering-zoomcamp'}, {'text': \"Use the PostgreSQL COPY FROM feature that is compatible with csv files\\nCOPY table_name [ ( column_name [, ...] ) ]\\nFROM { 'filename' | PROGRAM 'command' | STDIN }\\n[ [ WITH ] ( option [, ...] ) ]\\n[ WHERE condition ]\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'What is the fastest way to upload taxi data to dbt-postgres?', 'course': 'data-engineering-zoomcamp'}, {'text': 'Update the line:\\nWith:', 'section': 'Module 5: pyspark', 'question': 'When configuring the profiles.yml file for dbt-postgres with jinja templates with environment variables, I\\'m getting \"Credentials in profile \"PROFILE_NAME\", target: \\'dev\\', invalid: \\'5432\\'is not of type \\'integer\\'', 'course': 'data-engineering-zoomcamp'}, {'text': 'Install SDKMAN:\\ncurl -s \"https://get.sdkman.io\" | bash\\nsource \"$HOME/.sdkman/bin/sdkman-init.sh\"\\nUsing SDKMAN, install Java 11 and Spark 3.3.2:\\nsdk install java 11.0.22-tem\\nsdk install spark 3.3.2\\nOpen a new terminal or run the following in the same shell:\\nsource \"$HOME/.sdkman/bin/sdkman-init.sh\"\\nVerify the locations and versions of Java and Spark that were installed:\\necho $JAVA_HOME\\njava -version\\necho $SPARK_HOME\\nspark-submit --version', 'section': 'Module 5: pyspark', 'question': 'Setting up Java and Spark (with PySpark) on Linux (Alternative option using SDKMAN)', 'course': 'data-engineering-zoomcamp'}, {'text': 'If you’re seriously struggling to set things up \"locally\" (here locally meaning non/partly-managed environment like own laptop, a VM or Codespaces) you can use the following guide to use Spark in Google Colab:\\nhttps://medium.com/gitconnected/launch-spark-on-google-colab-and-connect-to-sparkui-342cad19b304\\nStarter notebook:\\nhttps://github.com/aaalexlit/medium_articles/blob/main/Spark_in_Colab.ipynb\\nIt’s advisable to spend some time setting things up locally rather than jumping right into this solution.', 'section': 'Module 5: pyspark', 'question': 'PySpark - Setting Spark up in Google Colab', 'course': 'data-engineering-zoomcamp'}, {'text': 'If after installing Java (either jdk or openjdk), Hadoop and Spark, and setting the corresponding environment variables you find the following error when spark-shell is run at CMD:\\njava.lang.IllegalAccessError: class org.apache.spark.storage.StorageUtils$ (in unnamed module @0x3c947bc5) cannot access class sun.nio.ch.DirectBuffer (in module java.base) because module java.base does not export sun.nio.ch to unnamed\\nmodule @0x3c947bc5\\nSolution: Java 17 or 19 is not supported by Spark. Spark 3.x: requires Java 8/11/16. Install Java 11 from the website provided in the windows.md setup file.', 'section': 'Module 5: pyspark', 'question': 'Spark-shell: unable to load native-hadoop library for platform - Windows', 'course': 'data-engineering-zoomcamp'}, {'text': 'I found this error while executing the user defined function in Spark (crazy_stuff_udf). I am working on Windows and using conda. After following the setup instructions, I found that the PYSPARK_PYTHON environment variable was not set correctly, given that conda has different python paths for each environment.\\nSolution:\\npip install findspark on the command line inside proper environment\\nAdd to the top of the script\\nimport findspark\\nfindspark.init()', 'section': 'Module 5: pyspark', 'question': 'PySpark - Python was not found; run without arguments to install from the Microsoft Store, or disable this shortcut from Settings > Manage App Execution Aliases.', 'course': 'data-engineering-zoomcamp'}, {'text': 'This is because Python 3.11 has some inconsistencies with such an old version of Spark. The solution is a downgrade in the Python version. Python 3.9 using a conda environment takes care of it. Or install newer PySpark >= 3.5.1 works for me (Ella) [source].', 'section': 'Module 5: pyspark', 'question': 'PySpark - TypeError: code() argument 13 must be str, not int  , while executing `import pyspark`  (Windows/ Spark 3.0.3 - Python 3.11)', 'course': 'data-engineering-zoomcamp'}, {'text': 'If anyone is a Pythonista or becoming one (which you will essentially be one along this journey), and desires to have all python dependencies under same virtual environment (e.g. conda) as done with prefect and previous exercises, simply follow these steps\\nInstall OpenJDK 11,\\non MacOS: $ brew install java11\\nAdd export PATH=\"/opt/homebrew/opt/openjdk@11/bin:$PATH\"\\nto ~/.bashrc or ~/zshrc\\nActivate working environment (by pipenv / poetry / conda)\\nRun $ pip install pyspark\\nWork with exercises as normal\\nAll default commands of spark will be also available at shell session under activated enviroment.\\nHope this can help!\\nP.s. you won’t need findspark to firstly initialize.\\nPy4J - Py4JJavaError: An error occurred while calling (...)  java.net.ConnectException: Connection refused: no further information;\\nIf you\\'re getting `Py4JavaError` with a generic root cause, such as the described above (Connection refused: no further information). You\\'re most likely using incompatible versions of the JDK or Python with Spark.\\nAs of the current latest Spark version (3.5.0), it supports JDK 8 / 11 / 17. All of which can be easily installed with SDKMan! on macOS or Linux environments\\n\\n$ sdk install java 17.0.10-librca\\n$ sdk install spark 3.5.0\\n$ sdk install hadoop 3.3.5\\nAs PySpark 3.5.0 supports Python 3.8+ make sure you\\'re setting up your virtualenv with either 3.8 / 3.9 / 3.10 / 3.11 (Most importantly avoid using 3.12 for now as not all libs in the data-science/engineering ecosystem are fully package for that)\\n\\n\\n$ conda create -n ENV_NAME python=3.11\\n$ conda activate ENV_NAME\\n$ pip install pyspark==3.5.0\\nThis setup makes installing `findspark` and the likes of it unnecessary. Happy coding.\\nPy4J - Py4JJavaError: An error occurred while calling o54.parquet. Or any kind of Py4JJavaError that show up after run df.write.parquet(\\'zones\\')(On window)\\nThis assume you already correctly set up the PATH in the nano ~/.bashrc\\nHere my\\nexport JAVA_HOME=\"/c/tools/jdk-11.0.21\"\\nexport PATH=\"${JAVA_HOME}/bin:${PATH}\"\\nexport HADOOP_HOME=\"/c/tools/hadoop-3.2.0\"\\nexport PATH=\"${HADOOP_HOME}/bin:${PATH}\"\\nexport SPARK_HOME=\"/c/tools/spark-3.3.2-bin-hadoop3\"\\nexport PATH=\"${SPARK_HOME}/bin:${PATH}\"\\nexport PYTHONPATH=\"${SPARK_HOME}/python/:$PYTHONPATH\"\\nexport PYTHONPATH=\"${SPARK_HOME}spark-3.5.1-bin-hadoop3py4j-0.10.9.5-src.zip:$PYTHONPATH\"\\nYou also need to add environment variables correctly which paths to java jdk, spark and hadoop through\\nGo to Stephenlaye2/winutils3.3.0: winutils.exe hadoop.dll and hdfs.dll binaries for hadoop windows (github.com), download the right winutils for hadoop-3.2.0. Then create a new folder,bin and put every thing in side to make a /c/tools/hadoop-3.2.0/bin(You might not need to do this, but after testing it without the /bin I could not make it to work)\\nThen follow the solution in this video: How To Resolve Issue with Writing DataFrame to Local File | winutils | msvcp100.dll (youtube.com)\\nRemember to restart IDE and computer, After the error An error occurred while calling o54.parquet.  is fixed but new errors like o31.parquet. Or o35.parquet. appear.', 'section': 'Module 5: pyspark', 'question': 'Java+Spark - Easy setup with miniconda env (worked on MacOS)', 'course': 'data-engineering-zoomcamp'}, {'text': 'After installing all including pyspark (and it is successfully imported), but then running this script on the jupyter notebook\\nimport pyspark\\nfrom pyspark.sql import SparkSession\\nspark = SparkSession.builder \\\\\\n.master(\"local[*]\") \\\\\\n.appName(\\'test\\') \\\\\\n.getOrCreate()\\ndf = spark.read \\\\\\n.option(\"header\", \"true\") \\\\\\n.csv(\\'taxi+_zone_lookup.csv\\')\\ndf.show()\\nit gives the error:\\nRuntimeError: Java gateway process exited before sending its port number\\n✅The solution (for me) was:\\npip install findspark on the command line and then\\nAdd\\nimport findspark\\nfindspark.init()\\nto the top of the script.\\nAnother possible solution is:\\nCheck that pyspark is pointing to the correct location.\\nRun pyspark.__file__. It should be list /home/<your user name>/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/__init__.py if you followed the videos.\\nIf it is pointing to your python site-packages remove the pyspark directory there and check that you have added the correct exports to you .bashrc file and that there are not any other exports which might supersede the ones provided in the course content.\\nTo add to the solution above, if the errors persist in regards to setting the correct path for spark,  an alternative solution for permanent path setting solve the error is  to set environment variables on system and user environment variables following this tutorial: Install Apache PySpark on Windows PC | Apache Spark Installation Guide\\nOnce everything is installed, skip to 7:14 to set up environment variables. This allows for the environment variables to be set permanently.', 'section': 'Module 5: pyspark', 'question': 'lsRuntimeError: Java gateway process exited before sending its port number', 'course': 'data-engineering-zoomcamp'}, {'text': 'Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\\nThe solution which worked for me(use following in jupyter notebook) :\\n!pip install findspark\\nimport findspark\\nfindspark.init()\\nThereafter , import pyspark and create spark contex<<t as usual\\nNone of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\\nFilter based on conditions based on multiple columns\\nfrom pyspark.sql.functions import col\\nnew_final.filter((new_final.a_zone==\"Murray Hill\") & (new_final.b_zone==\"Midwood\")).show()\\nKrishna Anand', 'section': 'Module 5: pyspark', 'question': 'Module Not Found Error in Jupyter Notebook .', 'course': 'data-engineering-zoomcamp'}, {'text': 'You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\\n` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\\nexport PYTHONPATH=”${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}”\\nMake sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named \\'py4j\\'` while executing `import pyspark`.\\nFor instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\\nThen the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\"` appropriately.\\nAdditionally, you can check for the version of ‘py4j’ of the spark you’re using from here and update as mentioned above.\\n~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.', 'section': 'Module 5: pyspark', 'question': \"Py4JJavaError - ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\", 'course': 'data-engineering-zoomcamp'}, {'text': 'If below does not work, then download the latest available py4j version with\\nconda install -c conda-forge py4j\\nTake care of the latest version number in the website to replace appropriately.\\nNow add\\nexport PYTHONPATH=\"${SPARK_HOME}/python/:$PYTHONPATH\"\\nexport PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH\"\\nin your  .bashrc file.', 'section': 'Module 5: pyspark', 'question': \"Py4J Error - ModuleNotFoundError: No module named 'py4j' (Solve with latest version)\", 'course': 'data-engineering-zoomcamp'}, {'text': 'Even after we have exported our paths correctly you may find that  even though Jupyter is installed you might not have Jupyter Noteboopgak for one reason or another. Full instructions are found here (for my walkthrough) or here (where I got the original instructions from) but are included below. These instructions include setting up a virtual environment (handy if you are on your own machine doing this and not a VM):\\nFull steps:\\nUpdate and upgrade packages:\\nsudo apt update && sudo apt -y upgrade\\nInstall Python:\\nsudo apt install python3-pip python3-dev\\nInstall Python virtualenv:\\nsudo -H pip3 install --upgrade pip\\nsudo -H pip3 install virtualenv\\nCreate a Python Virtual Environment:\\nmkdir notebook\\ncd notebook\\nvirtualenv jupyterenv\\nsource jupyterenv/bin/activate\\nInstall Jupyter Notebook:\\npip install jupyter\\nRun Jupyter Notebook:\\njupyter notebook', 'section': 'Module 5: pyspark', 'question': 'Exception: Jupyter command `jupyter-notebook` not found.', 'course': 'data-engineering-zoomcamp'}, {'text': 'Code executed:\\ndf = spark.read.parquet(pq_path)\\n… some operations on df …\\ndf.write.parquet(pq_path, mode=\"overwrite\")\\njava.io.FileNotFoundException: File file:/home/xxx/code/data/pq/fhvhv/2021/02/part-00021-523f9ad5-14af-4332-9434-bdcb0831f2b7-c000.snappy.parquet does not exist\\nThe problem is that Sparks performs lazy transformations, so the actual action that trigger the job is df.write, which does delete the parquet files that is trying to read (mode=”overwrite”)\\n✅Solution: Write to a different directorydf\\ndf.write.parquet(pq_path_temp, mode=\"overwrite\")', 'section': 'Module 5: pyspark', 'question': 'Error java.io.FileNotFoundException', 'course': 'data-engineering-zoomcamp'}, {'text': 'You need to create the Hadoop /bin directory manually and add the downloaded files in there, since the shell script provided for Windows installation just puts them in /c/tools/hadoop-3.2.0/ .', 'section': 'Module 5: pyspark', 'question': 'Hadoop - FileNotFoundException: Hadoop bin directory does not exist , when trying to write (Windows)', 'course': 'data-engineering-zoomcamp'}, {'text': 'Actually Spark SQL is one independent “type” of SQL - Spark SQL.\\nThe several SQL providers are very similar:\\nSELECT [attributes]\\nFROM [table]\\nWHERE [filter]\\nGROUP BY [grouping attributes]\\nHAVING [filtering the groups]\\nORDER BY [attribute to order]\\n(INNER/FULL/LEFT/RIGHT) JOIN [table2]\\nON [attributes table joining table2] (...)\\nWhat differs the most between several SQL providers are built-in functions.\\nFor Built-in Spark SQL function check this link: https://spark.apache.org/docs/latest/api/sql/index.html\\nExtra information on SPARK SQL :\\nhttps://databricks.com/glossary/what-is-spark-sql#:~:text=Spark%20SQL%20is%20a%20Spark,on%20existing%20deployments%20and%20data.', 'section': 'Module 5: pyspark', 'question': 'Which type of SQL is used in Spark? Postgres? MySQL? SQL Server?', 'course': 'data-engineering-zoomcamp'}, {'text': \"✅Solution: I had two notebooks running, and the one I wanted to look at had opened a port on localhost:4041.\\nIf a port is in use, then Spark uses the next available port number. It can be even 4044. Clean up after yourself when a port does not work or a container does not run.\\nYou can run spark.sparkContext.uiWebUrl\\nand result will be some like\\n'http://172.19.10.61:4041'\", 'section': 'Module 5: pyspark', 'question': 'The spark viewer on localhost:4040 was not showing the current run', 'course': 'data-engineering-zoomcamp'}, {'text': '✅Solution: replace Java Developer Kit 11 with Java Developer Kit 8.\\nJava - RuntimeError: Java gateway process exited before sending its port number\\nShows java_home is not set on the notebook log\\nhttps://sparkbyexamples.com/pyspark/pyspark-exception-java-gateway-process-exited-before-sending-the-driver-its-port-number/\\nhttps://twitter.com/drkrishnaanand/status/1765423415878463839', 'section': 'Module 5: pyspark', 'question': 'Java - java.lang.NoSuchMethodError: sun.nio.ch.DirectBuffer.cleaner()Lsun/misc/Cleaner Error during repartition call (conda pyspark installation)', 'course': 'data-engineering-zoomcamp'}, {'text': '✅I got it working using `gcs-connector-hadoop-2.2.5-shaded.jar` and Spark 3.1\\nI also added the google_credentials.json and .p12 to auth with gcs. These files are downloadable from GCP Service account.\\nTo create the SparkSession:\\nspark = SparkSession.builder.master(\\'local[*]\\') \\\\\\n.appName(\\'spark-read-from-bigquery\\') \\\\\\n.config(\\'BigQueryProjectId\\',\\'razor-project-xxxxxxx) \\\\\\n.config(\\'BigQueryDatasetLocation\\',\\'de_final_data\\') \\\\\\n.config(\\'parentProject\\',\\'razor-project-xxxxxxx) \\\\\\n.config(\"google.cloud.auth.service.account.enable\", \"true\") \\\\\\n.config(\"credentialsFile\", \"google_credentials.json\") \\\\\\n.config(\"GcpJsonKeyFile\", \"google_credentials.json\") \\\\\\n.config(\"spark.driver.memory\", \"4g\") \\\\\\n.config(\"spark.executor.memory\", \"2g\") \\\\\\n.config(\"spark.memory.offHeap.enabled\",True) \\\\\\n.config(\"spark.memory.offHeap.size\",\"5g\") \\\\\\n.config(\\'google.cloud.auth.service.account.json.keyfile\\', \"google_credentials.json\") \\\\\\n.config(\"fs.gs.project.id\", \"razor-project-xxxxxxx\") \\\\\\n.config(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\\\\n.config(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\") \\\\\\n.getOrCreate()', 'section': 'Module 5: pyspark', 'question': 'Spark fails when reading from BigQuery and using `.show()` on `SELECT` queries', 'course': 'data-engineering-zoomcamp'}, {'text': 'While creating a SparkSession using the config spark.jars.packages as com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.23.2\\nspark = SparkSession.builder.master(\\'local\\').appName(\\'bq\\').config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.23.2\").getOrCreate()\\nautomatically downloads the required dependency jars and configures the connector, removing the need to manage this dependency. More details available here', 'section': 'Module 5: pyspark', 'question': 'Spark BigQuery connector Automatic configuration', 'course': 'data-engineering-zoomcamp'}, {'text': 'Link to Slack Thread : has anyone figured out how to read from GCP data lake instead of downloading all the taxi data again?\\nThere’s a few extra steps to go into reading from GCS with PySpark\\n1.)  IMPORTANT: Download the Cloud Storage connector for Hadoop here: https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage#clusters\\nAs the name implies, this .jar file is what essentially connects PySpark with your GCS\\n2.) Move the .jar file to your Spark file directory. I installed Spark using homebrew on my MacOS machine and I had to create a /jars directory under \"/opt/homebrew/Cellar/apache-spark/3.2.1/ (where my spark dir is located)\\n3.) In your Python script, there are a few extra classes you’ll have to import:\\nimport pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.conf import SparkConf\\nfrom pyspark.context import SparkContext\\n4.) You must set up your configurations before building your SparkSession. Here’s my code snippet:\\nconf = SparkConf() \\\\\\n.setMaster(\\'local[*]\\') \\\\\\n.setAppName(\\'test\\') \\\\\\n.set(\"spark.jars\", \"/opt/homebrew/Cellar/apache-spark/3.2.1/jars/gcs-connector-hadoop3-latest.jar\") \\\\\\n.set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\\\\n.set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"path/to/google_credentials.json\")\\nsc = SparkContext(conf=conf)\\nsc._jsc.hadoopConfiguration().set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\\nsc._jsc.hadoopConfiguration().set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\\nsc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.json.keyfile\", \"path/to/google_credentials.json\")\\nsc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.enable\", \"true\")\\n5.) Once you run that, build your SparkSession with the new parameters we’d just instantiated in the previous step:\\nspark = SparkSession.builder \\\\\\n.config(conf=sc.getConf()) \\\\\\n.getOrCreate()\\n6.) Finally, you’re able to read your files straight from GCS!\\ndf_green = spark.read.parquet(\"gs://{BUCKET}/green/202*/\")', 'section': 'Module 5: pyspark', 'question': 'Spark Cloud Storage connector', 'course': 'data-engineering-zoomcamp'}, {'text': 'from pyarrow.parquet import ParquetFile\\npf = ParquetFile(\\'fhvhv_tripdata_2021-01.parquet\\')\\n#pyarrow builds tables, not dataframes\\ntbl_small = next(pf.iter_batches(batch_size = 1000))\\n#this function converts the table to a dataframe of manageable size\\ndf = tbl_small.to_pandas()\\nAlternatively without PyArrow:\\ndf = spark.read.parquet(\\'fhvhv_tripdata_2021-01.parquet\\')\\ndf1 = df.sort(\\'DOLocationID\\').limit(1000)\\npdf = df1.select(\"*\").toPandas()\\ngcsu', 'section': 'Module 5: pyspark', 'question': 'How can I read a small number of rows from the parquet file directly?', 'course': 'data-engineering-zoomcamp'}, {'text': 'Probably you’ll encounter this if you followed the video ‘5.3.1 - First Look at Spark/PySpark’ and used the parquet file from the TLC website (csv was used in the video).\\nWhen defining the schema, the PULocation and DOLocationID are defined as IntegerType. This will cause an error because the Parquet file is INT64 and you’ll get an error like:\\nParquet column cannot be converted in file [...] Column [...] Expected: int, Found: INT64\\nChange the schema definition from IntegerType to LongType and it should work', 'section': 'Module 5: pyspark', 'question': 'DataType error when creating Spark DataFrame with a specified schema?', 'course': 'data-engineering-zoomcamp'}, {'text': 'df_finalx=df_finalw.select([col(x).alias(x.replace(\" \",\"\")) for x in df_finalw.columns])\\nKrishna Anand', 'section': 'Module 5: pyspark', 'question': 'Remove white spaces from column names in Pyspark', 'course': 'data-engineering-zoomcamp'}, {'text': 'This error comes up on the Spark video 5.3.1 - First Look at Spark/PySpark,\\nbecause as at the creation of the video, 2021 data was the most recent which utilised csv files but as at now its parquet.\\nSo when you run the command spark.createDataFrame(df1_pandas).show(),\\nYou get the Attribute error. This is caused by the pandas version 2.0.0 which seems incompatible with Spark 3.3.2, so to fix it you have to downgrade pandas to 1.5.3 using the command pip install -U pandas==1.5.3\\nAnother option is adding the following after importing pandas, if one does not want to downgrade pandas version (source) :\\npd.DataFrame.iteritems = pd.DataFrame.items\\nNote that this problem is solved with Spark versions from 3.4.1', 'section': 'Module 5: pyspark', 'question': \"AttributeError: 'DataFrame' object has no attribute 'iteritems'\", 'course': 'data-engineering-zoomcamp'}, {'text': 'Another alternative is to install pandas 2.0.1 (it worked well as at the time of writing this), and it is compatible with Pyspark 3.5.1. Make sure to add or edit your environment variable like this:\\nexport SPARK_HOME=\"${HOME}/spark/spark-3.5.1-bin-hadoop3\"\\nexport PATH=\"${SPARK_HOME}/bin:${PATH}\"', 'section': 'Module 5: pyspark', 'question': \"AttributeError: 'DataFrame' object has no attribute 'iteritems'\", 'course': 'data-engineering-zoomcamp'}, {'text': 'Open a CMD terminal in administrator mode\\ncd %SPARK_HOME%\\nStart a master node: bin\\\\spark-class org.apache.spark.deploy.master.Master\\nStart a worker node: bin\\\\spark-class org.apache.spark.deploy.worker.Worker spark://<master_ip>:<port> --host <IP_ADDR>\\nbin/spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077 --host <IP_ADDR>\\nspark://<master_ip>:<port>: copy the address from the previous command, in my case it was spark://localhost:7077\\nUse --host <IP_ADDR> if you want to run the worker on a different machine. For now leave it empty.\\nNow you can access Spark UI through localhost:8080\\nHomework for Module 5:\\nDo not refer to the homework file located under /05-batch/code/. The correct file is located under\\nhttps://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/cohorts/2024/05-batch/homework.md', 'section': 'Module 5: pyspark', 'question': 'Spark Standalone Mode on Windows', 'course': 'data-engineering-zoomcamp'}, {'text': 'You can either type the export command every time you run a new session, add it to the .bashrc/ which you can find in /home or run this command at the beginning of your homebook:\\nimport findspark\\nfindspark.init()', 'section': 'Module 5: pyspark', 'question': 'Export PYTHONPATH command in linux is temporary', 'course': 'data-engineering-zoomcamp'}, {'text': 'I solved this issue: unzip the file with:\\nf\\nbefore creating head.csv', 'section': 'Module 5: pyspark', 'question': 'Compressed file ended before the end-of-stream marker was reached', 'course': 'data-engineering-zoomcamp'}, {'text': 'In the code along from Video 5.3.3 Alexey downloads the CSV files from the NYT website and gzips them in their bash script. If we now (2023) follow along but download the data from the GH course Repo, it will already be zippes as csv.gz files. Therefore we zip it again if we follow the code from the video exactly. This then leads to gibberish outcome when we then try to cat the contents or count the lines with zcat, because the file is zipped twitch and zcat only unzips it once.\\n✅solution: do not gzip the files downloaded from the course repo. Just wget them and save them as they are as csv.gz files. Then the zcat command and the showSchema command will also work\\nURL=\"${URL_PREFIX}/${TAXI_TYPE}/${TAXI_TYPE}_tripdata_${YEAR}-${FMONTH}.csv.gz\"\\nLOCAL_PREFIX=\"data/raw/${TAXI_TYPE}/${YEAR}/${FMONTH}\"\\nLOCAL_FILE=\"${TAXI_TYPE}_tripdata_${YEAR}_${FMONTH}.csv.gz\"\\nLOCAL_PATH=\"${LOCAL_PREFIX}/${LOCAL_FILE}\"\\necho \"downloading ${URL} to ${LOCAL_PATH}\"\\nmkdir -p ${LOCAL_PREFIX}\\nwget ${URL} -O ${LOCAL_PATH}\\necho \"compressing ${LOCAL_PATH}\"\\n# gzip ${LOCAL_PATH} <- uncomment this line', 'section': 'Module 5: pyspark', 'question': 'Compression Error: zcat output is gibberish, seems like still compressed', 'course': 'data-engineering-zoomcamp'}, {'text': 'Occurred while running : spark.createDataFrame(df_pandas).show()\\nThis error is usually due to the python version, since spark till date of 2 march 2023 doesn’t support python 3.11, try creating a new env with python version 3.8 and then run this command.\\nOn the virtual machine, you can create a conda environment (here called myenv) with python 3.10 installed:\\nconda create -n myenv python=3.10 anaconda\\nThen you must run conda activate myenv to run python 3.10. Otherwise you’ll still be running version 3.11. You can deactivate by typing conda deactivate.', 'section': 'Module 5: pyspark', 'question': 'PicklingError: Could not serialise object: IndexError: tuple index out of range.', 'course': 'data-engineering-zoomcamp'}, {'text': 'Make sure you have your credentials of your GCP in your VM under the location defined in the script.', 'section': 'Module 5: pyspark', 'question': 'Connecting from local Spark to GCS - Spark does not find my google credentials as shown in the video?', 'course': 'data-engineering-zoomcamp'}, {'text': 'To run spark in docker setup\\n1. Build bitnami spark docker\\na. clone bitnami repo using command\\ngit clone https://github.com/bitnami/containers.git\\n(tested on commit 9cef8b892d29c04f8a271a644341c8222790c992)\\nb. edit file `bitnami/spark/3.3/debian-11/Dockerfile` and update java and spark version as following\\n\"python-3.10.10-2-linux-${OS_ARCH}-debian-11\" \\\\\\n\"java-17.0.5-8-3-linux-${OS_ARCH}-debian-11\" \\\\\\nreference: https://github.com/bitnami/containers/issues/13409\\nc. build docker image by navigating to above directory and running docker build command\\nnavigate cd bitnami/spark/3.3/debian-11/\\nbuild command docker build -t spark:3.3-java-17 .\\n2. run docker compose\\nusing following file\\n```yaml docker-compose.yml\\nversion: \\'2\\'\\nservices:\\nspark:\\nimage: spark:3.3-java-17\\nenvironment:\\n- SPARK_MODE=master\\n- SPARK_RPC_AUTHENTICATION_ENABLED=no\\n- SPARK_RPC_ENCRYPTION_ENABLED=no\\n- SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\\n- SPARK_SSL_ENABLED=no\\nvolumes:\\n- \"./:/home/jovyan/work:rw\"\\nports:\\n- \\'8080:8080\\'\\n- \\'7077:7077\\'\\nspark-worker:\\nimage: spark:3.3-java-17\\nenvironment:\\n- SPARK_MODE=worker\\n- SPARK_MASTER_URL=spark://spark:7077\\n- SPARK_WORKER_MEMORY=1G\\n- SPARK_WORKER_CORES=1\\n- SPARK_RPC_AUTHENTICATION_ENABLED=no\\n- SPARK_RPC_ENCRYPTION_ENABLED=no\\n- SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\\n- SPARK_SSL_ENABLED=no\\nvolumes:\\n- \"./:/home/jovyan/work:rw\"\\nports:\\n- \\'8081:8081\\'\\nspark-nb:\\nimage: jupyter/pyspark-notebook:java-17.0.5\\nenvironment:\\n- SPARK_MASTER_URL=spark://spark:7077\\nvolumes:\\n- \"./:/home/jovyan/work:rw\"\\nports:\\n- \\'8888:8888\\'\\n- \\'4040:4040\\'\\n```\\nrun command to deploy docker compose\\ndocker-compose up\\nAccess jupyter notebook using link logged in docker compose logs\\nSpark master url is spark://spark:7077', 'section': 'Module 5: pyspark', 'question': 'Spark docker-compose setup', 'course': 'data-engineering-zoomcamp'}, {'text': 'To do this\\npip install gcsfs,\\nThereafter copy the uri path to the file and use \\ndf = pandas.read_csc(gs://path)', 'section': 'Module 5: pyspark', 'question': 'How do you read data stored in gcs on pandas with your local computer?', 'course': 'data-engineering-zoomcamp'}, {'text': 'Error:\\nspark.createDataFrame(df_pandas).schema\\nTypeError: field Affiliated_base_number: Can not merge type <class \\'pyspark.sql.types.StringType\\'> and <class \\'pyspark.sql.types.DoubleType\\'>\\nSolution:\\nAffiliated_base_number is a mix of letters and numbers (you can check this with a preview of the table), so it cannot be set to DoubleType (only for double-precision numbers). The suitable type would be StringType. Spark  inferSchema is more accurate than Pandas infer type method in this case. You can set it to  true  while reading the csv, so you don’t have to take out any data from your dataset. Something like this can help:\\ndf = spark.read \\\\\\n.options(\\nheader = \"true\", \\\\\\ninferSchema = \"true\", \\\\\\n) \\\\\\n.csv(\\'path/to/your/csv/file/\\')\\nSolution B:\\nIt\\'s because some rows in the affiliated_base_number are null and therefore it is assigned the datatype String and this cannot be converted to type Double. So if you really want to convert this pandas df to a pyspark df only take the  rows from the pandas df that are not null in the \\'Affiliated_base_number\\' column. Then you will be able to apply the pyspark function createDataFrame.\\n# Only take rows that have no null values\\npandas_df= pandas_df[pandas_df.notnull().all(1)]', 'section': 'Module 5: pyspark', 'question': 'TypeError when using spark.createDataFrame function on a pandas df', 'course': 'data-engineering-zoomcamp'}, {'text': 'Default executor memory is 1gb. This error appeared when working with the homework dataset.\\nError: MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\\nScaling row group sizes to 95.00% for 8 writers\\nSolution:\\nIncrease the memory of the executor when creating the Spark session like this:\\nRemember to restart the Jupyter session (ie. close the Spark session) or the config won’t take effect.', 'section': 'Module 5: pyspark', 'question': 'MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory', 'course': 'data-engineering-zoomcamp'}, {'text': 'Change the working directory to the spark directory:\\nif you have setup up your SPARK_HOME variable, use the following;\\ncd %SPARK_HOME%\\nif not, use the following;\\ncd <path to spark installation>\\nCreating a Local Spark Cluster\\nTo start Spark Master:\\nbin\\\\spark-class org.apache.spark.deploy.master.Master --host localhost\\nStarting up a cluster:\\nbin\\\\spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077 --host localhost', 'section': 'Module 5: pyspark', 'question': 'How to spark standalone cluster is run on windows OS', 'course': 'data-engineering-zoomcamp'}, {'text': 'I added PYTHONPATH, JAVA_HOME and SPARK_HOME to ~/.bashrc, import pyspark worked ok in iPython in terminal, but couldn’t be found in .ipynb opened in VS Code\\nAfter adding new lines to ~/.bashrc, need to restart the shell to activate the new lines, do either\\nsource ~/.bashrc\\nexec bash\\nInstead of configuring paths in ~/.bashrc, I created .env file in the root of my workspace:', 'section': 'Module 5: pyspark', 'question': 'Env variables set in ~/.bashrc are not loaded to Jupyter in VS Code', 'course': 'data-engineering-zoomcamp'}, {'text': 'I don’t use visual studio, so I did it the old fashioned way: ssh -L 8888:localhost:8888 <my user>@<VM IP> (replace user and IP with the ones used by the GCP VM, e.g. : ssh -L 8888:localhost:8888 myuser@34.140.188.1', 'section': 'Module 5: pyspark', 'question': 'How to port forward outside VS Code', 'course': 'data-engineering-zoomcamp'}, {'text': 'If you are doing wc -l fhvhv_tripdata_2021-01.csv.gz  with the gzip file as the file argument, you will get a different result, obviously! Since the file is compressed.\\nUnzip the file and then do wc -l fhvhv_tripdata_2021-01.csv to get the right results.', 'section': 'Module 5: pyspark', 'question': '“wc -l” is giving a different result then shown in the video', 'course': 'data-engineering-zoomcamp'}, {'text': 'when trying to:\\nURL=\"spark://$HOSTNAME:7077\"\\nspark-submit \\\\\\n--master=\"{$URL}\" \\\\\\n06_spark_sql.py \\\\\\n--input_green=data/pq/green/2021/*/ \\\\\\n--input_yellow=data/pq/yellow/2021/*/ \\\\\\n--output=data/report-2021\\nand you get errors like the following (SUMMARIZED):\\nWARN Utils: Your hostname, <HOSTNAME> resolves to a loopback address..\\nWARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address Setting default log level to \"WARN\".\\nException in thread \"main\" org.apache.spark.SparkException: Master must either be yarn or start with spark, mesos, k8s, or local at …\\nTry replacing --master=\"{$URL}\"\\nwith --master=$URL (edited)\\nExtra edit for spark version 3.4.2 - if encountering:\\n`Error: Unrecognized option: --master=`\\n→ Replace `--master=\"{$URL}\"` with  `--master \"${URL}\"`', 'section': 'Module 5: pyspark', 'question': '`spark-submit` errors', 'course': 'data-engineering-zoomcamp'}, {'text': 'If you are seeing this (or similar) error when attempting to write to parquet, it is likely an issue with your path variables.\\nFor Windows, create a new User Variable “HADOOP_HOME” that points to your Hadoop directory. Then add “%HADOOP_HOME%\\\\bin” to the PATH variable.\\nAdditional tips can be found here: https://stackoverflow.com/questions/41851066/exception-in-thread-main-java-lang-unsatisfiedlinkerror-org-apache-hadoop-io', 'section': 'Module 5: pyspark', 'question': 'Hadoop - Exception in thread \"main\" java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z', 'course': 'data-engineering-zoomcamp'}, {'text': \"Change the hadoop version to 3.0.1.Replace all the files in the local hadoop bin folder with the files in this repo:  winutils/hadoop-3.0.1/bin at master · cdarlint/winutils (github.com)\\nIf this does not work try to change other versions found in this repository.\\nFor more information please see this link: This version of %1 is not compatible with the version of Windows you're running · Issue #20 · cdarlint/winutils (github.com)\", 'section': 'Module 5: pyspark', 'question': 'Java.io.IOException. Cannot run program “C:\\\\hadoop\\\\bin\\\\winutils.exe”. CreateProcess error=216, This version of 1% is not compatible with the version of Windows you are using.', 'course': 'data-engineering-zoomcamp'}, {'text': 'Fix is to set the flag like the error states. Get your project ID from your dashboard and set it like so:\\ngcloud dataproc jobs submit pyspark \\\\\\n--cluster=my_cluster \\\\\\n--region=us-central1 \\\\\\n--project=my-dtc-project-1010101 \\\\\\ngs://my-dtc-bucket-id/code/06_spark_sql.py\\n-- \\\\\\n…', 'section': 'Module 5: pyspark', 'question': 'Dataproc - ERROR: (gcloud.dataproc.jobs.submit.pyspark) The required property [project] is not currently set. It can be set on a per-command basis by re-running your command with the [--project] flag.', 'course': 'data-engineering-zoomcamp'}, {'text': 'Go to %SPARK_HOME%\\\\bin\\nRun spark-class org.apache.spark.deploy.master.Master to run the master. This will give you a URL of the form spark://ip:port\\nRun spark-class org.apache.spark.deploy.worker.Worker spark://ip:port to run the worker. Make sure you use the URL you obtained in step 2.\\nCreate a new Jupyter notebook:\\nspark = SparkSession.builder \\\\\\n.master(\"spark://{ip}:7077\") \\\\\\n.appName(\\'test\\') \\\\\\n.getOrCreate()\\nCheck on Spark UI the master, worker and app.', 'section': 'Module 5: pyspark', 'question': 'Run Local Cluster Spark in Windows 10 with CMD', 'course': 'data-engineering-zoomcamp'}, {'text': 'This occurs because you are not logged in “gcloud auth login” and maybe the project id is not settled. Then type in a terminal:\\ngcloud auth login\\nThis will open a tab in the browser, accept the terms, after that close the tab if you want. Then set the project is like:\\ngcloud config set project <YOUR PROJECT_ID>\\nThen you can run the command to upload the pq dir to a GCS Bucket:\\ngsutil -m cp -r pq/ <YOUR URI from gsutil>/pq', 'section': 'Module 5: pyspark', 'question': \"lServiceException: 401 Anonymous caller does not have storage.objects.list access to the Google Cloud Storage bucket. Permission 'storage.objects.list' denied on resource (or it may not exist).\", 'course': 'data-engineering-zoomcamp'}, {'text': \"When submit a job, it might throw an error about Java in log panel within Dataproc. I changed the Versioning Control when I created a cluster, so it means that I delete the cluster and created a new one, and instead of choosing Debian-Hadoop-Spark, I switch to Ubuntu 20.02-Hadoop3.3-Spark3.3 for Versioning Control feature, the main reason to choose this is because I have the same Ubuntu version in mi laptop, I tried to find documentation to sustent this but unfortunately I couldn't nevertheless it works for me.\", 'section': 'Module 5: pyspark', 'question': 'py4j.protocol.Py4JJavaError  GCP', 'course': 'data-engineering-zoomcamp'}, {'text': \"Use both repartition and coalesce, like so:\\ndf = df.repartition(6)\\ndf = df.coalesce(6)\\ndf.write.parquet('fhv/2019/10', mode='overwrite')\", 'section': 'Module 5: pyspark', 'question': 'Repartition the Dataframe to 6 partitions using df.repartition(6) - got 8 partitions instead', 'course': 'data-engineering-zoomcamp'}, {'text': \"Possible solution - Try to forward the port using ssh cli instead of vs code.\\nRun > “ssh -L <local port>:<VM host/ip>:<VM port> <ssh hostname>”\\nssh hostname is the name you specified in the ~/.ssh/config file\\nIn case of Jupyter Notebook run\\n“ssh -L 8888:localhost:8888 gcp-vm”\\nfrom your local machine’s cli.\\nNOTE: If you logout from the session, the connection would break. Also while creating the spark session notice the block's log because sometimes it fails to run at 4040 and then switches to 4041.\\n~Abhijit Chakrborty: If you are having trouble accessing localhost ports from GCP VM consider adding the forwarding instructions to .ssh/config file as following:\\n```\\nHost <hostname>\\nHostname <external-gcp-ip>\\nUser xxxx\\nIdentityFile yyyy\\nLocalForward 8888 localhost:8888\\nLocalForward 8080 localhost:8080\\nLocalForward 5432 localhost:5432\\nLocalForward 4040 localhost:4040\\n```\\nThis should automatically forward all ports and will enable accessing localhost ports.\", 'section': 'Module 5: pyspark', 'question': 'Jupyter Notebook or SparkUI not loading properly at localhost after port forwarding from VS code?', 'course': 'data-engineering-zoomcamp'}, {'text': '~ Abhijit Chakraborty\\n`sdk list java`  to check for available java sdk versions.\\n`sdk install java 11.0.22-amzn`  as  java-11.0.22-amzn was available for my codespace.\\nclick on Y if prompted to change the default java version.\\nCheck for java version using `java -version `.\\nIf working fine great, else `sdk default java 11.0.22-amzn` or whatever version you have installed.', 'section': 'Module 5: pyspark', 'question': 'Installing Java 11 on codespaces', 'course': 'data-engineering-zoomcamp'}, {'text': 'Sometimes while creating a dataproc cluster on GCP, the following error is encountered.\\nSolution: As mentioned here, sometimes there might not be enough resources in the given region to allocate the request. Usually, gets freed up in a bit and one can create a cluster. – abhirup ghosh\\nSolution 2:  Changing the type of boot-disk from PD-Balanced to PD-Standard, in terraform, helped solve the problem.- Sundara Kumar Padmanabhan', 'section': 'Module 5: pyspark', 'question': \"Error: Insufficient 'SSD_TOTAL_GB' quota. Requested 500.0, available 470.0.\", 'course': 'data-engineering-zoomcamp'}, {'text': \"Pyspark converts the difference of two TimestampType values to Python's native datetime.timedelta object. The timedelta object only stores the duration in terms of days, seconds, and microseconds. Each of the three units of time must be manually converted into hours in order to express the total duration between the two timestamps using only hours.\\nAnother way for achieving this is using the datediff (sql function). It receives this parameters\\nUpper Date: the closest date you have. For example dropoff_datetime\\nLower Date: the farthest date you have.  For example pickup_datetime\\nAnd the result is returned in terms of days, so you could multiply the result for 24 in order to get the hours.\", 'section': 'Module 5: pyspark', 'question': 'Homework - how to convert the time difference of two timestamps to hours', 'course': 'data-engineering-zoomcamp'}, {'text': 'This version combination worked for me:\\nPySpark = 3.3.2\\nPandas = 1.5.3\\n\\nIf it still has an error,', 'section': 'Module 5: pyspark', 'question': 'PicklingError: Could not serialize object: IndexError: tuple index out of range', 'course': 'data-engineering-zoomcamp'}, {'text': \"Run this before SparkSession\\nimport os\\nimport sys\\nos.environ['PYSPARK_PYTHON'] = sys.executable\\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\", 'section': 'Module 5: pyspark', 'question': 'Py4JJavaError: An error occurred while calling o180.showString. : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 6) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.', 'course': 'data-engineering-zoomcamp'}, {'text': \"import os\\nimport sys\\nos.environ['PYSPARK_PYTHON'] = sys.executable\\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\\nDataproc Pricing: https://cloud.google.com/dataproc/pricing#on_gke_pricing\", 'section': 'Module 5: pyspark', 'question': 'RuntimeError: Python in worker has different version 3.11 than that in driver 3.10, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.', 'course': 'data-engineering-zoomcamp'}, {'text': 'Ans: No, you can submit a job to DataProc from your local computer by installing gsutil (https://cloud.google.com/storage/docs/gsutil_install) and configuring it. Then, you can execute the following command from your local computer.\\ngcloud dataproc jobs submit pyspark \\\\\\n--cluster=de-zoomcamp-cluster \\\\\\n--region=europe-west6 \\\\\\ngs://dtc_data_lake_de-zoomcamp-nytaxi/code/06_spark_sql.py \\\\\\n-- \\\\\\n--input_green=gs://dtc_data_lake_de-zoomcamp-nytaxi/pq/green/2020/*/ \\\\\\n--input_yellow=gs://dtc_data_lake_de-zoomcamp-nytaxi/pq/yellow/2020/*/ \\\\\\n--output=gs://dtc_data_lake_de-zoomcamp-nytaxi/report-2020 (edited)', 'section': 'Module 5: pyspark', 'question': 'Dataproc Qn: Is it essential to have a VM on GCP for running Dataproc and submitting jobs ?', 'course': 'data-engineering-zoomcamp'}, {'text': \"AttributeError: 'DataFrame' object has no attribute 'iteritems'\\nthis is because the method inside the pyspark refers to a package that has been already deprecated\\n(https://stackoverflow.com/questions/76404811/attributeerror-dataframe-object-has-no-attribute-iteritems)\\nYou can do this code below, which is mentioned in the stackoverflow link above:\\nQ: DE Zoomcamp 5.6.3 - Setting up a Dataproc Cluster I cannot create a cluster and get this message. I tried many times as the FAQ said, but it didn't work. What can I do?\\nError\\nInsufficient 'SSD_TOTAL_GB' quota. Requested 500.0, available 250.0.\\nRequest ID: 17942272465025572271\\nA: The master and worker nodes are allocated a maximum of 250 GB of memory combined. In the configuration section, adhere to the following specifications:\\nMaster Node:\\nMachine type: n2-standard-2\\nPrimary disk size: 85 GB\\nWorker Node:\\nNumber of worker nodes: 2\\nMachine type: n2-standard-2\\nPrimary disk size: 80 GB\\nYou can allocate up to 82.5 GB memory for worker nodes, keeping in mind that the total memory allocated across all nodes cannot exceed 250 GB.\", 'section': 'Module 5: pyspark', 'question': 'In module 5.3.1, trying to run spark.createDataFrame(df_pandas).show() returns error', 'course': 'data-engineering-zoomcamp'}, {'text': 'The MacOS setup instruction (https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/05-batch/setup/macos.md#installing-java) for setting the JAVA_HOME environment variable is for Intel-based Macs which have a default install location at /usr/local/. If you have an Apple Silicon mac, you will have to set JAVA_HOME to /opt/homebrew/, specifically in your .bashrc or .zshrc:\\nexport JAVA_HOME=\"/opt/homebrew/opt/openjdk/bin\"\\nexport PATH=\"$JAVA_HOME:$PATH\"\\nConfirm that your path was correctly set by running the command: which java\\nYou should expect to see the output:\\n/opt/homebrew/opt/openjdk/bin/java\\nReference: https://docs.brew.sh/Installation', 'section': 'Module 6: streaming with kafka', 'question': 'Setting JAVA_HOME with Homebrew on Apple Silicon', 'course': 'data-engineering-zoomcamp'}, {'text': 'Check Docker Compose File:\\nEnsure that your docker-compose.yaml file is correctly configured with the necessary details for the \"control-center\" service. Check the service name, image name, ports, volumes, environment variables, and any other configurations required for the container to start.\\nOn Mac OSX 12.2.1 (Monterey) I could not start the kafka control center. I opened Docker Desktop and saw docker images still running from week 4, which I did not see when I typed “docker ps.” I deleted them in docker desktop and then had no problem starting up the kafka environment.', 'section': 'Module 6: streaming with kafka', 'question': 'Could not start docker image “control-center” from the docker-compose.yaml file.', 'course': 'data-engineering-zoomcamp'}, {'text': \"Solution from Alexey: create a virtual environment and run requirements.txt and the python files in that environment.\\nTo create a virtual env and install packages (run only once)\\npython -m venv env\\nsource env/bin/activate\\npip install -r ../requirements.txt\\nTo activate it (you'll need to run it every time you need the virtual env):\\nsource env/bin/activate\\nTo deactivate it:\\ndeactivate\\nThis works on MacOS, Linux and Windows - but for Windows the path is slightly different (it's env/Scripts/activate)\\nAlso the virtual environment should be created only to run the python file. Docker images should first all be up and running.\", 'section': 'Module 6: streaming with kafka', 'question': 'Module “kafka” not found when trying to run producer.py', 'course': 'data-engineering-zoomcamp'}, {'text': 'ImportError: DLL load failed while importing cimpl: The specified module could not be found\\nVerify Python Version:\\nMake sure you are using a compatible version of Python with the Avro library. Check the Python version and compatibility requirements specified by the Avro library documentation.\\n... you may have to load librdkafka-5d2e2910.dll in the code. Add this before importing avro:\\nfrom ctypes import CDLL\\nCDLL(\"C:\\\\\\\\Users\\\\\\\\YOUR_USER_NAME\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\dtcde\\\\\\\\Lib\\\\\\\\site-packages\\\\\\\\confluent_kafka.libs\\\\librdkafka-5d2e2910.dll\")\\nIt seems that the error may occur depending on the OS and python version installed.\\nALTERNATIVE:\\nImportError: DLL load failed while importing cimpl\\n✅SOLUTION: $env:CONDA_DLL_SEARCH_MODIFICATION_ENABLE=1 in Powershell.\\nYou need to set this DLL manually in Conda Env.\\nSource: https://githubhot.com/repo/confluentinc/confluent-kafka-python/issues/1186?page=2', 'section': 'Module 6: streaming with kafka', 'question': 'Error importing cimpl dll when running avro examples', 'course': 'data-engineering-zoomcamp'}, {'text': \"✅SOLUTION: pip install confluent-kafka[avro].\\nFor some reason, Conda also doesn't include this when installing confluent-kafka via pip.\\nMore sources on Anaconda and confluent-kafka issues:\\nhttps://github.com/confluentinc/confluent-kafka-python/issues/590\\nhttps://github.com/confluentinc/confluent-kafka-python/issues/1221\\nhttps://stackoverflow.com/questions/69085157/cannot-import-producer-from-confluent-kafka\", 'section': 'Module 6: streaming with kafka', 'question': \"ModuleNotFoundError: No module named 'avro'\", 'course': 'data-engineering-zoomcamp'}, {'text': 'If you get an error while running the command python3 stream.py worker\\nRun pip uninstall kafka-python\\nThen run pip install kafka-python==1.4.6\\nWhat is the use of  Redpanda ?\\nRedpanda: Redpanda is built on top of the Raft consensus algorithm and is designed as a high-performance, low-latency alternative to Kafka. It uses a log-centric architecture similar to Kafka but with different underlying principles.\\nRedpanda is a powerful, yet simple, and cost-efficient streaming data platform that is compatible with Kafka® APIs while eliminating Kafka complexity.', 'section': 'Module 6: streaming with kafka', 'question': 'Error while running python3 stream.py worker', 'course': 'data-engineering-zoomcamp'}, {'text': 'Got this error because the docker container memory was exhausted. The dta file was upto 800MB but my docker container does not have enough memory to handle that.\\nSolution was to load the file in chunks with Pandas, then create multiple parquet files for each dat file I was processing. This worked smoothly and the issue was resolved.', 'section': 'Module 6: streaming with kafka', 'question': 'Negsignal:SIGKILL while converting dta files to parquet format', 'course': 'data-engineering-zoomcamp'}, {'text': 'Copy the file found in the Java example: data-engineering-zoomcamp/week_6_stream_processing/java/kafka_examples/src/main/resources/rides.csv', 'section': 'Module 6: streaming with kafka', 'question': 'data-engineering-zoomcamp/week_6_stream_processing/python/resources/rides.csv is missing', 'course': 'data-engineering-zoomcamp'}, {'text': 'tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.\\nKafka Python Videos - Rides.csv\\nThere is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.', 'section': 'Module 6: streaming with kafka', 'question': 'Kafka- python videos have low audio and hard to follow up', 'course': 'data-engineering-zoomcamp'}, {'text': 'If you have this error, it most likely that your kafka broker docker container is not working.\\nUse docker ps to confirm\\nThen in the docker compose yaml file folder, run docker compose up -d to start all the instances.', 'section': 'Module 6: streaming with kafka', 'question': 'kafka.errors.NoBrokersAvailable: NoBrokersAvailable', 'course': 'data-engineering-zoomcamp'}, {'text': 'Ankush said we can focus on horizontal scaling option.\\n“think of scaling in terms of scaling from consumer end. Or consuming message via horizontal scaling”', 'section': 'Module 6: streaming with kafka', 'question': 'Kafka homwork Q3, there are options that support scaling concept more than the others:', 'course': 'data-engineering-zoomcamp'}, {'text': 'If you get this error, know that you have not built your sparks and juypter images. This images aren’t readily available on dockerHub.\\nIn the spark folder, run ./build.sh from a bash cli to to build all images before running docker compose', 'section': 'Module 6: streaming with kafka', 'question': \"How to fix docker compose error: Error response from daemon: pull access denied for spark-3.3.1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", 'course': 'data-engineering-zoomcamp'}, {'text': 'Run this command in terminal in the same directory (/docker/spark):\\nchmod +x build.sh', 'section': 'Module 6: streaming with kafka', 'question': 'Python Kafka: ./build.sh: Permission denied Error', 'course': 'data-engineering-zoomcamp'}, {'text': 'Restarting all services worked for me:\\ndocker-compose down\\ndocker-compose up', 'section': 'Module 6: streaming with kafka', 'question': 'Python Kafka: ‘KafkaTimeoutError: Failed to update metadata after 60.0 secs.’ when running stream-example/producer.py', 'course': 'data-engineering-zoomcamp'}, {'text': 'While following tutorial 13.2 , when running ./spark-submit.sh streaming.py, encountered the following error:\\n…\\n24/03/11 09:48:36 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\\n24/03/11 09:48:36 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:7077 after 10 ms (0 ms spent in bootstraps)\\n24/03/11 09:48:54 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\\n24/03/11 09:48:56 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077…\\n24/03/11 09:49:16 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\\n24/03/11 09:49:36 WARN StandaloneSchedulerBackend: Application ID is not initialized yet.\\n24/03/11 09:49:36 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\\n…\\npy4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.sql.SparkSession.\\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\\n…\\nSolution:\\nDowngrade your local PySpark to 3.3.1 (same as Dockerfile)\\nThe reason for the failed connection in my case was the mismatch of PySpark versions. You can see that from the logs of spark-master in the docker container.\\nSolution 2:\\nCheck what Spark version your local machine has\\npyspark –version\\nspark-submit –version\\nAdd your version to SPARK_VERSION in build.sh', 'section': 'Module 6: streaming with kafka', 'question': 'Python Kafka: ./spark-submit.sh streaming.py - ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.', 'course': 'data-engineering-zoomcamp'}, {'text': 'Start a new terminal\\nRun: docker ps\\nCopy the CONTAINER ID of the spark-master container\\nRun: docker exec -it <spark_master_container_id> bash\\nRun: cat logs/spark-master.out\\nCheck for the log when the error happened\\nGoogle the error message from there', 'section': 'Module 6: streaming with kafka', 'question': 'Python Kafka: ./spark-submit.sh streaming.py - How to check why Spark master connection fails', 'course': 'data-engineering-zoomcamp'}, {'text': 'Make sure your java version is 11 or 8.\\nCheck your version by:\\njava --version\\nCheck all your versions by:\\n/usr/libexec/java_home -V\\nIf you already have got java 11 but just not selected as default, select the specific version by:\\nexport JAVA_HOME=$(/usr/libexec/java_home -v 11.0.22)\\n(or other version of 11)', 'section': 'Module 6: streaming with kafka', 'question': 'Python Kafka: ./spark-submit.sh streaming.py Error: py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.', 'course': 'data-engineering-zoomcamp'}, {'text': 'In my set up, all of the dependencies listed in gradle.build were not installed in <project_name>-1.0-SNAPSHOT.jar.\\nSolution:\\nIn build.gradle file, I added the following at the end:\\nshadowJar {\\narchiveBaseName = \"java-kafka-rides\"\\narchiveClassifier = \\'\\'\\n}\\nAnd then in the command line ran ‘gradle shadowjar’, and run the script from java-kafka-rides-1.0-SNAPSHOT.jar created by the shadowjar', 'section': 'Module 6: streaming with kafka', 'question': 'Java Kafka: <project_name>-1.0-SNAPSHOT.jar errors: package xxx does not exist even after gradle build', 'course': 'data-engineering-zoomcamp'}, {'text': 'confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\\nfastavro: pip install fastavro\\nAbhirup Ghosh\\nCan install Faust Library for Module 6 Python Version due to dependency conflicts?\\nThe Faust repository and library is no longer maintained - https://github.com/robinhood/faust\\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.', 'section': 'Module 6: streaming with kafka', 'question': 'Python Kafka: Installing dependencies for python3 06-streaming/python/avro_example/producer.py', 'course': 'data-engineering-zoomcamp'}, {'text': 'In the project directory, run:\\njava -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java', 'section': 'Module 6: streaming with kafka', 'question': 'Java Kafka: How to run producer/consumer/kstreams/etc in terminal', 'course': 'data-engineering-zoomcamp'}, {'text': 'For example, when running JsonConsumer.java, got:\\nConsuming form kafka started\\nRESULTS:::0\\nRESULTS:::0\\nRESULTS:::0\\nOr when running JsonProducer.java, got:\\nException in thread \"main\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\\nSolution:\\nMake sure in the scripts in src/main/java/org/example/ that you are running (e.g. JsonConsumer.java, JsonProducer.java), the StreamsConfig.BOOTSTRAP_SERVERS_CONFIG is the correct server url (e.g. europe-west3 from example vs europe-west2)\\nMake sure cluster key and secrets are updated in src/main/java/org/example/Secrets.java (KAFKA_CLUSTER_KEY and KAFKA_CLUSTER_SECRET)', 'section': 'Module 6: streaming with kafka', 'question': 'Java Kafka: When running the producer/consumer/etc java scripts, no results retrieved or no message sent', 'course': 'data-engineering-zoomcamp'}, {'text': 'Situation: in VS Code, usually there will be a triangle icon next to each test. I couldn’t see it at first and had to do some fixes.\\nSolution:\\n(Source)\\nVS Code\\n→ Explorer (first icon on the left navigation bar)\\n→ JAVA PROJECTS (bottom collapsable)\\n→  icon next in the rightmost position to JAVA PROJECTS\\n→  clean Workspace\\n→ Confirm by clicking Reload and Delete\\nNow you will be able to see the triangle icon next to each test like what you normally see in python tests.\\nE.g.:\\nYou can also add classes and packages in this window instead of creating files in the project directory', 'section': 'Module 6: streaming with kafka', 'question': 'Java Kafka: Tests are not picked up in VSCode', 'course': 'data-engineering-zoomcamp'}, {'text': 'In Confluent Cloud:\\nEnvironment → default (or whatever you named your environment as) → The right navigation bar →  “Stream Governance API” →  The URL under “Endpoint”\\nAnd create credentials from Credentials section below it', 'section': 'Module 6: streaming with kafka', 'question': 'Confluent Kafka: Where can I find schema registry URL?', 'course': 'data-engineering-zoomcamp'}, {'text': 'You can check the version of your local spark using spark-submit --version. In the build.sh file of the Python folder, make sure that SPARK_VERSION matches your local version. Similarly, make sure the pyspark you pip installed also matches this version.', 'section': 'Module 6: streaming with kafka', 'question': 'How do I check compatibility of local and container Spark versions?', 'course': 'data-engineering-zoomcamp'}, {'text': 'According to https://github.com/dpkp/kafka-python/\\n“DUE TO ISSUES WITH RELEASES, IT IS SUGGESTED TO USE https://github.com/wbarnha/kafka-python-ng FOR THE TIME BEING”\\nUse pip install kafka-python-ng instead', 'section': 'Project', 'question': 'How to fix the error \"ModuleNotFoundError: No module named \\'kafka.vendor.six.moves\\'\"?', 'course': 'data-engineering-zoomcamp'}, {'text': 'Each submitted project will be evaluated by 3 (three) randomly assigned students that have also submitted the project.\\nYou will also be responsible for grading the projects from 3 fellow students yourself. Please be aware that: not complying to this rule also implies you failing to achieve the Certificate at the end of the course.\\nThe final grade you get will be the median score of the grades you get from the peer reviewers.\\nAnd of course, the peer review criteria for evaluating or being evaluated must follow the guidelines defined here.', 'section': 'Project', 'question': 'How is my capstone project going to be evaluated?', 'course': 'data-engineering-zoomcamp'}, {'text': 'There is only ONE project for this Zoomcamp. You do not need to submit or create two projects. There are simply TWO chances to pass the course. You can use the Second Attempt if you a) fail the first attempt b) do not have the time due to other engagements such as holiday or sickness etc. to enter your project into the first attempt.', 'section': 'Project', 'question': 'Project 1 & Project 2', 'course': 'data-engineering-zoomcamp'}, {'text': 'See a list of datasets here: https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_7_project/datasets.md', 'section': 'Project', 'question': 'Does anyone know nice and relatively large datasets?', 'course': 'data-engineering-zoomcamp'}, {'text': 'You need to redefine the python environment variable to that of your user account', 'section': 'Project', 'question': 'How to run python as start up script?', 'course': 'data-engineering-zoomcamp'}, {'text': 'Initiate a Spark Session\\nspark = (SparkSession\\n.builder\\n.appName(app_name)\\n.master(master=master)\\n.getOrCreate())\\nspark.streams.resetTerminated()\\nquery1 = spark\\n.readStream\\n…\\n…\\n.load()\\nquery2 = spark\\n.readStream\\n…\\n…\\n.load()\\nquery3 = spark\\n.readStream\\n…\\n…\\n.load()\\nquery1.start()\\nquery2.start()\\nquery3.start()\\nspark.streams.awaitAnyTermination() #waits for any one of the query to receive kill signal or error failure. This is asynchronous\\n# On the contrary query3.start().awaitTermination() is a blocking ex call. Works well when we are reading only from one topic.', 'section': 'Project', 'question': 'Spark Streaming - How do I read from multiple topics in the same Spark Session', 'course': 'data-engineering-zoomcamp'}, {'text': 'Transformed data can be moved in to azure blob storage and then it can be moved in to azure SQL DB, instead of moving directly from databricks to Azure SQL DB.', 'section': 'Project', 'question': 'Data Transformation from Databricks to Azure SQL DB', 'course': 'data-engineering-zoomcamp'}, {'text': 'The trial dbt account provides access to dbt API. Job will still be needed to be added manually. Airflow will run the job using a python operator calling the API. You will need to provide api key, job id, etc. (be careful not committing it to Github).\\nDetailed explanation here: https://docs.getdbt.com/blog/dbt-airflow-spiritual-alignment\\nSource code example here: https://github.com/sungchun12/airflow-toolkit/blob/95d40ac76122de337e1b1cdc8eed35ba1c3051ed/dags/examples/dbt_cloud_example.py', 'section': 'Project', 'question': 'Orchestrating dbt with Airflow', 'course': 'data-engineering-zoomcamp'}, {'text': 'https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_api/airflow/providers/google/cloud/operators/dataproc/index.html\\nhttps://airflow.apache.org/docs/apache-airflow-providers-google/stable/_modules/airflow/providers/google/cloud/operators/dataproc.html\\nGive the following roles to you service account:\\nDataProc Administrator\\nService Account User (explanation here)\\nUse DataprocSubmitPySparkJobOperator, DataprocDeleteClusterOperator and  DataprocCreateClusterOperator.\\nWhen using  DataprocSubmitPySparkJobOperator, do not forget to add:\\ndataproc_jars = [\"gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.24.0.jar\"]\\nBecause DataProc does not already have the BigQuery Connector.', 'section': 'Project', 'question': 'Orchestrating DataProc with Airflow', 'course': 'data-engineering-zoomcamp'}, {'text': 'You can trigger your dbt job in Mage pipeline. For this get your dbt cloud api key under settings/Api tokens/personal tokens. Add it safely to  your .env\\nFor example\\ndbt_api_trigger=dbt_**\\nNavigate to job page and find api trigger  link\\nThen create a custom mage Python block with a simple http request like here\\nfrom dotenv import load_dotenv\\nfrom pathlib import Path\\ndotenv_path = Path(\\'/home/src/.env\\')\\nload_dotenv(dotenv_path=dotenv_path)\\ndbt_api_trigger= os.getenv(dbt_api_trigger)\\nurl = f\"https://cloud.getdbt.com/api/v2/accounts/{dbt_account_id}/jobs/<job_id>/run/\"\\nheaders = {\\n        \"Authorization\": f\"Token {dbt_api_trigger}\",\\n        \"Content-Type\": \"application/json\" }\\nbody = {\\n        \"cause\": \"Triggered via API\"\\n    }\\n    response = requests.post(url, headers=headers, json=body)\\nvoila! You triggered dbt job form your mage pipeline.', 'section': 'Project', 'question': 'Orchestrating dbt cloud with Mage', 'course': 'data-engineering-zoomcamp'}, {'text': \"The slack thread : thttps://datatalks-club.slack.com/archives/C01FABYF2RG/p1677678161866999\\nThe question is that sometimes even if you take plenty of effort to document every single step, and we can't even sure if the person doing the peer review will be able to follow-up, so how this criteria will be evaluated?\\nAlex clarifies: “Ideally yes, you should try to re-run everything. But I understand that not everyone has time to do it, so if you check the code by looking at it and try to spot errors, places with missing instructions and so on - then it's already great”\", 'section': 'Project', 'question': 'Project evaluation - Reproducibility', 'course': 'data-engineering-zoomcamp'}, {'text': 'The key valut in Azure cloud is used to store credentials or passwords or secrets of different tech stack used in Azure. For example if u do not want to expose the password in SQL database, then we can save the password under a given name and use them in other Azure stack.', 'section': 'Project', 'question': 'Key Vault in Azure cloud stack', 'course': 'data-engineering-zoomcamp'}, {'text': 'You can get the version of py4j from inside docker using this command\\ndocker exec -it --user airflow airflow-airflow-scheduler-1 bash -c \"ls /opt/spark/python/lib\"', 'section': 'Project', 'question': \"Spark docker - `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\", 'course': 'data-engineering-zoomcamp'}, {'text': 'Either use conda or pip for managing venv, using both of them together will cause incompatibility.\\nIf you’re using conda, install psycopg2 using the conda-forge channel, which may handle the architecture compatibility automatically\\nconda install -c conda-forge psycopg2\\nIf pip, do the normal install\\npip install psycopg2', 'section': 'Project', 'question': 'psycopg2 complains of incompatible environment e.g x86 instead of amd', 'course': 'data-engineering-zoomcamp'}, {'text': 'This is not a FAQ but more of an advice if you want to set up dbt locally, I did it in the following way:\\nI had the postgres instance from week 2 (year 2024) up (the docker-compose)\\nmkdir dbt\\nvi dbt/profiles.yml\\nAnd here I attached this content (only the required fields) and replaced them with the proper values (for instance mine where in the .env file of the folder of week 2 docker stuff)\\ncd dbt && git clone https://github.com/dbt-labs/dbt-starter-project\\nmkdir project && cd project && mv dbt-starter-project/* .\\nMake sure that you align the profile name in profiles.yml with the dbt_project.yml file\\nAdd this line anywhere on the dbt_project.yml file:\\nconfig-version: 2\\ndocker run --network=mage-zoomcamp_default --mount type=bind,source=/<your-path>/dbt/project,target=/usr/app --mount type=bind,source=/<your-path>/profiles.yml,target=/root/.dbt/profiles.yml ghcr.io/dbt-labs/dbt-postgres ls\\nIf you have trouble run\\ndocker run --network=mage-zoomcamp_default --mount type=bind,source=/<your-path>/dbt/project,target=/usr/app --mount type=bind,source=/<your-path>/profiles.yml,target=/root/.dbt/profiles.yml ghcr.io/dbt-labs/dbt-postgres debug', 'section': 'Project', 'question': 'Setting up dbt locally with Docker and Postgres', 'course': 'data-engineering-zoomcamp'}, {'text': 'The following line should be included in pyspark configuration\\n# Example initialization of SparkSession variable\\nspark = (SparkSession.builder\\n.master(...)\\n.appName(...)\\n# Add the following configuration\\n.config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-3.5-bigquery:0.37.0\")\\n)', 'section': 'Project', 'question': 'How to connect Pyspark with BigQuery?', 'course': 'data-engineering-zoomcamp'}, {'text': 'Install the astronomer-cosmos package as a dependency. (see Terraform example).\\nMake a new folder, dbt/, inside the dags/ folder of your Composer GCP bucket and copy paste your dbt-core project there. (see example)\\nEnsure your profiles.yml is configured to authenticate with a service account key. (see BigQuery example)\\nCreate a new DAG using the DbtTaskGroup class and a ProfileConfig specifying a profiles_yml_filepath that points to the location of your JSON key file. (see example)\\nYour dbt lineage graph should now appear as tasks inside a task group like this:', 'section': 'Course Management Form for Homeworks', 'question': 'How to run a dbt-core project as an Airflow Task Group on Google Cloud Composer using a service account JSON key', 'course': 'data-engineering-zoomcamp'}, {'text': 'The display name listed on the leaderboard is an auto-generated randomized name. You can edit it to be a nickname, or your real name, if you prefer. Your entry on the Leaderboard is the one highlighted in teal(?) / light green (?).\\nThe Certificate name should be your actual name that you want to appear on your certificate after completing the course.\\nThe \"Display on Leaderboard\" option indicates whether you want your name to be listed on the course leaderboard.\\nQuestion: Is it possible to create external tables in BigQuery using URLs, such as those from the NY Taxi data website?\\nAnswer: Not really, only Bigtable, Cloud Storage, and Google Drive are supported data stores.', 'section': 'Workshop 1 - dlthub', 'question': 'Edit Course Profile.', 'course': 'data-engineering-zoomcamp'}, {'text': \"Answer: To run the provided code, ensure that the 'dlt[duckdb]' package is installed. You can do this by executing the provided installation command: !pip install dlt[duckdb]. If you’re doing it locally, be sure to also have duckdb pip installed (even before the duckdb package is loaded).\", 'section': 'Workshop 1 - dlthub', 'question': 'How do I install the necessary dependencies to run the code?', 'course': 'data-engineering-zoomcamp'}, {'text': 'If you are running Jupyter Notebook on a fresh new Codespace or in local machine with a new Virtual Environment, you will need this package to run the starter Jupyter Notebook offered by the teacher. Execute this:\\npip install jupyter', 'section': 'Workshop 1 - dlthub', 'question': 'Other packages needed but not listed', 'course': 'data-engineering-zoomcamp'}, {'text': 'Alternatively, you can switch to in-file storage with:', 'section': 'Workshop 1 - dlthub', 'question': 'How can I use DuckDB In-Memory database with dlt ?', 'course': 'data-engineering-zoomcamp'}, {'text': 'After loading, you should have a total of 8 records, and ID 3 should have age 33\\nQuestion: Calculate the sum of ages of all the people loaded as described above\\nThe sum of all eight records\\' respective ages is too big to be in the choices. You need to first filter out the people whose occupation is equal to None in order to get an answer that is close to or present in the given choices. 😃\\n----------------------------------------------------------------------------------------\\nFIXED = use a raw string and keep the file:/// at the start of your file path\\nI\\'m having an issue with the dlt workshop notebook. The \\'Load to Parquet file\\' section specifically. No matter what I change the file path to, it\\'s still saving the dlt files directly to my C drive.\\n# Set the bucket_url. We can also use a local folder\\nos.environ[\\'DESTINATION__FILESYSTEM__BUCKET_URL\\'] = r\\'file:///content/.dlt/my_folder\\'\\nurl = \"https://storage.googleapis.com/dtc_zoomcamp_api/yellow_tripdata_2009-06.jsonl\"\\n# Define your pipeline\\npipeline = dlt.pipeline(\\npipeline_name=\\'my_pipeline\\',\\ndestination=\\'filesystem\\',\\ndataset_name=\\'mydata\\'\\n)\\n# Run the pipeline with the generator we created earlier.\\nload_info = pipeline.run(stream_download_jsonl(url), table_name=\"users\", loader_file_format=\"parquet\")\\nprint(load_info)\\n# Get a list of all Parquet files in the specified folder\\nparquet_files = glob.glob(\\'/content/.dlt/my_folder/mydata/users/*.parquet\\')\\n# show parquet files\\nfor file in parquet_files:\\nprint(file)', 'section': 'Workshop 2 - RisingWave', 'question': 'Homework - dlt Exercise 3 - Merge a generator concerns', 'course': 'data-engineering-zoomcamp'}, {'text': 'Check the contents of the repository with ls - the command.sh file should be in the root folder\\nIf it is not, verify that you had cloned the correct repository - https://github.com/risingwavelabs/risingwave-data-talks-workshop-2024-03-04', 'section': 'Workshop 2 - RisingWave', 'question': 'command.sh Error - source: no such file or directory: command.sh', 'course': 'data-engineering-zoomcamp'}, {'text': \"psql is a command line tool that is installed alongside PostgreSQL DB, but since we've always been running PostgreSQL in a container, you've only got `pgcli`, which lacks the feature to run a sql script into the DB. Besides, having a command line for each database flavor you'll have to deal with as a Data Professional is far from ideal.\\nSo, instead, you can use usql. Check the docs for details on how to install for your OS. On macOS, it supports `homebrew`, and on Windows, it supports scoop.\\nSo, to run the taxi_trips.sql script with usql:\", 'section': 'Workshop 2 - RisingWave', 'question': 'psql - command not found: psql (alternative install)', 'course': 'data-engineering-zoomcamp'}, {'text': 'If you encounter this error and are certain that you have docker compose installed, but typically run it as docker compose without the hyphen, then consider editing command.sh file by removing the hyphen from ‘docker-compose’. Example:\\nstart-cluster() {\\ndocker compose -f docker/docker-compose.yml up -d\\n}', 'section': 'Workshop 2 - RisingWave', 'question': 'Setup - source command.sh - error: “docker-compose” not found', 'course': 'data-engineering-zoomcamp'}, {'text': 'ERROR: The Compose file \\'./docker/docker-compose.yml\\' is invalid because:\\nInvalid top-level property \"x-image\". Valid top-level sections for this Compose file are: version, services, networks, volumes, secrets, configs, and extensions starting with \"x-\".\\nYou might be seeing this error because you\\'re using the wrong Compose file version. Either specify a supported version (e.g \"2.2\" or \"3.3\") and place your service definitions under the `services` key, or omit the `version` key and place your service definitions at the root of the file to use version 1.\\nFor more on the Compose file format versions, see https://docs.docker.com/compose/compose-file/\\nIf you encounter the above error and have docker-compose installed, try updating your version of docker-compose. At the time of reporting this issue (March 17 2024), Ubuntu does not seem to support a docker-compose version high enough to run the required docker images. If you have this error and are on a Ubuntu machine, consider starting a VM with a Debian machine or look for an alternative way to download docker-compose at the latest version on your machine.', 'section': 'Workshop 2 - RisingWave', 'question': 'Setup - start-cluster error: Invalid top-level property x-image', 'course': 'data-engineering-zoomcamp'}, {'text': 'Ans: [source] Yes, it is so that we can observe the changes as we’re working on the queries in real-time. The script is changing the date timestamp to the current time, so our queries with the now()filter would work. Open another terminal tab to copy+paste the queries while the stream-kafka script is running in the background.\\nNoel: I have recently increased this up to 100 at a time, you may pull the latest changes from the repository.', 'section': 'Workshop 2 - RisingWave', 'question': 'stream-kafka Qn: Is it expected that the records are being ingested 10 at a time?', 'course': 'data-engineering-zoomcamp'}, {'text': 'Ans: No, it is not.', 'section': 'Workshop 2 - RisingWave', 'question': 'Setup - Qn: Is kafka install required for the RisingWave workshop? [source]', 'course': 'data-engineering-zoomcamp'}, {'text': 'Ans: about 7GB free for all the containers to be provisioned and then the psql still needs to run and ingest the taxi data, so maybe 10gb in total?', 'section': 'Workshop 2 - RisingWave', 'question': 'Setup - Qn: How much free disk space should we have? [source]', 'course': 'data-engineering-zoomcamp'}, {'text': 'Replace psycopg2==2.9.9 with psycopg2-binary in the requirements.txt file [source] [another]\\nWhen you open another terminal to run the psql, remember to do the source command.sh step for each terminal session\\n---------------------------------------------------------------------------------------------', 'section': 'Workshop 2 - RisingWave', 'question': 'Psycopg2 - issues when running stream-kafka script', 'course': 'data-engineering-zoomcamp'}, {'text': \"If you’re using an Anaconda installation:\\nCd home/\\nConda install gcc\\nSource back to your RisingWave Venv - source .venv/bin/activate\\nPip install psycopg2-binary\\nPip install -r requirements.txt\\nFor some reason this worked - the Conda base doesn’t have the GCC installed - (GNU Compiler Collection) a compiler system that supports various programming languages. Without this the it fails to install pyproject.toml-based projects\\n“It's possible that in your specific environment, the gcc installation was required at the system level rather than within the virtual environment. This can happen if the build process for psycopg2 tries to access system-level dependencies during installation.\\nInstalling gcc in your main Python installation (Conda) would make it available system-wide, allowing any Python environment to access it when necessary for building packages.”\\ngcc stands for GNU Compiler Collection. It is a compiler system developed by the GNU Project that supports various programming languages, including C, C++, Objective-C, and Fortran.\\nGCC is widely used for compiling source code written in these languages into executable programs or libraries. It's a key tool in the software development process, particularly in the compilation stage where source code is translated into machine code that can be executed by a computer's processor.\\nIn addition to compiling source code, GCC also provides various optimization options, debugging support, and extensive documentation, making it a powerful and versatile tool for developers across different platforms and architectures.\\n—-----------------------------------------------------------------------------------\", 'section': 'Workshop 2 - RisingWave', 'question': 'Psycopg2 - `Could not build wheels for psycopg2, which is required to install pyproject.toml-based projects`', 'course': 'data-engineering-zoomcamp'}, {'text': \"Below I have listed some steps I took to rectify this and potentially other minor errors, in Windows:\\nUse the git bash terminal in windows.\\nActivate python venv from git bash: source .venv/Scripts/activate\\nModify the seed_kafka.py file: in the first line, replace python3 with python.\\nNow from git bash, run the seed-kafka cmd. It should work now.\\nAdditional Notes:\\nYou can connect to the RisingWave cluster from Powershell with the command psql -h localhost -p 4566 -d dev -U root , otherwise it asks for a password.\\nThe equivalent of source commands.sh  in Powershell is . .\\\\commands.sh from the workshop directory.\\nHope this can save you from some trouble in case you're doing this workshop on Windows like I am.\\n—--------------------------------------------------------------------------------------\", 'section': 'Workshop 2 - RisingWave', 'question': 'Psycopg2 InternalError: Failed to run the query - when running the seed-kafka command after initial setup.', 'course': 'data-engineering-zoomcamp'}, {'text': 'In case the script gets stuck on\\n%3|1709652240.100|FAIL|rdkafka#producer-2| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connect to ipv4#127.0.0.1:9092 failed: Connection refused (after 0ms in state CONNECT)gre\\nafter trying to load the trip data, check the logs of the message_queue container in docker. If it keeps restarting with Could not initialize seastar: std::runtime_error (insufficient physical memory: needed 4294967296 available 4067422208)  as the last message, then go to the docker-compose file in the docker folder of the project and change the ‘memory’ command for the message_queue service for some lower value.\\nSolution: lower the memory allocation of the service “message_queue” in your docker-compose file from 4GB. If you have the “insufficient physical memory” error message (try 3GB)\\nIssue: Running psql -f risingwave-sql/table/trip_data.sql after starting services with ‘default’ values using docker-compose up gives the error  “psql:risingwave-sql/table/trip_data.sql:61: ERROR:  syntax error at or near \".\" LINE 60:       properties.bootstrap.server=\\'message_queue:29092\\'”\\nSolution: Make sure you have run source commands.sh in each terminal window', 'section': 'Workshop 2 - RisingWave', 'question': 'Running stream-kafka script gets stuck on a loop with Connection Refused', 'course': 'data-engineering-zoomcamp'}, {'text': 'Use seed-kafka instead of stream-kafka to get a static set of results.', 'section': 'Workshop 2 - RisingWave', 'question': 'For the homework questions is there a specific number of records that have to be processed to obtain the final answer?', 'course': 'data-engineering-zoomcamp'}, {'text': 'It is best to use the order by and limit clause on the query to the materialized view instead of the materialized view creation in order to guarantee consistent results\\nHomework - The answers in the homework do not match the provided options: You must follow the following steps: 1. clean-cluster 2. docker volume prune and use seed-kafka instead of stream-kafka. Ensure that the number of records is 100K.', 'section': 'Workshop 2 - RisingWave', 'question': 'Homework - Materialized view does not guarantee order by warning', 'course': 'data-engineering-zoomcamp'}, {'text': 'For this workshop, and if you are following the view from Noel (2024) this requires you to install postgres to use it on your terminal. Found this steps (commands) to get it done [source]:\\nwget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\\nsudo sh -c \\'echo \"deb http://apt.postgresql.org/pub/repos/apt/ $(lsb_release -cs)-pgdg main\" >> /etc/apt/sources.list.d/pgdg.list\\'\\nsudo apt update\\napt install postgresql postgresql-contrib\\n(comment): now let’s check the service for postgresql\\nservice postgresql status\\n(comment) If down: use the next command\\nservice postgresql start\\n(comment) And your are done', 'section': 'Workshop 2 - RisingWave', 'question': 'How to install postgress on Linux like OS', 'course': 'data-engineering-zoomcamp'}, {'text': 'Refer to the solution given in the first solution here:\\nhttps://stackoverflow.com/questions/24683221/xdg-open-no-method-available-even-after-installing-xdg-utils\\nInstead of w3m use any other browser of your choice.\\nIt is just trying to open the index.html file. Which you can do from your File Explorer/Finder. If you’re on wsl try using explorer.exe index.html', 'section': 'Workshop 2 - RisingWave', 'question': 'Unable to Open Dashboard as xdg-open doesn’t open any browser', 'course': 'data-engineering-zoomcamp'}, {'text': 'Example Error:\\nWhen attempting to execute a Python script named seed-kafka.py or server.py with the following shebang line specifying Python 3 as the interpreter:\\nUsers may encounter the following error in a Unix-like environment:\\nThis error indicates that there is a problem with the Python interpreter path specified in the shebang line. The presence of the \\\\r character suggests that the script was edited or created in a Windows environment, causing the interpreter path to be incorrect when executed in Unix-like environments.\\n2 Solutions:\\nEither one or the other\\nUpdate Shebang Line:\\nVerify Python Interpreter Path: Use the which python3 command to determine the path to the Python 3 interpreter available in the current environment.\\nUpdate Shebang Line: Open the script file in a text editor. Modify the shebang line to point to the correct Python interpreter path found in the previous step. Ensure that the shebang line is consistent with the Python interpreter path in the execution environment.\\nExample Shebang Line:\\nReplace /usr/bin/env python3 with the correct Python interpreter path found using which python3.\\nConvert Line Endings:\\nUse the dos2unix command-line tool to convert the line endings of the script from Windows-style to Unix-style.\\nThis removes the extraneous carriage return characters (\\\\r), resolving issues related to unexpected tokens and ensuring compatibility with Unix-like environments.\\nExample Command:', 'section': 'Workshop 2 - RisingWave', 'question': 'Resolving Python Interpreter Path Inconsistencies in Unix-like Environments', 'course': 'data-engineering-zoomcamp'}, {'text': 'Ans : Windowing in streaming SQL involves defining a time-based or row-based boundary for data processing. It allows you to analyze and aggregate data over specific time intervals or based on the number of events received, providing a way to manage and organize streaming data for analysis.', 'section': 'Workshop 2 - RisingWave', 'question': 'How does windowing work in Sql?', 'course': 'data-engineering-zoomcamp'}, {'text': 'Python 3.12.1, is not compatible with kafka-python-2.0.2. Therefore, instead of running \"pip install kafka-python\", you can resolve the issue by using \"pip install git+https://github.com/dpkp/kafka-python.git\". If you have already installed kafka-python, you need to run \"pip uninstall kafka-python\" before executing \"pip install git+https://github.com/dpkp/kafka-python.git\" to resolve the compatibility issue.\\nQ:In the Mage pipeline, individual blocks run successfully. However, when executing the pipeline as a whole, some blocks fail.\\nA: I have the following key-value pair in io_config.yaml file configured but still Mage blocks failed to generate OAuth and authenticate with GCP: GOOGLE_SERVICE_ACC_KEY_FILEPATH: \"{{ env_var(\\'GCP_CREDENTIALS\\') }}\". The GCP_CREDENTIALS variable holds the full path to the service account key\\'s JSON file. Adding the following line within the failed code block resolved the issue: os.environ[\\'GOOGLE_APPLICATION_CREDENTIALS\\'] = os.environ.get(\\'GCP_CREDENTIALS\\').\\nThis occurs because the path to profiles.yml is not correctly specified. You can rectify this by:\\n“export DBT_PROFILES_DBT=path/to/profiles.yml”\\nEg., /home/src/magic-zoomcamp/dbt/project_name/\\nDo the similar for DBT_PROJECT_DIR if getting similar issue with dbt_project.yml.\\nOnce DIRs are set,:\\n“dbt debug –config-dir”\\nThis would update your paths. To maintain same path across sessions, use the path variables in your .env file.\\nTo add triggers in mage pipelines via CLI, you can create a trigger of type API, and copy the API links.\\nEg. link: http://localhost:6789/api/pipeline_schedules/10/pipeline_runs/f3a1a4228fc64cfd85295b668c93f3b2\\nThen create a trigger.py as such:\\nimport os\\nimport requests\\nclass MageTrigger:\\nOPTIONS = {\\n\"<pipeline_name>\": {\\n\"trigger_id\": 10,\\n\"key\": \"f3a1a4228fc64cfd85295b668c93f3b2\"\\n}\\n}\\n@staticmethod\\ndef trigger_pipeline(pipeline_name, variables=None):\\ntrigger_id = MageTrigger.OPTIONS[pipeline_name][\"trigger_id\"]\\nkey = MageTrigger.OPTIONS[pipeline_name][\"key\"]\\nendpoint = f\"http://localhost:6789/api/pipeline_schedules/{trigger_id}/pipeline_runs/{key}\"\\nheaders = {\\'Content-Type\\': \\'application/json\\'}\\npayload = {}\\nif variables is not None:\\npayload[\\'pipeline_run\\'] = {\\'variables\\': variables}\\nresponse = requests.post(endpoint, headers=headers, json=payload)\\nreturn response\\nMageTrigger.trigger_pipeline(\"<pipeline_name>\")\\nFinally, after the mage server is up an running, simply this command:\\npython trigger.py from mage directory in terminal.\\nCan I do data partitioning & clustering run by dbt pipeline, or I would need to do this manually in BigQuery afterwards?\\nYou can use this configuration in your DBT model:\\n{\\n\"field\": \"<field name>\",\\n\"data_type\": \"<timestamp | date | datetime | int64>\",\\n\"granularity\": \"<hour | day | month | year>\"\\n# Only required if data_type is \"int64\"\\n\"range\": {\\n\"start\": <int>,\\n\"end\": <int>,\\n\"interval\": <int>\\n}\\n}\\nand for clustering\\n{{\\nconfig(\\nmaterialized = \"table\",\\ncluster_by = \"order_id\",\\n)\\n}}\\nmore details in: https://docs.getdbt.com/reference/resource-configs/bigquery-configs', 'section': 'Triggers in Mage via CLI', 'question': 'Encountering the error \"ModuleNotFoundError: No module named \\'kafka.vendor.six.moves\\'\" when running \"from kafka import KafkaProducer\" in Jupyter Notebook for Module 6 Homework?', 'course': 'data-engineering-zoomcamp'}, {'text': 'Docker Commands\\n# Create a Docker Image from a base image\\nDocker run -it ubuntu bash\\n#List docker images\\nDocker images list\\n#List  Running containers\\nDocker ps -a\\n#List with full container ids\\nDocker ps -a --no-trunc\\n#Add onto existing image to create new image\\nDocker commit -a <User_Name> -m \"Message\" container_id New_Image_Name\\n# Create a Docker Image with an entrypoint from a base image\\nDocker run -it --entry_point=bash python:3.11\\n#Attach to a stopped container\\nDocker start -ai <Container_Name>\\n#Attach to a running container\\ndocker exec -it <Container_ID> bash\\n#copying from host to container\\nDocker cp <SRC_PATH/file> <containerid>:<dest_path>\\n#copying from container to host\\nDocker cp <containerid>:<Srct_path> <Dest Path on host/file>\\n#Create an image from a docker file\\nDocker build -t <Image_Name> <Location of Dockerfile>\\n#DockerFile Options and best practices\\nhttps://devopscube.com/build-docker-image/\\n#Docker delete all images forcefully\\ndocker rmi -f $(docker images -aq)\\n#Docker delete all containers forcefully\\ndocker rm -f $(docker ps -qa)\\n#docker compose creation\\nhttps://www.composerize.com/\\nGCP Commands\\n1.     Create SSH Keys\\n2.     Added to the Settings of Compute Engine VM Instance\\n3.     SSH-ed into the VM Instance with a config similar to following\\nHost my-website.com\\nHostName my-website.com\\nUser my-user\\nIdentityFile ~/.ssh/id_rsa\\n4.     Installed Anaconda by installing the sh file through bash <Anaconda.sh>\\n5.     Install Docker after\\na.     Sudo apt-get update\\nb.     Sudo apt-get docker\\n6.     To run Docker without SUDO permissions\\na.     https://github.com/sindresorhus/guides/blob/main/docker-without-sudo.md\\n7.     Google cloud remote copy\\na.     gcloud compute scp LOCAL_FILE_PATHVM_NAME:REMOTE_DIR\\nInstall GCP Cloud SDK on Docker Machine\\nhttps://stackoverflow.com/questions/23247943/trouble-installing-google-cloud-sdk-in-ubuntu\\nsudo apt-get install apt-transport-https ca-certificates gnupg && echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main\"| sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list&& curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add - && sudo apt-get update && sudo apt-get install google-cloud-sdk && sudo apt-get install google-cloud-sdk-app-engine-java && sudo apt-get install google-cloud-sdk-app-engine-python && gcloud init\\nAnaconda Commands\\n#Activate environment\\nConda Activate <environment_name>\\n#DeActivate environment\\nConda DeActivate <environment_name>\\n#Start iterm without conda environment\\nconda config --set auto_activate_base false\\n# Using Conda forge as default (Community driven packaging recipes and solutions)\\nhttps://conda-forge.org/docs/user/introduction.html\\nconda --version\\nconda update conda\\nconda config --add channels conda-forge\\nconda config --set channel_priority strict\\n#Using Libmamba as Solver\\nconda install pgcli  --solver=libmamba\\nLinux/MAC Commands\\nStarting and Stopping Services on Linux\\n●  \\tsudo systemctl start postgresql\\n●  \\tsudo systemctl stop postgresql\\nStarting and Stopping Services on MAC\\n●      launchctl start postgresql\\n●      launchctl stop postgresql\\nIdentifying processes listening to a Port across MAC/Linux\\nsudo lsof -i -P -n | grep LISTEN\\n$ sudo netstat -tulpn | grep LISTEN\\n$ sudo ss -tulpn | grep LISTEN\\n$ sudo lsof -i:22 ## see a specific port such as 22 ##\\n$ sudo nmap -sTU -O IP-address-Here\\nInstalling a package on Debian\\nsudo apt install <packagename>\\nListing all package on Debian\\nDpkg -l | grep <packagename>\\nUnInstalling a package on Debian\\nSudo apt remove <packagename>\\nSudo apt autoclean  && sudo apt autoremove\\nList all Processes on Debian/Ubuntu\\nPs -aux\\napt-get update && apt-get install procps\\napt-get install iproute2 for ss -tulpn\\n#Postgres Install\\nsudo sh -c \\'echo \"deb https://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main\" > /etc/apt/sources.list.d/pgdg.list\\'\\nwget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\\nsudo apt-get update\\nsudo apt-get -y install postgresql\\n#Changing Postgresql port to 5432\\n- sudo service postgresql stop - sed -e \\'s/^port.*/port = 5432/\\' /etc/postgresql/10/main/postgresql.conf > postgresql.conf\\n- sudo chown postgres postgresql.conf\\n- sudo mv postgresql.conf /etc/postgresql/10/main\\n- sudo systemctl restart postgresql', 'section': 'Triggers in Mage via CLI', 'question': 'Basic Commands', 'course': 'data-engineering-zoomcamp'}, {'text': 'Machine Learning Zoomcamp FAQ\\nThe purpose of this document is to capture frequently asked technical questions.\\nWe did this for our data engineering course and it worked quite well. Check this document for inspiration on how to structure your questions and answers:\\nData Engineering Zoomcamp FAQ\\nIn the course GitHub repository there’s a link. Here it is: https://airtable.com/shryxwLd0COOEaqXo\\nwork', 'section': 'General course-related questions', 'question': 'How do I sign up?', 'course': 'machine-learning-zoomcamp'}, {'text': 'The course videos are pre-recorded, you can start watching the course right now.\\nWe will also occasionally have office hours - live sessions where we will answer your questions. The office hours sessions are recorded too.\\nYou can see the office hours as well as the pre-recorded course videos in the course playlist on YouTube.', 'section': 'General course-related questions', 'question': 'Is it going to be live? When?', 'course': 'machine-learning-zoomcamp'}, {'text': 'Everything is recorded, so you won’t miss anything. You will be able to ask your questions for office hours in advance and we will cover them during the live stream. Also, you can always ask questions in Slack.', 'section': 'General course-related questions', 'question': 'What if I miss a session?', 'course': 'machine-learning-zoomcamp'}, {'text': \"The bare minimum. The focus is more on practice, and we'll cover the theory only on the intuitive level.: https://mlbookcamp.com/article/python\\nFor example, we won't derive the gradient update rule for logistic regression (there are other great courses for that), but we'll cover how to use logistic regression and make sense of the results.\", 'section': 'General course-related questions', 'question': 'How much theory will you cover?', 'course': 'machine-learning-zoomcamp'}, {'text': \"Yes! We'll cover some linear algebra in the course, but in general, there will be very few formulas, mostly code.\\nHere are some interesting videos covering linear algebra that you can already watch: ML Zoomcamp 1.8 - Linear Algebra Refresher from Alexey Grigorev or the excellent playlist from 3Blue1Brown Vectors | Chapter 1, Essence of linear algebra. Never hesitate to ask the community for help if you have any question.\\n(Mélanie Fouesnard)\", 'section': 'General course-related questions', 'question': \"I don't know math. Can I take the course?\", 'course': 'machine-learning-zoomcamp'}, {'text': \"The process is automated now, so you should receive the email eventually. If you haven’t, check your promotions tab in Gmail as well as spam.\\nIf you unsubscribed from our newsletter, you won't get course related updates too.\\nBut don't worry, it’s not a problem. To make sure you don’t miss anything, join the #course-ml-zoomcamp channel in Slack and our telegram channel with announcements. This is enough to follow the course.\", 'section': 'General course-related questions', 'question': \"I filled the form, but haven't received a confirmation email. Is it normal?\", 'course': 'machine-learning-zoomcamp'}, {'text': 'Approximately 4 months, but may take more if you want to do some extra activities (an extra project, an article, etc)', 'section': 'General course-related questions', 'question': 'How long is the course?', 'course': 'machine-learning-zoomcamp'}, {'text': 'Around ~10 hours per week. Timur Kamaliev did a detailed analysis of how much time students of the previous cohort needed to spend on different modules and projects. Full article', 'section': 'General course-related questions', 'question': 'How much time do I need for this course?', 'course': 'machine-learning-zoomcamp'}, {'text': 'Yes, if you finish at least 2 out of 3 projects and review 3 peers’ Projects by the deadline, you will get a certificate. This is what it looks like: link. There’s also a version without a robot: link.', 'section': 'General course-related questions', 'question': 'Will I get a certificate?', 'course': 'machine-learning-zoomcamp'}, {'text': \"Yes, it's possible. See the previous answer.\", 'section': 'General course-related questions', 'question': 'Will I get a certificate if I missed the midterm project?', 'course': 'machine-learning-zoomcamp'}, {'text': 'Check this article. If you know everything in this article, you know enough. If you don’t, read the article and join the coursIntroduction to Pythone too :)\\nIntroduction to Python – Machine Learning Bookcamp\\nYou can follow this English course from the OpenClassrooms e-learning platform, which is free and covers the python basics for data analysis: Learn Python Basics for Data Analysis - OpenClassrooms . It is important to know some basics such as: how to run a Jupyter notebook, how to import libraries (and what libraries are), how to declare a variable (and what variables are) and some important operations regarding data analysis.\\n(Mélanie Fouesnard)', 'section': 'General course-related questions', 'question': 'How much Python should I know?', 'course': 'machine-learning-zoomcamp'}, {'text': 'For the Machine Learning part, all you need is a working laptop with an internet connection. The Deep Learning part is more resource intensive, but for that you can use a cloud (we use Saturn cloud but can be anything else).\\n(Rileen Sinha; based on response by Alexey on Slack)', 'section': 'General course-related questions', 'question': \"Any particular hardware requirements for the course or everything is mostly cloud? TIA! Couldn't really find this in the FAQ.\", 'course': 'machine-learning-zoomcamp'}, {'text': 'Here is an article that worked for me: https://knowmledge.com/2023/12/07/ml-zoomcamp-2023-project/', 'section': 'General course-related questions', 'question': 'How to setup TensorFlow with GPU support on Ubuntu?', 'course': 'machine-learning-zoomcamp'}, {'text': \"Here’s how you join a in Slack: https://slack.com/help/articles/205239967-Join-a-channel\\nClick “All channels” at the top of your left sidebar. If you don't see this option, click “More” to find it.\\nBrowse the list of public channels in your workspace, or use the search bar to search by channel name or description.\\nSelect a channel from the list to view it.\\nClick Join Channel.\\nDo we need to provide the GitHub link to only our code corresponding to the homework questions?\\nYes. You are required to provide the URL to your repo in order to receive a grade\", 'section': 'General course-related questions', 'question': 'I’m new to Slack and can’t find the course channel. Where is it?', 'course': 'machine-learning-zoomcamp'}, {'text': 'Yes, you can. You won’t be able to submit some of the homeworks, but you can still take part in the course.\\nIn order to get a certificate, you need to submit 2 out of 3 course projects and review 3 peers’ Projects by the deadline. It means that if you join the course at the end of November and manage to work on two projects, you will still be eligible for a certificate.', 'section': 'General course-related questions', 'question': 'The course has already started. Can I still join it?', 'course': 'machine-learning-zoomcamp'}, {'text': 'The course is available in the self-paced mode too, so you can go through the materials at any time. But if you want to do it as a cohort with other students, the next iterations will happen in September 2023, September 2024 (and potentially other Septembers as well).', 'section': 'General course-related questions', 'question': 'When does the next iteration start?', 'course': 'machine-learning-zoomcamp'}, {'text': 'No, it’s not possible. The form is closed after the due date. But don’t worry, homework is not mandatory for finishing the course.', 'section': 'General course-related questions', 'question': 'Can I submit the homework after the due date?', 'course': 'machine-learning-zoomcamp'}, {'text': 'Welcome to the course! Go to the course page (http://mlzoomcamp.com/), scroll down and start going through the course materials. Then read everything in the cohort folder for your cohort’s year.\\nClick on the links and start watching the videos. Also watch office hours from previous cohorts. Go to DTC youtube channel and click on Playlists and search for {course yyyy}. ML Zoomcamp was first launched in 2021.\\nOr you can just use this link: http://mlzoomcamp.com/#syllabus', 'section': 'General course-related questions', 'question': 'I just joined. What should I do next? How can I access course materials?', 'course': 'machine-learning-zoomcamp'}, {'text': 'For the 2023 cohort, you can see the deadlines here (it’s taken from the 2023 cohort page)', 'section': 'General course-related questions', 'question': 'What are the deadlines in this course?', 'course': 'machine-learning-zoomcamp'}, {'text': 'There’s not much difference. There was one special module (BentoML) in the previous iteration of the course, but the rest of the modules are the same as in 2022. The homework this year is different.', 'section': 'General course-related questions', 'question': 'What’s the difference between the previous iteration of the course (2022) and this one (2023)?', 'course': 'machine-learning-zoomcamp'}, {'text': 'We won’t re-record the course videos. The focus of the course and the skills we want to teach remained the same, and the videos are still up-to-date.\\nIf you haven’t taken part in the previous iteration, you can start watching the videos. It’ll be useful for you and you will learn new things. However, we recommend using Python 3.10 now instead of Python 3.8.', 'section': 'General course-related questions', 'question': 'The course videos are from the previous iteration. Will you release new ones or we’ll use the videos from 2021?', 'course': 'machine-learning-zoomcamp'}, {'text': 'When you post about what you learned from the course on your social media pages, use the tag #mlzoomcamp. When you submit your homework, there’s a section in the form for putting the links there. Separate multiple links by any whitespace character (linebreak, space, tab, etc).\\nFor posting the learning in public links, you get extra scores. But the number of scores is limited to 7 points: if you put more than 7 links in your homework form, you’ll get only 7 points.\\nThe same content can be posted to 7 different social sites and still earn you 7 points if you add 7 URLs per week, see Alexey’s reply. (~ ellacharmed)\\nFor midterms/capstones, the awarded points are doubled as the duration is longer. So for projects the points are capped at 14 for 14 URLs.', 'section': 'General course-related questions', 'question': 'Submitting learning in public links', 'course': 'machine-learning-zoomcamp'}, {'text': \"You can create your own github repository for the course with your notes, homework, projects, etc.\\nThen fork the original course repo and add a link under the 'Community Notes' section to the notes that are in your own repo.\\nAfter that's done, create a pull request to sync your fork with the original course repo.\\n(By Wesley Barreto)\", 'section': 'General course-related questions', 'question': 'Adding community notes', 'course': 'machine-learning-zoomcamp'}, {'text': \"Leaderboard Links:\\n2023 - https://docs.google.com/spreadsheets/d/e/2PACX-1vSNK_yGtELX1RJK1SSRl4xiUbD0XZMYS6uwHnybc7Mql-WMnMgO7hHSu59w-1cE7FeFZjkopbh684UE/pubhtml\\n2022 - https://docs.google.com/spreadsheets/d/e/2PACX-1vQzLGpva63gb2rIilFnpZMRSb-buyr5oGh8jmDtIb8DANo4n6hDalra_WRCl4EZwO1JvaC4UIS62n5h/pubhtml\\nPython Code:\\nfrom hashlib import sha1\\ndef compute_hash(email):\\nreturn sha1(email.lower().encode('utf-8')).hexdigest()\\nYou need to call the function as follows:\\nprint(compute_hash('YOUR_EMAIL_HERE'))\\nThe quotes are required to denote that your email is a string.\\n(By Wesley Barreto)\\nYou can also use this website directly by entering your email: http://www.sha1-online.com. Then, you just have to copy and paste your hashed email in the “research” bar of the leaderboard to get your scores.\\n(Mélanie Fouesnard)\", 'section': '1. Introduction to Machine Learning', 'question': 'Computing the hash for the leaderboard and project review', 'course': 'machine-learning-zoomcamp'}, {'text': 'If you get “wget is not recognized as an internal or external command”, you need to install it.\\nOn Ubuntu, run\\nsudo apt-get install wget\\nOn Windows, the easiest way to install wget is to use Chocolatey:\\nchoco install wget\\nOr you can download a binary from here and put it to any location in your PATH (e.g. C:/tools/)\\nOn Mac, the easiest way to install wget is to use brew.\\nBrew install wget\\nAlternatively, you can use a Python wget library, but instead of simply using “wget” you’ll need eeeto use\\npython -m wget\\nYou need to install it with pip first:\\npip install wget\\nAnd then in your python code, for example in your jupyter notebook, use:\\nimport wget\\nwget.download(\"URL\")\\nThis should download whatever is at the URL in the same directory as your code.\\n(Memoona Tahira)\\nAlternatively, you can read a CSV file from a URL directly with pandas:\\nurl = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\ndf = pd.read_csv(url)\\nValid URL schemes include http, ftp, s3, gs, and file.\\nIn some cases you might need to bypass https checks:\\nimport ssl\\nssl._create_default_https_context = ssl._create_unverified_context\\nOr you can use the built-in Python functionality for downloading the files:\\nimport urllib.request\\nurl = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\nurllib.request.urlretrieve(url, \"housing.csv\")\\nUrllib.request.urlretrieve() is a standard Python library function available on all devices and platforms. URL requests and URL data retrieval are done with the urllib.request module.\\nThe urlretrieve() function allows you to download files from URLs and save them locally. Python programs use it to download files from the internet.\\nOn any Python-enabled device or platform, you can use the urllib.request.urlretrieve() function to download the file.\\n(Mohammad Emad Sharifi)', 'section': '1. Introduction to Machine Learning', 'question': 'wget is not recognized as an internal or external command', 'course': 'machine-learning-zoomcamp'}, {'text': 'You can use\\n!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nTo download the data too. The exclamation mark !, lets you execute shell commands inside your notebooks. This works generally for shell commands such as ls, cp, mkdir, mv etc . . .\\nFor instance, if you then want to move your data into a data directory alongside your notebook-containing directory, you could execute the following:\\n!mkdir -p ../data/\\n!mv housing.csv ../data/', 'section': '1. Introduction to Machine Learning', 'question': 'Retrieving csv inside notebook', 'course': 'machine-learning-zoomcamp'}, {'text': '(Tyler Simpson)', 'section': '1. Introduction to Machine Learning', 'question': 'Windows WSL and VS Code\\nIf you have a Windows 11 device and would like to use the built in WSL to access linux you can use the Microsoft Learn link Set up a WSL development environment | Microsoft Learn. To connect this to VS Code download the Microsoft verified VS Code extension ‘WSL’ this will allow you to remotely connect to your WSL Ubuntu instance as if it was a virtual machine.', 'course': 'machine-learning-zoomcamp'}, {'text': 'This is my first time using Github to upload a code. I was getting the below error message when I type\\ngit push -u origin master:\\nerror: src refspec master does not match any\\nerror: failed to push some refs to \\'https://github.com/XXXXXX/1st-Homework.git\\'\\nSolution:\\nThe error message got fixed by running below commands:\\ngit commit -m \"initial commit\"\\ngit push origin main\\nIf this is your first time to use Github, you will find a great & straightforward tutorial in this link https://dennisivy.com/github-quickstart\\n(Asia Saeed)\\nYou can also use the “upload file” functionality from GitHub for that\\nIf you write your code on Google colab you can also directly share it on your Github.\\n(By Pranab Sarma)', 'section': '1. Introduction to Machine Learning', 'question': 'Uploading the homework to Github', 'course': 'machine-learning-zoomcamp'}, {'text': \"I'm trying to invert the matrix but I got error that the matrix is singular matrix\\nThe singular matrix error is caused by the fact that not every matrix can be inverted. In particular, in the homework it happens because you have to pay close attention when dealing with multiplication (the method .dot) since multiplication is not commutative! X.dot(Y) is not necessarily equal to Y.dot(X), so respect the order otherwise you get the wrong matrix.\", 'section': '1. Introduction to Machine Learning', 'question': 'Singular Matrix Error', 'course': 'machine-learning-zoomcamp'}, {'text': 'I have a problem with my terminal. Command\\nconda create -n ml-zoomcamp python=3.9\\ndoesn’t work. Any of 3.8/ 3.9 / 3.10 should be all fine\\nIf you’re on Windows and just installed Anaconda, you can use Anaconda’s own terminal called “Anaconda Prompt”.\\nIf you don’t have Anaconda or Miniconda, you should install it first\\n(Tatyana Mardvilko)', 'section': '1. Introduction to Machine Learning', 'question': 'Conda is not an internal command', 'course': 'machine-learning-zoomcamp'}, {'text': 'How do I read the dataset with Pandas in Windows?\\nI used the code below but not working\\ndf = pd.read_csv(\\'C:\\\\Users\\\\username\\\\Downloads\\\\data.csv\\')\\nUnlike Linux/Mac OS, Windows uses the backslash (\\\\) to navigate the files that cause the conflict with Python. The problem with using the backslash is that in Python, the \\'\\\\\\' has a purpose known as an escape sequence. Escape sequences allow us to include special characters in strings, for example, \"\\\\n\" to add a new line or \"\\\\t\" to add spaces, etc. To avoid the issue we just need to add \"r\" before the file path and Python will treat it as a literal string (not an escape sequence).\\nHere’s how we should be loading the file instead:\\ndf = pd.read_csv(r\\'C:\\\\Users\\\\username\\\\Downloads\\\\data.csv\\')\\n(Muhammad Awon)', 'section': '1. Introduction to Machine Learning', 'question': 'Read-in the File in Windows OS', 'course': 'machine-learning-zoomcamp'}, {'text': 'Type the following command:\\ngit config -l | grep url\\nThe output should look like this:\\nremote.origin.url=https://github.com/github-username/github-repository-name.git\\nChange this to the following format and make sure the change is reflected using command in step 1:\\ngit remote set-url origin \"https://github-username@github.com/github-username/github-repository-name.git\"\\n(Added by Dheeraj Karra)', 'section': '1. Introduction to Machine Learning', 'question': \"'403 Forbidden' error message when you try to push to a GitHub repository\", 'course': 'machine-learning-zoomcamp'}, {'text': \"I had a problem when I tried to push my code from Git Bash:\\nremote: Support for password authentication was removed on August 13, 2021.\\nremote: Please see https://docs.github.com/en/get-started/getting-started-with-git/about-remote-repositories#cloning-with-https-urls for information on currently recommended modes of authentication.\\nfatal: Authentication failed for 'https://github.com/username\\nSolution:\\nCreate a personal access token from your github account and use it when you make a push of your last changes.\\nhttps://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent\\nBruno Bedón\", 'section': '1. Introduction to Machine Learning', 'question': \"Fatal: Authentication failed for 'https://github.com/username\", 'course': 'machine-learning-zoomcamp'}, {'text': \"In Kaggle, when you are trying to !wget a dataset from github (or any other public repository/location), you get the following error:\\nGetting  this error while trying to import data- !wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\n--2022-09-17 16:55:24--  https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... failed: Temporary failure in name resolution.\\nwget: unable to resolve host address 'raw.githubusercontent.com'\\nSolution:\\nIn your Kaggle notebook settings, turn on the Internet for your session. It's on the settings panel, on the right hand side of the Kaggle screen. You'll be asked to verify your phone number so Kaggle knows you are not a bot.\", 'section': '1. Introduction to Machine Learning', 'question': \"wget: unable to resolve host address 'raw.githubusercontent.com'\", 'course': 'machine-learning-zoomcamp'}, {'text': 'I found this video quite helpful: Creating Virtual Environment for Python from VS Code\\n[Native Jupiter Notebooks support in VS Code] In VS Code you can also have a native Jupiter Notebooks support, i.e. you do not need to open a web browser to code in a Notebook. If you have port forwarding enabled + run a ‘jupyter notebook ‘ command from a remote machine + have a remote connection configured in .ssh/config (as Alexey’s video suggests) - VS Code can execute remote Jupyter Notebooks files on a remote server from your local machine: https://code.visualstudio.com/docs/datascience/jupyter-notebooks.\\n[Git support from VS Code] You can work with Github from VSCode - staging and commits are easy from the VS Code’s UI:  https://code.visualstudio.com/docs/sourcecontrol/overview\\n(Added by Ivan Brigida)', 'section': '1. Introduction to Machine Learning', 'question': 'Setting up an environment using VS Code', 'course': 'machine-learning-zoomcamp'}, {'text': 'With regards to creating an environment for the project, do we need to run the command \"conda create -n .......\" and \"conda activate ml-zoomcamp\" everytime we open vs code to work on the project?\\nAnswer:\\n\"conda create -n ....\" is just run the first time to create the environment. Once created, you just need to run \"conda activate ml-zoomcamp\" whenever you want to use it.\\n(Added by Wesley Barreto)\\nconda env export > environment.yml will also allow you to reproduce your existing environment in a YAML file.  You can then recreate it with conda env create -f environment.yml', 'section': '1. Introduction to Machine Learning', 'question': 'Conda Environment Setup', 'course': 'machine-learning-zoomcamp'}, {'text': \"I was doing Question 7 from Week1 Homework and with step6: Invert XTX, I created the inverse. Now, an inverse when multiplied by the original matrix should return in an Identity matrix. But when I multiplied the inverse with the original matrix, it gave a matrix like this:\\nInverse * Original:\\n[[ 1.00000000e+00 -1.38777878e-16]\\n[ 3.16968674e-13  1.00000000e+00]]\\nSolution:\\nIt's because floating point math doesn't work well on computers as shown here: https://stackoverflow.com/questions/588004/is-floating-point-math-broken\\n(Added by Wesley Barreto)\", 'section': '1. Introduction to Machine Learning', 'question': 'Floating Point Precision', 'course': 'machine-learning-zoomcamp'}, {'text': 'Answer:\\nIt prints the information about the dataset like:\\nIndex datatype\\nNo. of entries\\nColumn information with not-null count and datatype\\nMemory usage by dataset\\nWe use it as:\\ndf.info()\\n(Added by Aadarsha Shrestha & Emoghena Itakpe)', 'section': '1. Introduction to Machine Learning', 'question': 'What does pandas.DataFrame.info() do?', 'course': 'machine-learning-zoomcamp'}, {'text': \"Pandas and numpy libraries are not being imported\\nNameError: name 'np' is not defined\\nNameError: name 'pd' is not defined\\nIf you're using numpy or pandas, make sure you use the first few lines before anything else.\\nimport pandas as pd\\nimport numpy as np\\nAdded by Manuel Alejandro Aponte\", 'section': '1. Introduction to Machine Learning', 'question': \"NameError: name 'np' is not defined\", 'course': 'machine-learning-zoomcamp'}, {'text': \"What if there were hundreds of columns? How do you get the columns only with numeric or object data in a more concise way?\\ndf.select_dtypes(include=np.number).columns.tolist()\\ndf.select_dtypes(include='object').columns.tolist()\\nAdded by Gregory Morris\", 'section': '1. Introduction to Machine Learning', 'question': 'How to select column by dtype', 'course': 'machine-learning-zoomcamp'}, {'text': 'There are many ways to identify the shape of dataset, one of them is using .shape attribute!\\ndf.shape\\ndf.shape[0] # for identify the number of rows\\ndf.shape[1] # for identify the number of columns\\nAdded by Radikal Lukafiardi', 'section': '1. Introduction to Machine Learning', 'question': 'How to identify the shape of dataset in Pandas', 'course': 'machine-learning-zoomcamp'}, {'text': 'First of all use np.dot for matrix multiplication. When you compute matrix-matrix multiplication you should understand that order of multiplying is crucial and affects the result of the multiplication!\\nDimension Mismatch\\nTo perform matrix multiplication, the number of columns in the 1st matrix should match the number of rows in the 2nd matrix. You can rearrange the order to make sure that this satisfies the condition.\\nAdded by Leah Gotladera', 'section': '1. Introduction to Machine Learning', 'question': 'How to avoid Value errors with array shapes in homework?', 'course': 'machine-learning-zoomcamp'}, {'text': 'You would first get the average of the column and save it to a variable, then replace the NaN values with the average variable.\\nThis method is called imputing - when you have NaN/ null values in a column, but you do not want to get rid of the row because it has valuable information contributing to other columns.\\nAdded by Anneysha Sarkar', 'section': '1. Introduction to Machine Learning', 'question': 'Question 5: How and why do we replace the NaN values with average of the column?', 'course': 'machine-learning-zoomcamp'}, {'text': 'In Question 7 we are asked to calculate\\nThe initial problem  can be solved by this, where a Matrix X is multiplied by some unknown weights w resulting in the target y.\\nAdditional reading and videos:\\nOrdinary least squares\\nMultiple Linear Regression in Matrix Form\\nPseudoinverse Solution to OLS\\nAdded by Sylvia Schmitt\\nwith commends from Dmytro Durach', 'section': '1. Introduction to Machine Learning', 'question': 'Question 7: Mathematical formula for linear regression', 'course': 'machine-learning-zoomcamp'}, {'text': 'This is most likely that you interchanged the first step of the multiplication\\nYou used  instead of\\nAdded by Emmanuel Ikpesu', 'section': '1. Introduction to Machine Learning', 'question': 'Question 7: FINAL MULTIPLICATION not having 5 column', 'course': 'machine-learning-zoomcamp'}, {'text': 'Note, that matrix multiplication (matrix-matrix, matrix-vector multiplication) can be written as * operator in some sources, but performed as @ operator or np.matmul() via numpy. * operator performs element-wise multiplication (Hadamard product).\\nnumpy.dot() or ndarray.dot() can be used, but for matrix-matrix multiplication @ or np.matmul() is preferred (as per numpy doc).\\nIf multiplying by a scalar numpy.multiply() or * is preferred.\\nAdded by Andrii Larkin', 'section': '1. Introduction to Machine Learning', 'question': 'Question 7: Multiplication operators.', 'course': 'machine-learning-zoomcamp'}, {'text': 'If you face an error kind of ImportError: cannot import name \\'contextfilter\\' from \\'jinja2\\' (anaconda\\\\lib\\\\site-packages\\\\jinja2\\\\__init__.py) when launching a new notebook for a brand new environment.\\nSwitch to the main environment and run \"pip install nbconvert --upgrade\".\\nAdded by George Chizhmak', 'section': '1. Introduction to Machine Learning', 'question': 'Error launching Jupyter notebook', 'course': 'machine-learning-zoomcamp'}, {'text': 'If you face this situation and see IPv6 addresses in the terminal, go to your System Settings > Network > your network connection > Details > Configure IPv6 > set to Manually > OK. Then try again', 'section': '1. Introduction to Machine Learning', 'question': 'wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv hangs on MacOS Ventura M1', 'course': 'machine-learning-zoomcamp'}, {'text': \"Wget doesn't ship with macOS, so there are other alternatives to use.\\nNo worries, we got curl:\\nexample:\\ncurl -o ./housing.csv https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nExplanations:\\ncurl: a utility for retrieving information from the internet.\\n-o: Tell it to store the result as a file.\\nfilename: You choose the file's name.\\nLinks: Put the web address (URL) here, and cURL will extract data from it and save it under the name you provide.\\nMore about it at:\\nCurl Documentation\\nAdded by David Espejo\", 'section': '1. Introduction to Machine Learning', 'question': 'In case you are using mac os and having trouble with WGET', 'course': 'machine-learning-zoomcamp'}, {'text': \"You can use round() function or f-strings\\nround(number, 4)  - this will round number up to 4 decimal places\\nprint(f'Average mark for the Homework is {avg:.3f}') - using F string\\nAlso there is pandas.Series. round idf you need to round values in the whole Series\\nPlease check the documentation\\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.round.html#pandas.Series.round\\nAdded by Olga Rudakova\", 'section': '2. Machine Learning for Regression', 'question': 'How to output only a certain number of decimal places', 'course': 'machine-learning-zoomcamp'}, {'text': 'Here are the crucial links for this Week 2 that starts September 18, 2023\\nAsk questions for Live Sessions: https://app.sli.do/event/vsUpjYsayZ8A875Hq8dpUa/live/questions\\nCalendar for weekly meetings: https://calendar.google.com/calendar/u/0/r?cid=cGtjZ2tkbGc1OG9yb2lxa2Vwc2g4YXMzMmNAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ&pli=1\\nWeek 2 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/02-regression/homework.md\\nSubmit HW Week 2: https://docs.google.com/forms/d/e/1FAIpQLSf8eMtnErPFqzzFsEdLap_GZ2sMih-H-Y7F_IuPGqt4fOmOJw/viewform (also available at the bottom of the above link)\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYoutube Link: 2.X --- https://www.youtube.com/watch?v=vM3SqPNlStE&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=12\\nFAQs: https://docs.google.com/document/d/1LpPanc33QJJ6BSsyxVg-pWNMplal84TdZtq10naIhD8/edit#heading=h.lpz96zg7l47j\\n~~Nukta Bhatia~~', 'section': '2. Machine Learning for Regression', 'question': 'How do I get started with Week 2?', 'course': 'machine-learning-zoomcamp'}, {'text': 'We can use histogram:\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n# Load the data\\nurl = \\'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\'\\ndf = pd.read_csv(url)\\n# EDA\\nsns.histplot(df[\\'median_house_value\\'], kde=False)\\nplt.show()\\nOR ceck skewness and describe:\\nprint(df[\\'median_house_value\\'].describe())\\n# Calculate the skewness of the \\'median_house_value\\' variable\\nskewness = df[\\'median_house_value\\'].skew()\\n# Print the skewness value\\nprint(\"Skewness of \\'median_house_value\\':\", skewness)\\n(Mohammad Emad Sharifi)', 'section': '2. Machine Learning for Regression', 'question': 'Checking long tail of data', 'course': 'machine-learning-zoomcamp'}, {'text': 'It’s possible that when you follow the videos, you’ll get a Singular Matrix error. We will explain why it happens in the Regularization video. Don’t worry, it’s normal that you have it.\\nYou can also have an error because you did the inverse of X once in your code and you’re doing it a second time.\\n(Added by Cécile Guillot)', 'section': '2. Machine Learning for Regression', 'question': 'LinAlgError: Singular matrix', 'course': 'machine-learning-zoomcamp'}, {'text': 'You can find a detailed description of the dataset ere https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html\\nKS', 'section': '2. Machine Learning for Regression', 'question': 'California housing dataset', 'course': 'machine-learning-zoomcamp'}, {'text': 'I was using for loops to apply rmse to list of y_val and y_pred. But the resulting rmse is all nan.\\nI found out that the problem was when my data reached the mean step after squaring the error in the rmse function. Turned out there were nan in the array, then I traced the problem back to where I first started to split the data: I had only use fillna(0) on the train data, not on the validation and test data. So the problem was fixed after I applied fillna(0) to all the dataset (train, val, test). Voila, my for loops to get rmse from all the seed values work now.\\nAdded by Sasmito Yudha Husada', 'section': '2. Machine Learning for Regression', 'question': 'Getting NaNs after applying .mean()', 'course': 'machine-learning-zoomcamp'}, {'text': 'Why should we transform the target variable to logarithm distribution? Do we do this for all machine learning projects?\\nOnly if you see that your target is highly skewed. The easiest way to evaluate this is by plotting the distribution of the target variable.\\nThis can help to understand skewness and how it can be applied to the distribution of your data set.\\nhttps://en.wikipedia.org/wiki/Skewness\\nPastor Soto', 'section': '2. Machine Learning for Regression', 'question': 'Target variable transformation', 'course': 'machine-learning-zoomcamp'}, {'text': 'The dataset can be read directly to pandas dataframe from the github link using the technique shown below\\ndfh=pd.read_csv(\"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\")\\nKrishna Anand', 'section': '2. Machine Learning for Regression', 'question': 'Reading the dataset directly from github', 'course': 'machine-learning-zoomcamp'}, {'text': \"For users of kaggle notebooks, the dataset can be loaded through widget using the below command. Please remember that ! before wget is essential\\n!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nOnce the dataset is loaded to the kaggle notebook server, it can be read through the below pandas command\\ndf = pd.read_csv('housing.csv')\\nHarish Balasundaram\", 'section': '2. Machine Learning for Regression', 'question': 'Loading the dataset directly through Kaggle Notebooks', 'course': 'machine-learning-zoomcamp'}, {'text': 'We can filter a dataset by using its values as below.\\ndf = df[(df[\"ocean_proximity\"] == \"<1H OCEAN\") | (df[\"ocean_proximity\"] == \"INLAND\")]\\nYou can use | for ‘OR’, and & for ‘AND’\\nAlternative:\\ndf = df[df[\\'ocean_proximity\\'].isin([\\'<1H OCEAN\\', \\'INLAND\\'])]\\nRadikal Lukafiardi', 'section': '2. Machine Learning for Regression', 'question': 'Filter a dataset by using its values', 'course': 'machine-learning-zoomcamp'}, {'text': 'Above users showed how to load the dataset directly from github. Here is another useful way of doing this using the `requests` library:\\n# Get data for homework\\nimport requests\\nurl = \\'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\'\\nresponse = requests.get(url)\\nif response.status_code == 200:\\nwith open(\\'housing.csv\\', \\'wb\\') as file:\\nfile.write(response.content)\\nelse:\\nprint(\"Download failed.\")\\nTyler Simpson', 'section': '2. Machine Learning for Regression', 'question': 'Alternative way to load the data using requests', 'course': 'machine-learning-zoomcamp'}, {'text': 'When creating a duplicate of your dataframe by doing the following:\\nX_train = df_train\\nX_val = df_val\\nYou’re still referencing the original variable, this is called a shallow copy. You can make sure that no references are attaching both variables and still keep the copy of the data do the following to create a deep copy:\\nX_train = df_train.copy()\\nX_val = df_val.copy()\\nAdded by Ixchel García', 'section': '2. Machine Learning for Regression', 'question': 'Null column is appearing even if I applied .fillna()', 'course': 'machine-learning-zoomcamp'}, {'text': 'Yes, you can. Here we implement it ourselves to better understand how it works, but later we will only rely on Scikit-Learn’s functions. If you want to start using it earlier — feel free to do it', 'section': '2. Machine Learning for Regression', 'question': 'Can I use Scikit-Learn’s train_test_split for this week?', 'course': 'machine-learning-zoomcamp'}, {'text': 'Yes, you can. We will also do that next week, so don’t worry, you will learn how to do it.', 'section': '2. Machine Learning for Regression', 'question': 'Can I use LinearRegression from Scikit-Learn for this week?', 'course': 'machine-learning-zoomcamp'}, {'text': 'What are equivalents in Scikit-Learn for the linear regression with and without regularization used in week 2.\\nCorresponding function for model without regularization:\\nsklearn.linear_model.LinearRegression\\nCorresponding function for model with regularization:\\nsklearn.linear_model.Ridge\\nThe linear model from Scikit-Learn are explained  here:\\nhttps://scikit-learn.org/stable/modules/linear_model.html\\nAdded by Sylvia Schmitt', 'section': '2. Machine Learning for Regression', 'question': 'Corresponding Scikit-Learn functions for Linear Regression (with and without Regularization)', 'course': 'machine-learning-zoomcamp'}, {'text': '`r` is a regularization parameter.\\nIt’s similar to `alpha` in sklearn.Ridge(), as both control the \"strength\" of regularization (increasing both will lead to stronger regularization), but mathematically not quite, here\\'s how both are used:\\nsklearn.Ridge()\\n||y - Xw||^2_2 + alpha * ||w||^2_2\\nlesson’s notebook (`train_linear_regression_reg` function)\\nXTX = XTX + r * np.eye(XTX.shape[0])\\n`r` adds “noise” to the main diagonal to prevent multicollinearity, which “breaks” finding inverse matrix.', 'section': '2. Machine Learning for Regression', 'question': 'Question 4: what is `r`, is it the same as `alpha` in sklearn.Ridge()?', 'course': 'machine-learning-zoomcamp'}, {'text': 'Q: “In lesson 2.8 why is y_pred different from y? After all, we trained X_train to get the weights that when multiplied by X_train should give exactly y, or?”\\nA: linear regression is a pretty simple model, it neither can nor should fit 100% (nor any other model, as this would be the sign of overfitting). This picture might illustrate some intuition behind this, imagine X is a single feature:\\nAs our model is linear, how would you draw a line to fit all the \"dots\"?\\nYou could \"fit\" all the \"dots\" on this pic using something like scipy.optimize.curve_fit (non-linear least squares) if you wanted to, but imagine how it would perform on previously unseen data.\\nAdded by Andrii Larkin', 'section': '2. Machine Learning for Regression', 'question': 'Why linear regression doesn’t provide a “perfect” fit?', 'course': 'machine-learning-zoomcamp'}, {'text': 'One of the questions on the homework calls for using a random seed of 42. When using 42, all my missing values ended up in my training dataframe and not my validation or test dataframes, why is that?\\nThe purpose of the seed value is to randomly generate the proportion split. Using a seed of 42 ensures that all learners are on the same page by getting the same behavior (in this case, all missing values ending up in the training dataframe). If using a different seed value (e.g. 9), missing values will then appear in all other dataframes.', 'section': '2. Machine Learning for Regression', 'question': 'Random seed 42', 'course': 'machine-learning-zoomcamp'}, {'text': 'It is possible to do the shuffling of the dataset with the pandas built-in function pandas.DataFrame.sample.The complete dataset can be shuffled including resetting the index with the following commands:\\nSetting frac=1 will result in returning a shuffled version of the complete Dataset.\\nSetting random_state=seed will result in the same randomization as used in the course resources.\\ndf_shuffled = df.sample(frac=1, random_state=seed)\\ndf_shuffled.reset_index(drop=True, inplace=True)\\nAdded by Sylvia Schmitt', 'section': '2. Machine Learning for Regression', 'question': 'Shuffling the initial dataset using pandas built-in function', 'course': 'machine-learning-zoomcamp'}, {'text': 'That’s normal. We all have different environments: our computers have different versions of OS and different versions of libraries — even different versions of Python.\\nIf it’s the case, just select the option that’s closest to your answer', 'section': '2. Machine Learning for Regression', 'question': \"The answer I get for one of the homework questions doesn't match any of the options. What should I do?\", 'course': 'machine-learning-zoomcamp'}, {'text': \"In question 3 of HW02 it is mentioned: ‘For computing the mean, use the training only’. What does that mean?\\nIt means that you should use only the training data set for computing the mean, not validation or  test data set. This is how you can calculate the mean\\ndf_train['column_name'].mean( )\\nAnother option:\\ndf_train[‘column_name’].describe()\\n(Bhaskar Sarma)\", 'section': '2. Machine Learning for Regression', 'question': 'Meaning of mean in homework 2, question 3', 'course': 'machine-learning-zoomcamp'}, {'text': 'When the target variable has a long tail distribution, like in prices, with a wide range, you can transform the target variable with np.log1p() method, but be aware if your target variable has negative values, this method will not work', 'section': '2. Machine Learning for Regression', 'question': 'When should we transform the target variable to logarithm distribution?', 'course': 'machine-learning-zoomcamp'}, {'text': 'If we try to perform an arithmetic operation between 2 arrays of different shapes or different dimensions, it throws an error like operands could not be broadcast together with shapes. There are some scenarios when broadcasting can occur and when it fails.\\nIf this happens sometimes we can use * operator instead of dot() method to solve the issue. So that the error is solved and also we get the dot product.\\n(Santhosh Kumar)', 'section': '2. Machine Learning for Regression', 'question': 'ValueError: shapes not aligned', 'course': 'machine-learning-zoomcamp'}, {'text': 'Copy of a dataframe is made with X_copy = X.copy().\\nThis is called creating a deep copy.  Otherwise it will keep changing the original dataframe if used like this: X_copy = X.\\nAny changes to X_copy will reflect back to X. This is not a real copy, instead it is a “view”.\\n(Memoona Tahira)', 'section': '2. Machine Learning for Regression', 'question': 'How to copy a dataframe without changing the original dataframe?', 'course': 'machine-learning-zoomcamp'}, {'text': 'One of the most important characteristics of the normal distribution is that mean=median=mode, this means that the most popular value, the mean of the distribution and 50% of the sample are under the same value, this is equivalent to say that the area under the curve (black) is the same on the left and on the right. The long tail (red curve) is the result of having a few observations with high values, now the behaviour of the distribution changes, first of all, the area is different on each side and now the mean, median and mode are different. As a consequence, the mean is no longer representative, the range is larger than before and the probability of being on the left or on the right is not the same.\\n(Tatiana Dávila)', 'section': '2. Machine Learning for Regression', 'question': 'What does ‘long tail’ mean?', 'course': 'machine-learning-zoomcamp'}, {'text': 'In statistics, the standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range. [Wikipedia] The formula to calculate standard deviation is:\\n(Aadarsha Shrestha)', 'section': '2. Machine Learning for Regression', 'question': 'What is standard deviation?', 'course': 'machine-learning-zoomcamp'}, {'text': 'The application of regularization depends on the specific situation and problem. It is recommended to consider it when training machine learning models, especially with small datasets or complex models, to prevent overfitting. However, its necessity varies depending on the data quality and size. Evaluate each case individually to determine if it is needed.\\n(Daniel Muñoz Viveros)', 'section': '2. Machine Learning for Regression', 'question': 'Do we need to apply regularization techniques always? Or only in certain scenarios?', 'course': 'machine-learning-zoomcamp'}, {'text': 'As it speeds up the development:\\nprepare_df(initial_df, seed, fill_na_type)  - that prepared all 3 dataframes and 3 y_vectors. Fillna() can be done before the initial_df is split.\\nOf course, you can reuse other functions: rmse() and train_linear_regression(X,y,r) from the class notebook\\n(Ivan Brigida)', 'section': '2. Machine Learning for Regression', 'question': 'Shortcut: define functions for faster execution', 'course': 'machine-learning-zoomcamp'}, {'text': 'If we have a list or series of data for example x = [1,2,3,4,5]. We can use pandas to find the standard deviation. We can pass our list into panda series and call standard deviation directly on the series pandas.Series(x).std().\\n(Quinn Avila)', 'section': '2. Machine Learning for Regression', 'question': 'How to use pandas to find standard deviation', 'course': 'machine-learning-zoomcamp'}, {'text': 'Numpy and Pandas packages use different equations to compute the standard deviation. Numpy uses  population standard deviation, whereas pandas uses sample standard deviation by default.\\nNumpy\\nPandas\\npandas default standard deviation is computed using one degree of freedom. You can change degree in of freedom in NumPy to change this to unbiased estimator by using ddof parameter:\\nimport numpy as np\\nnp.std(df.weight, ddof=1)\\nThe result will be similar if we change the dof = 1 in numpy\\n(Harish Balasundaram)', 'section': '2. Machine Learning for Regression', 'question': 'Standard Deviation Differences in Numpy and Pandas', 'course': 'machine-learning-zoomcamp'}, {'text': \"In pandas you can use built in Pandas function names std() to get standard deviation. For example\\ndf['column_name'].std() to get standard deviation of that column.\\ndf[['column_1', 'column_2']].std() to get standard deviation of multiple columns.\\n(Khurram Majeed)\", 'section': '2. Machine Learning for Regression', 'question': 'Standard deviation using Pandas built in Function', 'course': 'machine-learning-zoomcamp'}, {'text': 'Use ‘pandas.concat’ function (https://pandas.pydata.org/docs/reference/api/pandas.concat.html) to combine two dataframes. To combine two numpy arrays use numpy.concatenate (https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html) function. So the code would be as follows:\\ndf_train_combined = pd.concat([df_train, df_val])\\ny_train = np.concatenate((y_train, y_val), axis=0)\\n(George Chizhmak)', 'section': '2. Machine Learning for Regression', 'question': 'How to combine train and validation datasets', 'course': 'machine-learning-zoomcamp'}, {'text': 'The Root Mean Squared Error (RMSE) is one of the primary metrics to evaluate the performance of a regression model. It calculates the average deviation between the model\\'s predicted values and the actual observed values, offering insight into the model\\'s ability to accurately forecast the target variable. To calculate RMSE score:\\nLibraries needed\\nimport numpy as np\\nfrom sklearn.metrics import mean_squared_error\\nmse = mean_squared_error(actual_values, predicted_values)\\nrmse = np.sqrt(mse)\\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\\n(Aminat Abolade)', 'section': '2. Machine Learning for Regression', 'question': 'Understanding RMSE and how to calculate RMSE score', 'course': 'machine-learning-zoomcamp'}, {'text': 'If you would like to use multiple conditions as an example below you will get the error. The correct syntax for OR is |, and for AND is &\\n(Olga Rudakova)\\n–', 'section': '2. Machine Learning for Regression', 'question': 'What syntax use in Pandas for multiple conditions using logical AND and OR', 'course': 'machine-learning-zoomcamp'}, {'text': 'I found this video pretty usual for understanding how we got the normal form with linear regression Normal Equation Derivation for Regression', 'section': '2. Machine Learning for Regression', 'question': 'Deep dive into normal equation for regression', 'course': 'machine-learning-zoomcamp'}, {'text': '(Hrithik Kumar Advani)', 'section': '2. Machine Learning for Regression', 'question': 'Useful Resource for Missing Data Treatment\\nhttps://www.kaggle.com/code/parulpandey/a-guide-to-handling-missing-values-in-python/notebook', 'course': 'machine-learning-zoomcamp'}, {'text': 'The instruction for applying log transformation to the ‘median_house_value’ variable is provided before Q3 in the homework for Week-2 under the ‘Prepare and split the dataset’ heading.\\nHowever, this instruction is absent in the subsequent questions of the homework, and I got stuck with Q5 for a long time, trying to figure out why my RMSE was so huge, when it clicked to me that I forgot to apply log transformation to the target variable. Please remember to apply log transformation to the target variable for each question.\\n(Added by Soham Mundhada)', 'section': '2. Machine Learning for Regression', 'question': 'Caution for applying log transformation in Week-2 2023 cohort homework', 'course': 'machine-learning-zoomcamp'}, {'text': 'Version 0.24.2 and Python 3.8.11\\n(Added by Diego Giraldo)', 'section': '3. Machine Learning for Classification', 'question': 'What sklearn version is Alexey using in the youtube videos?', 'course': 'machine-learning-zoomcamp'}, {'text': 'Week 3 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/03-classification/homework.md\\nSubmit HW Week 3: https://docs.google.com/forms/d/e/1FAIpQLSeXS3pqsv_smRkYmVx-7g6KIZDnG29g2s7pdHo-ASKNqtfRFQ/viewform\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nEvaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYoutube Link: 3.X --- https://www.youtube.com/watch?v=0Zw04wdeTQo&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=29\\n~~Nukta Bhatia~~', 'section': '3. Machine Learning for Classification', 'question': 'How do I get started with Week 3?', 'course': 'machine-learning-zoomcamp'}, {'text': \"The error message “could not convert string to float: ‘Nissan’” typically occurs when a machine learning model or function is expecting numerical input, but receives a string instead. In this case, it seems like the model is trying to convert the car brand ‘Nissan’ into a numerical value, which isn’t possible.\\nTo resolve this issue, you can encode categorical variables like car brands into numerical values. One common method is one-hot encoding, which creates new binary columns for each category/label present in the original column.\\nHere’s an example of how you can perform one-hot encoding using pandas:\\nimport pandas as pd\\n# Assuming 'data' is your DataFrame and 'brand' is the column with car brands\\ndata_encoded = pd.get_dummies(data, columns=['brand'])\\nIn this code, pd.get_dummies() creates a new DataFrame where the ‘brand’ column is replaced with binary columns for each brand (e.g., ‘brand_Nissan’, ‘brand_Toyota’, etc.). Each row in the DataFrame has a 1 in the column that corresponds to its brand and 0 in all other brand columns.\\n-Mohammad Emad Sharifi-\", 'section': '3. Machine Learning for Classification', 'question': \"Could not convert string to float:’Nissan’rt string to float: 'Nissan'\", 'course': 'machine-learning-zoomcamp'}, {'text': 'Solution: Mutual Information score calculates the relationship between categorical variables or discrete variables. So in the homework, because the target which is median_house_value is continuous, we had to change it to binary format which in other words, makes its values discrete as either 0 or 1. If we allowed it to remain in the continuous variable format, the mutual information score could be calculated, but the algorithm would have to divide the continuous variables into bins and that would be highly subjective. That is why continuous variables are not used for mutual information score calculation.\\n—Odimegwu David—-', 'section': '3. Machine Learning for Classification', 'question': 'Why did we change the targets to binary format when calculating mutual information score in the homework?', 'course': 'machine-learning-zoomcamp'}, {'text': \"Q2 asks about correlation matrix and converting median_house_value from numeric to binary. Just to make sure here we are only dealing with df_train not df_train_full, right? As the question explicitly mentions the train dataset.\\nYes. I think it is only on df_train. The reason behind this is that df_train_full also contains the validation dataset, so at this stage we don't want to make conclusions based on the validation data, since we want to test how we did without using that portion of the data.\\nPastor Soto\", 'section': '3. Machine Learning for Classification', 'question': 'What data should we use for correlation matrix', 'course': 'machine-learning-zoomcamp'}, {'text': \"The background of any dataframe can be colored (not only the correlation matrix) based on the numerical values the dataframe contains by using the method pandas.io.formats.style.Styler.background_graident.\\nHere an example on how to color the correlation matrix. A color map of choice can get passed, here ‘viridis’ is used.\\n# ensure to have only numerical values in the dataframe before calling 'corr'\\ncorr_mat = df_numerical_only.corr()\\ncorr_mat.style.background_gradient(cmap='viridis')\\nHere is an example of how the coloring will look like using a dataframe containing random values and applying “background_gradient” to it.\\nnp.random.seed = 3\\ndf_random = pd.DataFrame(data=np.random.random(3*3).reshape(3,3))\\ndf_random.style.background_gradient(cmap='viridis')\\nAdded by Sylvia Schmitt\", 'section': '3. Machine Learning for Classification', 'question': 'Coloring the background of the pandas.DataFrame.corr correlation matrix directly', 'course': 'machine-learning-zoomcamp'}, {'text': 'data_corr = pd.DataFrame(data_num.corr().round(3).abs().unstack().sort_values(ascending=False))\\ndata_corr.head(10)\\nAdded by Harish Balasundaram\\nYou can also use seaborn to create a heatmap with the correlation. The code for doing that:\\nsns.heatmap(df[numerical_features].corr(),\\nannot=True,\\nsquare=True,\\nfmt=\".2g\",\\ncmap=\"crest\")\\nAdded by Cecile Guillot\\nYou can refine your heatmap and plot only a triangle, with a blue to red color gradient, that will show every correlation between your numerical variables without redundant information with this function:\\nWhich outputs, in the case of churn dataset:\\n(Mélanie Fouesnard)', 'section': '3. Machine Learning for Classification', 'question': 'Identifying highly correlated feature pairs easily through unstack', 'course': 'machine-learning-zoomcamp'}, {'text': \"Should we perform EDA on the base of train or train+validation or train+validation+test dataset?\\nIt's indeed good practice to only rely on the train dataset for EDA. Including validation might be okay. But we aren't supposed to touch the test dataset, even just looking at it isn't a good idea. We indeed pretend that this is the future unseen data\\nAlena Kniazeva\", 'section': '3. Machine Learning for Classification', 'question': 'What data should be used for EDA?', 'course': 'machine-learning-zoomcamp'}, {'text': 'Validation dataset helps to validate models and prediction on unseen data. This helps get an estimate on its performance on fresh data. It helps optimize the model.\\nEdidiong Esu\\nBelow is an extract of Alexey\\'s book explaining this point. Hope is useful\\nWhen we apply the fit method, this method is looking at the content of the df_train dictionaries we are passing to the DictVectorizer instance, and fit is figuring out (training) how to map the values of these dictionaries. If categorical, applies one-hot encoding, if numerical it will leave it as it is.\\nWith this context, if we apply the fit to the validation model, we are \"giving the answers\" and we are not letting the \"fit\" do its job for data that we haven\\'t seen. By not applying the fit to the validation model we can know how well it was trained.\\nBelow is an extract of Alexey\\'s book explaining this point.\\nHumberto Rodriguez\\nThere is no need to initialize another instance of dictvectorizer after fitting it on the train set as it will overwrite what it learnt from being fit on the train data.\\nThe correct way is to fit_transform the train set, and only transform the validation and test sets.\\nMemoona Tahira', 'section': '3. Machine Learning for Classification', 'question': 'Fitting DictVectorizer on validation', 'course': 'machine-learning-zoomcamp'}, {'text': 'For Q5 in homework, should we calculate the smallest difference in accuracy in real values (i.e. -0.001 is less than -0.0002) or in absolute values (i.e. 0.0002 is less than 0.001)?\\nWe should select the “smallest” difference, and not the “lowest”, meaning we should reason in absolute values.\\nIf the difference is negative, it means that the model actually became better when we removed the feature.', 'section': '3. Machine Learning for Classification', 'question': 'Feature elimination', 'course': 'machine-learning-zoomcamp'}, {'text': \"Instead use the method “.get_feature_names_out()” from DictVectorizer function and the warning will be resolved , but we need not worry about the waning as there won't be any warning\\nSanthosh Kumar\", 'section': '3. Machine Learning for Classification', 'question': 'FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2', 'course': 'machine-learning-zoomcamp'}, {'text': 'Fitting the logistic regression takes a long time / kernel crashes when calling predict() with the fitted model.\\nMake sure that the target variable for the logistic regression is binary.\\nKonrad Muehlberg', 'section': '3. Machine Learning for Classification', 'question': 'Logistic regression crashing Jupyter kernel', 'course': 'machine-learning-zoomcamp'}, {'text': 'Ridge regression is a linear regression technique used to mitigate the problem of multicollinearity (when independent variables are highly correlated) and prevent overfitting in predictive modeling. It adds a regularization term to the linear regression cost function, penalizing large coefficients.\\nsag Solver: The sag solver stands for \"Stochastic Average Gradient.\" It\\'s particularly suitable for large datasets, as it optimizes the regularization term using stochastic gradient descent (SGD). sag can be faster than some other solvers for large datasets.\\nAlpha: The alpha parameter  controls the strength of the regularization in Ridge regression. A higher alpha value leads to stronger regularization, which means the model will have smaller coefficient values, reducing the risk of overfitting.\\nfrom sklearn.linear_model import Ridge\\nridge = Ridge(alpha=alpha, solver=\\'sag\\', random_state=42)\\nridge.fit(X_train, y_train)\\nAminat Abolade', 'section': '3. Machine Learning for Classification', 'question': 'Understanding Ridge', 'course': 'machine-learning-zoomcamp'}, {'text': 'DictVectorizer(sparse=True) produces CSR format, which is both more memory efficient and converges better during fit(). Basically it stores non-zero values and indices instead of adding a column for each class of each feature (models of cars produced 900+ columns alone in the current task).\\nUsing “sparse” format like on the picture above, both via pandas.get_dummies() and DictVectorizer(sparse=False) - is slower (around 6-8min for Q6 task - Linear/Ridge Regression) for high amount of classes (like models of cars for eg) and gives a bit “worse” results in both Logistic and Linear/Ridge Regression, while also producing convergence warnings for Linear/Ridge Regression.\\nLarkin Andrii', 'section': '3. Machine Learning for Classification', 'question': 'pandas.get_dummies() and DictVectorizer(sparse=False) produce the same type of one-hot encodings:', 'course': 'machine-learning-zoomcamp'}, {'text': 'Ridge with sag solver requires feature to be of the same scale. You may get the following warning: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\\nPlay with different scalers. See notebook-scaling-ohe.ipynb\\nDmytro Durach\\n(Oscar Garcia)  Use a StandardScaler for the numeric fields and OneHotEncoder (sparce = False) for the categorical features.  This help with the warning. Separate the features (num/cat) without using the encoder first and see if that helps.', 'section': '3. Machine Learning for Classification', 'question': 'Convergence Problems in W3Q6', 'course': 'machine-learning-zoomcamp'}, {'text': \"When encountering convergence errors during the training of a Ridge regression model, consider the following steps:\\nFeature Normalization: Normalize your numerical features using techniques like MinMaxScaler or StandardScaler. This ensures that numerical features are on a \\tsimilar scale, preventing convergence issues.\\nCategorical Feature Encoding: If your dataset includes categorical features, apply \\tcategorical encoding techniques such as OneHotEncoder (OHE) to \\tconvert them into a numerical format. OHE is commonly used to represent categorical variables as binary vectors, making them compatible with regression models like Ridge.\\nCombine Features: After \\tnormalizing numerical features and encoding categorical features using OneHotEncoder, combine them to form a single feature matrix (X_train). This combined dataset serves as the input for training the Ridge regression model.\\nBy following these steps, you can address convergence errors and enhance the stability of your Ridge model training process. It's important to note that the choice of encoding method, such as OneHotEncoder, is appropriate for handling categorical features in this context.\\nYou can find an example here.\\n \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tOsman Ali\", 'section': '3. Machine Learning for Classification', 'question': 'Dealing with Convergence in Week 3 q6', 'course': 'machine-learning-zoomcamp'}, {'text': 'A sparse matrix is more memory-efficient because it only stores the non-zero values and their positions in memory. This is particularly useful when working with large datasets with many zero or missing values.\\nThe default DictVectorizer configuration is a sparse matrix. For week3 Q6 using the default sparse is an interesting option because of the size of the matrix. Training the model was also more performant and didn’t give an error message like dense mode.\\n \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tQuinn Avila', 'section': '3. Machine Learning for Classification', 'question': 'Sparse matrix compared dense matrix', 'course': 'machine-learning-zoomcamp'}, {'text': 'The warnings on the jupyter notebooks can be disabled/ avoided with the following comments:\\nImport warnings\\nwarnings.filterwarnings(“ignore”)\\nKrishna Anand', 'section': '3. Machine Learning for Classification', 'question': 'How  to Disable/avoid Warnings in Jupyter Notebooks', 'course': 'machine-learning-zoomcamp'}, {'text': 'Question: Regarding RMSE, how do we decide on the correct score to choose? In the study group discussion    about week two homework, all of us got it wrong and one person had the lowest score selected as well.\\nAnswer: You need to find RMSE for each alpha. If RMSE scores  are equal, you will select the lowest alpha.\\nAsia Saeed', 'section': '3. Machine Learning for Classification', 'question': 'How to select the alpha parameter in Q6', 'course': 'machine-learning-zoomcamp'}, {'text': 'Question: Could you please help me with HW3 Q3: \"Calculate the mutual information score with the (binarized) price for the categorical variable that we have. Use the training set only.\" What is the second variable that we need to use to calculate the mutual information score?\\nAnswer: You need to calculate the mutual info score between the binarized price (above_average) variable & ocean_proximity, the only original categorical variable in the dataset.\\nAsia Saeed', 'section': '3. Machine Learning for Classification', 'question': 'Second variable that we need to use to calculate the mutual information score', 'course': 'machine-learning-zoomcamp'}, {'text': 'Do we need to train the model only with the features: total_rooms, total_bedrooms, population and households? or with all the available features and then pop once at a time each of the previous features and train the model to make the accuracy comparison?\\nYou need to create a list of all features in this question and evaluate the model one time to obtain the accuracy, this will be the original accuracy, and then remove one feature each time, and in each time, train the model, find the accuracy and the difference between the original accuracy and the found accuracy. Finally, find out which feature has the smallest absolute accuracy difference.\\nWhile calculating differences between accuracy scores while training on the whole model, versus dropping one feature at a time and comparing its accuracy to the model to judge impact of the feature on the accuracy of the model, do we take the smallest difference or smallest absolute difference?\\nSince order of subtraction between the two accuracy scores can result in a negative number, we will take its absolute value as we are interested in the smallest value difference, not the lowest difference value. Case in point, if difference is -4 and -2, the smallest difference is abs(-2), and not abs(-4)', 'section': '3. Machine Learning for Classification', 'question': 'Features for homework Q5', 'course': 'machine-learning-zoomcamp'}, {'text': 'Both work in similar ways. That is, to convert categorical features to numerical variables for use in training the model. But the difference lies in the input. OneHotEncoder uses an array as input while DictVectorizer uses a dictionary.\\nBoth will produce the same result. But when we use OneHotEncoder, features are sorted alphabetically. When you use DictVectorizer you stack features that you want.\\nTanya Mard', 'section': '3. Machine Learning for Classification', 'question': 'What is the difference between OneHotEncoder and DictVectorizer?', 'course': 'machine-learning-zoomcamp'}, {'text': 'They are basically the same. There are some key differences with regards to their input/output types, handling of missing values, etc, but they are both techniques to one-hot-encode categorical variables with identical results. The biggest difference is get_dummies are a convenient choice when you are working with Pandas Dataframes, while if you are building a scikit-learn-based machine learning pipeline and need to handle categorical data as part of that pipeline, OneHotEncoder is a more suitable choice. [Abhirup Ghosh]', 'section': '3. Machine Learning for Classification', 'question': 'What is the difference between pandas get_dummies and sklearn OnehotEncoder?', 'course': 'machine-learning-zoomcamp'}, {'text': \"For the test_train_split question on week 3's homework, are we supposed to use 42 as the random_state in both splits or only the 1st one?\\nAnswer: for both splits random_state = 42 should be used\\n(Bhaskar Sarma)\", 'section': '3. Machine Learning for Classification', 'question': 'Use of random seed in HW3', 'course': 'machine-learning-zoomcamp'}, {'text': 'Should correlation be calculated after splitting or before splitting. And lastly I know how to find the correlation but how do i find the two most correlated features.\\nAnswer: Correlation matrix of your train dataset. Thus, after splitting. Two most correlated features are the ones having the highest correlation coefficient in terms of absolute values.', 'section': '3. Machine Learning for Classification', 'question': 'Correlation before or after splitting the data', 'course': 'machine-learning-zoomcamp'}, {'text': 'Make sure that the features used in ridge regression model are only NUMERICAL ones not categorical.\\nDrop all categorical features first before proceeding.\\n(Aileah Gotladera)\\nWhile it is True that ridge regression accepts only numerical values, the categorical ones can be useful for your model. You have to transform them using one-hot encoding before training the model. To avoid the error of non convergence, put sparse=True when doing so.\\n(Erjon)', 'section': '3. Machine Learning for Classification', 'question': 'Features in Ridge Regression Model', 'course': 'machine-learning-zoomcamp'}, {'text': \"You need to use all features. and price for target. Don't include the average variable we created before.\\nIf you use DictVectorizer then make sure to use sparce=True to avoid convergence errors\\nI also used StandardScalar for numerical variable you can try running with or without this\\n(Peter Pan)\", 'section': '3. Machine Learning for Classification', 'question': 'Handling Column Information for Homework 3 Question 6', 'course': 'machine-learning-zoomcamp'}, {'text': 'Use sklearn.preprocessing encoders and scalers, e.g. OneHotEncoder, OrdinalEncoder, and StandardScaler.', 'section': '3. Machine Learning for Classification', 'question': 'Transforming Non-Numerical Columns into Numerical Columns', 'course': 'machine-learning-zoomcamp'}, {'text': 'These both methods receive the dictionary as an input. While the DictVectorizer will store the big vocabulary and takes more memory. FeatureHasher create a vectors with predefined length. They are both used for categorical features.\\nWhen you have a high cardinality for categorical features better to use FeatureHasher. If you want to preserve feature names in transformed data and have a small number of unique values is DictVectorizer. But your choice will dependence on your data.\\nYou can read more by follow the link https://scikit-learn.org/stable/auto_examples/text/plot_hashing_vs_dict_vectorizer.html\\nOlga Rudakova', 'section': '3. Machine Learning for Classification', 'question': 'What is the better option FeatureHasher or DictVectorizer', 'course': 'machine-learning-zoomcamp'}, {'text': '(Question by Connie S.)\\nThe reason it\\'s good/recommended practice to do it after splitting is to avoid data leakage - you don\\'t want any data from the test set influencing the training stage (similarly from the validation stage in the initial training). See e.g. scikit-learn documentation on \"Common pitfalls and recommended practices\": https://scikit-learn.org/stable/common_pitfalls.html\\nAnswered/added by Rileen Sinha', 'section': '3. Machine Learning for Classification', 'question': \"Isn't it easier to use DictVertorizer or get dummies before splitting the data into train/val/test? Is there a reason we wouldn't do this? Or is it the same either way?\", 'course': 'machine-learning-zoomcamp'}, {'text': 'If you are getting 1.0 as accuracy then there is a possibility you have overfitted the model. Dropping the column msrp/price can help you solve this issue.\\nAdded by Akshar Goyal', 'section': '3. Machine Learning for Classification', 'question': 'HW3Q4 I am getting 1.0 as accuracy. Should I use the closest option?', 'course': 'machine-learning-zoomcamp'}, {'text': 'We can use sklearn & numpy packages to calculate Root Mean Squared Error\\nfrom sklearn.metrics import mean_squared_error\\nimport numpy as np\\nRmse = np.sqrt(mean_squared_error(y_pred, y_val/ytest)\\nAdded by Radikal Lukafiardi\\nYou can also refer to Alexey’s notebook for Week 2:\\nhttps://github.com/alexeygrigorev/mlbookcamp-code/blob/master/chapter-02-car-price/02-carprice.ipynb\\nwhich includes the following code:\\ndef rmse(y, y_pred):\\nerror = y_pred - y\\nmse = (error ** 2).mean()\\nreturn np.sqrt(mse)\\n(added by Rileen Sinha)', 'section': '3. Machine Learning for Classification', 'question': 'How to calculate Root Mean Squared Error?', 'course': 'machine-learning-zoomcamp'}, {'text': 'The solution is to use “get_feature_names_out” instead. See details: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html\\nGeorge Chizhmak', 'section': '3. Machine Learning for Classification', 'question': \"AttributeError: 'DictVectorizer' object has no attribute 'get_feature_names'\", 'course': 'machine-learning-zoomcamp'}, {'text': 'To use RMSE without math or numpy, ‘sklearn.metrics’ has a mean_squared_error function with a squared kwarg (defaults to True). Setting squared to False will return the RMSE.\\nfrom sklearn.metrics import mean_squared_error\\nrms = mean_squared_error(y_actual, y_predicted, squared=False)\\nSee details: https://stackoverflow.com/questions/17197492/is-there-a-library-function-for-root-mean-square-error-rmse-in-python\\nAhmed Okka', 'section': '3. Machine Learning for Classification', 'question': 'Root Mean Squared Error', 'course': 'machine-learning-zoomcamp'}, {'text': 'This article explains different encoding techniques used https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02\\nHrithik Kumar Advani', 'section': '3. Machine Learning for Classification', 'question': 'Encoding Techniques', 'course': 'machine-learning-zoomcamp'}, {'text': \"I got this error multiple times here is the code:\\n“accuracy_score(y_val, y_pred >= 0.5)”\\nTypeError: 'numpy.float64' object is not callable\\nI solve it using\\nfrom sklearn import metrics\\nmetrics.accuracy_score(y_train, y_pred>= 0.5)\\nOMAR Wael\", 'section': '4. Evaluation Metrics for Classification', 'question': 'Error in use of accuracy_score from sklearn in jupyter (sometimes)', 'course': 'machine-learning-zoomcamp'}, {'text': 'Week 4 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/04-evaluation/homework.md\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nEvaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYouTube Link: 4.X --- https://www.youtube.com/watch?v=gmg5jw1bM8A&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=40\\nSci-Kit Learn on Evaluation:\\nhttps://scikit-learn.org/stable/model_selection.html\\n~~Nukta Bhatia~~', 'section': '4. Evaluation Metrics for Classification', 'question': 'How do I get started with Week 4?', 'course': 'machine-learning-zoomcamp'}, {'text': 'https://datatalks-club.slack.com/archives/C0288NJ5XSA/p1696475675887119\\nMetrics can be used on a series or a dataframe\\n~~Ella Sahnan~~', 'section': '4. Evaluation Metrics for Classification', 'question': 'Using a variable to score', 'course': 'machine-learning-zoomcamp'}, {'text': 'Ie particularly in module-04 homework Qn2 vs Qn5. https://datatalks-club.slack.com/archives/C0288NJ5XSA/p1696760905214979\\nRefer to the sklearn docs, random_state is to ensure the “randomness” that is used to shuffle dataset is reproducible, and it usually requires both random_state and shuffle params to be set accordingly.\\n~~Ella Sahnan~~', 'section': '4. Evaluation Metrics for Classification', 'question': 'Why do we sometimes use random_state and not at other times?', 'course': 'machine-learning-zoomcamp'}, {'text': 'How to get classification metrics - precision, recall, f1 score, accuracy simultaneously\\nUse classification_report from sklearn. For more info check here.\\nAbhishek N', 'section': '4. Evaluation Metrics for Classification', 'question': 'How to get all classification metrics?', 'course': 'machine-learning-zoomcamp'}, {'text': 'I am getting multiple thresholds with the same F1 score, does this indicate I am doing something wrong or is there a method for choosing? I would assume just pick the lowest?\\nChoose the one closest to any of the options\\nAdded by Azeez Enitan Edunwale\\nYou can always use scikit-learn (or other standard libraries/packages) to verify results obtained using your own code, e.g. you can use  “classification_report” (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) to obtain precision, recall and F1-score.\\nAdded by Rileen Sinha', 'section': '4. Evaluation Metrics for Classification', 'question': 'Multiple thresholds for Q4', 'course': 'machine-learning-zoomcamp'}, {'text': \"Solution description: duplicating the\\ndf.churn = (df.churn == 'yes').astype(int)\\nThis is causing you to have only 0's in your churn column. In fact, match with the error you are getting:  ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0.\\nIt is telling us that it only contains 0's.\\nDelete one of the below cells and you will get the accuracy\\nHumberto Rodriguez\", 'section': '4. Evaluation Metrics for Classification', 'question': 'ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0', 'course': 'machine-learning-zoomcamp'}, {'text': 'Use Yellowbrick. Yellowbrick in a library that combines scikit-learn with matplotlib to produce visualizations for your models. It produces colorful classification reports.\\nKrishna Annad', 'section': '4. Evaluation Metrics for Classification', 'question': 'Method to get beautiful classification report', 'course': 'machine-learning-zoomcamp'}, {'text': 'That’s fine, use the closest option', 'section': '4. Evaluation Metrics for Classification', 'question': 'I’m not getting the exact result in homework', 'course': 'machine-learning-zoomcamp'}, {'text': 'Check the solutions from the 2021 iteration of the course. You should use roc_auc_score.', 'section': '4. Evaluation Metrics for Classification', 'question': 'Use AUC to evaluate feature importance of numerical variables', 'course': 'machine-learning-zoomcamp'}, {'text': 'When calculating the ROC AUC score using sklearn.metrics.roc_auc_score the function expects two parameters “y_true” and “y_score”. So for each numerical value in the dataframe it will be passed as the “y_score” to the function and the target variable will get passed a “y_true” each time.\\nSylvia Schmitt', 'section': '4. Evaluation Metrics for Classification', 'question': 'Help with understanding: “For each numerical value, use it as score and compute AUC”', 'course': 'machine-learning-zoomcamp'}, {'text': 'You must use the `dt_val` dataset to compute the metrics asked in Question 3 and onwards, as you did in Question 2.\\nDiego Giraldo', 'section': '4. Evaluation Metrics for Classification', 'question': 'What dataset should I use to compute the metrics in Question 3', 'course': 'machine-learning-zoomcamp'}, {'text': \"What does this line do?\\nKFold(n_splits=n_splits, shuffle=True, random_state=1)\\nIf I do it inside the loop [0.01, 0.1, 1, 10] or outside the loop in Q6, HW04 it doesn't make any difference to my answers. I am wondering why and what is the right way, although it doesn't make a difference!\\nDid you try using a different random_state? From my understanding, KFold just makes N (which is equal to n_splits) separate pairs of datasets (train+val).\\nhttps://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\\nIn my case changing random state changed results\\n(Arthur Minakhmetov)\\nChanging the random state makes a difference in my case too, but not whether it is inside or outside the for loop. I think I have got the answer. kFold = KFold(n_splits=n_splits, shuffle = True, random_state = 1)  is just a generator object and it contains only the information n_splits, shuffle and random_state. The k-fold splitting actually happens in the next for loop for train_idx, val_idx in kFold.split(df_full_train): . So it doesn't matter where we generate the object, before or after the first loop. It will generate the same information. But from the programming point of view, it is better to do it before the loop. No point doing it again and again inside the loop\\n(Bhaskar Sarma)\\nIn case of KFold(n_splits=n_splits, shuffle=True, random_state=1) and C= [0.01, 0.1, 1, 10], it is better to loop through the different values of Cs as the video explained. I had separate train() and predict() functions, which were reused after dividing the dataset via KFold. The model ran about 10 minutes and provided a good score.\\n(Ani Mkrtumyan)\", 'section': '4. Evaluation Metrics for Classification', 'question': 'What does KFold do?', 'course': 'machine-learning-zoomcamp'}, {'text': \"I’m getting “ValueError: multi_class must be in ('ovo', 'ovr')” when using roc_auc_score to evaluate feature importance of numerical variables in question 1.\\nI was getting this error because I was passing the parameters to roc_auc_score incorrectly (df_train[col] , y_train) . The correct way is to pass the parameters in this way: roc_auc_score(y_train, df_train[col])\\nAsia Saeed\", 'section': '4. Evaluation Metrics for Classification', 'question': \"ValueError: multi_class must be in ('ovo', 'ovr')\", 'course': 'machine-learning-zoomcamp'}, {'text': 'from tqdm.auto import tqdm\\nTqdm - terminal progress bar\\nKrishna Anand', 'section': '4. Evaluation Metrics for Classification', 'question': 'Monitoring Wait times and progress of the code execution can be done with:', 'course': 'machine-learning-zoomcamp'}, {'text': 'Inverting or negating variables with ROC AUC scores less than the threshold is a valuable technique to improve feature importance and model performance when dealing with negatively correlated features. It helps ensure that the direction of the correlation aligns with the expectations of most machine learning algorithms.\\nAileah Gotladera', 'section': '4. Evaluation Metrics for Classification', 'question': 'What is the use of inverting or negating the variables less than the threshold?', 'course': 'machine-learning-zoomcamp'}, {'text': 'In case of using predict(X) for this task we are getting the binary classification predictions which are 0 and 1. This may lead to incorrect evaluation values.\\nThe solution is to use predict_proba(X)[:,1], where we get the probability that the value belongs to one of the classes.\\nVladimir Yesipov\\nPredict_proba shows probailites per class.\\nAni Mkrtumyan', 'section': '4. Evaluation Metrics for Classification', 'question': 'Difference between predict(X) and predict_proba(X)[:, 1]', 'course': 'machine-learning-zoomcamp'}, {'text': 'For churn/not churn predictions, I need help to interpret the following scenario please, what is happening when:\\nThe threshold is 1.0\\nFPR is 0.0\\nAnd TPR is 0.0\\nWhen the threshold is 1.0, the condition for belonging to the positive class (churn class) is g(x)>=1.0 But g(x) is a sigmoid function for a binary classification problem. It has values between 0 and 1. This function  never becomes equal to outermost values, i.e. 0 and 1.\\nThat is why there is no object, for which churn-condition could be satisfied. And that is why there is no any positive  (churn) predicted value (neither true positive, nor false positive), if threshold is equal to 1.0\\nAlena Kniazeva', 'section': '4. Evaluation Metrics for Classification', 'question': 'Why are FPR and TPR equal to 0.0, when threshold = 1.0?', 'course': 'machine-learning-zoomcamp'}, {'text': \"Matplotlib has a cool method to annotate where you could provide an X,Y point and annotate with an arrow and text. For example this will show an arrow pointing to the x,y point optimal threshold.\\nplt.annotate(f'Optimal Threshold: {optimal_threshold:.2f}\\\\nOptimal F1 Score: {optimal_f1_score:.2f}',\\nxy=(optimal_threshold, optimal_f1_score),\\nxytext=(0.3, 0.5),\\ntextcoords='axes fraction',\\narrowprops=dict(facecolor='black', shrink=0.05))\\nQuinn Avila\", 'section': '4. Evaluation Metrics for Classification', 'question': 'How can I annotate a graph?', 'course': 'machine-learning-zoomcamp'}, {'text': \"It's a complex and abstract topic and it requires some time to understand. You can move on without fully understanding the concept.\\nNonetheless, it might be useful for you to rewatch the video, or even watch videos/lectures/notes by other people on this topic, as the ROC AUC is one of the most important metrics used in Binary Classification models.\", 'section': '4. Evaluation Metrics for Classification', 'question': 'I didn’t fully understand the ROC curve. Can I move on?', 'course': 'machine-learning-zoomcamp'}, {'text': 'One main reason behind that, is the way of splitting data. For example, we want to split data into train/validation/test with the ratios 60%/20%/20% respectively.\\nAlthough the following two options end up with the same ratio, the data itself is a bit different and not 100% matching in each case.\\n1)\\ndf_train, df_temp = train_test_split(df, test_size=0.4, random_state=42)\\ndf_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\\n2)\\ndf_full_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\\ndf_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=42)\\nTherefore, I would recommend using the second method which is more consistent with the lessons and thus the homeworks.\\nIbraheem Taha', 'section': '4. Evaluation Metrics for Classification', 'question': 'Why do I have different values of accuracy than the options in the homework?', 'course': 'machine-learning-zoomcamp'}, {'text': 'You can find the intercept between these two curves using numpy diff (https://numpy.org/doc/stable/reference/generated/numpy.diff.html ) and sign (https://numpy.org/doc/stable/reference/generated/numpy.sign.html):\\nI suppose here that you have your df_scores ready with your three columns ‘threshold’, ‘precision’ and ‘recall’:\\nYou want to know at which index (or indices) you have your intercept between precision and recall (namely: where the sign of the difference between precision and recall changes):\\nidx = np.argwhere(\\nnp.diff(\\nnp.sign(np.array(df_scores[\"precision\"]) - np.array(df_scores[\"recall\"]))\\n)\\n).flatten()\\nYou can print the result to easily read it:\\nprint(\\nf\"The precision and recall curves intersect at a threshold equal to {df_scores.loc[idx][\\'threshold\\']}.\"\\n)\\n(Mélanie Fouesnard)', 'section': '4. Evaluation Metrics for Classification', 'question': 'How to find the intercept between precision and recall curves by using numpy?', 'course': 'machine-learning-zoomcamp'}, {'text': \"In the demonstration video, we are shown how to calculate the precision and recall manually. You can use the Scikit Learn library to calculate the confusion matrix. precision, recall, f1_score without having to first define true positive, true negative, false positive, and false negative.\\nfrom sklearn.metrics import precision_score, recall_score, f1_score\\nprecision_score(y_true, y_pred, average='binary')\\nrecall_score(y_true, y_pred, average='binary')\\nf1_score(y_true, y_pred, average='binary')\\nRadikal Lukafiardi\", 'section': '4. Evaluation Metrics for Classification', 'question': 'Compute Recall, Precision, and F1 Score using scikit-learn library', 'course': 'machine-learning-zoomcamp'}, {'text': 'Cross-validation evaluates the performance of a model and chooses the best hyperparameters. Cross-validation does this by splitting the dataset into multiple parts (folds), typically 5 or 10. It then trains and evaluates your model multiple times, each time using a different fold as the validation set and the remaining folds as the training set.\\n\"C\" is a hyperparameter that is typically associated with regularization in models like Support Vector Machines (SVM) and logistic regression.\\nSmaller \"C\" values: They introduce more regularization, which means the model will try to find a simpler decision boundary, potentially underfitting the data. This is because it penalizes the misclassification of training examples more severely.\\nLarger \"C\" values: They reduce the regularization effect, allowing the model to fit the training data more closely, potentially overfitting. This is because it penalizes misclassification less severely, allowing the model to prioritize getting training examples correct.\\nAminat Abolade', 'section': '4. Evaluation Metrics for Classification', 'question': 'Why do we use cross validation?', 'course': 'machine-learning-zoomcamp'}, {'text': \"Model evaluation metrics can be easily computed using off the shelf calculations available in scikit learn library. This saves a lot of time and more precise compared to our own calculations from the scratch using numpy and pandas libraries.\\nfrom sklearn.metrics import (accuracy_score,\\nprecision_score,\\nrecall_score,\\nf1_score,\\nroc_auc_score\\n)\\naccuracy = accuracy_score(y_val, y_pred)\\nprecision = precision_score(y_val, y_pred)\\nrecall = recall_score(y_val, y_pred)\\nf1 = f1_score(y_val, y_pred)\\nroc_auc = roc_auc_score(y_val, y_pred)\\nprint(f'Accuracy: {accuracy}')\\nprint(f'Precision: {precision}')\\nprint(f'Recall: {recall}')\\nprint(f'F1-Score: {f1}')\\nprint(f'ROC AUC: {roc_auc}')\\n(Harish Balasundaram)\", 'section': '4. Evaluation Metrics for Classification', 'question': 'Evaluate the Model using scikit learn metrics', 'course': 'machine-learning-zoomcamp'}, {'text': 'Scikit-learn offers another way: precision_recall_fscore_support\\nExample:\\nfrom sklearn.metrics import precision_recall_fscore_support\\nprecision, recall, fscore, support = precision_recall_fscore_support(y_val, y_val_pred, zero_division=0)\\n(Gopakumar Gopinathan)', 'section': '4. Evaluation Metrics for Classification', 'question': 'Are there other ways to compute Precision, Recall and F1 score?', 'course': 'machine-learning-zoomcamp'}, {'text': '- ROC curves are appropriate when the observations are balanced between each class, whereas precision-recall curves are appropriate for imbalanced datasets.\\n- The reason for this recommendation is that ROC curves present an optimistic picture of the model on datasets with a class imbalance.\\n-This is because of the use of true negatives in the False Positive Rate in the ROC Curve and the careful avoidance of this rate in the Precision-Recall curve.\\n- If the proportion of positive to negative instances changes in a test set, the ROC curves will not change. Metrics such as accuracy, precision, lift and F scores use values from both columns of the confusion matrix. As a class distribution changes these measures will change as well, even if the fundamental classifier performance does not. ROC graphs are based upon TP rate and FP rate, in which each dimension is a strict columnar ratio, so cannot give an accurate picture of performance when there is class imbalance.\\n(Anudeep Vanjavakam)', 'section': '4. Evaluation Metrics for Classification', 'question': 'When do I use ROC vs Precision-Recall curves?', 'course': 'machine-learning-zoomcamp'}, {'text': 'You can use roc_auc_score function from sklearn.metrics module and pass the vector of the target variable (‘above_average’) as the first argument and the vector of feature values as the second one. This function will return AUC score for the feature that was passed as a second argument.\\n(Denys Soloviov)', 'section': '4. Evaluation Metrics for Classification', 'question': 'How to evaluate feature importance for numerical variables with AUC?', 'course': 'machine-learning-zoomcamp'}, {'text': 'Precision-recall curve, and thus the score, explicitly depends on the ratio  of positive to negative test cases. This means that comparison of the F-score across different problems with differing class ratios is problematic. One way to address this issue is to use a standard class ratio  when making such comparisons.\\n(George Chizhmak)', 'section': '4. Evaluation Metrics for Classification', 'question': 'Dependence of the F-score on class imbalance', 'course': 'machine-learning-zoomcamp'}, {'text': \"We can import precision_recall_curve from scikit-learn and plot the graph as follows:\\nfrom sklearn.metrics import precision_recall_curve\\nprecision, recall, thresholds = precision_recall_curve(y_val, y_predict)\\nplt.plot(thresholds, precision[:-1], label='Precision')\\nplt.plot(thresholds, recall[:-1], label='Recall')\\nplt.legend()\\nHrithik Kumar Advani\", 'section': '4. Evaluation Metrics for Classification', 'question': 'Quick way to plot Precision-Recall Curve', 'course': 'machine-learning-zoomcamp'}, {'text': 'For multiclass classification it is important to keep class balance when you split the data set. In this case Stratified k-fold returns folds that contains approximately the sme percentage of samples of each classes.\\nPlease check the realisation in sk-learn library:\\nhttps://scikit-learn.org/stable/modules/cross_validation.html#stratified-k-fold\\nOlga Rudakova', 'section': '5. Deploying Machine Learning Models', 'question': 'What is Stratified k-fold?', 'course': 'machine-learning-zoomcamp'}, {'text': 'Week 5 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/05-deployment/homework.md\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nHW 3 Solution: https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/cohorts/2022/03-classification/homework_3.ipynb\\nEvaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYouTube Link: 5.X --- https://www.youtube.com/watch?v=agIFak9A3m8&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=49\\n~~~ Nukta Bhatia ~~~', 'section': '5. Deploying Machine Learning Models', 'question': 'How do I get started with Week 5?', 'course': 'machine-learning-zoomcamp'}, {'text': 'While weeks 1-4 can relatively easily be followed and the associated homework completed with just about any default environment / local setup, week 5 introduces several layers of abstraction and dependencies.\\nIt is advised to prepare your “homework environment” with a cloud provider of your choice. A thorough step-by-step guide for doing so for an AWS EC2 instance is provided in an introductory video taken from the MLOPS course here:\\nhttps://www.youtube.com/watch?v=IXSiYkP23zo\\nNote that (only) small AWS instances can be run for free, and that larger ones will be billed hourly based on usage (but can and should be stopped when not in use).\\nAlternative ways are sketched here:\\nhttps://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/01-intro/06-environment.md', 'section': '5. Deploying Machine Learning Models', 'question': 'Errors related to the default environment: WSL, Ubuntu, proper Python version, installing pipenv etc.', 'course': 'machine-learning-zoomcamp'}, {'text': \"You’ll need a kaggle account\\nGo to settings, API and click `Create New Token`. This will download a `kaggle.json` file which contains your `username` and `key` information\\nIn the same location as your Jupyter NB, place the `kaggle.json` file\\nRun `!chmod 600 <ENTER YOUR FILEPATH>/kaggle.json`\\nMake sure to import os via `import os` and then run:\\nos.environ['KAGGLE_CONFIG_DIR'] = <STRING OF YOUR FILE PATH>\\nFinally you can run directly in your NB: `!kaggle datasets download -d kapturovalexander/bank-credit-scoring`\\nAnd then you can unzip the file and access the CSV via: `!unzip -o bank-credit-scoring.zip`\\n>>> Michael Fronda <<<\", 'section': '5. Deploying Machine Learning Models', 'question': 'How to download CSV data via Jupyter NB and the Kaggle API, for one seamless experience', 'course': 'machine-learning-zoomcamp'}, {'text': 'Cd .. (go back)\\nLs (see current folders)\\nCd ‘path’/ (go to this path)\\nPwd (home)\\nCat “file name’ --edit txt file in ubuntu\\nAileah Gotladera', 'section': '5. Deploying Machine Learning Models', 'question': 'Basic Ubuntu Commands:', 'course': 'machine-learning-zoomcamp'}, {'text': 'Open terminal and type the code below to check the version on your laptop\\npython3 --version\\nFor windows,\\nVisit the official python website at  https://www.python.org/downloads/ to download the python version you need for installation\\nRun the installer and  ensure to check the box that says “Add Python to PATH” during installation and complete the installation by following the prompts\\nOr\\nFor Python 3,\\nOpen your command prompt or terminal and run the following command:\\npip install --upgrade python\\nAminat Abolade', 'section': '5. Deploying Machine Learning Models', 'question': 'Installing and updating to the python version 3.10 and higher', 'course': 'machine-learning-zoomcamp'}, {'text': 'It is quite simple, and you can follow these instructions here:\\nhttps://www.youtube.com/watch?v=qYlgUDKKK5A&ab_channel=NeuralNine\\nMake sure that you have “Virtual Machine Platform” feature activated in your Windows “Features”. To do that, search “features” in the research bar and see if the checkbox is selected. You also need to make sure that your system (in the bios) is able to virtualize. This is usually the case.\\nIn the Microsoft Store: look for ‘Ubuntu’ or ‘Debian’ (or any linux distribution you want) and install it\\nOnce it is downloaded, open the app and choose a username and a password (secured one). When you type your password, nothing will show in the window, which is normal: the writing is invisible.\\nYou are now inside of your linux system. You can test some commands such as “pwd”. You are not in your Windows system.\\nTo go to your windows system: you need to go back two times with cd ../.. And then go to the “mnt” directory with cd mnt. If you list here your files, you will see your disks. You can move to the desired folder, for example here I moved to the ML_Zoomcamp folder:\\nPython should be already installed but you can check it by running sudo apt install python3 command.\\nYou can make your actual folder your default folder when you open your Ubuntu terminal with this command : echo \"cd ../../mnt/your/folder/path\" >> ~/.bashrc\\nYou can disable bell sounds (when you type something that does not exist for example) by modifying the inputrc file with this command: sudo vim /etc/inputrc\\nYou have to uncomment the set bell-style none line -> to do that, press the “i” keyboard letter (for insert) and go with your keyboard to this line. Delete the # and then press the Escape keyboard touch and finally press “:wq” to write (it saves your modifications) then quit.\\nYou can check that your modifications are taken into account by opening a new terminal (you can pin it to your task bar so you do not have to go to the Microsoft app each time).\\nYou will need to install pip by running this command sudo apt install python3-pip\\nNB: I had this error message when trying to install pipenv (https://github.com/microsoft/WSL/issues/5663):\\n/sbin/ldconfig.real: Can\\'t link /usr/lib/wsl/lib/libnvoptix_loader.so.1 to libnvoptix.so.1\\n/sbin/ldconfig.real: /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link\\nSo I had to create the following symbolic link:\\nsudo ln -s /usr/lib/wsl/lib/libcuda.so.1 /usr/lib64/libcuda.so\\n(Mélanie Fouesnard)', 'section': '5. Deploying Machine Learning Models', 'question': 'How to install WSL on Windows 10 and 11 ?', 'course': 'machine-learning-zoomcamp'}, {'text': \"Do you get errors building the Docker image on the Mac M1 chipset?\\nThe error I was getting was:\\nCould not open '/lib64/ld-linux-x86-64.so.2': No such file or directory\\nThe fix (from here): vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\\nOpen mlbookcamp-code/course-zoomcamp/01-intro/environment/Dockerfile\\nReplace line 1 with\\nFROM --platform=linux/amd64 ubuntu:latest\\nNow build the image as specified. In the end it took over 2 hours to build the image but it did complete in the end.\\nDavid Colton\", 'section': '5. Deploying Machine Learning Models', 'question': 'Error building Docker images on Mac with M1 silicon', 'course': 'machine-learning-zoomcamp'}, {'text': 'Import waitress\\nprint(waitress.__version__)\\nKrishna Anand', 'section': '5. Deploying Machine Learning Models', 'question': 'Method to find the version of any install python libraries in jupyter notebook', 'course': 'machine-learning-zoomcamp'}, {'text': 'Working on getting Docker installed - when I try running hello-world I am getting the error.\\nDocker: Cannot connect to the docker daemon at unix:///var/run/docker.sock. Is the Docker daemon running ?\\nSolution description\\nIf you’re getting this error on WSL, re-install your docker: remove the docker installation from WSL and install Docker Desktop on your host machine (Windows).\\nOn Linux, start the docker daemon with either of these commands:\\nsudo dockerd\\nsudo service docker start\\nAdded by Ugochukwu Onyebuchi', 'section': '5. Deploying Machine Learning Models', 'question': 'Cannot connect to the docker daemon. Is the Docker daemon running?', 'course': 'machine-learning-zoomcamp'}, {'text': 'After using the command “docker build -t churn-prediction .” to build the Docker image, the above error is raised and the image is not created.\\nIn your Dockerfile, change the Python version in the first line the Python version installed in your system:\\nFROM python:3.7.5-slim\\nTo find your python version, use the command python --version. For example:\\npython --version\\n>> Python 3.9.7\\nThen, change it on your Dockerfile:\\nFROM python:3.9.7-slim\\nAdded by Filipe Melo', 'section': '5. Deploying Machine Learning Models', 'question': \"The command '/bin/sh -c pipenv install --deploy --system &&  rm -rf /root/.cache' returned a non-zero code: 1\", 'course': 'machine-learning-zoomcamp'}, {'text': 'When the facilitator was adding sklearn to the virtual environment in the lectures, he used sklearn==0.24.1 and it ran smoothly. But while doing the homework and you are asked to use the 1.0.2 version of sklearn, it gives errors.\\nThe solution is to use the full name of sklearn. That is, run it as “pipenv install scikit-learn==1.0.2” and the error will go away, allowing you to install sklearn for the version in your virtual environment.\\nOdimegwu David\\nHomework asks you to install 1.3.1\\nPipenv install scikit-learn==1.3.1\\nUse Pipenv to install Scikit-Learn version 1.3.1\\nGopakumar Gopinathan', 'section': '5. Deploying Machine Learning Models', 'question': 'Running “pipenv install sklearn==1.0.2” gives errors. What should I do?', 'course': 'machine-learning-zoomcamp'}, {'text': 'What is the reason we don’t want to keep the docker image in our system and why do we need to run docker containers with `--rm` flag?\\nFor best practice, you don’t want to have a lot of abandoned docker images in your system. You just update it in your folder and trigger the build one more time.\\nThey consume extra space on your disk. Unless you don’t want to re-run the previously existing containers, it is better to use the `--rm` option.\\nThe right way to say: “Why do we remove the docker container in our system?”. Well the docker image is still kept; it is the container that is not kept. Upon execution, images are not modified; only containers are.\\nThe option `--rm` is for removing containers. The images remain until you remove them manually. If you don’t specify a version when building an image, it will always rebuild and replace the latest tag. `docker images` shows you all the image you have pulled or build so far.\\nDuring development and testing you usually specify `--rm` to get the containers auto removed upon exit. Otherwise they get accumulated in a stopped state, taking up space. `docker ps -a` shows you all the containers you have in your host. Each time you change Pipfile (or any file you baked into the container), you rebuild the image under the same tag or a new tag. It’s important to understand the difference between the term “docker image” and “docker container”. Image is what we build with all the resources baked in. You can move it around, maintain it in a repository, share it. Then we use the image to spin up instances of it and they are called containers.\\nAdded by Muhammad Awon', 'section': '5. Deploying Machine Learning Models', 'question': 'Why do we need the --rm flag', 'course': 'machine-learning-zoomcamp'}, {'text': 'When you create the dockerfile the name should be dockerfile and needs to be without extension. One of the problems we can get at this point is to create the dockerfile as a dockerfile extension Dockerfile.dockerfile which creates an error when we build the docker image. Instead we just need to create the file without extension: Dockerfile and will run perfectly.\\nAdded by Pastor Soto', 'section': '5. Deploying Machine Learning Models', 'question': 'Failed to read Dockerfile', 'course': 'machine-learning-zoomcamp'}, {'text': 'Refer to the page https://docs.docker.com/desktop/install/mac-install/ remember to check if you have apple chip or intel chip.', 'section': '5. Deploying Machine Learning Models', 'question': 'Install docker on MacOS', 'course': 'machine-learning-zoomcamp'}, {'text': 'Problem: When I am trying to pull the image with the docker pull svizor/zoomcamp-model command I am getting an error:\\nUsing default tag: latest\\nError response from daemon: manifest for svizor/zoomcamp-model:latest not found: manifest unknown: manifest unknown\\nSolution: The docker by default uses the latest tag to avoid this use the correct tag from image description. In our case use command:\\ndocker pull svizor/zoomcamp-model:3.10.12-slim\\nAdded by Vladimir Yesipov', 'section': '5. Deploying Machine Learning Models', 'question': 'I cannot pull the image with docker pull command', 'course': 'machine-learning-zoomcamp'}, {'text': 'Using the command docker images or docker image ls will dump all information for all local Docker images. It is possible to dump the information only for a specified image by using:\\ndocker image ls <image name>\\nOr alternatively:\\ndocker images <image name>\\nIn action to that it is possible to only dump specific information provided using the option --format which will dump only the size for the specified image name when using the command below:\\ndocker image ls --format \"{{.Size}}\" <image name>\\nOr alternatively:\\ndocker images --format \"{{.Size}}\" <image name>\\nSylvia Schmitt', 'section': '5. Deploying Machine Learning Models', 'question': 'Dumping/Retrieving only the size of for a specific Docker image', 'course': 'machine-learning-zoomcamp'}, {'text': \"It creates them in\\nOSX/Linux: ~/.local/share/virtualenvs/folder-name_cyrptic-hash\\nWindows: C:\\\\Users\\\\<USERNAME>\\\\.virtualenvs\\\\folder-name_cyrptic-hash\\nEg: C:\\\\Users\\\\Ella\\\\.virtualenvs\\\\code-qsdUdabf (for module-05 lesson)\\nThe environment name is the name of the last folder in the folder directory where we used the pipenv install command (or any other pipenv command). E.g. If you run any pipenv command in folder path ~/home/user/Churn-Flask-app, it will create an environment named Churn-Flask-app-some_random_characters, and it's path will be like this: /home/user/.local/share/virtualenvs/churn-flask-app-i_mzGMjX.\\nAll libraries of this environment will be installed inside this folder. To activate this environment, I will need to cd into the project folder again, and type pipenv shell. In short, the location of the project folder acts as an identifier for an environment, in place of any name.\\n(Memoona Tahira)\", 'section': '5. Deploying Machine Learning Models', 'question': 'Where does pipenv create environments and how does it name them?', 'course': 'machine-learning-zoomcamp'}, {'text': 'Launch the container image in interactive mode and overriding the entrypoint, so that it starts a bash command.\\ndocker run -it --entrypoint bash <image>\\nIf the container is already running, execute a command in the specific container:\\ndocker ps (find the container-id)\\ndocker exec -it <container-id> bash\\n(Marcos MJD)', 'section': '5. Deploying Machine Learning Models', 'question': 'How do I debug a docker container?', 'course': 'machine-learning-zoomcamp'}, {'text': \"$ docker exec -it 1e5a1b663052 bash\\nthe input device is not a TTY.  If you are using mintty, try prefixing the command with 'winpty'\\nFix:\\nwinpty docker exec -it 1e5a1b663052 bash\\nA TTY is a terminal interface that supports escape sequences, moving the cursor around, etc.\\nWinpty is a Windows software package providing an interface similar to a Unix pty-master for communicating with Windows console programs.\\nMore info on terminal, shell, console applications hi and so on:\\nhttps://conemu.github.io/en/TerminalVsShell.html\\n(Marcos MJD)\", 'section': '5. Deploying Machine Learning Models', 'question': 'The input device is not a TTY when running docker in interactive mode (Running Docker on Windows in GitBash)', 'course': 'machine-learning-zoomcamp'}, {'text': 'Initially, I did not assume there was a model2. I copied the original model1.bin and dv.bin. Then when I tried to load using\\nCOPY [\"model2.bin\", \"dv.bin\", \"./\"]\\nthen I got the error above in MINGW64 (git bash) on Windows.\\nThe temporary solution I found was to use\\nCOPY [\"*\", \"./\"]\\nwhich I assume combines all the files from the original docker image and the files in your working directory.\\nAdded by Muhammed Tan', 'section': '5. Deploying Machine Learning Models', 'question': 'Error: failed to compute cache key: \"/model2.bin\" not found: not found', 'course': 'machine-learning-zoomcamp'}, {'text': 'Create a virtual environment using the Cmd command (command) and use pip freeze command to write the requirements in the text file\\nKrishna Anand', 'section': '5. Deploying Machine Learning Models', 'question': 'Failed to write the dependencies to pipfile and piplock file', 'course': 'machine-learning-zoomcamp'}, {'text': 'f-String not properly keyed in: does anyone knows why i am getting error after import pickle?\\nThe first error showed up because your f-string is using () instead of {} around C. So, should be: f’model_C={C}.bin’\\nThe second error as noticed by Sriniketh, your are missing one parenthesis it should be pickle.dump((dv, model), f_out)\\n(Humberto R.)', 'section': '5. Deploying Machine Learning Models', 'question': 'f-strings', 'course': 'machine-learning-zoomcamp'}, {'text': \"This error happens because pipenv is already installed but you can't access it from the path.\\nThis error comes out if you run.\\npipenv  --version\\npipenv shell\\nSolution for Windows\\nOpen this option\\nClick here\\nClick in Edit Button\\nMake sure the next two locations are on the PATH, otherwise, add it.\\nC:\\\\Users\\\\AppData\\\\....\\\\Python\\\\PythonXX\\\\\\nC:\\\\Users\\\\AppData\\\\....\\\\Python\\\\PythonXX\\\\Scripts\\\\\\nAdded by Alejandro Aponte\\nNote: this answer assumes you don’t use Anaconda. For Windows, using Anaconda would be a better choice and less prone to errors.\", 'section': '5. Deploying Machine Learning Models', 'question': \"'pipenv' is not recognized as an internal or external command, operable program or batch file.\", 'course': 'machine-learning-zoomcamp'}, {'text': 'Following the instruction from video week-5.6, using pipenv to install python libraries throws below error\\nSolution to this error is to make sure that you are working with python==3.9 (as informed in the very first lesson of the zoomcamp) and not python==3.10.\\nAdded by Hareesh Tummala', 'section': '5. Deploying Machine Learning Models', 'question': 'AttributeError: module ‘collections’ has no attribute ‘MutableMapping’', 'course': 'machine-learning-zoomcamp'}, {'text': 'After entering `pipenv shell` don’t forget to use `exit` before `pipenv --rm`, as it may cause errors when trying to install packages, it is unclear whether you are “in the shell”(using Windows) at the moment as there are no clear markers for it.\\nIt can also mess up PATH, if that’s the case, here’s terminal commands for fixing that:\\n# for Windows\\nset VIRTUAL_ENV \"\"\\n# for Unix\\nexport VIRTUAL_ENV=\"\"\\nAlso manually re-creating removed folder at `C:\\\\Users\\\\username\\\\.virtualenvs\\\\removed-envname` can help, removed-envname can be seen at the error message.\\nAdded by Andrii Larkin', 'section': '5. Deploying Machine Learning Models', 'question': \"Q: ValueError: Path not found or generated: WindowsPath('C:/Users/username/.virtualenvs/envname/Scripts')\", 'course': 'machine-learning-zoomcamp'}, {'text': 'Set the host to ‘0.0.0.0’ on the flask app and dockerfile then RUN the url using localhost.\\n(Theresa S.)', 'section': '5. Deploying Machine Learning Models', 'question': \"ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\", 'course': 'machine-learning-zoomcamp'}, {'text': 'Solution:\\nThis error occurred because I used single quotes around the filenames. Stick to double quotes', 'section': '5. Deploying Machine Learning Models', 'question': 'docker  build ERROR [x/y] COPY …', 'course': 'machine-learning-zoomcamp'}, {'text': 'I tried the first solution on Stackoverflow which recommended running `pipenv lock` to update the Pipfile.lock. However, this didn’t resolve it. But the following switch to the pipenv installation worked\\nRUN pipenv install --system --deploy --ignore-pipfile', 'section': '5. Deploying Machine Learning Models', 'question': 'Fix error during installation of Pipfile inside Docker container', 'course': 'machine-learning-zoomcamp'}, {'text': 'Solution\\nThis error was because there was another instance of gunicorn running. So I thought of removing this along with the zoomcamp_test image. However, it didn’t let me remove the orphan container. So I did the following\\nRunning the following commands\\ndocker ps -a <to list all docker containers>\\ndocker images <to list images>\\ndocker stop <container ID>\\ndocker rm <container ID>\\ndocker rmi image\\nI rebuilt the Docker image, and ran it once again; this time it worked correctly and I was able to serve the test script to the endpoint.', 'section': '5. Deploying Machine Learning Models', 'question': 'How to fix error after running the Docker run command', 'course': 'machine-learning-zoomcamp'}, {'text': 'I was getting the below error when I rebuilt the docker image although the port was not allocated, and it was working fine.\\nError message:\\nError response from daemon: driver failed programming external connectivity on endpoint beautiful_tharp (875be95c7027cebb853a62fc4463d46e23df99e0175be73641269c3d180f7796): Bind for 0.0.0.0:9696 failed: port is already allocated.\\nSolution description\\nIssue has been resolved running the following command:\\ndocker kill $(docker ps -q)\\nhttps://github.com/docker/for-win/issues/2722\\nAsia Saeed', 'section': '5. Deploying Machine Learning Models', 'question': 'Bind for 0.0.0.0:9696 failed: port is already allocated', 'course': 'machine-learning-zoomcamp'}, {'text': 'I was getting the error on client side with this\\nClient Side:\\nFile \"C:\\\\python\\\\lib\\\\site-packages\\\\urllib3\\\\connectionpool.py\", line 703, in urlopen …………………..\\nraise ConnectionError(err, request=request)\\nrequests.exceptions.ConnectionError: (\\'Connection aborted.\\', RemoteDisconnected(\\'Remote end closed connection without response\\'))\\nSevrer Side:\\nIt showed error for gunicorn\\nThe waitress  cmd was running smoothly from server side\\nSolution:\\nUse the ip-address as 0.0.0.0:8000 or 0.0.0.0:9696.They are the ones which do work max times\\nAamir Wani', 'section': '5. Deploying Machine Learning Models', 'question': 'Bind for 127.0.0.1:5000 showing error', 'course': 'machine-learning-zoomcamp'}, {'text': 'Install it by using command\\n% brew install md5sha1sum\\nThen run command to check hash for file to check if they the same with the provided\\n% md5sum model1.bin dv.bin\\nOlga Rudakova', 'section': '5. Deploying Machine Learning Models', 'question': 'Installing md5sum on Macos', 'course': 'machine-learning-zoomcamp'}, {'text': 'Problem description:\\nI started a web-server in terminal (command window, powershell, etc.). How can I run another python script, which makes a request to this server?\\nSolution description:\\nJust open another terminal (command window, powershell, etc.) and run a python script.\\nAlena Kniazeva', 'section': '5. Deploying Machine Learning Models', 'question': 'How to run a script while a web-server is working?', 'course': 'machine-learning-zoomcamp'}, {'text': \"Problem description:\\nIn video 5.5 when I do pipenv shell and then pipenv run gunicorn --bind 0.0.0.0:9696 predict:app, I get the following warning:\\nUserWarning: Trying to unpickle estimator DictVectorizer from version 1.1.1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\\nSolution description:\\nWhen you create a virtual env, you should use the same version of Scikit-Learn that you used for training the model on this case it's 1.1.1. There is version conflicts so we need to make sure our model and dv files are created from the version we are using for the project.\\nBhaskar Sarma\", 'section': '5. Deploying Machine Learning Models', 'question': 'Version-conflict in pipenv', 'course': 'machine-learning-zoomcamp'}, {'text': \"If you install packages via pipenv install, and get an error that ends like this:\\npipenv.vendor.plette.models.base.ValidationError: {'python_version': '3.9', 'python_full_version': '3.9.13'}\\npython_full_version: 'python_version' must not be present with 'python_full_version'\\npython_version: 'python_full_version' must not be present with 'python_version'\\nDo this:\\nopen Pipfile in nano editor, and remove either the python_version or python_full_version line, press CTRL+X, type Y and click Enter to save changed\\nType pipenv lock to create the Pipfile.lock.\\nDone. Continue what you were doing\", 'section': '5. Deploying Machine Learning Models', 'question': 'Python_version and Python_full_version error after running pipenv install:', 'course': 'machine-learning-zoomcamp'}, {'text': 'If during running the  docker build command, you get an error like this:\\nYour Pipfile.lock (221d14) is out of date. Expected: (939fe0).\\nUsage: pipenv install [OPTIONS] [PACKAGES]...\\nERROR:: Aborting deploy\\nOption 1: Delete the pipfile.lock via rm Pipfile, and then rebuild the lock via  pipenv lock from the terminal before retrying the docker build command.\\nOption 2:  If it still doesn’t work, remove the pipenv environment, Pipfile and Pipfile.lock, and create a new one before building docker again. Commands to remove pipenv environment and removing pipfiles:\\npipenv  --rm\\nrm Pipfile*', 'section': '5. Deploying Machine Learning Models', 'question': 'Your Pipfile.lock (221d14) is out of date (during Docker build)', 'course': 'machine-learning-zoomcamp'}, {'text': 'Ans: Pip uninstall waitress mflow. Then reinstall just mlflow. By this time you should have successfully built your docker image so you dont need to reinstall waitress. All good. Happy learning.\\nAdded by 🅱🅻🅰🆀', 'section': '5. Deploying Machine Learning Models', 'question': 'You are using windows. Conda environment. You then use waitress instead of gunicorn. After a few runs, suddenly mlflow server fails to run.', 'course': 'machine-learning-zoomcamp'}, {'text': \"Ans: so you have created the env. You need to make sure you're in eu-west-1 (ireland) when you check the EB environments. Maybe you're in a different region in your console.\\nAdded by Edidiong Esu\", 'section': '5. Deploying Machine Learning Models', 'question': 'Completed creating the environment locally but could not find the environment on AWS.', 'course': 'machine-learning-zoomcamp'}, {'text': 'Running \\'pip install waitress\\' as a command on GitBash was not downloading the executable file \\'waitress-serve.exe\\'. You need this file to be able to run commands with waitress in Git Bash. To solve this:\\nopen a Jupyter notebook and run the same command \\' pip install waitress\\'. This way the executable file will be downloaded. The notebook may give you this warning : \\'WARNING: The script waitress-serve.exe is installed in \\'c:\\\\Users\\\\....\\\\anaconda3\\\\Scripts\\' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\\'\\nAdd the path where \\'waitress-serve.exe\\' is installed into gitbash\\'s PATH as such:\\nenter the following command in gitbash: nano ~/.bashrc\\nadd the path to \\'waitress-serve.exe\\' to PATH using this command: export PATH=\"/path/to/waitress:$PATH\"\\nclose gitbash and open it again and you should be good to go\\nAdded by Bachar Kabalan', 'section': '5. Deploying Machine Learning Models', 'question': 'Installing waitress on Windows via GitBash: “waitress-serve” command not found', 'course': 'machine-learning-zoomcamp'}, {'text': 'Q2.1: Use Pipenv to install Scikit-Learn version 1.3.1\\nThis is an error I got while executing the above step in the ml-zoomcamp conda environment. The error is not fatal and just warns you that explicit language specifications are not set out in our bash profile. A quick-fix is here:\\nhttps://stackoverflow.com/questions/49436922/getting-error-while-trying-to-run-this-command-pipenv-install-requests-in-ma\\nBut one can proceed without addressing it.\\nAdded by Abhirup Ghosh', 'section': '5. Deploying Machine Learning Models', 'question': 'Warning: the environment variable LANG is not set!', 'course': 'machine-learning-zoomcamp'}, {'text': 'The provided image FROM svizor/zoomcamp-model:3.10.12-slim has a model and dictvectorizer that should be used for question 6. \"model2.bin\", \"dv.bin\"\\nAdded by Quinn Avila', 'section': '5. Deploying Machine Learning Models', 'question': 'Module5 HW Question 6', 'course': 'machine-learning-zoomcamp'}, {'text': 'https://apps.microsoft.com/detail/windows-terminal/9N0DX20HK701?hl=es-419&gl=CO\\nAdded by Dawuta Smit', 'section': '5. Deploying Machine Learning Models', 'question': 'Terminal Used in Week 5 videos:', 'course': 'machine-learning-zoomcamp'}, {'text': \"Question:\\nWhen running\\npipenv run waitress-serve --listen=localhost:9696 q4-predict:app\\nI get the following:\\nThere was an exception (ValueError) importing your module.\\nIt had these arguments:\\n1. Malformed application 'q4-predict:app'\\nAnswer:\\nWaitress doesn’t accept a dash in the python file name.\\nThe solution is to rename the file replacing a dash with something else for instance with an underscore eg q4_predict.py\\nAdded by Alex Litvinov\", 'section': '5. Deploying Machine Learning Models', 'question': 'waitress-serve shows Malformed application', 'course': 'machine-learning-zoomcamp'}, {'text': 'I wanted to have a fast and simple way to check if the HTTP POST requests are working just running a request from command line. This can be done running ‘curl’. \\n(Used with WSL2 on Windows, should also work on Linux and MacOS)\\ncurl --json \\'<json data>\\' <url>\\n# piping the structure to the command\\ncat <json file path> | curl --json @- <url>\\necho \\'<json data>\\' | curl --json @- <url>\\n# example using piping\\necho \\'{\"job\": \"retired\", \"duration\": 445, \"poutcome\": \"success\"}\\'\\\\\\n| curl --json @- http://localhost:9696/predict\\nAdded by Sylvia Schmitt', 'section': '5. Deploying Machine Learning Models', 'question': 'Testing HTTP POST requests from command line using curl', 'course': 'machine-learning-zoomcamp'}, {'text': 'Question:\\nWhen executing\\neb local run  --port 9696\\nI get the following error:\\nERROR: NotSupportedError - You can use \"eb local\" only with preconfigured, generic and multicontainer Docker platforms.\\nAnswer:\\nThere are two options to fix this:\\nRe-initialize by running eb init -i and choosing the options from a list (the first default option for docker platform should be fine).\\nEdit the ‘.elasticbeanstalk/config.yml’ directly changing the default_platform from Docker to default_platform: Docker running on 64bit Amazon Linux 2023\\nThe disadvantage of the second approach is that the option might not be available the following years\\nAdded by Alex Litvinov', 'section': '5. Deploying Machine Learning Models', 'question': 'NotSupportedError - You can use \"eb local\" only with preconfigured, generic and multicontainer Docker platforms.', 'course': 'machine-learning-zoomcamp'}, {'text': \"You need to include the protocol scheme: 'http://localhost:9696/predict'.\\nWithout the http:// part, requests has no idea how to connect to the remote server.\\nNote that the protocol scheme must be all lowercase; if your URL starts with HTTP:// for example, it won’t find the http:// connection adapter either.\\nAdded by George Chizhmak\", 'section': '5. Deploying Machine Learning Models', 'question': \"Requests Error: No connection adapters were found for 'localhost:9696/predict'.\", 'course': 'machine-learning-zoomcamp'}, {'text': 'While running the docker image if you get the same result check which model you are using.\\nRemember you are using a model downloading model + python version so remember to change the model in your file when running your prediction test.\\nAdded by Ahmed Okka', 'section': '5. Deploying Machine Learning Models', 'question': 'Getting the same result', 'course': 'machine-learning-zoomcamp'}, {'text': 'Ensure that you used pipenv to install the necessary modules including gunicorn. As pipfiles for virtual environments, you can use pipenv shell and then build+run your docker image. - Akshar Goyal', 'section': '5. Deploying Machine Learning Models', 'question': 'Trying to run a docker image I built but it says it’s unable to start the container process', 'course': 'machine-learning-zoomcamp'}, {'text': \"You can copy files from your local machine into a Docker container using the docker cp command. Here's how to do it:\\nTo copy a file or directory from your local machine into a running Docker container, you can use the `docker cp command`. The basic syntax is as follows:\\ndocker cp /path/to/local/file_or_directory container_id:/path/in/container\\nHrithik Kumar Advani\", 'section': '5. Deploying Machine Learning Models', 'question': 'How do I copy files from my local machine to docker container?', 'course': 'machine-learning-zoomcamp'}, {'text': 'You can copy files from your local machine into a Docker container using the docker cp command. Here\\'s how to do it:\\nIn the Dockerfile, you can provide the folder containing the files that you want to copy over. The basic syntax is as follows:\\nCOPY [\"src/predict.py\", \"models/xgb_model.bin\", \"./\"]\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tGopakumar Gopinathan', 'section': '5. Deploying Machine Learning Models', 'question': 'How do I copy files from a different folder into docker container’s working directory?', 'course': 'machine-learning-zoomcamp'}, {'text': 'I struggled with the command :\\neb init -p docker tumor-diagnosis-serving -r eu-west-1\\nWhich resulted in an error when running : eb local run --port 9696\\nERROR: NotSupportedError - You can use \"eb local\" only with preconfigured, generic and multicontainer Docker platforms.\\nI replaced it with :\\neb init -p \"Docker running on 64bit Amazon Linux 2\" tumor-diagnosis-serving -r eu-west-1\\nThis allowed the recognition of the Dockerfile and the build/run of the docker container.\\nAdded by Mélanie Fouesnard', 'section': '5. Deploying Machine Learning Models', 'question': 'I can’t create the environment on AWS Elastic Beanstalk with the command proposed during the video', 'course': 'machine-learning-zoomcamp'}, {'text': \"I had this error when creating a AWS ElasticBean environment: eb create tumor-diagnosis-env\\nERROR   Instance deployment: Both 'Dockerfile' and 'Dockerrun.aws.json' are missing in your source bundle. Include at least one of them. The deployment failed.\\nI did not committed the files used to build the container, particularly the Dockerfile. After a git add and git commit of the modified files, the command works.\\nAdded by Mélanie Fouesnard\", 'section': '6. Decision Trees and Ensemble Learning', 'question': 'Dockerfile missing when creating the AWS ElasticBean environment', 'course': 'machine-learning-zoomcamp'}, {'text': 'Week 6 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/06-trees/homework.md\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nHW 4 Solution: https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/cohorts/2022/04-evaluation/homework_4.ipynb\\nEvaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYouTube Link: 6.X --- https://www.youtube.com/watch?v=GJGmlfZoCoU&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=57\\nFAQs: https://docs.google.com/document/d/1LpPanc33QJJ6BSsyxVg-pWNMplal84TdZtq10naIhD8/edit#heading=h.lpz96zg7l47j\\n~~~Nukta Bhatia~~~', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'How to get started with Week 6?', 'course': 'machine-learning-zoomcamp'}, {'text': 'During the XGBoost lesson, we created a parser to extract the training and validation auc from the standard output. However, we can accomplish that in a more straightforward way.\\nWe can use the evals_result  parameters, which takes an empty dictionary and updates it for each tree. Additionally, you can store the data in a dataframe and plot it in an easier manner.\\nAdded by Daniel Coronel', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'How to get the training and validation metrics from XGBoost?', 'course': 'machine-learning-zoomcamp'}, {'text': 'You should create sklearn.ensemble.RandomForestRegressor object. It’s rather similar to sklearn.ensemble.RandomForestClassificator for classification problems. Check https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html for more information.\\nAlena Kniazeva', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'How to solve regression problems with random forest in scikit-learn?', 'course': 'machine-learning-zoomcamp'}, {'text': 'In question 6, I was getting ValueError: feature_names must be string, and may not contain [, ] or < when I was creating DMatrix for train and validation\\nSolution description\\nThe cause of this error is that some of the features names contain special characters like = and <, and I fixed the error by removing them as  follows:\\nfeatures= [i.replace(\"=<\", \"_\").replace(\"=\",\"_\") for i in features]\\nAsia Saeed\\nAlternative Solution:\\nIn my case the equal sign “=” was not a problem, so in my opinion the first part of Asias solution features= [i.replace(\"=<\", \"_\") should work as well.\\nFor me this works:\\nfeatures = []\\nfor f in dv.feature_names_:\\nstring = f.replace(“=<”, “-le”)\\nfeatures.append(string)\\nPeter Ernicke', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'ValueError: feature_names must be string, and may not contain [, ] or <', 'course': 'machine-learning-zoomcamp'}, {'text': 'If you’re getting this error, It is likely because the feature names in dv.get_feature_names_out() are a np.ndarray instead of a list so you have to convert them into a list by using the to_list() method.\\nAli Osman', 'section': '6. Decision Trees and Ensemble Learning', 'question': \"`TypeError: Expecting a sequence of strings for feature names, got: <class 'numpy.ndarray'> ` when training xgboost model.\", 'course': 'machine-learning-zoomcamp'}, {'text': \"If you’re getting TypeError:\\n“TypeError: Expecting a sequence of strings for feature names, got: <class 'numpy.ndarray'>”,\\nprobably you’ve done this:\\nfeatures = dv.get_feature_names_out()\\nIt gets you np.ndarray instead of list. Converting to list list(features) will not fix this, read below.\\nIf you’re getting ValueError:\\n“ValueError: feature_names must be string, and may not contain [, ] or <”,\\nprobably you’ve either done:\\nfeatures = list(dv.get_feature_names_out())\\nor:\\nfeatures = dv.feature_names_\\nreason is what you get from DictVectorizer here looks like this:\\n['households',\\n'housing_median_age',\\n'latitude',\\n'longitude',\\n'median_income',\\n'ocean_proximity=<1H OCEAN',\\n'ocean_proximity=INLAND',\\n'population',\\n'total_bedrooms',\\n'total_rooms']\\nit has symbols XGBoost doesn’t like ([, ] or <).\\nWhat you can do, is either do not specify “feature_names=” while creating xgb.DMatrix or:\\nimport re\\nfeatures = dv.feature_names_\\npattern = r'[\\\\[\\\\]<>]'\\nfeatures = [re.sub(pattern, '  ', f) for f in features]\\nAdded by Andrii Larkin\", 'section': '6. Decision Trees and Ensemble Learning', 'question': 'Q6: ValueError or TypeError while setting xgb.DMatrix(feature_names=)', 'course': 'machine-learning-zoomcamp'}, {'text': 'To install Xgboost, use the code below directly in your jupyter notebook:\\n(Pip 21.3+ is required)\\npip install xgboost\\nYou can update your pip by using the code below:\\npip install --upgrade pip\\nFor more about xgbboost and installation, check here:\\nhttps://xgboost.readthedocs.io/en/stable/install.html\\nAminat Abolade', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'How to Install Xgboost', 'course': 'machine-learning-zoomcamp'}, {'text': 'Sometimes someone might wonder what eta means in the tunable hyperparameters of XGBoost and how it helps the model.\\nETA is the learning rate of the model. XGBoost uses gradient descent to calculate and update the model. In gradient descent, we are looking for the minimum weights that help the model to learn the data very well. This minimum weights for the features is updated each time the model passes through the features and learns the features during training. Tuning the learning rate helps you tell the model what speed it would use in deriving the minimum for the weights.', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'What is eta in XGBoost', 'course': 'machine-learning-zoomcamp'}, {'text': 'For ensemble algorithms, during the week 6, one bagging algorithm and one boosting algorithm were presented: Random Forest and XGBoost, respectively.\\nRandom Forest trains several models in parallel. The output can be, for example, the average value of all the outputs of each model. This is called bagging.\\nXGBoost trains several models sequentially: the previous model error is used to train the following model. Weights are used to ponderate the models such as the best models have higher weights and are therefore favored for the final output. This method is called boosting.\\nNote that boosting is not necessarily better than bagging.\\nMélanie Fouesnard\\nBagging stands for “Bootstrap Aggregation” - it involves taking multiple samples with replacement to derive multiple training datasets from the original training dataset (bootstrapping), training a classifier (e.g. decision trees or stumps for Random Forests) on each such training dataset, and then combining the the predictions (aggregation) to obtain the final prediction. For classification, predictions are combined via voting; for regression, via averaging. Bagging can be done in parallel, since the various classifiers are independent. Bagging decreases variance (but not bias) and is robust against overfitting.\\nBoosting, on the other hand, is sequential - each model learns from the mistakes of its predecessor. Observations are given different weights - observations/samples misclassified by the previous classifier are given a higher weight, and this process is continued until a stopping condition is reached (e.g. max. No. of models is reached, or error is acceptably small, etc.). Boosting reduces bias & is generally more accurate than bagging, but can be prone to overfitting.\\nRileen', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'What is the difference between bagging and boosting?', 'course': 'machine-learning-zoomcamp'}, {'text': 'I wanted to directly capture the output from the xgboost training for multiple eta values to a dictionary without the need to run the same cell multiple times and manually editing the eta value in between or copy the code for a second eta value.\\nUsing the magic cell command “%%capture output” I was only able to capture the complete output for all iterations for the loop, but. I was able to solve this using the following approach. This is just a code sample to grasp the idea.\\n# This would be the content of the Jupyter Notebook cell\\nfrom IPython.utils.capture import capture_output\\nimport sys\\ndifferent_outputs = {}\\nfor i in range(3):\\nwith capture_output(sys.stdout) as output:\\nprint(i)\\nprint(\"testing capture\")\\ndifferent_outputs[i] = output.stdout\\n# different_outputs\\n# {0: \\'0\\\\ntesting capture\\\\n\\',\\n#  1: \\'1\\\\ntesting capture\\\\n\\',\\n#  2: \\'2\\\\ntesting capture\\\\n\\'}\\nAdded by Sylvia Schmitt', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'Capture stdout for each iterations of a loop separately', 'course': 'machine-learning-zoomcamp'}, {'text': 'Calling roc_auc_score() to get auc is throwing the above error.\\nSolution to this issue is to make sure that you pass y_actuals as 1st argument and y_pred as 2nd argument.\\nroc_auc_score(y_train, y_pred)\\nHareesh Tummala', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'ValueError: continuous format is not supported', 'course': 'machine-learning-zoomcamp'}, {'text': 'When rmse stops improving means, when it stops to decrease or remains almost similar.\\nPastor Soto', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'Question 3 of homework 6 if i see that rmse goes up at a certain number of n_estimators but then goes back down lower than it was before, should the answer be the number of n_estimators after which rmse initially went up, or the number after which it was its overall lowest value?', 'course': 'machine-learning-zoomcamp'}, {'text': 'dot_data = tree.export_graphviz(regr, out_file=None,\\nfeature_names=boston.feature_names,\\nfilled=True)\\ngraphviz.Source(dot_data, format=\"png\")\\nKrishna Anand\\nfrom sklearn import tree\\ntree.plot_tree(dt,feature_names=dv.feature_names_)\\nAdded By Ryan Pramana', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'One of the method to visualize the decision trees', 'course': 'machine-learning-zoomcamp'}, {'text': 'Solution: This problem happens because you use DecisionTreeClassifier instead of DecisionTreeRegressor. You should check if you want to use a Decision tree for classification or regression.\\nAlejandro Aponte', 'section': '6. Decision Trees and Ensemble Learning', 'question': \"ValueError: Unknown label type: 'continuous'\", 'course': 'machine-learning-zoomcamp'}, {'text': 'When I run dt = DecisionTreeClassifier() in jupyter in same laptop, each time I re-run it or do (restart kernel + run) I get different values of auc. Some of them are 0.674, 0.652, 0.642, 0.669 and so on.  Anyone knows why it could be? I am referring to 7:40-7:45 of video 6.3.\\nSolution: try setting the random seed e.g\\ndt = DecisionTreeClassifier(random_state=22)\\nBhaskar Sarma', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'Different values of auc, each time code is re-run', 'course': 'machine-learning-zoomcamp'}, {'text': \"They both do the same, it's just less typing from the script.\\nAsked by Andrew Katoch, Added by Edidiong Esu\", 'section': '6. Decision Trees and Ensemble Learning', 'question': 'Does it matter if we let the Python file create the server or if we run gunicorn directly?', 'course': 'machine-learning-zoomcamp'}, {'text': 'When I tried to run example from the video using function ping and can not import it. I use import ping and it was unsuccessful. To fix it I use the statement:\\n\\nfrom [file name] import ping\\nOlga Rudakova', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'No module named ‘ping’?', 'course': 'machine-learning-zoomcamp'}, {'text': 'The DictVectorizer has a function to get the feature names get_feature_names_out(). This is helpful for example if you need to analyze feature importance but use the dict vectorizer for one hot encoding. Just keep in mind it does return a numpy array so you may need to convert this to a list depending on your usage for example dv.get_feature_names_out() will return a ndarray array of string objects. list(dv.get_feature_names_out()) will convert to a standard list of strings. Also keep in mind that you first need to fit the predictor and response arrays before you have access to the feature names.\\nQuinn Avila', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'DictVectorizer feature names', 'course': 'machine-learning-zoomcamp'}, {'text': \"They both do the same, it's just less typing from the script.\", 'section': '6. Decision Trees and Ensemble Learning', 'question': 'Does it matter if we let the Python file create the server or if we run gunicorn directly?', 'course': 'machine-learning-zoomcamp'}, {'text': 'This error occurs because the list of feature names contains some characters like \"<\" that are not supported. To fix this issue, you can replace those problematic characters with supported ones. If you want to create a consistent list of features with no special characters, you can achieve it like this:\\nYou can address this error by replacing problematic characters in the feature names with underscores, like so:\\nfeatures = [f.replace(\\'=<\\', \\'_\\').replace(\\'=\\', \\'_\\') for f in features]\\nThis code will go through the list of features and replace any instances of \"=<\" with \"\", as well as any \"=\" with \"\", ensuring that the feature names only consist of supported characters.', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'ValueError: feature_names must be string, and may not contain [, ] or <', 'course': 'machine-learning-zoomcamp'}, {'text': \"To make it easier for us to determine which features are important, we can use a horizontal bar chart to illustrate feature importance sorted by value.\\n1. # extract the feature importances from the model\\nfeature_importances = list(zip(features_names, rdr_model.feature_importances_))\\nimportance_df = pd.DataFrame(feature_importances, columns=['feature_names', 'feature_importances'])\\n2. # sort descending the dataframe by using feature_importances value\\nimportance_df = importance_df.sort_values(by='feature_importances', ascending=False)\\n3. # create a horizontal bar chart\\nplt.figure(figsize=(8, 6))\\nsns.barplot(x='feature_importances', y='feature_names', data=importance_df, palette='Blues_r')\\nplt.xlabel('Feature Importance')\\nplt.ylabel('Feature Names')\\nplt.title('Feature Importance Chart')\\nRadikal Lukafiardi\", 'section': '6. Decision Trees and Ensemble Learning', 'question': 'Visualize Feature Importance by using horizontal bar chart', 'course': 'machine-learning-zoomcamp'}, {'text': 'Instead of using np.sqrt() as the second step. You can extract it using like this way :\\nmean_squared_error(y_val, y_predict_val,squared=False)\\nAhmed Okka', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'RMSE using metrics.root_meas_square()', 'course': 'machine-learning-zoomcamp'}, {'text': 'I like this visual implementation of features importance in scikit-learn library:\\nhttps://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\\nIt actually adds std.errors to features importance -> so that you can trace stability of features (important for a model’s explainability) over the different params of the model.\\nIvan Brigida', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'Features Importance graph', 'course': 'machine-learning-zoomcamp'}, {'text': 'Expanded error says: xgboost.core.XGBoostError: sklearn needs to be installed in order to use this module. So, sklearn in requirements solved the problem.\\nGeorge Chizhmak', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'xgboost.core.XGBoostError: This app has encountered an error. The original error message is redacted to prevent data leaks.', 'course': 'machine-learning-zoomcamp'}, {'text': 'Information gain  in Y due to X, or the mutual information of Y and X\\nWhere  is the entropy of Y. \\n\\nIf X is completely uninformative about Y:\\nIf X is completely informative about Y: )\\nHrithik Kumar Advani', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'Information Gain', 'course': 'machine-learning-zoomcamp'}, {'text': 'Filling in missing values using an entire dataset before splitting for training/testing/validation causes', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'Data Leakage', 'course': 'machine-learning-zoomcamp'}, {'text': 'Save model by calling ‘booster.save_model’, see eg\\nLoad model:\\nDawuta Smit\\nThis section is moved to Projects', 'section': '8. Neural Networks and Deep Learning', 'question': 'Serialized Model Xgboost error', 'course': 'machine-learning-zoomcamp'}, {'text': 'TODO', 'section': '8. Neural Networks and Deep Learning', 'question': 'How to get started with Week 8?', 'course': 'machine-learning-zoomcamp'}, {'text': 'Create or import your notebook into Kaggle.\\nClick on the Three dots at the top right hand side\\nClick on Accelerator\\nChoose T4 GPU\\nKhurram Majeed', 'section': '8. Neural Networks and Deep Learning', 'question': 'How to use Kaggle for Deep Learning?', 'course': 'machine-learning-zoomcamp'}, {'text': 'Create or import your notebook into Google Colab.\\nClick on the Drop Down at the top right hand side\\nClick on “Change runtime type”\\nChoose T4 GPU\\nKhurram Majeed', 'section': '8. Neural Networks and Deep Learning', 'question': 'How to use Google Colab for Deep Learning?', 'course': 'machine-learning-zoomcamp'}, {'text': 'Connecting your GPU on Saturn Cloud to Github repository is not compulsory, since you can just download the notebook and copy it to the Github folder. But if you like technology to do things for you, then follow the solution description below:\\nSolution description: Follow the instructions in these github docs to create an SSH private and public key:\\nhttps://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-ke\\ny-and-adding-it-to-the-ssh-agenthttps://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account?tool=webui\\nThen the second video on this module about saturn cloud would show you how to add the ssh keys to secrets and authenticate through a terminal.\\nOr alternatively, you could just use the public keys provided by Saturn Cloud by default. To do so, follow these steps:\\nClick on your username and on manage\\nDown below you will see the Git SSH keys section.\\nCopy the default public key provided by Saturn Cloud\\nPaste these key into the SSH keys section of your github repo\\nOpen a terminal on Saturn Cloud and run this command “ssh -T git@github.com”\\nYou will receive a successful authentication notice.\\nOdimegwu David', 'section': '8. Neural Networks and Deep Learning', 'question': 'How do I push from Saturn Cloud to Github?', 'course': 'machine-learning-zoomcamp'}, {'text': 'This template is referred to in the video 8.1b Setting up the Environment on Saturn Cloud\\nbut the location shown in the video is no longer correct.\\nThis template has been moved to “python deep learning tutorials’ which is shown on the Saturn Cloud home page.\\nSteven Christolis', 'section': '8. Neural Networks and Deep Learning', 'question': 'Where is the Python TensorFlow template on Saturn Cloud?', 'course': 'machine-learning-zoomcamp'}, {'text': 'The above error happens since module scipy is not installed in the saturn cloud tensorflow image. While creating the Jupyter server resource, in the “Extra Packages” section under pip in the textbox write scipy. Below the textbox, the pip install scipy command will be displayed. This will ensure when the resource spins up, the scipy package will be automatically installed. This approach can also be followed for additional python packages.\\nSumeet Lalla', 'section': '8. Neural Networks and Deep Learning', 'question': 'Getting error module scipy not found during model training in Saturn Cloud tensorflow image', 'course': 'machine-learning-zoomcamp'}, {'text': 'Problem description: Uploading the data to saturn cloud from kaggle can be time saving, specially if the dataset is large.\\nYou can just download to your local machine and then upload to a folder on saturn cloud, but there is a better solution that needs to be set once and you have access to all kaggle datasets in saturn cloud.\\nOn your notebook run:\\n!pip install -q kaggle\\nGo to Kaggle website (you need to have an account for this):\\nClick on your profile image -> Account\\nScroll down to the API box\\nClick on Create New API token\\nIt will download a json file with the name kaggle.json store on your local computer. We need to upload this file in the .kaggle folder\\nOn the notebook click on folder icon on the left upper corner\\nThis will take you to the root folder\\nClick on the .kaggle folder\\nOnce inside of the .kaggle folder upload the kaggle.json file that you downloaded\\nRun this command on your notebook:\\n!chmod 600 /home/jovyan/.kaggle/kaggle.json\\nDownload the data using this command:\\n!kaggle datasets download -d agrigorev/dino-or-dragon\\nCreate a folder to unzip your files:\\n!mkdir data\\nUnzip your files inside that folder\\n!unzip dino-or-dragon.zip -d data\\nPastor Soto', 'section': '8. Neural Networks and Deep Learning', 'question': 'How to upload kaggle data to Saturn Cloud?', 'course': 'machine-learning-zoomcamp'}, {'text': 'In order to run tensorflow with gpu on your local machine you’ll need to setup cuda and cudnn.\\nThe process can be overwhelming. Here’s a simplified guide\\nOsman Ali', 'section': '8. Neural Networks and Deep Learning', 'question': 'How to install CUDA & cuDNN on Ubuntu 22.04', 'course': 'machine-learning-zoomcamp'}, {'text': 'Problem description:\\nWhen loading saved model getting error: ValueError: Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights.\\nSolution description:\\nBefore loading model need to evaluate the model on input data: model.evaluate(train_ds)\\nAdded by Vladimir Yesipov', 'section': '8. Neural Networks and Deep Learning', 'question': 'Error: (ValueError: Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights.) when loading model.', 'course': 'machine-learning-zoomcamp'}, {'text': 'Problem description:\\nWhen follow module 8.1b video to setup git in Saturn Cloud, run `ssh -T git@github.com` lead error `git@github.com: Permission denied (publickey).`\\nSolution description:\\nAlternative way, we can setup git in our Saturn Cloud env with generate SSH key in our Saturn Cloud and add it to our git account host. After it, we can access/manage our git through Saturn’s jupyter server. All steps detailed on this following tutorial: https://saturncloud.io/docs/using-saturn-cloud/gitrepo/\\nAdded by Ryan Pramana', 'section': '8. Neural Networks and Deep Learning', 'question': 'Getting error when connect git on Saturn Cloud: permission denied', 'course': 'machine-learning-zoomcamp'}, {'text': \"Problem description:\\nGetting an error using <git clone git@github.com:alexeygrigorev/clothing-dataset-small.git>\\nThe error:\\nCloning into 'clothing-dataset'...\\nHost key verification failed.\\nfatal: Could not read from remote repository.\\nPlease make sure you have the correct access rights\\nand the repository exists.\\nSolution description:\\nwhen cloning the repo, you can also chose https - then it should work. This happens when you don't have your ssh key configured.\\n<git clone https://github.com/alexeygrigorev/clothing-dataset-small.git>\\nAdded by Gregory Morris\", 'section': '8. Neural Networks and Deep Learning', 'question': 'Host key verification failed.', 'course': 'machine-learning-zoomcamp'}, {'text': \"Problem description\\nThe accuracy and the loss are both still the same or nearly the same while training.\\nSolution description\\nIn the homework, you should set class_mode='binary' while reading the data.\\nAlso, problem occurs when you choose the wrong optimizer, batch size, or learning rate\\nAdded by Ekaterina Kutovaia\", 'section': '8. Neural Networks and Deep Learning', 'question': 'The same accuracy on epochs', 'course': 'machine-learning-zoomcamp'}, {'text': 'Problem:\\nWhen resuming training after augmentation, the loss skyrockets (1000+ during first epoch) and accuracy settles around 0.5 – i.e. the model becomes as good as a random coin flip.\\nSolution:\\nCheck that the augmented ImageDataGenerator still includes the option “rescale” as specified in the preceding step.\\nAdded by Konrad Mühlberg', 'section': '8. Neural Networks and Deep Learning', 'question': 'Model breaking after augmentation – high loss + bad accuracy', 'course': 'machine-learning-zoomcamp'}, {'text': \"While doing:\\nimport tensorflow as tf\\nfrom tensorflow import keras\\nmodel = tf.keras.models.load_model('model_saved.h5')\\nIf you get an error message like this:\\nValueError: The channel dimension of the inputs should be defined. The input_shape received is (None, None, None, None), where axis -1 (0-based) is the channel dimension, which found to be `None`.\\nSolution:\\nSaving a model (either yourself via model.save() or via checkpoint when save_weights_only = False) saves two things: The trained model weights (for example the best weights found during training) and the model architecture.  If the number of channels is not explicitly specified in the Input layer of the model, and is instead defined as a variable, the model architecture will not have the value in the variable stored. Therefore when the model is reloaded, it will complain about not knowing the number of channels. See the code below, in the first line, you need to specify number of channels explicitly:\\n# model architecture:\\ninputs = keras.Input(shape=(input_size, input_size, 3))\\nbase = base_model(inputs, training=False)\\nvectors = keras.layers.GlobalAveragePooling2D()(base)\\ninner = keras.layers.Dense(size_inner, activation='relu')(vectors)\\ndrop = keras.layers.Dropout(droprate)(inner)\\noutputs = keras.layers.Dense(10)(drop)\\nmodel = keras.Model(inputs, outputs)\\n(Memoona Tahira)\", 'section': '8. Neural Networks and Deep Learning', 'question': 'Missing channel value error while reloading model:', 'course': 'machine-learning-zoomcamp'}, {'text': \"Problem:\\nA dataset for homework is in a zipped folder. If you unzip it within a jupyter notebook by means of ! unzip command, you’ll see a huge amount of output messages about unzipping of each image. So you need to suppress this output\\nSolution:\\nExecute the next cell:\\n%%capture\\n! unzip zipped_folder_name.zip -d destination_folder_name\\nAdded by Alena Kniazeva\\nInside a Jupyter Notebook:\\nimport zipfile\\nlocal_zip = 'data.zip'\\nzip_ref = zipfile.ZipFile(local_zip, 'r')\\nzip_ref.extractall('data')\\nzip_ref.close()\", 'section': '8. Neural Networks and Deep Learning', 'question': 'How to unzip a folder with an image dataset and suppress output?', 'course': 'machine-learning-zoomcamp'}, {'text': 'Problem:\\nWhen we run train_gen.flow_from_directory() as in video 8.5, it finds images belonging to 10 classes. Does it understand the names of classes from the names of folders? Or, there is already something going on deep behind?\\nSolution:\\nThe name of class is the folder name\\nIf you just create some random folder with the name \"xyz\", it will also be considered as a class!! The name itself is saying flow_from_directory\\na clear explanation below:\\nhttps://vijayabhaskar96.medium.com/tutorial-image-classification-with-keras-flow-from-directory-and-generators-95f75ebe5720\\nAdded by Bhaskar Sarma', 'section': '8. Neural Networks and Deep Learning', 'question': 'How keras flow_from_directory know the names of classes in images?', 'course': 'machine-learning-zoomcamp'}, {'text': 'Problem:\\nI created a new environment in SaturnCloud and chose the image corresponding to Saturn with Tensorflow, but when I tried to fit the model it showed an error about the missing module: scipy\\nSolution:\\nInstall the module in a new cell: !pip install scipy\\nRestart the kernel and fit the model again\\nAdded by Erick Calderin', 'section': '8. Neural Networks and Deep Learning', 'question': 'Error with scipy missing module in SaturnCloud', 'course': 'machine-learning-zoomcamp'}, {'text': 'The command to read folders in the dataset in the tensorflow source code is:\\nfor subdir in sorted(os.listdir(directory)):\\n…\\nReference: https://github.com/keras-team/keras/blob/master/keras/preprocessing/image.py, line 563\\nThis means folders will be read in alphabetical order. For example, in the case of a folder named dino, and another named dragon, dino will read first and will have class label 0, whereas dragon will be read in next and will have class label 1.\\nWhen a Keras model predicts binary labels, it will only return one value, and this is the probability of class 1 in case of sigmoid activation function in the last dense layer with 2 neurons. The probability of class 0 can be found out by:\\nprob(class(0)) = 1- prob(class(1))\\nIn case of using from_logits to get results, you will get two values for each of the labels.\\nA prediction of 0.8 is saying the probability that the image has class label 1 (in this case dragon), is 0.8, and conversely we can infer the probability that the image has class label 0 is 0.2.\\n(Added by Memoona Tahira)', 'section': '8. Neural Networks and Deep Learning', 'question': 'How are numeric class labels determined in flow_from_directroy using binary class mode and what is meant by the single probability predicted by a binary Keras model:', 'course': 'machine-learning-zoomcamp'}, {'text': \"It's fine, some small changes are expected\\nAlexey Grigorev\", 'section': '8. Neural Networks and Deep Learning', 'question': 'Does the actual values matter after predicting with a neural network or it should be treated as like hood of falling in a class?', 'course': 'machine-learning-zoomcamp'}, {'text': 'Problem:\\nI found running the wasp/bee model on my mac laptop had higher reported accuracy and lower std deviation than the HW answers. This may be because of the SGD optimizer. Running this on my mac printed a message about a new and legacy version that could be used.\\nSolution:\\nTry running the same code on google collab or another way. The answers were closer for me on collab. Another tip is to change the runtime to use T4 and the model run’s faster than just CPU\\nAdded by Quinn Avila', 'section': '8. Neural Networks and Deep Learning', 'question': 'What if your accuracy and std training loss don’t match HW?', 'course': 'machine-learning-zoomcamp'}, {'text': 'When running “model.fit(...)” an additional parameter “workers” can be specified for speeding up the data loading/generation. The default value is “1”. Try out which value between 1 and the cpu count on your system performs best.\\nhttps://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\\nAdded by Sylvia Schmitt', 'section': '8. Neural Networks and Deep Learning', 'question': 'Using multi-threading for data generation in “model.fit()”', 'course': 'machine-learning-zoomcamp'}, {'text': 'Reproducibility for training runs can be achieved following these instructions: \\nhttps://www.tensorflow.org/versions/r2.8/api_docs/python/tf/config/experimental/enable_op_determinism\\nseed = 1234\\ntf.keras.utils.set_random_seed(seed)\\ntf.config.experimental.enable_op_determinism()\\nThis will work for a script, if this gets executed multiple times.\\nAdded by Sylvia Schmitt', 'section': '8. Neural Networks and Deep Learning', 'question': 'Reproducibility with TensorFlow using a seed point', 'course': 'machine-learning-zoomcamp'}, {'text': 'Pytorch is also a deep learning framework that allows to do equivalent tasks as keras. Here is a tutorial to create a CNN from scratch using pytorch :\\nhttps://blog.paperspace.com/writing-cnns-from-scratch-in-pytorch/\\nThe functions have similar goals. The syntax can be slightly different. For the lessons and the homework, we use keras, but one can feel free to make a pull request with the equivalent with pytorch for lessons and homework!\\nMélanie Fouesnard', 'section': '8. Neural Networks and Deep Learning', 'question': 'Can we use pytorch for this lesson/homework ?', 'course': 'machine-learning-zoomcamp'}, {'text': \"While training a Keras model you get the error “Failed to find data adapter that can handle input: <class 'keras.src.preprocessing.image.ImageDataGenerator'>, <class 'NoneType'>” you may have unintentionally passed the image generator instead of the dataset to the model\\ntrain_gen = ImageDataGenerator(rescale=1./255)\\ntrain_ds = train_gen.flow_from_directory(…)\\nhistory_after_augmentation = model.fit(\\ntrain_gen, # this should be train_ds!!!\\nepochs=10,\\nvalidation_data=test_gen # this should be test_ds!!!\\n)\\nThe fix is simple and probably obvious once pointed out, use the training and validation dataset (train_ds and val_ds) returned from flow_from_directory\\nAdded by Tzvi Friedman\", 'section': '8. Neural Networks and Deep Learning', 'question': 'Keras model training fails with “Failed to find data adapter”', 'course': 'machine-learning-zoomcamp'}, {'text': 'The command ‘nvidia-smi’ has a built-in function which will run it in subsequently updating it every N seconds without the need of using the command ‘watch’.\\nnvidia-smi -l <N seconds>\\nThe following command will run ‘nvidia-smi’ every 2 seconds until interrupted using CTRL+C.\\nnvidia-smi -l 2\\nAdded by Sylvia Schmitt', 'section': '8. Neural Networks and Deep Learning', 'question': 'Running ‘nvidia-smi’ in a loop without using ‘watch’', 'course': 'machine-learning-zoomcamp'}, {'text': 'The Python package ‘’ is an interactive GPU process viewer similar to ‘htop’ for CPU.\\nhttps://pypi.org/project//\\nImage source: https://pypi.org/project//\\nAdded by Sylvia Schmitt', 'section': '8. Neural Networks and Deep Learning', 'question': 'Checking GPU and CPU utilization using ‘nvitop’', 'course': 'machine-learning-zoomcamp'}, {'text': \"Let’s say we define our Conv2d layer like this:\\n>> tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3))\\nIt means our input image is RGB (3 channels, 150 by 150 pixels), kernel is 3x3 and number of filters (layer’s width) is 32.\\nIf we check model.summary() we will get this:\\n_________________________________________________________________\\nLayer (type)                Output Shape              Param #\\n=================================================================\\nconv2d (Conv2D)             (None, 148, 148, 32)      896\\nSo where does 896 params come from? It’s computed like this:\\n>>> (3*3*3 +1) * 32\\n896\\n# 3x3 kernel, 3 channels RGB, +1 for bias, 32 filters\\nWhat about the number of “features” we get after the Flatten layer?\\nFor our homework model.summary() for last MaxPooling2d and Flatten layers looked like this:\\n_________________________________________________________________\\nLayer (type)                Output Shape              Param #\\n=================================================================\\nmax_pooling2d_3       (None, 7, 7, 128)         0\\nflatten (Flatten)           (None, 6272)              0\\nSo where do 6272 vectors come from? It’s computed like this:\\n>>> 7*7*128\\n6272\\n# 7x7 “image shape” after several convolutions and poolings, 128 filters\\nAdded by Andrii Larkin\", 'section': '8. Neural Networks and Deep Learning', 'question': 'Q: Where does the number of Conv2d layer’s params come from? Where does the number of “features” we get after the Flatten layer come from?', 'course': 'machine-learning-zoomcamp'}, {'text': 'It’s quite useful to understand that all types of models in the course are a plain stack of layers where each layer has exactly one input tensor and one output tensor (Sequential model TF page, Sequential class).\\nYou can simply start from an “empty” model and add more and more layers in a sequential order.\\nThis mode is called “Sequential Model API”  (easier)\\nIn Alexey’s videos it is implemented as chained calls of different entities (“inputs”,“base”, “vectors”,  “outputs”) in a more advanced mode “Functional Model API”.\\nMaybe a more complicated way makes sense when you do Transfer Learning and want to separate “Base” model vs. rest, but in the HW you need to recreate the full model from scratch ⇒ I believe it is easier to work with a sequence of “similar” layers.\\nYou can read more about it in this TF2 tutorial.\\nA really useful Sequential model example is shared in the Kaggle’s “Bee or Wasp” dataset folder with code: notebook\\nAdded by Ivan Brigida\\nFresh Run on Neural Nets\\nWhile correcting an error on neural net architecture, it is advised to do fresh run by restarting kernel, else the model learns on top of previous runs.\\nAdded by Abhijit Chakraborty', 'section': '8. Neural Networks and Deep Learning', 'question': 'Sequential vs. Functional Model Modes in Keras (TF2)', 'course': 'machine-learning-zoomcamp'}, {'text': \"I found this code snippet fixed my OOM errors, as I have an Nvidia GPU. Can't speak to OOM errors on CPU, though.\\nhttps://www.tensorflow.org/api_docs/python/tf/config/experimental/set_memory_growth\\n```\\nphysical_devices = tf.configlist_physical_devices('GPU')\\ntry:\\ntf.config.experimental.set_memory_growth(physical_devices[0],True)\\nexcept:\\n# Invalid device or cannot modify virtual devices once initialized.\\npass\\n```\", 'section': '8. Neural Networks and Deep Learning', 'question': 'Out of memory errors when running tensorflow', 'course': 'machine-learning-zoomcamp'}, {'text': 'When training the models, in the fit function, you can specify the number of workers/threads.\\nThe number of threads apparently also works for GPUs, and came very handy in google colab for the T4 GPU, since it was very very slow, and workers default value is 1.\\nI changed the workers variable to 2560, following this thread in stackoverflow. I am using the free T4 GPU.  (https://stackoverflow.com/questions/68208398/how-to-find-the-number-of-cores-in-google-colabs-gpu)\\nAdded by Ibai Irastorza', 'section': '8. Neural Networks and Deep Learning', 'question': 'Model training very slow in google colab with T4 GPU', 'course': 'machine-learning-zoomcamp'}, {'text': 'From the keras documentation:\\nDeprecated: tf.keras.preprocessing.image.ImageDataGenerator is not recommended for new code. Prefer loading images with tf.keras.utils.image_dataset_from_directory and transforming the output tf.data.Dataset with preprocessing layers. For more information, see the tutorials for loading images and augmenting images, as well as the preprocessing layer guide.\\nHrithik Kumar Advani', 'section': '9. Serverless Deep Learning', 'question': 'Using image_dataset_from_directory instead of ImageDataGeneratorn for loading images', 'course': 'machine-learning-zoomcamp'}, {'text': 'TODO', 'section': '9. Serverless Deep Learning', 'question': 'How to get started with Week 9?', 'course': 'machine-learning-zoomcamp'}, {'text': 'The week 9 uses a link to github to fetch the models.\\nThe original link was moved to here:\\nhttps://github.com/DataTalksClub/machine-learning-zoomcamp/releases', 'section': '9. Serverless Deep Learning', 'question': 'Where is the model for week 9?', 'course': 'machine-learning-zoomcamp'}, {'text': 'Solution description\\nIn the unit 9.6, Alexey ran the command echo ${REMOTE_URI} which turned the URI address in the terminal. There workaround is to set a local variable (REMOTE_URI) and assign your URI address in the terminal and use it to login the registry, for instance, REMOTE_URI=2278222782.dkr.ecr.ap-south-1.amazonaws.com/clothing-tflite-images (fake address). One caveat is that you will lose this variable once the session is terminated.\\nI also had the same problem on Ubuntu terminal. I executed the following two commands:\\n$ export REMOTE_URI=1111111111.dkr.ecr.us-west-1.amazonaws.com/clothing-tflite-images:clothing-model-xception-v4-001\\n$ echo $REMOTE_URI\\n111111111.dkr.ecr.us-west-1.amazonaws.com/clothing-tflite-images:clothing-model-xception-v4-001\\nNote: 1. no curly brackets (e.g. echo ${REMOTE_URI}) needed unlike in video 9.6,\\n2. Replace REMOTE_URI with your URI\\n(Bhaskar Sarma)', 'section': '9. Serverless Deep Learning', 'question': 'Executing the command echo ${REMOTE_URI} returns nothing.', 'course': 'machine-learning-zoomcamp'}, {'text': 'The command aws ecr get-login --no-include-email returns an invalid choice error:\\nThe solution is to use the following command instead:  aws ecr get-login-password\\nCould simplify the login process with, just replace the <ACCOUNT_NUMBER> and <REGION> with your values:\\nexport PASSWORD=`aws ecr get-login-password`\\ndocker login -u AWS -p $PASSWORD <ACCOUNT_NUMBER>.dkr.ecr.<REGION>.amazonaws.com/clothing-tflite-images\\nAdded by Martin Uribe', 'section': '9. Serverless Deep Learning', 'question': 'Getting a syntax error while trying to get the password from aws-cli', 'course': 'machine-learning-zoomcamp'}, {'text': 'We can use the keras.models.Sequential() function to pass many parameters of the cnn at once.\\nKrishna Anand', 'section': '9. Serverless Deep Learning', 'question': 'Pass many parameters in the model at once', 'course': 'machine-learning-zoomcamp'}, {'text': 'This error is produced sometimes when building your docker image from the Amazon python base image.\\nSolution description: The following could solve the problem.\\nUpdate your docker desktop if you haven’t done so.\\nOr restart docker desktop and terminal and then build the image all over again.\\nOr if all else fails, first run the following command: DOCKER_BUILDKIT=0  docker build .  then build your image.\\n(optional) Added by Odimegwu David', 'section': '9. Serverless Deep Learning', 'question': 'Getting  ERROR [internal] load metadata for public.ecr.aws/lambda/python:3.8', 'course': 'machine-learning-zoomcamp'}, {'text': \"When trying to run the command  !ls -lh in windows jupyter notebook  , I was getting an error message that says “'ls' is not recognized as an internal or external command,operable program or batch file.\\nSolution description :\\nInstead of !ls -lh , you can use this command !dir , and you will get similar output\\nAsia Saeed\", 'section': '9. Serverless Deep Learning', 'question': \"Problem: 'ls' is not recognized as an internal or external command, operable program or batch file.\", 'course': 'machine-learning-zoomcamp'}, {'text': 'When I run   import tflite_runtime.interpreter as tflite , I get an error message says “ImportError: generic_type: type \"InterpreterWrapper\" is already registered!”\\nSolution description\\nThis error occurs when you import both tensorflow  and tflite_runtime.interpreter  “import tensorflow as tf” and “import tflite_runtime.interpreter as tflite” in the same notebook.  To fix the issue, restart the kernel and import only tflite_runtime.interpreter \" import tflite_runtime.interpreter as tflite\".\\nAsia Saeed', 'section': '9. Serverless Deep Learning', 'question': 'ImportError: generic_type: type \"InterpreterWrapper\" is already registered!', 'course': 'machine-learning-zoomcamp'}, {'text': 'Problem description:\\nIn command line try to do $ docker build -t dino_dragon\\ngot this Using default tag: latest\\n[2022-11-24T06:48:47.360149000Z][docker-credential-desktop][W] Windows version might not be up-to-date: The system cannot find the file specified.\\nerror during connect: This error may indicate that the docker daemon is not running.: Post\\n.\\nSolution description:\\nYou need to make sure that Docker is not stopped by a third-party program.\\nAndrei Ilin', 'section': '9. Serverless Deep Learning', 'question': 'Windows version might not be up-to-date', 'course': 'machine-learning-zoomcamp'}, {'text': 'When running docker build -t dino-dragon-model it returns the above error\\nThe most common source of this error in this week is because Alex video shows a version of the wheel with python 8, we need to find a wheel with the version that we are working on. In this case python 9. Another common error is to copy the link, this will also produce the same error, we need to download the raw format:\\nhttps://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.7.0-cp39-cp39-linux_x86_64.whl\\nPastor Soto', 'section': '9. Serverless Deep Learning', 'question': 'WARNING: You are using pip version 22.0.4; however, version 22.3.1 is available', 'course': 'machine-learning-zoomcamp'}, {'text': 'Problem description:\\nIn video 9.6, after installing aswcli, we should configure it with aws configure . There it asks for Access Key ID, Secret Access Key, Default Region Name and also Default output format. What we should put for Default output format? Leaving it as  None is okay?\\nSolution description:\\nYes, in my I case I left everything as the provided defaults (except, obviously, the Access key and the secret access key)\\nAdded by Bhaskar Sarma', 'section': '9. Serverless Deep Learning', 'question': 'How to do AWS configure after installing awscli', 'course': 'machine-learning-zoomcamp'}, {'text': 'Problem:\\nWhile passing local testing of the lambda function without issues, trying to test the same input with a running docker instance results in an error message like\\n{‘errorMessage’: ‘Unable to marshal response: Object of type float32 is not JSON serializable’, ‘errorType’: ‘Runtime.MarshalError’, ‘requestId’: ‘f155492c-9af2-4d04-b5a4-639548b7c7ac’, ‘stackTrace’: []}\\nThis happens when a model (in this case the dino vs dragon model) returns individual estimation values as numpy float32 values (arrays). They need to be converted individually to base-Python floats in order to become “serializable”.\\nSolution:\\nIn my particular case, I set up the dino vs dragon model in such a way as to return a label + predicted probability for each class as follows (below is a two-line extract of function predict() in the lambda_function.py):\\npreds = [interpreter.get_tensor(output_index)[0][0], \\\\\\n1-interpreter.get_tensor(output_index)[0][0]]\\nIn which case the above described solution will look like this:\\npreds = [float(interpreter.get_tensor(output_index)[0][0]), \\\\\\nfloat(1-interpreter.get_tensor(output_index)[0][0])]\\nThe rest can be made work by following the chapter 9 (and/or chapter 5!) lecture videos step by step.\\nAdded by Konrad Muehlberg', 'section': '9. Serverless Deep Learning', 'question': 'Object of type float32 is not JSON serializable', 'course': 'machine-learning-zoomcamp'}, {'text': 'I had this error when running the command line : interpreter.set_tensor(input_index, x) that can be seen in the video 9.3 around 12 minutes.\\nValueError: Cannot set tensor: Got value of type UINT8 but expected type FLOAT32 for input 0, name: serving_default_conv2d_input:0\\nThis is because the X is an int but a float is expected.\\nSolution:\\nI found this solution from this question here https://stackoverflow.com/questions/76102508/valueerror-cannot-set-tensor-got-value-of-type-float64-but-expected-type-float :\\n# Need to convert to float32 before set_tensor\\nX = np.float32(X)\\nThen, it works. I work with tensorflow 2.15.0, maybe the fact that this version is more recent involves this change ?\\nAdded by Mélanie Fouesnard', 'section': '9. Serverless Deep Learning', 'question': 'Error with the line “interpreter.set_tensor(input_index, X”)', 'course': 'machine-learning-zoomcamp'}, {'text': 'To check your file size using the powershell terminal, you can do the following command lines:\\n$File = Get-Item -Path path_to_file\\n$FileSize = (Get-Item -Path $FilePath).Length\\nNow you can check the size of your file, for example in MB:\\nWrite-host \"MB\":($FileSize/1MB)\\nSource: https://www.sharepointdiary.com/2020/10/powershell-get-file-size.html#:~:text=To%20get%20the%20size%20of,the%20file%2C%20including%20its%20size.\\nAdded by Mélanie Fouesnard', 'section': '9. Serverless Deep Learning', 'question': 'How to easily get file size in powershell terminal ?', 'course': 'machine-learning-zoomcamp'}, {'text': 'I wanted to understand how lambda container images work in depth and how lambda functions are initialized, for this reason, I found the following documentation\\nhttps://docs.aws.amazon.com/lambda/latest/dg/images-create.html\\nhttps://docs.aws.amazon.com/lambda/latest/dg/runtimes-api.html\\nAdded by Alejandro aponte', 'section': '9. Serverless Deep Learning', 'question': 'How do Lambda container images work?', 'course': 'machine-learning-zoomcamp'}, {'text': 'The docker image for aws lambda can be created and pushed to aws ecr and the same can be exposed as a REST API through APIGatewayService in a single go using AWS Serverless Framework. Refer the below article for a detailed walkthrough.\\nhttps://medium.com/hoonio/deploy-containerized-serverless-flask-to-aws-lambda-c0eb87c1404d\\nAdded by Sumeet Lalla', 'section': '9. Serverless Deep Learning', 'question': 'How to use AWS Serverless Framework to deploy on AWS Lambda and expose it as REST API through APIGatewayService?', 'course': 'machine-learning-zoomcamp'}, {'text': 'Problem:\\nWhile trying to build docker image in Section 9.5 with the command:\\ndocker build -t clothing-model .\\nIt throws a pip install error for the tflite runtime whl\\nERROR: failed to solve: process \"/bin/sh -c pip install https://github.com/alexeygrigorev/tflite-aws-lambda/blob/main/tflite/tflite_runtime-2.14.0-cp310-cp310-linux_x86_64.whl\" did not complete successfully: exit code: 1\\nTry to use this link: https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.14.0-cp310-cp310-linux_x86_64.whl\\nIf the link above does not work:\\nThe problem is because of the arm architecture of the M1. You will need to run the code on a PC or Ubuntu OS.\\nOr try the code bellow.\\nAdded by Dashel Ruiz Perez\\nSolution:\\nTo build the Docker image, use the command:\\ndocker build --platform linux/amd64 -t clothing-model .\\nTo run the built image, use the command:\\ndocker run -it --rm -p 8080:8080 --platform linux/amd64 clothing-model:latest\\nAdded by Daniel Egbo', 'section': '9. Serverless Deep Learning', 'question': 'Error building docker image on M1 Mac', 'course': 'machine-learning-zoomcamp'}, {'text': \"Problem: Trying to test API gateway in 9.7 - API Gateway: Exposing the Lambda Function, running: $ python test.py\\nWith error message:\\n{'message': 'Missing Authentication Token'}\\nSolution:\\nNeed to get the deployed API URL for the specific path you are invoking. Example:\\nhttps://<random string>.execute-api.us-east-2.amazonaws.com/test/predict\\nAdded by Andrew Katoch\", 'section': '9. Serverless Deep Learning', 'question': 'Error invoking API Gateway deploy API locally', 'course': 'machine-learning-zoomcamp'}, {'text': 'Problem: When trying to install tflite_runtime with\\n!pip install --extra-index-url https://google-coral.github.io/py-repo/ tflite_runtime\\none gets an error message above.\\nSolution:\\nfflite_runtime is only available for the os-python version combinations that can be found here: https://google-coral.github.io/py-repo/tflite-runtime/\\nyour combination must be missing here\\nyou can see if any of these work for you https://github.com/alexeygrigorev/tflite-aws-lambda/tree/main/tflite\\nand install the needed one using pip\\neg\\npip install https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.7.0-cp38-cp38-linux_x86_64.whl\\nas it is done in the lectures code:\\nhttps://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/09-serverless/code/Dockerfile#L4\\nAlternatively, use a virtual machine (with VM VirtualBox, for example) with a Linux system. The other way is to run a code at a virtual machine within cloud service, for example you can use Vertex AI Workbench at GCP (notebooks and terminals are provided there, so all tasks may be performed).\\nAdded by Alena Kniazeva, modified by Alex Litvinov', 'section': '9. Serverless Deep Learning', 'question': 'Error: Could not find a version that satisfies the requirement tflite_runtime (from versions:none)', 'course': 'machine-learning-zoomcamp'}, {'text': 'docker: Error response from daemon: mkdir /var/lib/docker/overlay2/37be849565da96ac3fce34ee9eb2215bd6cd7899a63ebc0ace481fd735c4cb0e-init: read-only file system.\\nYou need to restart the docker services to get rid of the above error\\nKrishna Anand', 'section': '9. Serverless Deep Learning', 'question': 'Docker run error', 'course': 'machine-learning-zoomcamp'}, {'text': 'The docker image can be saved/exported to tar format in local machine using the below command:\\ndocker image save <image-name> -o <name-of-tar-file.tar>\\nThe individual layers of the docker image for the filesystem content can be viewed by extracting the layer.tar present in the <name-of-tar-file.tar> created from above.\\nSumeet Lalla', 'section': '9. Serverless Deep Learning', 'question': 'Save Docker Image to local machine and view contents', 'course': 'machine-learning-zoomcamp'}, {'text': 'On vscode running jupyter notebook. After I ‘pip install pillow’, my notebook did not recognize using the import for example from PIL import image. After restarting the jupyter notebook the imports worked.\\nQuinn Avila', 'section': '9. Serverless Deep Learning', 'question': 'Jupyter notebook not seeing package', 'course': 'machine-learning-zoomcamp'}, {'text': 'Due to experimenting back and forth so much without care for storage, I just ran out of it on my 30-GB AWS instance. It turns out that deleting docker images does not actually free up any space as you might expect. After removing images, you also need to run docker system prune', 'section': '9. Serverless Deep Learning', 'question': 'Running out of space for AWS instance.', 'course': 'machine-learning-zoomcamp'}, {'text': 'Using the 2.14 version with python 3.11 works fine.\\nIn case it doesn’t work, I tried with tensorflow 2.4.4 whl, however, make sure to run it on top of supported python versions like 3.8, else there will be issues installing tf==2.4.4\\nAdded by Abhijit Chakraborty', 'section': '9. Serverless Deep Learning', 'question': 'Using Tensorflow 2.15 for AWS deployment', 'course': 'machine-learning-zoomcamp'}, {'text': 'see here', 'section': '9. Serverless Deep Learning', 'question': 'Command aws ecr get-login --no-include-email returns “aws: error: argument operation: Invalid choice…”', 'course': 'machine-learning-zoomcamp'}, {'text': 'Sign in to the AWS Console: Log in to the AWS Console.\\nNavigate to IAM: Go to the IAM service by clicking on \"Services\" in the top left corner and selecting \"IAM\" under the \"Security, Identity, & Compliance\" section.\\nCreate a new policy: In the left navigation pane, select \"Policies\" and click on \"Create policy.\"\\nSelect the service and actions:\\nClick on \"JSON\" and copy and paste the JSON policy you provided earlier for the specific ECR actions.\\nReview and create the policy:\\nClick on \"Review policy.\"\\nProvide a name and description for the policy.\\nClick on \"Create policy.\"\\nJSON policy:\\n{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"VisualEditor0\",\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"ecr:CreateRepository\",\\n\"ecr:GetAuthorizationToken\",\\n\"ecr:BatchCheckLayerAvailability\",\\n\"ecr:BatchGetImage\",\\n\"ecr:InitiateLayerUpload\",\\n\"ecr:UploadLayerPart\",\\n\"ecr:CompleteLayerUpload\",\\n\"ecr:PutImage\"\\n],\\n\"Resource\": \"*\"\\n}\\n]\\n}\\nAdded by: Daniel Muñoz-Viveros\\nERROR: failed to solve: public.ecr.aws/lambda/python:3.10: error getting credentials - err: exec: \"docker-credential-desktop.exe\": executable file not found in $PATH, out: ``\\n(WSL2 system)\\nSolved: Delete the file ~/.docker/config.json\\nYishan Zhan', 'section': '9. Serverless Deep Learning', 'question': 'What IAM permission policy is needed to complete Week 9: Serverless?', 'course': 'machine-learning-zoomcamp'}, {'text': 'Add the next lines to vim /etc/docker/daemon.json\\n{\\n\"dns\": [\"8.8.8.8\", \"8.8.4.4\"]\\n}\\nThen, restart docker:  sudo service docker restart\\nIbai Irastorza', 'section': '9. Serverless Deep Learning', 'question': 'Docker Temporary failure in name resolution', 'course': 'machine-learning-zoomcamp'}, {'text': \"Solution: add compile = False to the load_model function\\nkeras.models.load_model('model_name.h5', compile=False)\\nNadia Paz\", 'section': '9. Serverless Deep Learning', 'question': 'Keras model *.h5 doesn’t load. Error: weight_decay is not a valid argument, kwargs should be empty  for `optimizer_experimental.Optimizer`', 'course': 'machine-learning-zoomcamp'}, {'text': 'This deployment setup can be tested locally using AWS RIE (runtime interface emulator).\\nBasically, if your Docker image was built upon base AWS Lambda image (FROM public.ecr.aws/lambda/python:3.10) - just use certain ports for “docker run” and a certain “localhost link” for testing:\\ndocker run -it --rm -p 9000:8080 name\\nThis command runs the image as a container and starts up an endpoint locally at:\\nlocalhost:9000/2015-03-31/functions/function/invocations\\nPost an event to the following endpoint using a curl command:\\ncurl -XPOST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d \\'{}\\'\\nExamples of curl testing:\\n* windows testing:\\ncurl -XPOST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d \"{\\\\\"url\\\\\": \\\\\"https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg\\\\\"}\"\\n* unix testing:\\ncurl -XPOST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d \\'{\"url\": \"https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg\"}\\'\\nIf during testing you encounter an error like this:\\n# {\"errorMessage\": \"Unable to marshal response: Object of type float32 is not JSON serializable\", \"errorType\": \"Runtime.MarshalError\", \"requestId\": \"7ea5d17a-e0a2-48d5-b747-a16fc530ed10\", \"stackTrace\": []}\\njust turn your response at lambda_handler() to string - str(result).\\nAdded by Andrii Larkin', 'section': '9. Serverless Deep Learning', 'question': 'How to test AWS Lambda + Docker locally?', 'course': 'machine-learning-zoomcamp'}, {'text': 'Make sure all codes in test.py dont have any dependencies with tensorflow library. One of most common reason that lead the this error is tflite still imported from tensorflow. Change import tensorflow.lite as tflite to import tflite_runtime.interpreter as tflite\\nAdded by Ryan Pramana', 'section': '9. Serverless Deep Learning', 'question': '\"Unable to import module \\'lambda_function\\': No module named \\'tensorflow\\'\" when run python test.py', 'course': 'machine-learning-zoomcamp'}, {'text': 'I’ve tried to do everything in Google Colab. Here is a way to work with Docker in Google Colab:\\nhttps://gist.github.com/mwufi/6718b30761cd109f9aff04c5144eb885\\n\\uec03%%shell\\npip install udocker\\nudocker --allow-root install\\n\\uec02!udocker --allow-root run hello-world\\nAdded by Ivan Brigida\\nLambda API Gateway errors:\\n`Authorization header requires \\'Credential\\' parameter. Authorization header requires \\'Signature\\' parameter. Authorization header requires \\'SignedHeaders\\' parameter. Authorization header requires existence of either a \\'X-Amz-Date\\' or a \\'Date\\' header.`\\n`Missing Authentication Token`\\nimport boto3\\nclient = boto3.client(\\'apigateway\\')\\nresponse = client.test_invoke_method(\\nrestApiId=\\'your_rest_api_id\\',\\nresourceId=\\'your_resource_id\\',\\nhttpMethod=\\'POST\\',\\npathWithQueryString=\\'/test/predict\\', #depend how you set up the api\\nbody=\\'{\"url\": \"https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg\"}\\'\\n)\\nprint(response[\\'body\\'])\\nYishan Zhan\\nUnable to run pip install tflite_runtime from github wheel links?\\nTo overcome this issue, you can download the whl file to your local project folder and in the Docker file add the following lines:\\nCOPY <file-name> .\\nRUN pip install <file-name>\\nAbhijit Chakraborty', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Install Docker (udocker) in Google Colab', 'course': 'machine-learning-zoomcamp'}, {'text': 'TODO', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'How to get started with Week 10?', 'course': 'machine-learning-zoomcamp'}, {'text': 'Running a CNN on your CPU can take a long time and once you’ve run out of free time on some cloud providers, it’s time to pay up. Both can be tackled by installing tensorflow with CUDA support on your local machine if you have the right hardware.\\nI was able to get it working by using the following resources:\\nCUDA on WSL :: CUDA Toolkit Documentation (nvidia.com)\\nInstall TensorFlow with pip\\nStart Locally | PyTorch\\nI included the link to PyTorch so that you can get that one installed and working too while everything is fresh on your mind. Just select your options, and for Computer Platform, I chose CUDA 11.7 and it worked for me.\\nAdded by Martin Uribe', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'How to install Tensorflow in Ubuntu WSL2', 'course': 'machine-learning-zoomcamp'}, {'text': 'If you are running tensorflow on your own machine and you start getting the following errors:\\nAllocator (GPU_0_bfc) ran out of memory trying to allocate 6.88GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\\nTry adding this code in a cell at the beginning of your notebook:\\nconfig = tf.compat.v1.ConfigProto()\\nconfig.gpu_options.allow_growth = True\\nsession = tf.compat.v1.Session(config=config)\\nAfter doing this most of my issues went away. I say most because there was one instance when I still got the error once more, but only during one epoch. I ran the code again, right after it finished, and I never saw the error again.\\nAdded by Martin Uribe', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Getting: Allocator ran out of memory errors?', 'course': 'machine-learning-zoomcamp'}, {'text': 'In session 10.3, when creating the virtual environment with pipenv and trying to run the script gateway.py, you might get this error:\\nTypeError: Descriptors cannot not be created directly.\\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\\n1. Downgrade the protobuf package to 3.20.x or lower.\\n2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates\\nThis will happen if your version of protobuf is one of the newer ones. As a workaround, you can fix the protobuf version to an older one. In my case I got around the issue by creating the environment with:\\npipenv install --python 3.9.13 requests grpcio==1.42.0 flask gunicorn \\\\\\nkeras-image-helper tensorflow-protobuf==2.7.0 protobuf==3.19.6\\nAdded by Ángel de Vicente', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Problem with recent version of protobuf', 'course': 'machine-learning-zoomcamp'}, {'text': 'Due to the uncertainties associated with machines, sometimes you can get the error message like this when you try to run a docker command:\\n”Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?”\\nSolution: The solution is simple. The Docker Desktop might no longer be connecting to the WSL Linux distro. What you need to do is go to your Docker Desktop setting and then click on resources. Under resources, click on WSL Integration. You will get a tab like the image below:\\nJust enable additional distros. That’s all. Even if the additional distro is the same as the default WSL distro.\\nOdimegwu David', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'WSL Cannot Connect To Docker Daemon', 'course': 'machine-learning-zoomcamp'}, {'text': 'In case the HPA instance does not run correctly even after installing the latest version of Metrics Server from the components.yaml manifest with:\\n>>kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\\nAnd the targets still appear as <unknown>\\nRun >>kubectl edit deploy -n kube-system metrics-server\\nAnd search for this line:\\nargs:\\n- --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\\nAdd this line in the middle:  - --kubelet-insecure-tls\\nSo that it stays like this:\\nargs:\\n- --kubelet-insecure-tls\\n- --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\\nSave and run again >>kubectl get hpa\\nAdded by Marilina Orihuela', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'HPA instance doesn’t run properly', 'course': 'machine-learning-zoomcamp'}, {'text': 'In case the HPA instance does not run correctly even after installing the latest version of Metrics Server from the components.yaml manifest with:\\n>>kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\\nAnd the targets still appear as <unknown>\\nRun the following command:\\nkubectl apply -f https://raw.githubusercontent.com/Peco602/ml-zoomcamp/main/10-kubernetes/kube-config/metrics-server-deployment.yaml\\nWhich uses a metrics server deployment file already embedding the - --kubelet-insecure-tls option.\\nAdded by Giovanni Pecoraro', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'HPA instance doesn’t run properly (easier solution)', 'course': 'machine-learning-zoomcamp'}, {'text': \"When I run pip install grpcio==1.42.0 tensorflow-serving-api==2.7.0 to install the libraries in windows machine,  I was getting the below error :\\nERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\\\\\Users\\\\\\\\Asia\\\\\\\\anaconda3\\\\\\\\Lib\\\\\\\\site-packages\\\\\\\\google\\\\\\\\protobuf\\\\\\\\internal\\\\\\\\_api_implementation.cp39-win_amd64.pyd'\\nConsider using the `--user` option or check the permissions.\\nSolution description :\\nI was able to install the libraries using below command:\\npip --user install grpcio==1.42.0 tensorflow-serving-api==2.7.0\\nAsia Saeed\", 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Could not install packages due to an OSError: [WinError 5] Access is denied', 'course': 'machine-learning-zoomcamp'}, {'text': 'Problem description\\nI was getting the below error message when I run gateway.py after modifying the code & creating virtual environment in  video 10.3 :\\nFile \"C:\\\\Users\\\\Asia\\\\Data_Science_Code\\\\Zoompcamp\\\\Kubernetes\\\\gat.py\", line 9, in <module>\\nfrom tensorflow_serving.apis import predict_pb2\\nFile \"C:\\\\Users\\\\Asia\\\\.virtualenvs\\\\Kubernetes-Ge6Ts1D5\\\\lib\\\\site-packages\\\\tensorflow_serving\\\\apis\\\\predict_pb2.py\", line 14, in <module>\\nfrom tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\\nFile \"C:\\\\Users\\\\Asia\\\\.virtualenvs\\\\Kubernetes-Ge6Ts1D5\\\\lib\\\\site-packages\\\\tensorflow\\\\core\\\\framework\\\\tensor_pb2.py\", line 14, in <module>\\nfrom tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2\\nFile \"C:\\\\Users\\\\Asia\\\\.virtualenvs\\\\Kubernetes-Ge6Ts1D5\\\\lib\\\\site-packages\\\\tensorflow\\\\core\\\\framework\\\\resource_handle_pb2.py\", line 14, in <module>\\nfrom tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\\nFile \"C:\\\\Users\\\\Asia\\\\.virtualenvs\\\\Kubernetes-Ge6Ts1D5\\\\lib\\\\site-packages\\\\tensorflow\\\\core\\\\framework\\\\tensor_shape_pb2.py\", line 36, in <module>\\n_descriptor.FieldDescriptor(\\nFile \"C:\\\\Users\\\\Asia\\\\.virtualenvs\\\\Kubernetes-Ge6Ts1D5\\\\lib\\\\site-packages\\\\google\\\\protobuf\\\\descriptor.py\", line 560, in __new__\\n_message.Message._CheckCalledFromGeneratedFile()\\nTypeError: Descriptors cannot not be created directly.\\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\\n1. Downgrade the protobuf package to 3.20.x or lower.\\n2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\\nSolution description:\\nIssue has been resolved by downgrading protobuf to version 3.20.1.\\npipenv install protobuf==3.20.1\\nAsia Saeed', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'TypeError: Descriptors cannot not be created directly.', 'course': 'machine-learning-zoomcamp'}, {'text': 'To install kubectl on windows using the terminal in vscode (powershell), I followed this tutorial: https://medium.com/@ggauravsigra/install-kubectl-on-windows-af77da2e6fff\\nI first downloaded kubectl with curl, with these command lines: https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/#install-kubectl-binary-with-curl-on-windows\\nAt step 3, I followed the tutorial with the copy of the exe file in a specific folder on C drive.\\nThen I added this folder path to PATH in my environment variables.\\nKind can be installed the same way with the curl command on windows, by specifying a folder that will be added to the path environment variable.\\nAdded by Mélanie Fouesnard', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'How to install easily kubectl on windows ?', 'course': 'machine-learning-zoomcamp'}, {'text': \"First you need to launch a powershell terminal with administrator privilege.\\nFor this we need to install choco library first through the following syntax in powershell:\\nSet-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1'))\\nKrishna Anand\", 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Install kind through choco library', 'course': 'machine-learning-zoomcamp'}, {'text': 'If you are having challenges installing Kind through the Windows Powershell as provided on the website and Choco Library as I did, you can simply install Kind through Go.\\n> Download and Install Go (https://go.dev/doc/install)\\n> Confirm installation by typing the following in Command Prompt -  go version\\n> Proceed by installing Kind by following this command - go install sigs.k8s.io/kind@v0.20.0\\n>Confirm Installation kind --version\\nIt works perfectly.', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Install Kind via Go package', 'course': 'machine-learning-zoomcamp'}, {'text': \"I ran into an issue where kubectl wasn't working.\\nI kept getting the following error:\\nkubectl get service\\nThe connection to the server localhost:8080 was refused - did you specify the right host or port?\\nI searched online for a resolution, but everyone kept talking about creating an environment variable and creating some admin.config file in my home directory.\\nAll hogwash.\\nThe solution to my problem was to just start over.\\nkind delete cluster\\nrm -rf ~/.kube\\nkind create cluster\\nNow when I try the same command again:\\nkubectl get service\\nNAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\\nkubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   53s\\nAdded by Martin Uribe\", 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'The connection to the server localhost:8080 was refused - did you specify the right host or port?', 'course': 'machine-learning-zoomcamp'}, {'text': 'Problem description\\nDue to experimenting back and forth so much without care for storage, I just ran out of it on my 30-GB AWS instance.\\nMy first reflex was to remove some zoomcamp directories, but of course those are mostly code so it didn’t help much.\\nSolution description\\n> docker images\\nrevealed that I had over 20 GBs worth of superseded / duplicate models lying around, so I proceeded to > docker rmi\\na bunch of those — but to no avail!\\nIt turns out that deleting docker images does not actually free up any space as you might expect. After removing images, you also need to run\\n> docker system prune\\nSee also: https://stackoverflow.com/questions/36799718/why-removing-docker-containers-and-images-does-not-free-up-storage-space-on-wind\\nAdded by Konrad Mühlberg', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Running out of storage after building many docker images', 'course': 'machine-learning-zoomcamp'}, {'text': 'Yes, the question does require for you to specify values for CPU and memory in the yaml file, however the question that it is use in the form only refers to the port which do have a define correct value for this specific homework.\\nPastor Soto', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'In HW10 Q6 what does it mean “correct value for CPU and memory”? Aren’t they arbitrary?', 'course': 'machine-learning-zoomcamp'}, {'text': 'In Kubernetes resource specifications, such as CPU requests and limits, the \"m\" stands for milliCPU, which is a unit of computing power. It represents one thousandth of a CPU core.\\ncpu: \"100m\" means the container is requesting 100 milliCPUs, which is equivalent to 0.1 CPU core.\\ncpu: \"500m\" means the container has a CPU limit of 500 milliCPUs, which is equivalent to 0.5 CPU core.\\nThese values are specified in milliCPUs to allow fine-grained control over CPU resources. It allows you to express CPU requirements and limits in a more granular way, especially in scenarios where your application might not need a full CPU core.\\nAdded by Andrii Larkin', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Why cpu vals for Kubernetes deployment.yaml look like “100m” and “500m”? What does \"m\" mean?', 'course': 'machine-learning-zoomcamp'}, {'text': 'Problem: Failing to load docker-image to cluster (when you’ved named a cluster)\\nkind load docker-image zoomcamp-10-model:xception-v4-001\\nERROR: no nodes found for cluster \"kind\"\\nSolution: Specify cluster name with -n\\nkind -n clothing-model load docker-image zoomcamp-10-model:xception-v4-001\\nAndrew Katoch', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Kind cannot load docker image', 'course': 'machine-learning-zoomcamp'}, {'text': \"Problem: I download kind from the next command:\\ncurl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.17.0/kind-windows-amd64\\nWhen I try\\nkind --version\\nI get: 'kind' is not recognized as an internal or external command, operable program or batch file\\nSolution: The default name of executable is kind-windows-amd64.exe, so that you have to rename this file to  kind.exe. Put this file in specific folder, and add it to PATH\\nAlejandro Aponte\", 'section': '10. Kubernetes and TensorFlow Serving', 'question': \"'kind' is not recognized as an internal or external command, operable program or batch file. (In Windows)\", 'course': 'machine-learning-zoomcamp'}, {'text': 'Using kind with Rootless Docker or Rootless Podman requires some changes on the system (Linux), see kind – Rootless (k8s.io).\\nSylvia Schmitt', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Running kind on Linux with Rootless Docker or Rootless Podman', 'course': 'machine-learning-zoomcamp'}, {'text': 'Deploy and Access the Kubernetes Dashboard\\nLuke', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Kubernetes-dashboard', 'course': 'machine-learning-zoomcamp'}, {'text': 'Make sure you are on AWS CLI v2 (check with aws --version)\\nhttps://docs.aws.amazon.com/cli/latest/userguide/cliv2-migration-instructions.html', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Correct AWS CLI version for eksctl', 'course': 'machine-learning-zoomcamp'}, {'text': 'Problem Description:\\nIn video 10.3, when I was testing a flask service, I got the above error. I ran docker run .. in one terminal. When in second terminal I run python gateway.py, I get the above error.\\nSolution: This error has something to do with versions of Flask and Werkzeug. I got the same error, if I just import flask with from flask import Flask.\\nBy running pip freeze > requirements.txt,I found that their versions are Flask==2.2.2 and Werkzeug==2.2.2. This error appears while using an old version of werkzeug (2.2.2) with new version of flask (2.2.2). I solved it by pinning version of Flask into an older version with pipenv install Flask==2.1.3.\\nAdded by Bhaskar Sarma', 'section': '10. Kubernetes and TensorFlow Serving', 'question': \"TypeError: __init__() got an unexpected keyword argument 'unbound_message' while importing Flask\", 'course': 'machine-learning-zoomcamp'}, {'text': 'As per AWS documentation:\\nhttps://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html\\nYou need to do: (change the fields in red)\\naws ecr get-login-password --region region | docker login --username AWS --password-stdin aws_account_id.dkr.ecr.region.amazonaws.com\\nAlternatively you can run the following command without changing anything given you have a default region configured\\naws ecr get-login-password --region $(aws configure get region) | docker login --username AWS --password-stdin \"$(aws sts get-caller-identity --query \"Account\" --output text).dkr.ecr.$(aws configure get region).amazonaws.com\"\\nAdded by Humberto Rodriguez', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Command aws ecr get-login --no-include-email returns “aws: error: argument operation: Invalid choice…”', 'course': 'machine-learning-zoomcamp'}, {'text': 'While trying to run the docker code on M1:\\ndocker run --platform linux/amd64 -it --rm \\\\\\n-p 8500:8500 \\\\\\n-v $(pwd)/clothing-model:/models/clothing-model/1 \\\\\\n-e MODEL_NAME=\"clothing-model\" \\\\\\ntensorflow/serving:2.7.0\\nIt outputs the error:\\nError:\\nStatus: Downloaded newer image for tensorflow/serving:2.7.0\\n[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/generated_message_reflection.cc:2345] CHECK failed: file != nullptr:\\nterminate called after throwing an instance of \\'google::protobuf::FatalException\\'\\nwhat():  CHECK failed: file != nullptr:\\nqemu: uncaught target signal 6 (Aborted) - core dumped\\n/usr/bin/tf_serving_entrypoint.sh: line 3:     8 Aborted                 tensorflow_model_server --port=8500 --rest_api_port=8501 --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \"$@\"\\nSolution\\ndocker pull emacski/tensorflow-serving:latest\\ndocker run -it --rm \\\\\\n-p 8500:8500 \\\\\\n-v $(pwd)/clothing-model:/models/clothing-model/1 \\\\\\n-e MODEL_NAME=\"clothing-model\" \\\\\\nemacski/tensorflow-serving:latest-linux_arm64\\nSee more here: https://github.com/emacski/tensorflow-serving-arm\\nAdded by Daniel Egbo', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Error downloading  tensorflow/serving:2.7.0 on Apple M1 Mac', 'course': 'machine-learning-zoomcamp'}, {'text': 'Similar to the one above but with a different solution the main reason is that emacski doesn’t seem to maintain the repo any more, the latest image is from 2 years ago at the time of writing (December 2023)\\nProblem:\\nWhile trying to run the docker code on Mac M2 apple silicon:\\ndocker run --platform linux/amd64 -it --rm \\\\\\n-p 8500:8500 \\\\\\n-v $(pwd)/clothing-model:/models/clothing-model/1 \\\\\\n-e MODEL_NAME=\"clothing-model\" \\\\\\ntensorflow/serving\\nYou get an error:\\n/usr/bin/tf_serving_entrypoint.sh: line 3:     7 Illegal instruction     tensorflow_model_server --port=8500 --rest_api_port=8501 --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \"$@\"\\nSolution:\\nUse bitnami/tensorflow-serving base image\\nLaunch it either using docker run\\ndocker run -d \\\\\\n--name tf_serving \\\\\\n-p 8500:8500 \\\\\\n-p 8501:8501 \\\\\\n-v $(pwd)/clothing-model:/bitnami/model-data/1 \\\\\\n-e TENSORFLOW_SERVING_MODEL_NAME=clothing-model \\\\\\nbitnami/tensorflow-serving:2\\nOr the following docker-compose.yaml\\nversion: \\'3\\'\\nservices:\\ntf_serving:\\nimage: bitnami/tensorflow-serving:2\\nvolumes:\\n- ${PWD}/clothing-model:/bitnami/model-data/1\\nports:\\n- 8500:8500\\n- 8501:8501\\nenvironment:\\n- TENSORFLOW_SERVING_MODEL_NAME=clothing-model\\nAnd run it with\\ndocker compose up\\nAdded by Alex Litvinov', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Illegal instruction error when running tensorflow/serving image on Mac M2 Apple Silicon (potentially on M1 as well)', 'course': 'machine-learning-zoomcamp'}, {'text': 'Problem: CPU metrics Shows Unknown\\nNAME         REFERENCE           TARGETS         MINPODS   MAXPODS   REPLICAS   AGE\\ncredit-hpa   Deployment/credit   <unknown>/20%   1         3         1          18s\\nFailedGetResourceMetric       2m15s (x169 over 44m)  horizontal-pod-autoscaler  failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API:\\nSolution:\\n-> Delete HPA (kubectl delete hpa credit-hpa)\\n-> kubectl apply -f https://raw.githubusercontent.com/pythianarora/total-practice/master/sample-kubernetes-code/metrics-server.yaml\\n-> Create HPA\\nThis should solve the cpu metrics report issue.\\nAdded by Priya V', 'section': '11. KServe', 'question': 'HPA doesn’t show CPU metrics', 'course': 'machine-learning-zoomcamp'}, {'text': 'Problem description:\\nRunning this:\\ncurl -s \"https://raw.githubusercontent.com/kserve/kserve/release-0.9/hack/quick_install.sh\" | bash\\nFails with errors because of istio failing to update resources, and you are on kubectl > 1.25.0.\\nCheck kubectl version with kubectl version\\nSolution description\\nEdit the file “quick_install.bash” by downloading it with curl without running bash. Edit the versions of Istio and Knative as per the matrix on the KServe website.\\nRun the bash script now.\\nAdded by Andrew Katoch', 'section': '11. KServe', 'question': 'Errors with istio during installation', 'course': 'machine-learning-zoomcamp'}, {'text': 'Problem description\\nSolution description\\n(optional) Added by Name', 'section': 'Projects (Midterm and Capstone)', 'question': 'Problem title', 'course': 'machine-learning-zoomcamp'}, {'text': 'Answer: You can see them here (it’s taken from the 2022 cohort page). Go to the cohort folder for your own cohort’s deadline.', 'section': 'Projects (Midterm and Capstone)', 'question': 'What are the project deadlines?', 'course': 'machine-learning-zoomcamp'}, {'text': 'Answer: All midterms and capstones are meant to be solo projects. [source @Alexey]', 'section': 'Projects (Midterm and Capstone)', 'question': 'Are projects solo or collaborative/group work?', 'course': 'machine-learning-zoomcamp'}, {'text': 'Answer: Ideally midterms up to module-06, capstones include all modules in that cohort’s syllabus. But you can include anything extra that you want to feature. Just be sure to document anything not covered in class.\\nAlso watch office hours from previous cohorts. Go to DTC youtube channel and click on Playlists and search for {course yyyy}. ML Zoomcamp was first launched in 2021.\\nMore discussions:\\n[source1] [source2] [source3]', 'section': 'Projects (Midterm and Capstone)', 'question': 'What modules, topics, problem-sets should a midterm/capstone project cover? Can I do xyz?', 'course': 'machine-learning-zoomcamp'}, {'text': \"These links apply to all projects, actually. Again, for some cohorts, the modules/syllabus might be different, so always check in your cohort’s folder as well for additional or different instructions, if any.\\nMidterm Project Sample: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp/cohorts/2021/07-midterm-project\\nMidTerm Project Deliverables: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp/projects\\nSubmit MidTerm Project: https://docs.google.com/forms/d/e/1FAIpQLSfgmOk0QrmHu5t0H6Ri1Wy_FDVS8I_nr5lY3sufkgk18I6S5A/viewform\\nDatasets:\\nhttps://www.kaggle.com/datasets and https://www.kaggle.com/competitions\\nhttps://archive.ics.uci.edu/ml/index.php\\nhttps://data.europa.eu/en\\nhttps://www.openml.org/search?type=data\\nhttps://newzealand.ai/public-data-sets\\nhttps://datasetsearch.research.google.com\\nWhat to do and Deliverables\\nThink of a problem that's interesting for you and find a dataset for that\\nDescribe this problem and explain how a model could be used\\nPrepare the data and doing EDA, analyze important features\\nTrain multiple models, tune their performance and select the best model\\nExport the notebook into a script\\nPut your model into a web service and deploy it locally with Docker\\nBonus points for deploying the service to the cloud\", 'section': 'Projects (Midterm and Capstone)', 'question': 'Crucial Links', 'course': 'machine-learning-zoomcamp'}, {'text': 'Answer: Previous cohorts projects page has instructions (youtube).\\nhttps://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2022/projects.md#midterm-project\\nAlexey and his team will compile a g-sheet with links to submitted projects with our hashed emails (just like when we check leaderboard for homework) that are ours to review within the evaluation deadline.\\n~~~ Added by Nukta Bhatia ~~~', 'section': 'Projects (Midterm and Capstone)', 'question': 'How to conduct peer reviews for projects?', 'course': 'machine-learning-zoomcamp'}, {'text': 'See the answer here.', 'section': 'Projects (Midterm and Capstone)', 'question': 'Computing the hash for project review', 'course': 'machine-learning-zoomcamp'}, {'text': 'For the learning in public for this midterm project it seems that has a total value of 14!, Does this mean that we need make 14 posts?, Or the regular seven posts for each module and each one with a value of 2?, Or just one with a total value of 14?\\n14 posts, one for each day', 'section': 'Projects (Midterm and Capstone)', 'question': 'Learning in public links for the projects', 'course': 'machine-learning-zoomcamp'}, {'text': 'You can use git-lfs (https://git-lfs.com/) for upload large file to github repository.\\nRyan Pramana', 'section': 'Projects (Midterm and Capstone)', 'question': \"My dataset is too large and I can't loaded in GitHub , do anyone knows about a solution?\", 'course': 'machine-learning-zoomcamp'}, {'text': 'If you have submitted two projects (and peer-reviewed at least 3 course-mates’ projects for each submission), you will get the certificate for the course. According to the course coordinator, Alexey Grigorev, only two projects are needed to get the course certificate.\\n(optional) David Odimegwu', 'section': 'Projects (Midterm and Capstone)', 'question': 'What If I submitted only two projects and failed to submit the third?', 'course': 'machine-learning-zoomcamp'}, {'text': 'Yes. You only need to review peers when you submit your project.\\nConfirmed on Slack by Alexey Grigorev (added by Rileen Sinha)', 'section': 'Projects (Midterm and Capstone)', 'question': \"I did the first two projects and skipped the last one so I wouldn't have two peer review in second capstone right?\", 'course': 'machine-learning-zoomcamp'}, {'text': 'Regarding Point 4 in the midterm deliverables, which states, \"Train multiple models, tune their performance, and select the best model,\" you might wonder, how many models should you train? The answer is simple: train as many as you can. The term \"multiple\" implies having more than one model, so as long as you have more than one, you\\'re on the right track.', 'section': 'Projects (Midterm and Capstone)', 'question': 'How many models should I train?', 'course': 'machine-learning-zoomcamp'}, {'text': 'I am not sure how the project evaluate assignment works? Where do I find this? I have access to all the capstone 2 project, perhaps, I can randomly pick any to review.\\nAnswer:\\nThe link provided for example (2023/Capstone link ): https://docs.google.com/forms/d/e/1FAIpQLSdgoepohpgbM4MWTAHWuXa6r3NXKnxKcg4NDOm0bElAdXdnnA/viewform contains a list of all submitted projects to be evaluated. More specific, you are to review 3 assigned peer projects. In the spreadsheet are 3 hash values of your assigned peer projects. However, you need to derive the your hash value of your email address and find the value on the spreadsheet under the (reviewer_hash) heading.\\nTo calculate your hash value run the python code below:\\nfrom hashlib import sha1\\ndef compute_hash(email):\\nreturn sha1(email.lower().encode(\\'utf-8\\')).hexdigest()\\n# Example usage **** enter your email below (Example1@gmail.com)****\\nemail = \"Example1@gmail.com\"\\nhashed_email = compute_hash(email)\\nprint(\"Original Email:\", email)\\nprint(\"Hashed Email (SHA-1):\", hashed_email)\\nEdit the above code to replace Example1@gmail.com as your email address\\nStore and run the above python code from your terminal. See below as the Hashed Email (SHA-1) value\\nYou then go to the link: https://docs.google.com/spreadsheets/d/e/2PACX-1vR-7RRtq7AMx5OzI-tDbkzsbxNLm-NvFOP5OfJmhCek9oYcDx5jzxtZW2ZqWvBqc395UZpHBv1of9R1/pubhtml?gid=876309294&single=true\\nLastly, copy the “Hashed Email (SHA-1): bd9770be022dede87419068aa1acd7a2ab441675” value and search for 3 identical entries. There you should see your peer project to be reviewed.\\nBy Emmanuel Ayeni', 'section': 'Projects (Midterm and Capstone)', 'question': 'How does the project evaluation work for you as a peer reviewer?', 'course': 'machine-learning-zoomcamp'}, {'text': 'Alexey Grigorev: “It’s based on all the scores to make sure most of you pass.”                                                   By Annaliese Bronz\\nOther course-related questions that don’t fall into any of the categories above or can apply to more than one category/module', 'section': 'Miscellaneous', 'question': 'Do you pass a project based on the average of everyone else’s scores or based on the total score you earn?', 'course': 'machine-learning-zoomcamp'}, {'text': 'Answer: The train.py file will be used by your peers to review your midterm project. It is for them to cross-check that your training process works on someone else’s system. It should also be included in the environment in conda or with pipenv.\\nOdimegwu David', 'section': 'Miscellaneous', 'question': 'Why do I need to provide a train.py file when I already have the notebook.ipynb file?', 'course': 'machine-learning-zoomcamp'}, {'text': \"Pip install pillow - install pillow library\\nfrom PIL import Image\\nimg = Image.open('aeroplane.png')\\nFrom numpy import asarray\\nnumdata=asarray(img)\\nKrishna Anand\", 'section': 'Miscellaneous', 'question': 'Loading the Image with PILLOW library and converting to numpy array', 'course': 'machine-learning-zoomcamp'}, {'text': \"Ans: train.py has to be a python file. This is because running a python script for training a model is much simpler than running a notebook and that's how training jobs usually look like in real life.\", 'section': 'Miscellaneous', 'question': 'Is a train.py file necessary when you have a train.ipynb file in your midterm project directory?', 'course': 'machine-learning-zoomcamp'}, {'text': 'Yes, you can create a mobile app or interface that manages these forms and validations. But you should also perform validations on backend.\\nYou can also check Streamlit: https://github.com/DataTalksClub/project-of-the-week/blob/main/2022-08-14-frontend.md\\nAlejandro Aponte', 'section': 'Miscellaneous', 'question': 'Is there a way to serve up a form for users to enter data for the model to crunch on?', 'course': 'machine-learning-zoomcamp'}, {'text': \"Using model.feature_importances_ can gives you an error:\\nAttributeError: 'Booster' object has no attribute 'feature_importances_'\\nAnswer: if you train the model like this: model = xgb.train you should use get_score() instead\\nEkaterina Kutovaia\", 'section': 'Miscellaneous', 'question': 'How to get feature importance for XGboost model', 'course': 'machine-learning-zoomcamp'}, {'text': 'In the Elastic Container Service task log, error “[Errno 12] Cannot allocate memory” showed up.\\nJust increase the RAM and CPU in your task definition.\\nHumberto Rodriguez', 'section': 'Miscellaneous', 'question': '[Errno 12] Cannot allocate memory in AWS Elastic Container Service', 'course': 'machine-learning-zoomcamp'}, {'text': \"When running a docker container with waitress serving the app.py for making predictions, pickle will throw an error that can't get attribute <name_of_class> on module __main__.\\nThis does not happen when Flask is used directly, i.e. not through waitress.\\nThe problem is that the model uses a custom column transformer class, and when the model was saved, it was saved from the __main__ module (e.g. python train.py). Pickle will reference the class in the global namespace (top-level code): __main__.<custom_class>.\\nWhen using waitress, waitress will load the predict_app module and this will call pickle.load, that will try to find __main__.<custom_class> that does not exist.\\nSolution:\\nPut the class into a separate module and import it in both the script that saves the model (e.g. train.py) and the script that loads the model (e.g. predict.py)\\nNote: If Flask is used (no waitress) in predict.py, and predict.py has the definition of the class, When  it is run: python predict.py, it will work because the class is in the same namespace as the one used when the model was saved (__main__).\\nDetailed info: https://stackoverflow.com/questions/27732354/unable-to-load-files-using-pickle-and-multiple-modules\\nMarcos MJD\", 'section': 'Miscellaneous', 'question': 'Pickle error: can’t get attribute XXX on module __main__', 'course': 'machine-learning-zoomcamp'}, {'text': 'There are different techniques, but the most common used are the next:\\nDataset transformation (for example, log transformation)\\nClipping high values\\nDropping these observations\\nAlena Kniazeva', 'section': 'Miscellaneous', 'question': 'How to handle outliers in a dataset?', 'course': 'machine-learning-zoomcamp'}, {'text': 'I was getting the below error message when I was trying to create docker image using bentoml\\n[bentoml-cli] `serve` failed: Failed loading Bento from directory /home/bentoml/bento: Failed to import module \"service\": No module named \\'sklearn\\'\\nSolution description\\nThe cause was because , in bentofile.yaml, I wrote sklearn instead of scikit-learn. Issue was fixed after I modified the packages list as below.\\npackages: # Additional pip packages required by the service\\n- xgboost\\n- scikit-learn\\n- pydantic\\nAsia Saeed', 'section': 'Miscellaneous', 'question': 'Failed loading Bento from directory /home/bentoml/bento: Failed to import module \"service\": No module named \\'sklearn\\'', 'course': 'machine-learning-zoomcamp'}, {'text': \"You might see a long error message with something about sparse matrices, and in the swagger UI, you get a code 500 error with “” (empty string) as output.\\nPotential reason: Setting DictVectorizer or OHE to sparse while training, and then storing this in a pipeline or custom object in the benotml model saving stage in train.py. This means that when the custom object is called in service.py, it will convert each input to a different sized sparse matrix, and this can't be batched due to inconsistent length. In this case, bentoml model signatures should have batchable set to False for production during saving the bentoml mode in train.py.\\n(Memoona Tahira)\", 'section': 'Miscellaneous', 'question': 'BentoML not working with –production flag at any stage: e.g. with bentoml serve and while running the bentoml container', 'course': 'machine-learning-zoomcamp'}, {'text': 'Problem description:\\nDo we have to run everything?\\nYou are encouraged, if you can, to run them. As this provides another opportunity to learn from others.\\nNot everyone will be able to run all the files, in particular the neural networks.\\nSolution description:\\nAlternatively, can you see that everything you need to reproduce is there: the dataset is there, the instructions are there, are there any obvious errors and so on.\\nRelated slack conversation here.\\n(Gregory Morris)', 'section': 'Miscellaneous', 'question': 'Reproducibility', 'course': 'machine-learning-zoomcamp'}, {'text': \"If your model is too big for github one option is to try and compress the model using joblib. For example joblib.dump(model, model_filename, compress=('zlib', 6) will use zlib to compress the model. Just note this could take a few moments as the model is being compressed.\\nQuinn Avila\", 'section': 'Miscellaneous', 'question': 'Model too big', 'course': 'machine-learning-zoomcamp'}, {'text': \"When you try to push the docker image to Google Container Registry and get this message “unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials.”, type this below on console, but first install https://cloud.google.com/sdk/docs/install, this is to be able to use gcloud in console:\\ngcloud auth configure-docker\\n(Jesus Acuña)\", 'section': 'Miscellaneous', 'question': 'Permissions to push docker to Google Container Registry', 'course': 'machine-learning-zoomcamp'}, {'text': 'I am getting this error message when I tried to install tflite in a pipenv environment\\nError:  An error occurred while installing tflite_runtime!\\nError text:\\nERROR: Could not find a version that satisfies the requirement tflite_runtime (from versions: none)\\nERROR: No matching distribution found for tflite_runtime\\nThis version of tflite do not run on python 3.10, the way we can make it work is by install python 3.9, after that it would install the tflite_runtime without problem.\\nPastor Soto\\nCheck all available versions here:\\nhttps://google-coral.github.io/py-repo/tflite-runtime/\\nIf you don’t find a combination matching your setup, try out the options at\\nhttps://github.com/alexeygrigorev/tflite-aws-lambda/tree/main/tflite\\nwhich you can install as shown in the lecture, e.g.\\npip install https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.7.0-cp38-cp38-linux_x86_64.whl\\nFinally, if nothing works, use the TFLite included in TensorFlow for local development, and use Docker for testing Lambda.\\nRileen Sinha (based on discussions on Slack)', 'section': 'Miscellaneous', 'question': 'Tflite_runtime unable to install', 'course': 'machine-learning-zoomcamp'}, {'text': \"Error: ImageDataGenerator name 'scipy' is not defined.\\nCheck that scipy is installed in your environment.\\nRestart jupyter kernel and try again.\\nMarcos MJD\", 'section': 'Miscellaneous', 'question': 'Error when running ImageDataGenerator.flow_from_dataframe', 'course': 'machine-learning-zoomcamp'}, {'text': 'Tim from BentoML has prepared a dedicated video tutorial wrt this use case here:\\nhttps://www.youtube.com/watch?v=7gI1UH31xb4&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=97\\nKonrad Muehlberg', 'section': 'Miscellaneous', 'question': 'How to pass BentoML content / docker container to Amazon Lambda', 'course': 'machine-learning-zoomcamp'}, {'text': \"In deploying model part, I wanted to test my model locally on a test-image data and I had this silly error after the following command:\\nurl = 'https://github.com/bhasarma/kitchenware-classification-project/blob/main/test-image.jpg'\\nX = preprocessor.from_url(url)\\nI got the error:\\nUnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x7f797010a590>\\nSolution:\\nAdd ?raw=true after .jpg in url. E.g. as below\\nurl = ‘https://github.com/bhasarma/kitchenware-classification-project/blob/main/test-image.jpg?raw=true’\\nBhaskar Sarma\", 'section': 'Miscellaneous', 'question': 'Error UnidentifiedImageError: cannot identify image file', 'course': 'machine-learning-zoomcamp'}, {'text': 'Problem: If you run pipenv install and get this message. Maybe manually change Pipfile and Pipfile.lock.\\nSolution: Run: ` pipenv lock` for fix this problem and dependency files\\nAlejandro Aponte', 'section': 'Miscellaneous', 'question': '[pipenv.exceptions.ResolutionFailure]: Warning: Your dependencies could not be resolved. You likely have a mismatch in your sub-dependencies', 'course': 'machine-learning-zoomcamp'}, {'text': 'Problem: In the course this function worked to get the features from the dictVectorizer instance: dv.get_feature_names(). But in my computer did not work. I think it has to do with library versions and but apparently that function will be deprecated soon:\\nOld: https://scikit-learn.org/0.22/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.get_feature_names\\nNew: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.get_feature_names\\nSolution: change the line dv.get_feature_names() to list(dv.get_feature_names_out))\\nIbai Irastorza', 'section': 'Miscellaneous', 'question': 'Get_feature_names() not found', 'course': 'machine-learning-zoomcamp'}, {'text': 'Problem happens when contacting the server waiting to send your predict-test and your data here in the correct shape.\\nThe problem was the format input to the model wasn’t in the right shape. Server receives the data in json format (dict) which is not suitable for the model. U should convert it to like numpy arrays.\\nAhmed Okka', 'section': 'Miscellaneous', 'question': 'Error decoding JSON response: Expecting value: line 1 column 1 (char 0)', 'course': 'machine-learning-zoomcamp'}, {'text': \"Q: Hii folks, I tried deploying my docker image on Render, but it won't I get SIGTERM everytime.\\nI think .5GB RAM is not enough, is there any other free alternative available ?\\nA: aws (amazon), gcp (google), saturn.\\nBoth aws and gcp give microinstance for free for a VERY long time, and a bunch more free stuff.\\nSaturn even provides free GPU instances. Recent promo link from mlzoomcamp for Saturn:\\n“You can sign up here: https://bit.ly/saturn-mlzoomcamp\\nWhen you sign up, write in the chat box that you're an ML Zoomcamp student and you should get extra GPU hours (something like 150)”\\nAdded by Andrii Larkin\", 'section': 'Miscellaneous', 'question': 'Free cloud alternatives', 'course': 'machine-learning-zoomcamp'}, {'text': \"Problem description: I have one column day_of_the_month . It has values 1, 2, 20, 25 etc. and int . I have a second column month_of_the_year. It has values jan, feb, ..dec. and are string. I want to convert these two columns into one column day_of_the_year and I want them to be int. 2 and jan should give me 2, i.e. 2nd day of the year, 1 and feb should give me 32, i.e. 32 nd day of the year. What is the simplest pandas-way to do it?\\nSolution description:\\nconvert dtype in day_of_the_month column from int to str with df['day_of_the_month'] = df['day_of_the_month'].map(str)\\nconvert month_of_the_year column in jan, feb ...,dec into 1,2, ..,12 string using map()\\nconvert day and month into a datetime object with:\\ndf['date_formatted'] = pd.to_datetime(\\ndict(\\nyear='2055',\\nmonth=df['month'],\\nday=df['day']\\n)\\n)\\nget day of year with: df['day_of_year']=df['date_formatted'].dt.dayofyear\\n(Bhaskar Sarma)\", 'section': 'Miscellaneous', 'question': 'Getting day of the year from day and month column', 'course': 'machine-learning-zoomcamp'}, {'text': 'How to visualize the predictions per classes after training a neural net\\nSolution description\\nclasses, predictions = zip(*dict(zip(classes, predictions)).items())\\nplt.figure(figsize=(12, 3))\\nplt.bar(classes, predictions)\\nLuke', 'section': 'Miscellaneous', 'question': 'Chart for classes and predictions', 'course': 'machine-learning-zoomcamp'}, {'text': 'You can convert the prediction output values to a datafarme using \\ndf = pd.DataFrame.from_dict(dict, orient=\\'index\\' , columns=[\"Prediction\"])\\nEdidiong Esu', 'section': 'Miscellaneous', 'question': 'Convert dictionary values to Dataframe table', 'course': 'machine-learning-zoomcamp'}, {'text': 'The image dataset for the competition was in a different layout from what we used in the dino vs dragon lesson. Since that’s what was covered, some folks were more comfortable with that setup, so I wrote a script that would generate it for them\\nIt can be found here: kitchenware-dataset-generator | Kaggle\\nMartin Uribe', 'section': 'Miscellaneous', 'question': 'Kitchenware Classification Competition Dataset Generator', 'course': 'machine-learning-zoomcamp'}, {'text': 'Install Nvidia drivers: https://www.nvidia.com/download/index.aspx.\\nWindows:\\nInstall Anaconda prompt https://www.anaconda.com/\\nTwo options:\\nInstall package ‘tensorflow-gpu’ in Anaconda\\nInstall the Tensorflow way https://www.tensorflow.org/install/pip#windows-native\\nWSL/Linux:\\nWSL: Use the Windows Nvida drivers, do not touch that.\\nTwo options:\\nInstall the Tensorflow way https://www.tensorflow.org/install/pip#linux_1\\nMake sure to follow step 4 to install CUDA by environment\\nAlso run:\\necho ‘export XLA_FLAGS=--xla_gpu_cuda_data_dir=$CONDA_PREFIX/lib/> $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh\\nInstall CUDA toolkit 11.x.x https://developer.nvidia.com/cuda-toolkit-archive\\nInstall https://developer.nvidia.com/rdp/cudnn-download\\nNow you should be able to do training/inference with GPU in Tensorflow\\n(Learning in public links Links to social media posts where you share your progress with others (LinkedIn, Twitter, etc). Use #mlzoomcamp tag. The scores for this part will be capped at 7 points. Please make sure the posts are valid URLs starting with \"https://\" Does it mean that I should provide my linkedin link? or it means that I should write a post that I have completed my first assignement? (\\nANS (by ezehcp7482@gmail.com): Yes, provide the linkedIN link to where you posted.\\nezehcp7482@gmail.com:\\nPROBLEM: Since I had to put up a link to a public repository, I had to use Kaggle and uploading the dataset therein was a bit difficult; but I had to ‘google’ my way out.\\nANS: See this link for a guide (https://www.kaggle.com/code/dansbecker/finding-your-files-in-kaggle-kernels/notebook)', 'section': 'Miscellaneous', 'question': 'CUDA toolkit and cuDNN Install for Tensorflow', 'course': 'machine-learning-zoomcamp'}, {'text': 'When multiplying matrices, the order of multiplication is important.\\nFor example:\\nA (m x n) * B (n x p) = C (m x p)\\nB (n x p) * A (m x n) = D (n x n)\\nC and D are matrices of different sizes and usually have different values. Therefore the order is important in matrix multiplication and changing the order changes the result.\\nBaran Akın', 'section': 'Miscellaneous', 'question': 'About getting the wrong result when multiplying matrices', 'course': 'machine-learning-zoomcamp'}, {'text': 'Refer to https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/01-intro/06-environment.md\\n(added by Rileen Sinha)', 'section': 'Miscellaneous', 'question': 'None of the videos have how to install the environment in Mac, does someone have instructions for Mac with M1 chip?', 'course': 'machine-learning-zoomcamp'}, {'text': \"Depends on whether the form will still be open. If you're lucky and it's open, you can submit your homework and it will be evaluated. if closed - it's too late.\\n(Added by Rileen Sinha, based on answer by Alexey on Slack)\", 'section': 'Miscellaneous', 'question': 'I may end up submitting the assignment late. Would it be evaluated?', 'course': 'machine-learning-zoomcamp'}, {'text': 'Yes. Whoever corrects the homework will only be able to access the link if the repository is public.\\n(added by Tano Bugelli)\\nHow to install Conda environment in my local machine?\\nWhich ide is recommended for machine learning?', 'section': 'Miscellaneous', 'question': 'Does the github repository need to be public?', 'course': 'machine-learning-zoomcamp'}, {'text': 'Install w get:\\n!which wget\\nDownload data:\\n!wget -P /content/drive/My\\\\ Drive/Downloads/ URL\\n(added by Paulina Hernandez)', 'section': 'Miscellaneous', 'question': 'How to use wget with Google Colab?', 'course': 'machine-learning-zoomcamp'}, {'text': \"Features (X) must always be formatted as a 2-D array to be accepted by scikit-learn.\\nUse reshape to reshape a 1D array to a 2D.\\n\\t\\t\\t\\t\\t\\t\\t(-Aileah) :>\\n(added by Tano\\nfiltered_df = df[df['ocean_proximity'].isin(['<1H OCEAN', 'INLAND'])]\\n# Select only the desired columns\\nselected_columns = [\\n'latitude',\\n'longitude',\\n'housing_median_age',\\n'total_rooms',\\n'total_bedrooms',\\n'population',\\n'households',\\n'median_income',\\n'median_house_value'\\n]\\nfiltered_df = filtered_df[selected_columns]\\n# Display the first few rows of the filtered DataFrame\\nprint(filtered_df.head())\", 'section': 'Miscellaneous', 'question': 'Features in scikit-learn?', 'course': 'machine-learning-zoomcamp'}, {'text': 'FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead', 'section': 'Miscellaneous', 'question': 'When I plotted using Matplot lib to check if median has a tail, I got the error below how can one bypass?', 'course': 'machine-learning-zoomcamp'}, {'text': 'When trying to rerun the docker file in Windows, as opposed to developing in WSL/Linux, I got the error of:\\n```\\nWarning: Python 3.11 was not found on your system…\\nNeither ‘pipenv’ nor ‘asdf’ could be found to install Python.\\nYou can specify specific versions of Python with:\\n$ pipenv –python path\\\\to\\\\python\\n```\\nThe solution was to add Python311 installation folder to the PATH and restart the system and run the docker file again. That solved the error.\\n(Added by Abhijit Chakraborty)', 'section': 'Miscellaneous', 'question': 'Reproducibility in different OS', 'course': 'machine-learning-zoomcamp'}, {'text': 'You may quickly deploy your project to DigitalOcean App Cloud. The process is relatively straightforward. The deployment costs about 5 USD/month. The container needs to be up until the end of the project evaluation.\\nSteps:\\nRegister in DigitalOcean\\nGo to Apps -> Create App.\\nYou will need to choose GitHub as a service provider.\\nEdit Source Directory (if your project is not in the repo root)\\nIMPORTANT: Go to settings -> App Spec and edit the Dockerfile path so it looks like ./project/Dockerfile path relative to your repo root\\nRemember to add model files if they are not built automatically during the container build process.\\nBy Dmytro Durach', 'section': 'Miscellaneous', 'question': 'Deploying to Digital Ocean', 'course': 'machine-learning-zoomcamp'}, {'text': \"I’m just looking back at the lessons in week 3 (churn prediction project), and lesson 3.6 talks about Feature Importance for categorical values. At 8.12, the mutual info scores show that the some features are more important than others, but then in lesson 3.10 the Logistic Regression model is trained on all of the categorical variables (see 1:35). Once we have done feature importance, is it best to train your model only on the most important features?\\nNot necessarily - rather, any feature that can offer additional predictive value should be included (so, e.g. predict with & without including that feature; if excluding it drops performance, keep it, else drop it). A few individually important features might in fact be highly correlated with others, & dropping some might be fine. There are many feature selection algorithms, it might be interesting to read up on them (among the methods we've learned so far in this course, L1 regularization (Lasso) implicitly does feature selection by shrinking some weights all the way to zero).\\nBy Rileen Sinha\", 'section': 'Miscellaneous', 'question': 'Is it best to train your model only on the most important features?', 'course': 'machine-learning-zoomcamp'}, {'text': 'You can consider several different approaches:\\nSampling: In the exploratory phase, you can use random samples of the data.\\nChunking: When you do need all the data, you can read and process it in chunks that do fit in the memory.\\nOptimizing data types: Pandas’ automatic data type inference (when reading data in) might result in e.g. float64 precision being used to represent integers, which wastes space. You might achieve substantial memory reduction by optimizing the data types.\\nUsing Dask, an open-source python project which parallelizes Numpy and Pandas.\\n(see, e.g. https://www.vantage-ai.com/en/blog/4-strategies-how-to-deal-with-large-datasets-in-pandas)\\nBy Rileen Sinha', 'section': 'Miscellaneous', 'question': 'How can I work with very large datasets, e.g. the New York Yellow Taxi dataset, with over a million rows?', 'course': 'machine-learning-zoomcamp'}, {'text': 'Technically, yes. Advisable? Not really. Reasons:\\nSome homework(s) asks for specific python library versions.\\nAnswers may not match in MCQ options if using different languages other than Python 3.10 (the recommended version for 2023 cohort)\\nAnd as for midterms/capstones, your peer-reviewers may not know these other languages. Do you want to be penalized for others not knowing these other languages?\\nYou can create a separate repo using course’s lessons but written in other languages for your own learnings, but not advisable for submissions.\\ntx[source]', 'section': 'Miscellaneous', 'question': 'Can I do the course in other languages, like R or Scala?', 'course': 'machine-learning-zoomcamp'}, {'text': 'Yes, it’s allowed (as per Alexey).\\nAdded By Rileen Sinha', 'section': 'Miscellaneous', 'question': 'Is use of libraries like fast.ai or huggingface allowed in the capstone and competition, or are they considered to be \"too much help\"?', 'course': 'machine-learning-zoomcamp'}, {'text': 'The TF and TF Serving versions have to match (as per solution from the slack channel)\\nAdded by Chiedu Elue', 'section': 'Miscellaneous', 'question': 'Flask image was built and tested successfully, but tensorflow serving image was built and unable to test successfully. What could be the problem?', 'course': 'machine-learning-zoomcamp'}, {'text': 'I’ve seen LinkedIn users list DataTalksClub as Experience with titles as:\\nMachine Learning Fellow\\nMachine Learning Student\\nMachine Learning Participant\\nMachine Learning Trainee\\nPlease note it is best advised that you do not list the experience as an official “job” or “internship” experience since DataTalksClub did not hire you, nor financially compensate you.\\nOther ways you can incorporate the experience in the following sections:\\nOrganizations\\nProjects\\nSkills\\nFeatured\\nOriginal posts\\nCertifications\\nCourses\\nBy Annaliese Bronz\\nInteresting question, I put the link of my project into my CV as showcase and make posts to show my progress.\\nBy Ani Mkrtumyan', 'section': 'Miscellaneous', 'question': 'Any advice for adding the Machine Learning Zoomcamp experience to your LinkedIn profile?', 'course': 'machine-learning-zoomcamp'}, {'text': 'MLOps Zoomcamp FAQ\\nThe purpose of this document is to capture frequently asked technical questions.\\nWe did this for our data engineering course, and it worked quite well. Check this document for inspiration on how to structure your questions and answers:\\nData Engineering Zoomcamp FAQ\\n[Problem description]\\n[Solution description]\\n(optional) Added by Name', 'section': '+-General course questions', 'question': 'Format for questions: [Problem title]', 'course': 'mlops-zoomcamp'}, {'text': 'Approximately 3 months. For each module, about 1 week with possible deadline extensions (in total 6~9 weeks), 2 weeks for working on the capstone project and 1 week for peer review.', 'section': '+-General course questions', 'question': 'What is the expected duration of this course or that for each module?', 'course': 'mlops-zoomcamp'}, {'text': 'The difference is the Orchestration and Monitoring modules. Those videos will be re-recorded. The rest should mostly be the same.\\nAlso all of the homeworks will be changed for the 2023 cohort.', 'section': '+-General course questions', 'question': 'What’s the difference between the 2023 and 2022 course?', 'course': 'mlops-zoomcamp'}, {'text': 'Yes, it will start in May 2024', 'section': '+-General course questions', 'question': 'Will there be a 2024 Cohort? When will the 2024 cohort start?', 'course': 'mlops-zoomcamp'}, {'text': 'Please choose the closest one to your answer. Also do not post your answer in the course slack channel.', 'section': '+-General course questions', 'question': 'What if my answer is not exactly the same as the choices presented?', 'course': 'mlops-zoomcamp'}, {'text': 'Please pick up a problem you want to solve yourself. Potential datasets can be found on either Kaggle, Hugging Face, Google, AWS, or the UCI Machine Learning Datasets Repository.', 'section': '+-General course questions', 'question': 'Are we free to choose our own topics for the final project?', 'course': 'mlops-zoomcamp'}, {'text': 'In order to obtain the certificate, completion of the final capstone project is mandatory. The completion of weekly homework assignments is optional, but they can contribute to your overall progress and ranking on the top 100 leaderboard.', 'section': '+-General course questions', 'question': 'Can I still graduate when I didn’t complete homework for week x?', 'course': 'mlops-zoomcamp'}, {'text': 'You can get a few cloud points by using kubernetes even if you deploy it only locally. Or you can use local stack too to mimic AWS\\nAdded by Ming Jun, Asked by Ben Pacheco, Answered by Alexey Grigorev', 'section': 'Module 1: Introduction', 'question': 'For the final project, is it required to be put on the cloud?', 'course': 'mlops-zoomcamp'}, {'text': 'For those who are not using VSCode (or other similar IDE), you can automate port-forwarding for Jupyter Notebook by adding the following line of code to your\\n~/.ssh/config file (under the mlops-zoomcamp host):\\nLocalForward 127.0.0.1:8899 127.0.0.1:8899\\nThen you can launch Jupyter Notebook using the following command: jupyter notebook --port=8899 --no-browser and copy paste the notebook URL into your browser.\\nAdded by Vishal', 'section': 'Module 1: Introduction', 'question': 'Port-forwarding without Visual Studio', 'course': 'mlops-zoomcamp'}, {'text': 'You can install the Jupyter extension to open notebooks in VSCode.\\nAdded by Khubaib', 'section': 'Module 1: Introduction', 'question': 'Opening Jupyter in VSCode', 'course': 'mlops-zoomcamp'}, {'text': 'In case one would like to set a github repository (e.g. for Homeworks), one can follow 2 great tutorials that helped a lot\\nSetting up github on AWS instance - this\\nSetting up keys on AWS instance - this\\nThen, one should be able to push to its repo\\nAdded by Daniel Hen (daniel8hen@gmail.com)', 'section': 'Module 1: Introduction', 'question': 'Configuring Github to work from the remote VM', 'course': 'mlops-zoomcamp'}, {'text': \"Faced issue while setting up JUPYTER NOTEBOOK on AWS. I was unable to access it from my desktop. (I am not using visual studio and hence faced problem)\\nRun\\njupyter notebook --generate-config\\nEdit file /home/ubuntu/.jupyter/jupyter_notebook_config.py to add following line:\\nNotebookApp.ip = '*'\\nAdded by Atul Gupta (samatul@gmail.com)\", 'section': 'Module 1: Introduction', 'question': 'Opening Jupyter in AWS', 'course': 'mlops-zoomcamp'}, {'text': 'If you wish to use WSL on your windows machine, here are the setup instructions:\\nCommand: Sudo apt install wget\\nGet Anaconda download address here. wget <download address>\\nTurn on Docker Desktop WFree Download | AnacondaSL2\\nCommand: git clone <github repository address>\\nVSCODE on WSL\\nJupyter: pip3 install jupyter\\nAdded by Gregory Morris (gwm1980@gmail.com)\\nAll in all softwares at one shop:\\nYou can use anaconda which has all built in services like pycharm, jupyter\\nAdded by Khaja Zaffer (khajazaffer@aln.iseg.ulisboa.pt)\\nFor windows “wsl --install” in Powershell\\nAdded by Vadim Surin (vdmsurin@gmai.com)', 'section': 'Module 1: Introduction', 'question': 'WSL instructions', 'course': 'mlops-zoomcamp'}, {'text': 'If you create a folder data and download datasets or raw files in your local repository. Then to push all your code to remote repository without this files or folder please use gitignore file. The simple way to create it do the following steps\\n1. Create empty .txt file (using text editor or command line)\\n2. Safe as .gitignore (. must use the dot symbol)\\n3. Add rules\\n *.parquet - to ignore all parquet files\\ndata/ - to ignore all files in folder data\\n\\nFor more pattern read GIT documentation\\nhttps://git-scm.com/docs/gitignore\\nAdded by Olga Rudakova (olgakurgan@gmail.com)', 'section': 'Module 1: Introduction', 'question': '.gitignore how-to', 'course': 'mlops-zoomcamp'}, {'text': \"Make sure when you stop an EC2 instance that it actually stops (there's a meme about it somewhere). There are green circles (running), orange (stopping), and red (stopped). Always refresh the page to make sure you see the red circle and status of stopped.\\nEven when an EC2 instance is stopped, there WILL be other charges that are incurred (e.g. if you uploaded data to the EC2 instance, this data has to be stored somewhere, usually an EBS volume and this storage incurs a cost).\\nYou can set up billing alerts. (I've never done this, so no advice on how to do this).\\n(Question by: Akshit Miglani (akshit.miglani09@gmail.com) and Answer by Anna Vasylytsya)\", 'section': 'Module 1: Introduction', 'question': 'AWS suggestions', 'course': 'mlops-zoomcamp'}, {'text': 'You can get invitation code by coursera and use it in account to verify it it has different characteristics.\\nI really love it\\nhttps://www.youtube.com/watch?v=h_GdX6KtXjo', 'section': 'Module 1: Introduction', 'question': 'IBM Cloud an alternative for AWS', 'course': 'mlops-zoomcamp'}, {'text': \"I am worried about the cost of keeping an AWS instance running during the course.\\nWith the instance specified during working environment setup, if you remember to Stop Instance once you finished your work for the day.  Using that strategy, in a day with about 5 hours of work you will pay around $0.40 USD which will account for $12 USD per month, which seems to be an affordable amount.\\nYou must remember that you would have a different IP public address every time you Restart your instance, and you would need to edit your ssh Config file.  It's worth the time though.\\nAdditionally, AWS enables you to set up an automatic email alert if a predefined budget is exceeded.\\nHere is a tutorial to set this up.\\nAlso, you can estimate the cost yourself, using AWS pricing calculator (to use it you don’t even need to be logged in).\\nAt the time of writing (20.05.2023) t3a.xlarge instance with 2 hr/day usage (which translates to 10 hr/week that should be enough to complete the course) and 30GB EBS monthly cost is 10.14 USD\\nHere’s a link to the estimate\\nAdded by Alex Litvinov (aaalex.lit@gmail.com)\", 'section': 'Module 1: Introduction', 'question': 'AWS costs', 'course': 'mlops-zoomcamp'}, {'text': 'For many parts - yes. Some things like kinesis are not in AWS free tier, but you can do it locally with localstack.', 'section': 'Module 1: Introduction', 'question': 'Is the AWS free tier enough for doing this course?', 'course': 'mlops-zoomcamp'}, {'text': 'When I click an open IP-address in an AWS EC2 instance I get an error: “This site can’t be reached”. What should I do?\\nThis ip-address is not required to be open in a browser. It is needed to connect to the running EC2 instance via terminal from your local machine or via terminal from a remote server with such command, for example if:\\nip-address is 11.111.11.111\\ndownloaded key name is razer.pem (the key should be moved to a hidden folder .ssh)\\nyour user name is user_name\\nssh -i /Users/user_name/.ssh/razer.pem ubuntu@11.111.11.111', 'section': 'Module 1: Introduction', 'question': 'AWS EC2: this site can’t be reached', 'course': 'mlops-zoomcamp'}, {'text': 'After this command `ssh -i ~/.ssh/razer.pem ubuntu@XX.XX.XX.XX` I got this error: \"unprotected private key file\". This page (https://99robots.com/how-to-fix-permission-error-ssh-amazon-ec2-instance/) explains how to fix this error. Basically you need to change the file permissions of the key file with this command: chmod 400 ~/.ssh/razer.pem', 'section': 'Module 1: Introduction', 'question': 'Unprotected private key file!', 'course': 'mlops-zoomcamp'}, {'text': 'My SSH connection to AWS cannot last more than a few minutes, whether via terminal or VS code.\\nMy config:\\n# Copy Configuration in local nano editor, then Save it!\\nHost mlops-zoomcamp                                         # ssh connection calling name\\nUser ubuntu                                             # username AWS EC2\\nHostName <instance-public-IPv4-addr>                    # Public IP, it changes when Source EC2 is turned off.\\nIdentityFile ~/.ssh/name-of-your-private-key-file.pem   # Private SSH key file path\\nLocalForward 8888 localhost:8888                        # Connecting to a service on an internal network from the outside, static forward or set port user forward via on vscode\\nStrictHostKeyChecking no\\nAdded by Muhammed Çelik\\nThe disconnection will occur whether I SSH via WSL2 or via VS Code, and usually occurs after I run some code, i.e. “import mlflow”, so not particularly intense computation.\\nI cannot reconnect to the instance without stopping and restarting with a new IPv4 address.\\nI’ve gone through steps listed on this page: https://aws.amazon.com/premiumsupport/knowledge-center/ec2-linux-resolve-ssh-connection-errors/\\nInbound rule should allow all incoming IPs for SSH.\\nWhat I expect to happen:\\nSSH connection should remain while I’m actively using the instance, and if it does disconnect, I should be able to reconnect back.\\nSolution: sometimes the hang ups are caused by the instance running out of memory. In one instance, using EC2 feature to view screenshot of the instance as a means to troubleshoot, it was the OS out-of-memory feature which killed off some critical processes. In this case, if we can’t use a higher compute VM with more RAM, try adding a swap file, which uses the disk as RAM substitute and prevents the OOM error. Follow Ubuntu’s documentation here: https://help.ubuntu.com/community/SwapFaq.\\nAlternatively follow AWS’s own doc, which mirrors Ubuntu’s: https://aws.amazon.com/premiumsupport/knowledge-center/ec2-memory-swap-file/', 'section': 'Module 1: Introduction', 'question': 'AWS EC2 instance constantly drops SSH connection', 'course': 'mlops-zoomcamp'}, {'text': 'Everytime I restart my EC2 instance I keep getting different IP and need to update the config file manually.\\n\\nSolution: You can create a script like this to automatically update the IP address of your EC2 instance.https://github.com/dimzachar/mlops-zoomcamp/blob/master/notes/Week_1/update_ssh_config.md', 'section': 'Module 1: Introduction', 'question': 'AWS EC2 IP Update', 'course': 'mlops-zoomcamp'}, {'text': 'Make sure to use an instance with enough compute capabilities such as a t2.xlarge. You can check the monitoring tab in the EC2 dashboard to monitor your instance.', 'section': 'Module 1: Introduction', 'question': 'VS Code crashes when connecting to Jupyter', 'course': 'mlops-zoomcamp'}, {'text': 'Error “ValueError: X has 526 features, but LinearRegression is expecting 525 features as input.” when running your Linear Regression Model on the validation data set:\\nSolution: The DictVectorizer creates an initial mapping for the features (columns). When calling the DictVecorizer again for the validation dataset transform should be used as it will ignore features that it did not see when fit_transform was last called. E.g.\\nX_train = dv.fit_transform(train_dict)\\nX_test = dv.transform(test_dict)', 'section': 'Module 1: Introduction', 'question': 'X has 526 features, but expecting 525 features', 'course': 'mlops-zoomcamp'}, {'text': 'If some dependencies are missing\\nInstall following packages\\npandas\\nmatplotlib\\nscikit-learn\\nfastparquet\\npyarrow\\nseaborn\\npip install -r requirements.txt\\nI have seen this error when using pandas.read_parquet(), the solution is to install pyarrow or fastparquet by doing !pip install pyarrow in the notebook\\nNOTE: if you’re using Conda instead of pip, install fastparquet rather than pyarrow, as it is much easier to install and it’s functionally identical to pyarrow for our needs.', 'section': 'Module 1: Introduction', 'question': 'Missing dependencies', 'course': 'mlops-zoomcamp'}, {'text': 'The evaluation RMSE I get doesn’t figure within the options!\\nIf you’re evaluating the model on the entire February data, try to filter outliers using the same technique you used on the train data (0≤duration≤60) and you’ll get a RMSE which is (approximately) in the options. Also don’t forget to convert the columns data types to str before using the DictVectorizer.\\nAnother option: Along with filtering outliers, additionally filter on null values by replacing them with -1.  You will get a RMSE which is (almost same as) in the options. Use ‘.round(2)’ method to round it to 2 decimal points.\\nWarning deprecation\\nThe python interpreter warning of modules that have been deprecated  and will be removed in future releases as well as making suggestion how to go about your code.\\nFor example\\nC:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\seaborn\\\\distributions.py:2619:\\nFutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\\nwarnings.warn(msg, FutureWarning)\\nTo suppress the warnings, you can include this code at the beginning of your notebook\\nimport warnings\\nwarnings.filterwarnings(\"ignore\")', 'section': 'Module 1: Introduction', 'question': 'No RMSE value in the options', 'course': 'mlops-zoomcamp'}, {'text': 'sns.distplot(df_train[\"duration\"])\\nCan be replaced with\\nsns.histplot(\\ndf_train[\"duration\"] , kde=True,\\nstat=\"density\", kde_kws=dict(cut=3), bins=50,\\nalpha=.4, edgecolor=(1, 1, 1, 0.4),\\n)\\nTo get almost identical result', 'section': 'Module 1: Introduction', 'question': 'How to replace distplot with histplot', 'course': 'mlops-zoomcamp'}, {'text': 'You need to replace the capital letter “L” with a small one “l”', 'section': 'Module 1: Introduction', 'question': \"KeyError: 'PULocationID'  or  'DOLocationID'\", 'course': 'mlops-zoomcamp'}, {'text': 'I have faced a problem while reading the large parquet file. I tried some workarounds but they were NOT successful with Jupyter.\\nThe error message is:\\nIndexError: index 311297 is out of bounds for axis 0 with size 131743\\nI solved it by performing the homework directly as a python script.\\nAdded by Ibraheem Taha (ibraheemtaha91@gmail.com)\\nYou can try using the Pyspark library\\nAnswered by kamaldeen (kamaldeen32@gmail.com)', 'section': 'Module 1: Introduction', 'question': 'Reading large parquet files', 'course': 'mlops-zoomcamp'}, {'text': 'First remove the outliers (trips with unusual duration) before plotting\\nAdded by Ibraheem Taha (ibraheemtaha91@gmail.com)', 'section': 'Module 1: Introduction', 'question': 'Distplot takes too long', 'course': 'mlops-zoomcamp'}, {'text': 'Problem: RMSE on test set was too high when hot encoding the validation set with a previously fitted OneHotEncoder(handle_unknown=’ignore’) on the training set, while DictVectorizer would yield the correct RMSE.\\nIn principle both transformers should behave identically when treating categorical features (at least in this week’s homework where we don’t have sequences of strings in each row):\\nFeatures are put into binary columns encoding their presence (1) or absence (0)\\nUnknown categories are imputed as zeroes in the hot-encoded matrix', 'section': 'Module 1: Introduction', 'question': 'RMSE on test set too high', 'course': 'mlops-zoomcamp'}, {'text': 'A: Alexey’s answer https://www.youtube.com/watch?v=8uJ36ZZr_Is&t=13s\\nIn summary,\\npd.get_dummies or OHE can come up with result in different orders and handle missing data differently, so train and val set would have different columns during train and validation\\nDictVectorizer would ignore missing (in train) and new (in val) datasets\\nOther sources:\\nhttps://datascience.stackexchange.com/questions/9443/when-to-use-one-hot-encoding-vs-labelencoder-vs-dictvectorizor\\nhttps://scikit-learn.org/stable/modules/feature_extraction.html\\nhttps://innovation.alteryx.com/encode-smarter/\\n~ ellacharmed', 'section': 'Module 1: Introduction', 'question': 'Q: Using of OneHotEncoder instead of DictVectorizer', 'course': 'mlops-zoomcamp'}, {'text': \"Why didn't get_dummies in pandas library or OneHotEncoder in scikit-learn library be used for one-hot encoding? I know OneHotEncoder is the most common and useful. One-hot coding can also be done using the eye or identity components of the NumPy library.\\nM.Sari\\nOneHotEncoder has the option to output a row column tuple matrix. DictVectorizer is a one step method to encode and support row column tuple matrix output.\\nHarinder(sudwalh@gmail.com)\", 'section': 'Module 1: Introduction', 'question': 'Q: Why did we not use OneHotEncoder(sklearn) instead of DictVectorizer ?', 'course': 'mlops-zoomcamp'}, {'text': 'How to check that we removed the outliers?\\nUse the pandas function describe() which can provide a report of the data distribution along with the statistics to describe the data. For example, after clipping the outliers using boolean expression, the min and max can be verified using\\ndf[‘duration’].describe()', 'section': 'Module 1: Introduction', 'question': 'Clipping outliers', 'course': 'mlops-zoomcamp'}, {'text': 'pd.get_dummies and DictVectorizer both create a one-hot encoding on string values. Therefore you need to convert the values in PUlocationID and DOlocationID to string.\\nIf you convert the values in PUlocationID and DOlocationID from numeric to string, the NaN values get converted to the string \"nan\".  With DictVectorizer the RMSE is the same whether you use \"nan\" or \"-1\" as string representation for the NaN values. Therefore the representation doesn\\'t have to be \"-1\" specifically, it could also be some other string.', 'section': 'Module 1: Introduction', 'question': 'Replacing NaNs for pickup location and drop off location with -1 for One-Hot Encoding', 'course': 'mlops-zoomcamp'}, {'text': 'Problem: My LinearRegression RSME is very close to the answer but not exactly the same. Is this normal?\\nAnswer: No, LinearRegression is an deterministic model, it should always output the same results when given the same inputs.\\nAnswer:\\nCheck if you have treated the outlier properly for both train and validation sets\\nCheck if the one hot encoding has been done properly by looking at the shape of one hot encoded feature matrix. If it shows 2 features, there is wrong with one hot encoding. Hint: the drop off and pick up codes need to be converted to proper data format and then DictVectorizer is fitted.\\nHarshit Lamba (hlamba19@gmail.com)', 'section': 'Module 1: Introduction', 'question': 'Slightly different RSME', 'course': 'mlops-zoomcamp'}, {'text': 'Problem: I’m facing an extremely low RMSE score (eg: 4.3451e-6) - what shall I do?\\nAnswer: Recheck your code to see if your model is learning the target prior to making the prediction. If the target variable is passed in as a parameter while fitting the model, chances are the model would score extremely low. However, that’s not what you would want and would much like to have your model predict that. A good way to check that is to make sure your X_train doesn’t contain any part of your y_train. The same stands for validation too.\\nSnehangsu De (desnehangsu@gmail.com)', 'section': 'Module 1: Introduction', 'question': 'Extremely low RSME', 'course': 'mlops-zoomcamp'}, {'text': 'Problem: how to enable auto completion in jupyter notebook? Tab doesn’t work for me\\nSolution: !pip install --upgrade jedi==0.17.2\\nChristopher R.J.(romanjaimesc@gmail.com)', 'section': 'Module 1: Introduction', 'question': 'Enabling Auto-completion in jupyter notebook', 'course': 'mlops-zoomcamp'}, {'text': \"Problem: While following the steps in the videos you may have problems trying to download with wget the files. Usually it is a 403 error type (Forbidden access).\\nSolution: The links point to files on cloudfront.net, something like this:\\nhttps://d37ci6vzurychx.cloudfront.net/tOSError: Could not open parquet input source '<Buffer>': Invalid: Parquet OSError: Could not open parquet input source '<Buffer>': Invalid: Parquet rip+data/green_tripdata_2021-01.parquet\\nI’m not download the dataset directly, i use dataset URL and run this in the file.\\nUpdate(27-May-2023): Vikram\\nI am able to download the data from the below link. This is from the official  NYC trip record page (https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page). Copy link from page directly as the below url might get changed if the NYC decides to move away from this. Go to the page , right click and use copy link.\\nwget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2021-01.parquet\\n(Asif)\\nCopy the link address and replace the cloudfront.net part with s3.amazonaws.com/nyc-tlc/, so it looks like this:\\nhttps://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2021-01.parquet\\nMario Tormo (mario@tormo-romero.eu)\\nOSError: Could not open parquet input source '<Buffer>': Invalid: Parquet\", 'section': 'Module 1: Introduction', 'question': 'Downloading the data from the NY Taxis datasets gives error : 403 Forbidden', 'course': 'mlops-zoomcamp'}, {'text': 'Problem: PyCharm (remote) doesn’t see conda execution path. So, I cannot use conda env (which is located on a remote server).\\nSolution: In remote server in command line write “conda activate envname”, after write “which python” - it gives you python execution path. After you can use this path when you will add new interpreter in PyCharm: add local interpreter -> system interpreter -> and put the path with python.\\nSalimov Ilnaz (salimovilnaz777@gmail.com)', 'section': 'Module 1: Introduction', 'question': 'Using PyCharm & Conda env in remote development', 'course': 'mlops-zoomcamp'}, {'text': 'Problem: The output of DictVectorizer was taking up too much memory. So much so, that I couldn’t even fit the linear regression model before running out of memory on my 16 GB machine.\\nSolution: In the example for DictVectorizer in the scikit-learn website, they set the parameter “sparse” as False. Although this helps with viewing the results, this results in a lot of memory usage. The solution is to either use “sparse=True” instead, or leave it at the default which is also True.\\nAhmed Fahim (afahim03@yahoo.com)', 'section': 'Module 1: Introduction', 'question': 'Running out of memory', 'course': 'mlops-zoomcamp'}, {'text': 'Problem: For me, Installing anaconda didn’t modify the .bashrc profile. That means Anaconda env was not activated even after exiting and relaunching the unix shell.\\nSolution:\\nFor bash : Initiate conda again, which will add entries for anaconda in .bashrc file.\\n$ cd YOUR_PATH_ANACONDA/bin $ ./conda init bash\\nThat will automatically edit your .bashrc.\\nReload:\\n$ source ~/.bashrc\\nAhamed Irshad (daisyfuentesahamed@gmail.com)', 'section': 'Module 1: Introduction', 'question': 'Activating Anaconda env in .bashrc', 'course': 'mlops-zoomcamp'}, {'text': 'While working through the HW1, you will realize that the training and the validation data set feature sizes are different. I was trying to figure out why and went down the entire rabbit hole only to see that I wasn’t doing ```transform``` on the premade dictionary vectorizer instead of ```fit_transform```. You already have the dictionary vectorizer made so no need to execute the fit pipeline on the model.\\nSam Lim(changhyeonlim@gmail.com)', 'section': 'Module 1: Introduction', 'question': 'The feature size is different for training set and validation set', 'course': 'mlops-zoomcamp'}, {'text': 'I found a good guide how to get acces to your machine again when you removed your public key.\\nUsing the following link you can go to Session Manager and log in to your instance and create public key again. https://repost.aws/knowledge-center/ec2-linux-fix-permission-denied-errors\\nThe main problem for me here was to get my old public key, so for doing this you should run the following command: ssh-keygen -y -f /path_to_key_pair/my-key-pair.pem\\nFor more information: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/describe-keys.html#retrieving-the-public-key\\nHanna Zhukavets (a.zhukovec1901@gmail.com)', 'section': 'Module 1: Introduction', 'question': 'Permission denied (publickey) Error (when you remove your public key on the AWS machine)', 'course': 'mlops-zoomcamp'}, {'text': 'Problem: The February dataset has been used as a validation/test dataset and been stripped of the outliers in a similar manner to the train dataset (taking only the rows for the duration between 1 and 60, inclusive). The RMSE obtained afterward is in the thousands.\\nAnswer: The sparsematrix result from DictVectorizer shouldn’t be turned into an ndarray. After removing that part of the code, I ended up receiving a correct result .\\nTahina Mahatoky (tahinadanny@gmail.com)', 'section': 'Module 1: Introduction', 'question': 'Overfitting: Absurdly high RMSE on the validation dataset', 'course': 'mlops-zoomcamp'}, {'text': 'more specific error line:\\nfrom sklearn.feature_extraction import DictVectorizer\\nI had this issue and to solve it I did\\n!pip install scikit-learn\\nJoel Auccapuclla (auccapuclla 2013@gmail.com)', 'section': 'Module 2: Experiment tracking', 'question': 'Can’t import sklearn', 'course': 'mlops-zoomcamp'}, {'text': 'Problem: Localhost:5000 Unavailable // Access to Localhost Denied // You don’t have authorization to view this page (127.0.0.1:5000)\\n\\nSolution: If you are on an chrome browser you need to head to `chrome://net-internals/#sockets` and press “Flush Socket Pools”', 'section': 'Module 2: Experiment tracking', 'question': 'Access Denied at Localhost:5000 - Authorization Issue', 'course': 'mlops-zoomcamp'}, {'text': \"You have something running on the 5000 port. You need to stop it.\\nAnswer: On terminal in mac .\\nRun ps -A | grep gunicorn\\nLook for the number process id which is the 1st number after running the command\\nkill 13580\\nwhere 13580  represents the process number.\\nSource\\nwarrie.warrieus@gmail.com\\nOr by executing the following command it will kill all the processes using port 5000:\\n>> sudo fuser -k 5000/tcp\\nAnswered by Vaibhav Khandelwal\\nJust execute in the command below in he command line to kill the running port\\n->> kill -9 $(ps -A | grep python | awk '{print $1}')\\nAnswered by kamaldeen (kamaldeen32@gmail.com)\\nChange to different port (5001 in this case)\\n>> mlflow ui --backend-store-uri sqlite:///mlflow.db --port 5001\\nAnswered by krishna (nellaikrishna@gmail.com)\", 'section': 'Module 2: Experiment tracking', 'question': \"Connection in use: ('127.0.0.1', 5000)\", 'course': 'mlops-zoomcamp'}, {'text': 'Running python register_model.py results in the following error:\\nValueError: could not convert string to float: \\'0 int\\\\n1   float\\\\n2     hyperopt_param\\\\n3       Literal{n_estimators}\\\\n4       quniform\\\\n5         Literal{10}\\\\n6         Literal{50}\\\\n7         Literal{1}\\'\\nFull Traceback:\\nTraceback (most recent call last):\\nFile \"/Users/name/Desktop/Programming/DataTalksClub/MLOps-Zoomcamp/2. Experiment tracking and model management/homework/scripts/register_model.py\", line 101, in <module>\\nrun(args.data_path, args.top_n)\\nFile \"/Users/name/Desktop/Programming/DataTalksClub/MLOps-Zoomcamp/2. Experiment tracking and model management/homework/scripts/register_model.py\", line 67, in run\\ntrain_and_log_model(data_path=data_path, params=run.data.params)\\nFile \"/Users/name/Desktop/Programming/DataTalksClub/MLOps-Zoomcamp/2. Experiment tracking and model management/homework/scripts/register_model.py\", line 41, in train_and_log_model\\nparams = space_eval(SPACE, params)\\nFile \"/Users/name/miniconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/hyperopt/fmin.py\", line 618, in space_eval\\nrval = pyll.rec_eval(space, memo=memo)\\nFile \"/Users/name/miniconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/hyperopt/pyll/base.py\", line 902, in rec_eval\\nrval = scope._impls[node.name](*args, **kwargs)\\nValueError: could not convert string to float: \\'0 int\\\\n1   float\\\\n2     hyperopt_param\\\\n3       Literal{n_estimators}\\\\n4       quniform\\\\n5         Literal{10}\\\\n6         Literal{50}\\\\n7         Literal{1}\\'\\nSolution: There are two plausible errors to this. Both are in the hpo.py file where the hyper-parameter tuning is run. The objective function should look like this.\\n\\n   def objective(params):\\n# It\\'s important to set the \"with\" statement and the \"log_params\" function here\\n# in order to properly log all the runs and parameters.\\nwith mlflow.start_run():\\n# Log the parameters\\nmlflow.log_params(params)\\nrf = RandomForestRegressor(**params)\\nrf.fit(X_train, y_train)\\ny_pred = rf.predict(X_valid)\\n# Calculate and log rmse\\nrmse = mean_squared_error(y_valid, y_pred, squared=False)\\nmlflow.log_metric(\\'rmse\\', rmse)\\nIf you add the with statement before this function, and just after the following line\\nX_valid, y_valid = load_pickle(os.path.join(data_path, \"valid.pkl\"))\\nand you log the parameters just after the search_space dictionary is defined, like this\\nsearch_space = {....}\\n# Log the parameters\\nmlflow.log_params(search_space)\\nThen there is a risk that the parameters will be logged in group. As a result, the\\nparams = space_eval(SPACE, params)\\nregister_model.py file will receive the parameters in group, while in fact it expects to receive them one by one. Thus, make sure that the objective function looks as above.\\nAdded by Jakob Salomonsson', 'section': 'Module 2: Experiment tracking', 'question': 'Could not convert string to float - ValueError', 'course': 'mlops-zoomcamp'}, {'text': 'Make sure you launch the mlflow UI from the same directory as thec that is running the experiments (same directory that has the mlflow directory and the database that stores the experiments).\\nOr navigate to the correct directory when specifying the tracking_uri.\\nFor example:\\nIf the mlflow.db is in a subdirectory called database, the tracking uri would be ‘sqllite:///database/mlflow.db’\\nIf the mlflow.db is a directory above your current directory: the tracking uri would be ‘sqlite:///../mlflow.db’\\nAnswered by Anna Vasylytsya\\nAnother alternative is to use an absolute path to mlflow.db rather than relative path\\nAnd yet another alternative is to launch the UI from the same notebook by executing the following code cell\\nimport subprocess\\nMLFLOW_TRACKING_URI = \"sqlite:///data/mlflow.db\"\\nsubprocess.Popen([\"mlflow\", \"ui\", \"--backend-store-uri\", MLFLOW_TRACKING_URI])\\nAnd then using the same MLFLOW_TRACKING_URI when initializing mlflow or the client\\nclient = MlflowClient(tracking_uri=MLFLOW_TRACKING_URI)\\nmlflow.set_tracking_uri(MLFLOW_TRACKING_URI)', 'section': 'Module 2: Experiment tracking', 'question': 'Experiment not visible in MLflow UI', 'course': 'mlops-zoomcamp'}, {'text': \"Problem:\\nGetting\\nERROR: THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE\\nduring MLFlow's installation process, particularly while installing the Numpy package using pip\\nWhen I installed mlflow using ‘pip install mlflow’ on 27th May 2022, I got the following error while numpy was getting installed through mlflow:\\n\\nCollecting numpy\\nDownloading numpy-1.22.4-cp310-cp310-win_amd64.whl (14.7 MB)\\n|██████████████              \\t| 6.3 MB 107 kB/s eta 0:01:19\\nERROR: THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE.\\nIf you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.\\nnumpy from https://files.pythonhosted.org/packages/b5/50/d7978137464251c393df28fe0592fbb968110f752d66f60c7a53f7158076/numpy-1.22.4-cp310-cp310-win_amd64.whl#sha256=3e1ffa4748168e1cc8d3cde93f006fe92b5421396221a02f2274aab6ac83b077 (from mlflow):\\nExpected sha256 3e1ffa4748168e1cc8d3cde93f006fe92b5421396221a02f2274aab6ac83b077\\nGot    \\t15e691797dba353af05cf51233aefc4c654ea7ff194b3e7435e6eec321807e90\\nSolution:\\nThen when I install numpy separately (and not as part of mlflow), numpy gets installed (same version), and then when I do 'pip install mlflow', it also goes through.\\nPlease note that the above may not be consistently simulatable, but please be aware of this issue that could occur during pip install of mlflow.\\nAdded by Venkat Ramakrishnan\", 'section': 'Module 2: Experiment tracking', 'question': 'Hash Mismatch Error with Package Installation', 'course': 'mlops-zoomcamp'}, {'text': 'After deleting an experiment from UI, the deleted experiment still persists in the database.\\nSolution: To delete this experiment permanently, follow these steps.\\nAssuming you are using sqlite database;\\nInstall ipython sql using the following command: pip install ipython-sql\\nIn your jupyter notebook, load the SQL magic scripts with this: %load_ext sql\\nLoad the database with this: %sql sqlite:///nameofdatabase.db\\nRun the following SQL script to delete the experiment permanently: check link', 'section': 'Module 2: Experiment tracking', 'question': 'How to Delete an Experiment Permanently from MLFlow UI', 'course': 'mlops-zoomcamp'}, {'text': 'Problem: I cloned the public repo, made edits, committed and pushed them to my own repo. Now I want to get the recent commits from the public repo without overwriting my own changes to my own repo. Which command(s) should I use?\\nThis is what my config looks like (in case this might be useful):\\n[core]\\nrepositoryformatversion = 0\\nfilemode = true\\nbare = false\\nlogallrefupdates = true\\nignorecase = true\\nprecomposeunicode = true\\n[remote \"origin\"]\\nurl = git@github.com:my_username/mlops-zoomcamp.git\\nfetch = +refs/heads/*:refs/remotes/origin/*\\n[branch \"main\"]\\nremote = origin\\nmerge = refs/heads/main\\nSolution: You should fork DataClubsTak’s repo instead of cloning it. On GitHub, click “Fetch and Merge” under the menu “Fetch upstream” at the main page of your own', 'section': 'Module 2: Experiment tracking', 'question': 'How to Update Git Public Repo Without Overwriting Changes', 'course': 'mlops-zoomcamp'}, {'text': 'This is caused by ```mlflow.xgboost.autolog()``` when version 1.6.1 of xgboost\\nDowngrade to 1.6.0\\n```pip install xgboost==1.6.0``` or update requirements file with xgboost==1.6.0 instead of xgboost\\nAdded by Nakul Bajaj', 'section': 'Module 2: Experiment tracking', 'question': 'Image size of 460x93139 pixels is too large. It must be less than 2^16 in each direction.', 'course': 'mlops-zoomcamp'}, {'text': 'Since the version 1.29 the list_experiments method was deprecated and then removed in the later version\\nYou should use search_experiments instead\\nAdded by Alex Litvinov', 'section': 'Module 2: Experiment tracking', 'question': \"MlflowClient object has no attribute 'list_experiments'\", 'course': 'mlops-zoomcamp'}, {'text': 'Make sure `mlflow.autolog()` ( or framework-specific autolog ) written BEFORE `with mlflow.start_run()` not after.\\nAlso make sure that all dependencies for the autologger are installed, including matplotlib. A warning about uninstalled dependencies will be raised.\\nMohammed Ayoub Chettouh', 'section': 'Module 2: Experiment tracking', 'question': 'MLflow Autolog not working', 'course': 'mlops-zoomcamp'}, {'text': 'If you’re running MLflow on a remote VM, you need to forward the port too like we did in Module 1 for Jupyter notebook port 8888. Simply connect your server to VS Code, as we did, and add 5000 to the PORT like in the screenshot:\\nAdded by Sharon Ibejih\\nIf you are running MLflow locally and 127.0.0.1:5000 shows a blank page navigate to localhost:5000 instead.', 'section': 'Module 2: Experiment tracking', 'question': 'MLflow URL (http://127.0.0.1:5000), doesn’t open.', 'course': 'mlops-zoomcamp'}, {'text': 'Got the same warning message as Warrie Warrie when using “mlflow.xgboost.autolog()”\\nIt turned out that this was just a warning message and upon checking MLflow UI (making sure that no “tag” filters were included), the model was actually automatically tracked in the MLflow.\\nAdded by Bengsoon Chuah, Asked by Warrie Warrie, Answered by Anna Vasylytsya & Ivan Starovit', 'section': 'Module 2: Experiment tracking', 'question': 'MLflow.xgboost Autolog Model Signature Failure', 'course': 'mlops-zoomcamp'}, {'text': \"mlflow.exceptions.MlflowException: Cannot set a deleted experiment 'cross-sell' as the active experiment. You can restore the experiment, or permanently delete the  experiment to create a new one.\\nThere are many options to solve in this link: https://stackoverflow.com/questions/60088889/how-do-you-permanently-delete-an-experiment-in-mlflow\", 'section': 'Module 2: Experiment tracking', 'question': 'MlflowException: Unable to Set a Deleted Experiment', 'course': 'mlops-zoomcamp'}, {'text': 'You do not have enough disk space to install the requirements. You can either increase the base EBS volume by following this link or add an external disk to your instance and configure conda installation to happen on the external disk.\\nAbinaya Mahendiran\\nOn GCP: I added another disk to my vm and followed this guide to mount the disk. Confirm the mount by running df -H (disk free) command in bash shell. I also deleted Anaconda and instead used miniconda. I downloaded miniconda in the additional disk that I mounted and when installing miniconda, enter the path to the extra disk instead of the default disk, this way conda is installed on the extra disk.\\nYang Cao', 'section': 'Module 2: Experiment tracking', 'question': 'No Space Left on Device - OSError[Errno 28]', 'course': 'mlops-zoomcamp'}, {'text': 'I was using an old version of sklearn due to which I got the wrong number of parameters because in the latest version min_impurity_split for randomforrestRegressor was deprecated. Had to upgrade to the latest version to get the correct number of params.', 'section': 'Module 2: Experiment tracking', 'question': 'Parameters Mismatch in Homework Q3', 'course': 'mlops-zoomcamp'}, {'text': \"Error: I installed all the libraries from the requirements.txt document in a new environment as follows:\\npip install -r requirementes.txt\\nThen when I run mlflow from my terminal like this:\\nmlflow\\nI get this error:\\nSOLUTION: You need to downgrade the version of 'protobuf' module to 3.20.x or lower. Initially, it was version=4.21, I installed protobuf==3.20\\npip install protobuf==3.20\\nAfter which I was able to run mlflow from my terminal.\\n-Submitted by Aashnna Soni\", 'section': 'Module 2: Experiment tracking', 'question': 'Protobuf error when installing MLflow', 'course': 'mlops-zoomcamp'}, {'text': 'Please check your current directory while running the mlflow ui command. You need to run mlflow ui or mlflow server command in the right directory.', 'section': 'Module 2: Experiment tracking', 'question': 'Setting up Artifacts folders', 'course': 'mlops-zoomcamp'}, {'text': 'If you have problem with setting up MLflow for experiment tracking on GCP, you can check these two links:\\nhttps://kargarisaac.github.io/blog/mlops/data%20engineering/2022/06/15/MLFlow-on-GCP.html\\nhttps://kargarisaac.github.io/blog/mlops/2022/08/26/machine-learning-workflow-orchestration-zenml.html', 'section': 'Module 2: Experiment tracking', 'question': 'Setting up MLflow experiment tracker on GCP', 'course': 'mlops-zoomcamp'}, {'text': 'Solution: Downgrade setuptools (I downgraded 62.3.2 -> 49.1.0)', 'section': 'Module 2: Experiment tracking', 'question': 'Setuptools Replacing Distutils - MLflow Autolog Warning', 'course': 'mlops-zoomcamp'}, {'text': 'I can’t sort runs in MLFlow\\nMake sure you are in table view (not list view) in the MLflow UI.\\nAdded and Answered by Anna Vasylytsya', 'section': 'Module 2: Experiment tracking', 'question': 'Sorting runs in MLflow UI', 'course': 'mlops-zoomcamp'}, {'text': 'Problem: When I ran `$ mlflow ui` on a remote server and try to open it in my local browser I got an exception  and the page with mlflow ui wasn’t loaded.\\nSolution: You should `pip uninstall flask` on your remote server on conda env and after it install Flask `pip install Flask`. It is because the base conda env has ~flask<1.2, and when you clone it to your new work env, you are stuck with this old version.\\nAdded by Salimov Ilnaz', 'section': 'Module 2: Experiment tracking', 'question': \"TypeError: send_file() unexpected keyword 'max_age' during MLflow UI Launch\", 'course': 'mlops-zoomcamp'}, {'text': 'Problem: After successfully installing mlflow using pip install mlflow on my Windows system, I am trying to run the mlflow ui command but it throws the following error:\\nFileNotFoundError: [WinError 2] The system cannot find the file specified\\nSolution: Add C:\\\\Users\\\\{User_Name}\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\Scripts to the PATH\\nAdded by Alex Litvinov', 'section': 'Module 2: Experiment tracking', 'question': 'mlflow ui on Windows FileNotFoundError: [WinError 2] The system cannot find the file specified', 'course': 'mlops-zoomcamp'}, {'text': 'Running “python hpo.py --data_path=./your-path --max_evals=50” for the homework leads to the following error: TypeError: unsupported operand type(s) for -: \\'str\\' and \\'int\\'\\nFull Traceback:\\nFile \"~/repos/mlops/02-experiment-tracking/homework/hpo.py\", line 73, in <module>\\nrun(args.data_path, args.max_evals)\\nFile \"~/repos/mlops/02-experiment-tracking/homework/hpo.py\", line 47, in run\\nfmin(\\nFile \"~/Library/Caches/pypoetry/virtualenvs/mlflow-intro-SyTqwt0D-py3.9/lib/python3.9/site-packages/hyperopt/fmin.py\", line 540, in fmin\\nreturn trials.fmin(\\nFile \"~/Library/Caches/pypoetry/virtualenvs/mlflow-intro-SyTqwt0D-py3.9/lib/python3.9/site-packages/hyperopt/base.py\", line 671, in fmin\\nreturn fmin(\\nFile \"~/Library/Caches/pypoetry/virtualenvs/mlflow-intro-SyTqwt0D-py3.9/lib/python3.9/site-packages/hyperopt/fmin.py\", line 586, in fmin\\nrval.exhaust()\\nFile \"~/Library/Caches/pypoetry/virtualenvs/mlflow-intro-SyTqwt0D-py3.9/lib/python3.9/site-packages/hyperopt/fmin.py\", line 364, in exhaust\\nself.run(self.max_evals - n_done, block_until_done=self.asynchronous)\\nTypeError: unsupported operand type(s) for -: \\'str\\' and \\'int\\'\\nSolution:\\nThe --max_evals argument in hpo.py has no defined datatype and will therefore implicitly be treated as string. It should be an integer, so that the script can work correctly. Add type=int to the argument definition:\\nparser.add_argument(\\n\"--max_evals\",\\ntype=int,\\ndefault=50,\\nhelp=\"the number of parameter evaluations for the optimizer to explore.\"\\n)', 'section': 'Module 2: Experiment tracking', 'question': 'Unsupported Operand Type Error in hpo.py', 'course': 'mlops-zoomcamp'}, {'text': 'Getting the following warning when running mlflow.sklearn:\\n\\n2022/05/28 04:36:36 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of sklearn. If you encounter errors during autologging, try upgrading / downgrading sklearn to a supported version, or try upgrading MLflow. […]\\nSolution: use 0.22.1 <= scikit-learn <= 1.1.0\\nReference: https://www.mlflow.org/docs/latest/python_api/mlflow.sklearn.html', 'section': 'Module 2: Experiment tracking', 'question': 'Unsupported Scikit-Learn version', 'course': 'mlops-zoomcamp'}, {'text': 'Problem: CLI commands (mlflow experiments list) do not return experiments\\nSolution description: need to set environment variable for the Tracking URI:\\n$ export MLFLOW_TRACKING_URI=http://127.0.0.1:5000\\nAdded and Answered by Dino Vitale', 'section': 'Module 2: Experiment tracking', 'question': 'Mlflow CLI does not return experiments', 'course': 'mlops-zoomcamp'}, {'text': 'Problem: After starting the tracking server, when we try to use the mlflow cli commands as listed here, most of them can’t seem to find the experiments that have been run with the tracking server\\nSolution: We need to set the environment variable MLFLOW_TRACKING_URI to the URI of the sqlite database. This is something like “export MLFLOW_TRACKING_URI=sqlite:///{path to sqlite database}” . After this, we can view the experiments from the command line using commands like “mlflow experiments search”\\nEven after this commands like “mlflow gc” doesn’t seem to get the tracking uri, and they have to be passed explicitly as an argument every time the command is run.\\nAhmed Fahim (afahim03@yahoo.com)', 'section': 'Module 2: Experiment tracking', 'question': 'Viewing MLflow Experiments using MLflow CLI', 'course': 'mlops-zoomcamp'}, {'text': 'All the experiment and other tracking information in mlflow are stored in sqllite database provided while initiating the mlflow ui command. This database can be inspected using Pycharm’s Database tab by using the SQLLite database type. Once the connection is created as below, the tables can be queried and inspected using regular SQL. The same applies for any SQL backed database such as postgres as well.\\nThis is very useful to understand the entity structure of the data being stored within mlflow and useful for any kind of systematic archiving of model tracking for longer periods.\\nAdded by Senthilkumar Gopal', 'section': 'Module 2: Experiment tracking', 'question': 'Viewing SQLlite Data Raw & Deleting Experiments Manually', 'course': 'mlops-zoomcamp'}, {'text': 'Solution : It is another way to start it for remote hosting a mlflow server. For example, if you are multiple colleagues working together on something you most likely would not run mlflow on one laptop but rather everyone would connect to the same server running mlflow\\nAnswer by Christoffer Added by Akshit Miglani (akshit.miglani09@gmail.com)', 'section': 'Module 2: Experiment tracking', 'question': 'What does launching the tracking server locally mean?', 'course': 'mlops-zoomcamp'}, {'text': 'Problem: parameter was not recognized during the model registry\\nSolution: parameters should be added in previous to the model registry. The parameters can be added by mlflow.log_params(params) so that the dictionary can be directly appended to the data.run.params.\\nAdded and Answered by Sam Lim', 'section': 'Module 2: Experiment tracking', 'question': 'Parameter adding in case of max_depth not recognized', 'course': 'mlops-zoomcamp'}, {'text': 'Problem: Max_depth is not recognize even when I add the mlflow.log_params\\nSolution: the mlflow.log_params(params) should be added to the hpo.py script, but if you run it it will append the new model to the previous run that doesn’t contain the parameters, you should either remove the previous experiment or change it\\nPastor Soto', 'section': 'Module 2: Experiment tracking', 'question': 'Max_depth is not recognize even when I add the mlflow.log_params', 'course': 'mlops-zoomcamp'}, {'text': \"Problem: About week_2 homework: The register_model.py  script, when I copy it into a jupyter notebook fails and spits out the following error. AttributeError: 'tuple' object has no attribute 'tb_frame'\\nSolution: remove click decorators\", 'section': 'Module 2: Experiment tracking', 'question': \"AttributeError: 'tuple' object has no attribute 'tb_frame'\", 'course': 'mlops-zoomcamp'}, {'text': 'Problem: when running the preprocess_data.py file you get the following error:\\n\\nwandb: ERROR api_key not configured (no-tty). call wandb.login(key=[your_api_key])\\nSolution: Go to your WandB profile (top RHS) → user settings → scroll down to “Danger Zone” and copy your API key. \\n\\nThen before running preprocess_data.py, add and run the following cell in your notebook:\\n\\n%%bash\\n\\nWandb login <YOUR_API_KEY_HERE>.\\nAdded and Answered by James Gammerman (jgammerman@gmail.com)', 'section': 'Module 2: Experiment tracking', 'question': 'WandB API error', 'course': 'mlops-zoomcamp'}, {'text': 'Please make sure you following the order below nd enabling the autologging before constructing the dataset. If you still have this issue check that your data is in format compatible with XGBoost.\\n# Enable MLflow autologging for XGBoost\\nmlflow.xgboost.autolog()\\n# Construct your dataset\\nX_train, y_train = ...\\n# Train your XGBoost model\\nmodel = xgb.XGBRegressor(...)\\nmodel.fit(X_train, y_train)\\nAdded by Olga Rudakova', 'section': 'Module 2: Experiment tracking', 'question': 'WARNING mlflow.xgboost: Failed to infer model signature: could not sample data to infer model signature: please ensure that autologging is enabled before constructing the dataset.', 'course': 'mlops-zoomcamp'}, {'text': 'Problem\\nUsing wget command to download either data or python scripts on Windows, I am using the notebook provided by Visual Studio and despite having a python virtual env, it did not recognize the pip command.\\nSolution: Use python -m pip, this same for any other command. Ie. python -m wget\\nAdded by Erick Calderin', 'section': 'Module 2: Experiment tracking', 'question': 'wget not working', 'course': 'mlops-zoomcamp'}, {'text': \"Problem: Open/run github notebook(.ipynb) directly in Google Colab\\nSolution: Change the domain from 'github.com' to 'githubtocolab.com'. The notebook will open in Google Colab.\\nOnly works with Public repo.\\nAdded by Ming Jun\\nNavigating in Wandb UI became difficult to me, I had to intuit some options until I found the correct one.\\nSolution: Go to the official doc.\\nAdded by Erick Calderin\", 'section': 'Module 2: Experiment tracking', 'question': 'Open/run github notebook(.ipynb) directly in Google Colab', 'course': 'mlops-zoomcamp'}, {'text': 'Problem: Someone asked why we are using this type of split approach instead of just a random split.\\nSolution: For example, I have some models at work that train on Jan 1 2020 — Aug 1 2021 time period, and then test on Aug 1 - Dec 31 2021, and finally validate on Jan - March or something\\nWe do these “out of time”  validations to do a few things:\\nCheck for seasonality of our data\\nWe know if the RMSE for Test is 5 say, and then RMSE for validation is 20, then there’s serious seasonality to the data we are looking at, and now we might change to Time Series approaches\\nIf I’m predicting on Mar 30 2023 the outcomes for the next 3 months, the “random sample” in our train/test would have caused data leakage, overfitting, and poor model performance in production. We mustn’t take information about the future and apply it to the present when we are predicting in a model context.\\nThese are two of, I think, the biggest points for why we are doing jan/feb/march. I wouldn’t do it any other way.\\nTrain: Jan\\nTest: Feb\\nValidate: March\\nThe point of validation is to report out model metrics to leadership, regulators, auditors, and record the models performance to then later analyze target drift\\nAdded by Sam LaFell\\nProblem: If you get an error while trying to run the mlflow server on AWS CLI with S3 bucket and POSTGRES database:\\nReproducible Command:\\nmlflow server -h 0.0.0.0 -p 5000 --backend-store-uri postgresql://<DB_USERNAME>:<DB_PASSWORD>@<DB_ENDPOINT>:<DB_PORT>/<DB_NAME> --default-artifact-root s3://<BUCKET_NAME>\\nError:\\n\"urllib3 v2.0 only supports OpenSSL 1.1.1+, currently \"\\nImportError: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the \\'ssl\\' module is compiled with \\'OpenSSL 1.0.2k-fips  26 Jan 2017\\'. See: https://github.com/urllib3/urllib3/issues/2168\\nSolution: Upgrade mlflow using\\nCode: pip3 install --upgrade mlflow\\nResolution: It downgrades urllib3 2.0.3 to 1.26.16 which is compatible with mlflow and ssl 1.0.2\\nInstalling collected packages: urllib3\\nAttempting uninstall: urllib3\\nFound existing installation: urllib3 2.0.3\\nUninstalling urllib3-2.0.3:\\nSuccessfully uninstalled urllib3-2.0.3\\nSuccessfully installed urllib3-1.26.16\\nAdded by Sarvesh Thakur', 'section': 'Module 3: Orchestration', 'question': 'Why do we use Jan/Feb/March for Train/Test/Validation Purposes?', 'course': 'mlops-zoomcamp'}, {'text': 'Problem description\\nSolution description\\n(optional) Added by Name', 'section': 'Module 3: Orchestration', 'question': 'Problem title', 'course': 'mlops-zoomcamp'}, {'text': 'Here', 'section': 'Module 4: Deployment', 'question': 'Where is the FAQ for Prefect questions?', 'course': 'mlops-zoomcamp'}, {'text': 'Windows with AWS CLI already installed\\nAWS CLI version:\\naws-cli/2.4.24 Python/3.8.8 Windows/10 exe/AMD64 prompt/off\\nExecuting\\n$(aws ecr get-login --no-include-email)\\nshows error\\naws.exe: error: argument operation: Invalid choice, valid choices are…\\nUse this command instead. More info here:\\nhttps://docs.aws.amazon.com/cli/latest/reference/ecr/get-login-password.html\\naws ecr get-login-password \\\\\\n--region <region> \\\\\\n| docker login \\\\\\n--username AWS \\\\\\n--password-stdin <aws_account_id>.dkr.ecr.<region>.amazonaws.com\\nAdded by MarcosMJD', 'section': 'Module 4: Deployment', 'question': 'aws.exe: error: argument operation: Invalid choice — Docker can not login to ECR.', 'course': 'mlops-zoomcamp'}, {'text': 'Use ` at the end of each line except the last. Note that multiline string does not need `.\\nEscape “ to “\\\\ .\\nUse $env: to create env vars (non-persistent). E.g.:\\n$env:KINESIS_STREAM_INPUT=\"ride_events\"\\naws kinesis put-record --cli-binary-format raw-in-base64-out `\\n--stream-name $env:KINESIS_STREAM_INPUT `\\n--partition-key 1 `\\n--data \\'{\\n\\\\\"ride\\\\\": {\\n\\\\\"PULocationID\\\\\": 130,\\n\\\\\"DOLocationID\\\\\": 205,\\n\\\\\"trip_distance\\\\\": 3.66\\n},\\n\\\\\"ride_id\\\\\": 156\\n}\\'\\nAdded by MarcosMJD', 'section': 'Module 4: Deployment', 'question': 'Multiline commands in Windows Powershell', 'course': 'mlops-zoomcamp'}, {'text': \"If one gets pipenv failures for pipenv install command -\\nAttributeError: module 'collections' has no attribute 'MutableMapping'\\nIt happens because you use the system Python (3.10) for pipenv.\\nIf you previously installed pipenv with apt-get, remove it - sudo-apt remove pipenv\\nMake sure you have a non-system Python installed in your environment. The easiest way to do it is to install anaconda or miniconda\\nNext, install pipenv to your non-system Python. If you use the setup from the lectures, it’s just this: pip install pipenv\\nNow re-run pipenv install XXXX (relevant dependencies) - should work\\nTested and worked on AWS instance, similar to the config Alexey presented in class.\\nAdded by Daniel HenSSL\", 'section': 'Module 4: Deployment', 'question': \"Pipenv installation not working (AttributeError: module 'collections' has no attribute 'MutableMapping')\", 'course': 'mlops-zoomcamp'}, {'text': 'First check if SSL module configured with following command:\\nPython -m ssl\\n\\nIf the output of this is empty there is no problem with SSL configuration.\\n\\nThen you should upgrade your pipenv package in your current environment to resolve the problem.\\nAdded by Kenan Arslanbay', 'section': 'Module 4: Deployment', 'question': \"module is not available (Can't connect to HTTPS URL)\", 'course': 'mlops-zoomcamp'}, {'text': \"During scikit-learn installation via the command:\\npipenv install scikit-learn==1.0.2\\nThe following error is raised:\\nModuleNotFoundError: No module named 'pip._vendor.six'\\nThen, one should:\\nsudo apt install python-six\\npipenv --rm\\npipenv install scikit-learn==1.0.2\\nAdded by Giovanni Pecoraro\", 'section': 'Module 4: Deployment', 'question': \"No module named 'pip._vendor.six'\", 'course': 'mlops-zoomcamp'}, {'text': 'Problem description. How can we use Jupyter notebooks with the Pipenv environment?\\nSolution: Refer to this stackoverflow question. Basically install jupyter and ipykernel using pipenv. And then register the kernel with `python -m ipykernel install --user --name=my-virtualenv-name` inside the Pipenv shell. If you are using Jupyter notebooks in VS Code, doing this will also add the virtual environment in the list of kernels.\\nAdded by Ron Medina', 'section': 'Module 4: Deployment', 'question': 'Pipenv with Jupyter', 'course': 'mlops-zoomcamp'}, {'text': \"Problem: I tried to run starter notebook on pipenv environment but had issues with no output on prints. \\nI used scikit-learn==1.2.2 and python==3.10\\nTornado version was 6.3.2\\n\\nSolution: The error you're encountering seems to be a bug related to Tornado, which is a Python web server and networking library. It's used by Jupyter under the hood to handle networking tasks.\\nDowngrading to tornado==6.1 fixed the issue\\nhttps://stackoverflow.com/questions/54971836/no-output-jupyter-notebook\", 'section': 'Module 4: Deployment', 'question': 'Pipenv with Jupyter no output', 'course': 'mlops-zoomcamp'}, {'text': 'Problem description:  You might get an error ‘Invalid base64’ after running the ‘aws kinesis put-record’ command on your local machine. This might be the case if you are using the AWS CLI version 2 (note that in the video 4.4, around 57:42, you can see a warning since the instructor is using v1 of the CLI.\\nSolution description: To get around this, pass the argument ‘--cli-binary-format raw-in-base64-out’. This will encode your data string into base64 before passing it to kinesis\\nAdded by M', 'section': 'Module 4: Deployment', 'question': '‘Invalid base64’ error after running `aws kinesis put-record`', 'course': 'mlops-zoomcamp'}, {'text': 'Problem description:   Running starter.ipynb in homework’s Q1 will show up this error.\\nSolution description: Update pandas (actually pandas version was the latest, but several dependencies are updated).\\nAdded by Marcos Jimenez', 'section': 'Module 4: Deployment', 'question': 'Error index 311297 is out of bounds for axis 0 with size 131483 when loading parquet file.', 'course': 'mlops-zoomcamp'}, {'text': 'Use command $pipenv lock to force the creation of Pipfile.lock\\nAdded by Bijay P.', 'section': 'Module 4: Deployment', 'question': 'Pipfile.lock was not created along with Pipfile', 'course': 'mlops-zoomcamp'}, {'text': 'This issue is usually due to the pythonfinder module in pipenv.\\nThe solution to this involves manually changing the scripts as describe here python_finder_fix\\nAdded by Ridwan Amure', 'section': 'Module 4: Deployment', 'question': 'Permission Denied using Pipenv', 'course': 'mlops-zoomcamp'}, {'text': 'When passing arguments to a script via command line and converting it to a 4 digit number using f’{year:04d}’, this error showed up.\\nThis happens because all inputs from the command line are read as string by the script. They need to be converted to numeric/integer before transformation in fstring.\\nyear = int(sys.argv[1])\\nf’{year:04d}’\\nIf you use click library just edit a decorator\\n@click.command()\\n@click.option( \"--year\",  help=\"Year for evaluation\",   type=int)\\ndef  your_function(year):\\n<<Your code>>\\nAdded by Taras Sh', 'section': 'Module 4: Deployment', 'question': \"Error while parsing arguments via CLI  [ValueError: Unknown format code 'd' for object of type 'str']\", 'course': 'mlops-zoomcamp'}, {'text': 'Ensure the correct image is being used to derive from.\\nCopy the data from local to the docker image using the COPY command to a relative path. Using absolute paths within the image might be troublesome.\\nUse paths starting from /app and don’t forget to do WORKDIR /app before actually performing the code execution.\\nMost common commands\\nBuild container using docker build -t mlops-learn .\\nExecute the script using docker run -it --rm mlops-learn\\n<mlops-learn> is just a name used for the image and does not have any significance.', 'section': 'Module 4: Deployment', 'question': 'Dockerizing tips', 'course': 'mlops-zoomcamp'}, {'text': 'If you are trying to run Flask gunicorn & MLFlow server from the same container, defining both in Dockerfile with CMD will only run MLFlow & not Flask.\\nSolution: Create separate shell script with server run commands, for eg:\\n> \\tscript1.sh\\n#!/bin/bash\\ngunicorn --bind=0.0.0.0:9696 predict:app\\nAnother script with e.g. MLFlow server:\\n>\\tscript2.sh\\n#!/bin/bash\\nmlflow server -h 0.0.0.0 -p 5000 --backend-store-uri=sqlite:///mlflow.db --default-artifact-root=g3://zc-bucket/mlruns/\\nCreate a wrapper script to run above 2 scripts:\\n>\\twrapper_script.sh\\n#!/bin/bash\\n# Start the first process\\n./script1.sh &\\n# Start the second process\\n./script2.sh &\\n# Wait for any process to exit\\nwait -n\\n# Exit with status of process that exited first\\nexit $?\\nGive executable permissions to all scripts:\\nchmod +x *.sh\\nNow we can define last line of Dockerfile as:\\n> \\tDockerfile\\nCMD ./wrapper_script.sh\\nDont forget to expose all ports defined by services!', 'section': 'Module 4: Deployment', 'question': 'Running multiple services in a Docker container', 'course': 'mlops-zoomcamp'}, {'text': 'Problem description cannot generate pipfile.lock raise InstallationError( pip9.exceptions.InstallationError: Command \"python setup.py egg_info\" failed with error code 1\\nSolution: you need to force and upgrade wheel and pipenv\\nJust run the command line :\\npip install --user --upgrade --upgrade-strategy eager pipenv wheel', 'section': 'Module 4: Deployment', 'question': 'Cannot generate pipfile.lock raise InstallationError( pip9.exceptions.InstallationError)', 'course': 'mlops-zoomcamp'}, {'text': \"Problem description. How can we connect s3 bucket to MLFLOW?\\nSolution: Use boto3 and AWS CLI to store access keys. The access keys are what will be used by boto3 (AWS' Python API tool) to connect with the AWS servers. If there are no Access Keys how can they make sure that they have the right to access this Bucket? Maybe you're a malicious actor (Hacker for ex). The keys must be present for boto3 to talk to the AWS servers and they will provide access to the Bucket if you possess the right permissions. You can always set the Bucket as public so anyone can access it, now you don't need access keys because AWS won't care.\\nRead more here: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html\\nAdded by Akshit Miglani\", 'section': 'Module 4: Deployment', 'question': 'Connecting s3 bucket to MLFLOW', 'course': 'mlops-zoomcamp'}, {'text': 'Even though the upload works using aws cli and boto3 in Jupyter notebook.\\nSolution set the AWS_PROFILE environment variable (the default profile is called default)', 'section': 'Module 4: Deployment', 'question': 'Uploading to s3 fails with An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The AWS Access Key Id you provided does not exist in our records.\"', 'course': 'mlops-zoomcamp'}, {'text': 'Problem description: lib_lightgbm.so Reason: image not found\\nSolution description: Add “RUN apt-get install libgomp1” to your docker. (change installer command based on OS)\\nAdded by Kazeem Hakeem', 'section': 'Module 4: Deployment', 'question': 'Dockerizing lightgbm', 'course': 'mlops-zoomcamp'}, {'text': 'When the request is processed in lambda function, mlflow library raises:\\n2022/09/19 21:18:47 WARNING mlflow.pyfunc: Encountered an unexpected error (AttributeError(\"module \\'dataclasses\\' has no attribute \\'__version__\\'\")) while detecting model dependency mismatches. Set logging level to DEBUG to see the full traceback.\\nSolution: Increase the memory of the lambda function.\\nAdded by MarcosMJD', 'section': 'Module 4: Deployment', 'question': 'Error raised when executing mlflow’s pyfunc.load_model in lambda function.', 'course': 'mlops-zoomcamp'}, {'text': 'Just a note if you are following the video but also using the repo’s notebook The notebook is the end state of the video which eventually uses mlflow pipelines.\\nJust watch the video and be patient. Everything will work :)\\nAdded by Quinn Avila', 'section': 'Module 4: Deployment', 'question': '4.3 FYI Notebook is end state of Video -', 'course': 'mlops-zoomcamp'}, {'text': 'Problem description: I was having issues because my python script was not reading AWS credentials from env vars, after building the image I was running it like this:\\ndocker run -it homework-04 -e AWS_ACCESS_KEY_ID=xxxxxxxx -e AWS_SECRET_ACCESS_KEY=xxxxxx\\nSolution 1:\\n\\nEnvironment Variables: \\nYou can set the AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and AWS_SESSION_TOKEN (if you are using AWS STS) environment variables. You can set these in your shell, or you can include them in your Docker run command like this:\\nI found out by myself that those variables must be passed before specifying the name of the image, as follow:\\ndocker run -e AWS_ACCESS_KEY_ID=xxxxxxxx -e AWS_SECRET_ACCESS_KEY=xxxxxx -it homework-04\\nAdded by Erick Cal\\nSolution 2 (if AWS credentials were not found):\\nAWS Configuration Files: \\nThe AWS SDKs and CLI will check the ~/.aws/credentials and ~/.aws/config files for credentials if they exist. You can map these files into your Docker container using volumes:\\n\\ndocker run -it --rm -v ~/.aws:/root/.aws homework:v1', 'section': 'Module 4: Deployment', 'question': 'Passing envs to my docker image', 'course': 'mlops-zoomcamp'}, {'text': 'If anyone is troubleshooting or just interested in seeing the model listed on the image svizor/zoomcamp-model:mlops-3.10.0-slim.\\nCreate a dockerfile. (yep thats all) and build “docker build -t zoomcamp_test .”\\nFROM svizor/zoomcamp-model:mlops-3.10.0-slim\\nRun “docker run -it zoomcamp_test ls /app” output -> model.bin\\nThis will list the contents of the app directory and “model.bin” should output. With this you could just copy your files, for example “copy myfile .” maybe a requirements file and this can be run for example “docker run -it myimage myscript arg1 arg2 ”. Of course keep in mind a build is needed everytime you change the Dockerfile.\\nAnother variation is to have it run when you run the docker file.\\n“””\\nFROM svizor/zoomcamp-model:mlops-3.10.0-slim\\nWORKDIR /app\\nCMD ls\\n“””\\nJust keep in mind CMD is needed because the RUN commands are used for building the image and the CMD is used at container runtime. And in your example you probably want to run a script or should we say CMD a script.\\nQuinn Avila', 'section': 'Module 4: Deployment', 'question': 'How to see the model in the docker container in app/?', 'course': 'mlops-zoomcamp'}, {'text': 'To resolve this make sure to build the docker image with the platform tag, like this:\\n“docker build -t homework:v1 --platform=linux/arm64 .”', 'section': 'Module 4: Deployment', 'question': \"WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\", 'course': 'mlops-zoomcamp'}, {'text': \"Solution: instead of input_file = f'https://s3.amazonaws.com/nyc-tlc/trip+data/{taxi_type}_tripdata_{year:04d}-{month:02d}.parquet'  use input_file = f'https://d37ci6vzurychx.cloudfront.net/trip-data/{taxi_type}_tripdata_{year:04d}-{month:02d}.parquet'\\nIlnaz Salimov\\nsalimovilnaz777@gmail.com\", 'section': 'Module 4: Deployment', 'question': 'HTTPError: HTTP Error 403: Forbidden when call apply_model() in score.ipynb', 'course': 'mlops-zoomcamp'}, {'text': 'i\\'m getting this error ModuleNotFoundError: No module named \\'pipenv.patched.pip._vendor.urllib3.response\\'\\nand Resolved from this command pip install pipenv --force-reinstall\\ngetting this errror site-packages\\\\pipenv\\\\patched\\\\pip\\\\_vendor\\\\urllib3\\\\connectionpool.py\"\\nResolved from this command pip install -U pip and pip install requests\\nAsif', 'section': 'Module 5: Monitoring', 'question': \"ModuleNotFoundError: No module named 'pipenv.patched.pip._vendor.urllib3.response'\", 'course': 'mlops-zoomcamp'}, {'text': 'Problem description: When running docker-compose up as shown in the video 5.2 if you go to http://localhost:3000/ you get asked for a username and a password.\\nSolution: for both of them the default is “admin”. Then you can enter your new password. \\nSee also here\\nAdded by JaimeRV', 'section': 'Module 5: Monitoring', 'question': 'Login window in Grafana', 'course': 'mlops-zoomcamp'}, {'text': 'Problem Description : In Linux, when starting services using docker compose up --build  as shown in video 5.2, the services won’t start and instead we get message unknown flag: --build in command prompt.\\nSolution : Since we install docker-compose separately in Linux, we have to run docker-compose up --build instead of docker compose up --build\\nAdded by Ashish Lalchandani', 'section': 'Module 5: Monitoring', 'question': 'Error in starting monitoring services in Linux', 'course': 'mlops-zoomcamp'}, {'text': 'Problem: When running prepare.py getting KeyError: ‘content-length’\\nSolution: From Emeli Dral:\\nIt seems to me that the link we used in prepare.py to download taxi data does not work anymore. I substituted the instruction:\\nurl = f\"https://nyc-tlc.s3.amazonaws.com/trip+data/{file}\\nby the\\nurl = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/{file}\"\\nin the prepare.py and it worked for me. Hopefully, if you do the same you will be able to get those data.', 'section': 'Module 5: Monitoring', 'question': 'KeyError ‘content-length’ when running prepare.py', 'course': 'mlops-zoomcamp'}, {'text': 'Problem description\\nWhen I run the command “docker-compose up –build” and send the data to the real-time prediction service. The service will return “Max retries exceeded with url: /api”.\\nIn my case it because of my evidently service exit with code 2 due to the “app.py” in evidently service cannot import “from pyarrow import parquet as pq”.\\nSolution description\\nThe first solution is just install the pyarrow module “pip install pyarrow”\\nThe second solution is restart your machine.\\nThe third solution is if the first and second one didn’t work with your machine. I found that “app.py” of evidently service didn’t use that module. So comment the pyarrow module out and the problem was solved for me.\\nAdded by Surawut Jirasaktavee', 'section': 'Module 5: Monitoring', 'question': 'Evidently service exit with code 2', 'course': 'mlops-zoomcamp'}, {'text': 'When using evidently if you get this error.\\nYou probably forgot to and parentheses () just and opening and closing and you are good to go.\\nQuinn Avila', 'section': 'Module 5: Monitoring', 'question': 'ValueError: Incorrect item instead of a metric or metric preset was passed to Report', 'course': 'mlops-zoomcamp'}, {'text': 'You will get an error if you didn’t add a target=’duration_min’\\nIf you want to use RegressionQualityMetric() you need a target=’duration_min and you need this added to you current_data[‘duration_min’]\\nQuinn Avila', 'section': 'Module 5: Monitoring', 'question': 'For the report RegressionQualityMetric()', 'course': 'mlops-zoomcamp'}, {'text': 'Problem description\\nValueError: Found array with 0 sample(s) (shape=(0, 6)) while a minimum of 1 is required by LinearRegression.\\nSolution description\\nThis happens because the generated data is based on an early date therefore the training dataset would be empty.\\nAdjust the following\\nbegin = datetime.datetime(202X, X, X, 0, 0)\\nAdded by Luke', 'section': 'Module 5: Monitoring', 'question': 'Found array with 0 sample(s)', 'course': 'mlops-zoomcamp'}, {'text': 'Problem description\\nGetting “target columns” “prediction columns” not present errors after adding a metric\\nSolution description\\nMake sure to read through the documentation on what is required or optional when adding the metric. I added DatasetCorrelationsMetric which doesn’t require any parameters because the metric evaluates for correlations among the features.\\nSam Lim', 'section': 'Module 5: Monitoring', 'question': 'Adding additional metric', 'course': 'mlops-zoomcamp'}, {'text': 'When you try to login in Grafana with standard requisites (admin/admin) it throw up an error.\\nAfter run grafana-cli admin reset-admin-password admin in Grafana container the problem will be fixed\\nAdded by Artem Glazkov', 'section': 'Module 5: Monitoring', 'question': 'Standard login in Grafana does not work', 'course': 'mlops-zoomcamp'}, {'text': 'Problem description. While my metric generation script was still running, I noticed that the charts in Grafana don’t get updated.\\nSolution description. There are two things to pay attention to:\\nRefresh interval: set it to a small value: 5-10-30 seconds\\nUse your local timezone in a call to `pytz.timezone` – I couldn’t get updates before changing this from the original value “Europe/London” to my own zone', 'section': 'Module 5: Monitoring', 'question': 'The chart in Grafana doesn’t get updates', 'course': 'mlops-zoomcamp'}, {'text': 'Problem description. Prefect server was not running locally, I ran `prefect server start` command but it stopped immediately..\\nSolution description. I used Prefect cloud to run the script, however I created an issue on the Prefect github.\\nBy Erick Calderin', 'section': 'Module 5: Monitoring', 'question': 'Prefect server was not running locally', 'course': 'mlops-zoomcamp'}, {'text': 'Solution. Using docker CLI run docker system prune to remove unused things (build cache, containers, images etc)\\nAlso, to see what’s taking space before pruning you can run docker system df\\nBy Alex Litvinov', 'section': 'Module 5: Monitoring', 'question': 'no disk space left error when doing docker compose up', 'course': 'mlops-zoomcamp'}, {'text': 'Problem: when run docker-compose up –build, you may see this error. To solve, add `command: php -S 0.0.0.0:8080 -t /var/www/html` in adminer block in yml file like:\\nadminer:\\ncommand: php -S 0.0.0.0:8080 -t /var/www/html\\nimage: adminer\\n…\\nIlnaz Salimov\\nsalimovilnaz777@gmail.com', 'section': 'Module 5: Monitoring', 'question': 'Failed to listen on :::8080 (reason: php_network_getaddresses: getaddrinfo failed: Address family for hostname not supported)', 'course': 'mlops-zoomcamp'}, {'text': 'Problem: Can we generate charts like Evidently inside Grafana?\\nSolution: In Grafana that would be a stat panel (just a number) and scatter plot panel (I believe it requires a plug-in). However, there is no native way to quickly recreate this exact Evidently dashboard. You\\'d need to make sure you have all the relevant information logged to your Grafana data source, and then design your own plots in Grafana.\\nIf you want to recreate the Evidently visualizations externally, you can export the Evidently output in JSON with include_render=True\\n(more details here https://docs.evidentlyai.com/user-guide/customization/json-dict-output) and then parse information from it for your external visualization layer. To include everything you need for non-aggregated visuals, you should also add \"raw_data\": True  option (more details here https://docs.evidentlyai.com/user-guide/customization/report-data-aggregation).\\nOverall, this specific plot with under- and over-performance segments is more useful during debugging, so might be easier to access it ad hoc using Evidently.\\nAdded by Ming Jun, Asked by Luke, Answered by Elena Samuylova', 'section': 'Module 6: Best practices', 'question': 'Generate Evidently Chart in Grafana', 'course': 'mlops-zoomcamp'}, {'text': \"You may get an error ‘{'errorMessage': 'Unable to locate credentials', …’ from the print statement in test_docker.py after running localstack with kinesis.\\nTo fix this, in the docker-compose.yaml file, in addition to the environment variables like AWS_DEFAULT_REGION, add two other variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY. Their value is not important; anything like abc will suffice\\nAdded by M\\nOther possibility is just to run\\naws --endpoint-url http://localhost:4566 configure\\nAnd providing random values for AWS Access Key ID , AWS Secret Access Key, Default region name, and Default output format.\\nAdded by M.A. Monjas\", 'section': 'Module 6: Best practices', 'question': 'Get an error ‘Unable to locate credentials’ after running localstack with kinesis', 'course': 'mlops-zoomcamp'}, {'text': \"You may get an error while creating a bucket with localstack and the boto3 client:\\nbotocore.exceptions.ClientError: An error occurred (IllegalLocationConstraintException) when calling the CreateBucket operation: The unspecified location constraint is incompatible for the region specific endpoint this request was sent to.\\nTo fix this, instead of creating a bucket via\\ns3_client.create_bucket(Bucket='nyc-duration')\\nCreate it with\\ns3_client.create_bucket(Bucket='nyc-duration', CreateBucketConfiguration={\\n'LocationConstraint': AWS_DEFAULT_REGION})\\nyam\\nAdded by M\", 'section': 'Module 6: Best practices', 'question': 'Get an error ‘ unspecified location constraint is incompatible ’', 'course': 'mlops-zoomcamp'}, {'text': 'When executing an AWS CLI command (e.g., aws s3 ls), you can get the error <botocore.awsrequest.AWSRequest object at 0x7fbaf2666280>.\\nTo fix it, simply set the AWS CLI environment variables:\\nexport AWS_DEFAULT_REGION=eu-west-1\\nexport AWS_ACCESS_KEY_ID=foobar\\nexport AWS_SECRET_ACCESS_KEY=foobar\\nTheir value is not important; anything would be ok.\\nAdded by Giovanni Pecoraro', 'section': 'Module 6: Best practices', 'question': 'Get an error “<botocore.awsrequest.AWSRequest object at 0x7fbaf2666280>” after running an AWS CLI command', 'course': 'mlops-zoomcamp'}, {'text': 'At every commit the above error is thrown and no pre-commit hooks are ran.\\nMake sure the indentation in .pre-commit-config.yaml is correct. Especially the 4 spaces ahead of every `repo` statement\\nAdded by M. Ayoub C.', 'section': 'Module 6: Best practices', 'question': 'Pre-commit triggers an error at every commit: “mapping values are not allowed in this context”', 'course': 'mlops-zoomcamp'}, {'text': 'No option to remove pytest test\\nRemove .vscode folder located on the folder you previously used for testing, e.g. folder code (from week6-best-practices) was chosen to test, so you may remove .vscode inside the folder.\\nAdded by Rizdi Aprilian', 'section': 'Module 6: Best practices', 'question': 'Could not reconfigure pytest from zero after getting done with previous folder', 'course': 'mlops-zoomcamp'}, {'text': 'Problem description\\nFollowing video 6.3, at minute 11:23, get records command returns empty Records.\\nSolution description\\nAdd --no-sign-request to Kinesis get records call:\\n aws --endpoint-url=http://localhost:4566 kinesis get-records --shard-iterator […] --no-sign-request', 'section': 'Module 6: Best practices', 'question': 'Empty Records in Kinesis Get Records with LocalStack', 'course': 'mlops-zoomcamp'}, {'text': \"Problem description\\ngit commit -m 'Updated xxxxxx'\\nAn error has occurred: InvalidConfigError:\\n==> File .pre-commit-config.yaml\\n=====> 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\\nSolution description\\nSet uft-8 encoding when creating the pre-commit yaml file:\\npre-commit sample-config | out-file .pre-commit-config.yaml -encoding utf8\\nAdded by MarcosMJD\", 'section': 'Module 6: Best practices', 'question': 'In Powershell, Git commit raises utf-8 encoding error after creating pre-commit yaml file', 'course': 'mlops-zoomcamp'}, {'text': \"Problem description\\ngit commit -m 'Updated xxxxxx'\\n[INFO] Initializing environment for https://github.com/pre-commit/pre-commit-hooks.\\n[INFO] Installing environment for https://github.com/pre-commit/pre-commit-hooks.\\n[INFO] Once installed this environment will be reused.\\nAn unexpected error has occurred: CalledProcessError: command:\\n…\\nreturn code: 1\\nexpected return code: 0\\nstdout:\\nAttributeError: 'PythonInfo' object has no attribute 'version_nodot'\\nSolution description\\nClear app-data of the virtualenv\\npython -m virtualenv api -vvv --reset-app-data\\nAdded by MarcosMJD\", 'section': 'Module 6: Best practices', 'question': \"Git commit with pre-commit hook raises error ‘'PythonInfo' object has no attribute 'version_nodot'\", 'course': 'mlops-zoomcamp'}, {'text': 'Problem description\\nProject structure:\\n/sources/production/model_service.py\\n/sources/tests/unit_tests/test_model_service.py (“from production.model_service import ModelService)\\nWhen running python test_model_service.py from the sources directory, it works.\\nWhen running pytest ./test/unit_tests fails. ‘No module named ‘production’’\\nSolution description\\nUse python -m pytest ./test/unit_tests\\nExplanation: pytest does not add to the sys.path the path where pytest is run.\\nYou can run python -m pytest, or alternatively export PYTHONPATH=. Before executing pytest\\nAdded by MarcosMJD', 'section': 'Module 6: Best practices', 'question': 'Pytest error ‘module not found’ when if using custom packages in the source code', 'course': 'mlops-zoomcamp'}, {'text': 'Problem description\\nProject structure:\\n/sources/production/model_service.py\\n/sources/tests/unit_tests/test_model_service.py (“from production.model_service import ModelService)\\ngit commit -t ‘test’ raises ‘No module named ‘production’’ when calling pytest hook\\n- repo: local\\nhooks:\\n- id: pytest-check\\nname: pytest-check\\nentry: pytest\\nlanguage: system\\npass_filenames: false\\nalways_run: true\\nargs: [\\n\"tests/\"\\n]\\nSolution description\\nUse this hook instead:\\n- repo: local\\nhooks:\\n- id: pytest-check\\nname: pytest-check\\nentry: \"./sources/tests/unit_tests/run.sh\"\\nlanguage: system\\ntypes: [python]\\npass_filenames: false\\nalways_run: true\\nAnd make sure that run.sh sets the right directory and run pytest:\\ncd \"$(dirname \"$0\")\"\\ncd ../..\\nexport PYTHONPATH=.\\npipenv run pytest ./tests/unit_tests\\nAdded by MarcosMJD', 'section': 'Module 6: Best practices', 'question': 'Pytest error ‘module not found’ when using pre-commit hooks if using custom packages in the source code', 'course': 'mlops-zoomcamp'}, {'text': 'Problem description\\nThis is the step in the ci yml file definition:\\n- name: Run Unit Tests\\nworking-directory: \"sources\"\\nrun: ./tests/unit_tests/run.sh\\nWhen executing github ci action, error raises:\\n…/tests/unit_test/run.sh Permission error\\nError: Process completed with error code 126\\nSolution description\\nAdd execution  permission to the script and commit+push:\\ngit update-index --chmod=+x .\\\\sources\\\\tests\\\\unit_tests\\\\run.sh\\nAdded by MarcosMJD', 'section': 'Module 6: Best practices', 'question': 'Github actions: Permission denied error when executing script file', 'course': 'mlops-zoomcamp'}, {'text': 'Problem description\\nWhen a docker-compose file contains a lot of containers, running the containers may take too much resource. There is a need to easily select only a group of containers while ignoring irrelevant containers during testing.\\nSolution description\\nAdd profiles: [“profile_name”] in the service definition.\\nWhen starting up the service, add `--profile profile_name` in the command.\\nAdded by Ammar Chalifah', 'section': 'Module 6: Best practices', 'question': 'Managing Multiple Docker Containers with docker-compose profile', 'course': 'mlops-zoomcamp'}, {'text': 'Problem description\\nIf you are having problems with the integration tests and kinesis double check that your aws regions match on the docker-compose and local config. Otherwise you will be creating a stream in the wrong region\\nSolution description\\nFor example set ~/.aws/config region = us-east-1 and the docker-compose.yaml - AWS_DEFAULT_REGION=us-east-1\\nAdded by Quinn Avila', 'section': 'Module 6: Best practices', 'question': 'AWS regions need to match docker-compose', 'course': 'mlops-zoomcamp'}, {'text': 'Problem description\\nPre-commit command was failing with isort repo.\\nSolution description\\nSet version to 5.12.0\\nAdded by Erick Calderin', 'section': 'Module 6: Best practices', 'question': 'Isort Pre-commit', 'course': 'mlops-zoomcamp'}, {'text': 'Problem description\\nInfrastructure created in AWS with CD-Deploy Action needs to be destroyed\\nSolution description\\nFrom local:\\nterraform init -backend-config=\"key=mlops-zoomcamp-prod.tfstate\" --reconfigure\\nterraform destroy --var-file vars/prod.tfvars\\nAdded by Erick Calderin', 'section': 'Module 6: Best practices', 'question': 'How to destroy infrastructure created via GitHub Actions', 'course': 'mlops-zoomcamp'}]\n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "\n",
    "docs_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/01-intro/documents.json?raw=1'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)\n",
    "\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9880aca9-ae97-42f5-9aa5-37bb46448841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon’t forget to register in DataTalks.Club's Slack and join the channel.\", 'section': 'General course-related questions', 'question': 'Course - When will the course start?', 'course': 'data-engineering-zoomcamp', 'doc_id': 'bae7a31e6abaddb52b4061dcf238fc61'}, {'text': 'GitHub - DataTalksClub data-engineering-zoomcamp#prerequisites', 'section': 'General course-related questions', 'question': 'Course - What are the prerequisites for this course?', 'course': 'data-engineering-zoomcamp', 'doc_id': '3e5d4959603c68a1e154fa2a6bd9d1e8'}, {'text': \"Yes, even if you don't register, you're still eligible to submit the homeworks.\\nBe aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\", 'section': 'General course-related questions', 'question': 'Course - Can I still join the course after the start date?', 'course': 'data-engineering-zoomcamp', 'doc_id': '60a31bbef930b3d6b127405fcd0b618e'}, {'text': \"You don't need it. You're accepted. You can also just start learning and submitting homework without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.\", 'section': 'General course-related questions', 'question': 'Course - I have registered for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?', 'course': 'data-engineering-zoomcamp', 'doc_id': '386dcf67c83b203e5b424f2ba7489370'}, {'text': 'You can start by installing and setting up all the dependencies and requirements:\\nGoogle cloud account\\nGoogle Cloud SDK\\nPython 3 (installed with Anaconda)\\nTerraform\\nGit\\nLook over the prerequisites and syllabus to see if you are comfortable with these subjects.', 'section': 'General course-related questions', 'question': 'Course - What can I do before the course starts?', 'course': 'data-engineering-zoomcamp', 'doc_id': '6e3550ba00f652ce2fa74706751c4983'}, {'text': \"There are 3 Zoom Camps in a year, as of 2024. However, they are for separate courses:\\nData-Engineering (Jan - Apr)\\nMLOps (May - Aug)\\nMachine Learning (Sep - Jan)\\nThere's only one Data-Engineering Zoomcamp “live” cohort per year, for the certification. Same as for the other Zoomcamps.\\nThey follow pretty much the same schedule for each cohort per zoomcamp. For Data-Engineering it is (generally) from Jan-Apr of the year. If you’re not interested in the Certificate, you can take any zoom camps at any time, at your own pace, out of sync with any “live” cohort.\", 'section': 'General course-related questions', 'question': 'Course - how many Zoomcamps in a year?', 'course': 'data-engineering-zoomcamp', 'doc_id': 'f8323339d264dc9d40d9dad5a34c06b5'}, {'text': 'Yes. For the 2024 edition we are using Mage AI instead of Prefect and re-recorded the terraform videos, For 2023, we used Prefect instead of Airflow..', 'section': 'General course-related questions', 'question': 'Course - Is the current cohort going to be different from the previous cohort?', 'course': 'data-engineering-zoomcamp', 'doc_id': 'd10eed624489c36f17500750ff21c868'}, {'text': 'Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\\nYou can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.', 'section': 'General course-related questions', 'question': 'Course - Can I follow the course after it finishes?', 'course': 'data-engineering-zoomcamp', 'doc_id': 'cb86516adcdcafa29f0758ae6ca28a0b'}, {'text': 'Yes, the slack channel remains open and you can ask questions there. But always sDocker containers exit code w search the channel first and second, check the FAQ (this document), most likely all your questions are already answered here.\\nYou can also tag the bot @ZoomcampQABot to help you conduct the search, but don’t rely on its answers 100%, it is pretty good though.', 'section': 'General course-related questions', 'question': 'Course - Can I get support if I take the course in the self-paced mode?', 'course': 'data-engineering-zoomcamp', 'doc_id': 'fa5c1523945f27f6bb5d9a04f2146a7a'}, {'text': 'All the main videos are stored in the Main “DATA ENGINEERING” playlist (no year specified). The Github repository has also been updated to show each video with a thumbnail, that would bring you directly to the same playlist below.\\nBelow is the MAIN PLAYLIST’. And then you refer to the year specific playlist for additional videos for that year like for office hours videos etc. Also find this playlist pinned to the slack channel.\\nh\\nttps://youtube.com/playlist?list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&si=NspQhtZhZQs1B9F-', 'section': 'General course-related questions', 'question': 'Course - Which playlist on YouTube should I refer to?', 'course': 'data-engineering-zoomcamp', 'doc_id': 'a5737de4f33219a4fcfe02f2c746d5a3'}, {'text': 'It depends on your background and previous experience with modules. It is expected to require about 5 - 15 hours per week. [source1] [source2]\\nYou can also calculate it yourself using this data and then update this answer.', 'section': 'General course-related questions', 'question': 'Course - \\u200b\\u200bHow many hours per week am I expected to spend on this  course?', 'course': 'data-engineering-zoomcamp', 'doc_id': 'c3b5714cc4d5a6db4fd5912404f30c31'}, {'text': \"No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\", 'section': 'General course-related questions', 'question': 'Certificate - Can I follow the course in a self-paced mode and get a certificate?', 'course': 'data-engineering-zoomcamp', 'doc_id': 'c47f302e889f61f7fd3cee025ef939b6'}, {'text': 'The zoom link is only published to instructors/presenters/TAs.\\nStudents participate via Youtube Live and submit questions to Slido (link would be pinned in the chat when Alexey goes Live). The video URL should be posted in the announcements channel on Telegram & Slack before it begins. Also, you will see it live on the DataTalksClub YouTube Channel.\\nDon’t post your questions in chat as it would be off-screen before the instructors/moderators have a chance to answer it if the room is very active.', 'section': 'General course-related questions', 'question': 'Office Hours - What is the video/zoom link to the stream for the “Office Hour” or workshop sessions?', 'course': 'data-engineering-zoomcamp', 'doc_id': '6a439fc15426567b38e84acc6142c461'}, {'text': 'Yes! Every “Office Hours” will be recorded and available a few minutes after the live session is over; so you can view (or rewatch) whenever you want.', 'section': 'General course-related questions', 'question': 'Office Hours - I can’t attend the “Office hours” / workshop, will it be recorded?', 'course': 'data-engineering-zoomcamp', 'doc_id': '093b06705cc2cc1141f667e241e14a06'}, {'text': 'You can find the latest and up-to-date deadlines here: https://docs.google.com/spreadsheets/d/e/2PACX-1vQACMLuutV5rvXg5qICuJGL-yZqIV0FBD84CxPdC5eZHf8TfzB-CJT_3Mo7U7oGVTXmSihPgQxuuoku/pubhtml\\nAlso, take note of Announcements from @Au-Tomator for any extensions or other news. Or, the form may also show the updated deadline, if Instructor(s) has updated it.', 'section': 'General course-related questions', 'question': 'Homework - What are homework and project deadlines?', 'course': 'data-engineering-zoomcamp', 'doc_id': '1e1c6528694d3abbfc7488bb3abea5e2'}, {'text': 'No, late submissions are not allowed. But if the form is still not closed and it’s after the due date, you can still submit the homework. confirm your submission by the date-timestamp on the Course page.y\\nOlder news:[source1] [source2]', 'section': 'General course-related questions', 'question': 'Homework - Are late submissions of homework allowed?', 'course': 'data-engineering-zoomcamp', 'doc_id': '3a90237d0692b97f3f9b322a855cc4fd'}, {'text': 'Answer: In short, it’s your repository on github, gitlab, bitbucket, etc\\nIn long, your repository or any other location you have your code where a reasonable person would look at it and think yes, you went through the week and exercises.', 'section': 'General course-related questions', 'question': 'Homework - What is the homework URL in the homework link?', 'course': 'data-engineering-zoomcamp', 'doc_id': '9a0107663b3bab95ee3787a55673cde6'}, {'text': 'After you submit your homework it will be graded based on the amount of questions in a particular homework. You can see how many points you have right on the page of the homework up top. Additionally in the leaderboard you will find the sum of all points you’ve earned - points for Homeworks, FAQs and Learning in Public. If homework is clear, others work as follows: if you submit something to FAQ, you get one point, for each learning in a public link you get one point.\\n(https://datatalks-club.slack.com/archives/C01FABYF2RG/p1706846846359379?thread_ts=1706825019.546229&cid=C01FABYF2RG)', 'section': 'General course-related questions', 'question': 'Homework and Leaderboard - what is the system for points in the course management platform?', 'course': 'data-engineering-zoomcamp', 'doc_id': 'b666ae0b62d6a6dfc38396ba510cdc6b'}, {'text': 'When you set up your account you are automatically assigned a random name such as “Lucid Elbakyan” for example. If you want to see what your Display name is.\\nGo to the Homework submission link →  https://courses.datatalks.club/de-zoomcamp-2024/homework/hw2 - Log in > Click on ‘Data Engineering Zoom Camp 2024’ > click on ‘Edit Course Profile’ - your display name is here, you can also change it should you wish:', 'section': 'General course-related questions', 'question': 'Leaderboard - I am not on the leaderboard / how do I know which one I am on the leaderboard?', 'course': 'data-engineering-zoomcamp', 'doc_id': 'd418a703e60b8555c41dc80863f589dc'}, {'text': 'Yes, for simplicity (of troubleshooting against the recorded videos) and stability. [source]\\nBut Python 3.10 and 3.11 should work fine.', 'section': 'General course-related questions', 'question': 'Environment - Is Python 3.9 still the recommended version to use in 2024?', 'course': 'data-engineering-zoomcamp', 'doc_id': '2a0887600c276fd6f9d5a5ead914df0b'}, {'text': 'You can set it up on your laptop or PC if you prefer to work locally from your laptop or PC.\\nYou might face some challenges, especially for Windows users. If you face cnd2\\nIf you prefer to work on the local machine, you may start with the week 1 Introduction to Docker and follow through.\\nHowever, if you prefer to set up a virtual machine, you may start with these first:\\nUsing GitHub Codespaces\\nSetting up the environment on a cloudV Mcodespace\\nI decided to work on a virtual machine because I have different laptops & PCs for my home & office, so I can work on this boot camp virtually anywhere.', 'section': 'General course-related questions', 'question': 'Environment - Should I use my local machine, GCP, or GitHub Codespaces for my environment?', 'course': 'data-engineering-zoomcamp', 'doc_id': '821ffc7eef9760e690cf6f6f3d01f191'}, {'text': 'GitHub Codespaces offers you computing Linux resources with many pre-installed tools (Docker, Docker Compose, Python).\\nYou can also open any GitHub repository in a GitHub Codespace.', 'section': 'General course-related questions', 'question': 'Environment - Is GitHub codespaces an alternative to using cli/git bash to ingest the data and create a docker file?', 'course': 'data-engineering-zoomcamp', 'doc_id': 'c4138ffefdd3d00b85496250ace2c836'}, {'text': \"It's up to you which platform and environment you use for the course.\\nGithub codespaces or GCP VM are just possible options, but you can do the entire course from your laptop.\", 'section': 'General course-related questions', 'question': 'Environment - Do we really have to use GitHub codespaces? I already have PostgreSQL & Docker installed.', 'course': 'data-engineering-zoomcamp', 'doc_id': '13cc1cfada6c696dab0c83e781774b93'}, {'text': 'Choose the approach that aligns the most with your idea for the end project\\nOne of those should suffice. However, BigQuery, which is part of GCP, will be used, so learning that is probably a better option. Or you can set up a local environment for most of this course.', 'section': 'General course-related questions', 'question': 'Environment - Do I need both GitHub Codespaces and GCP?', 'course': 'data-engineering-zoomcamp', 'doc_id': '3cc081b0deeb66dac1f6e6f80b859bc3'}, {'text': '1. To open Run command window, you can either:\\n(1-1) Use the shortcut keys: \\'Windows + R\\', or\\n(1-2) Right Click \"Start\", and click \"Run\" to open.\\n2. Registry Values Located in Registry Editor, to open it: Type \\'regedit\\' in the Run command window, and then press Enter.\\' 3. Now you can change the registry values \"Autorun\" in \"HKEY_CURRENT_USER\\\\Software\\\\Microsoft\\\\Command Processor\" from \"if exists\" to a blank.\\nAlternatively, You can simplify the solution by deleting the fingerprint saved within the known_hosts file. In Windows, this file is placed at  C:\\\\Users\\\\<your_user_name>\\\\.ssh\\\\known_host', 'section': 'General course-related questions', 'question': 'This happens when attempting to connect to a GCP VM using VSCode on a Windows machine. Changing registry value in registry editor', 'course': 'data-engineering-zoomcamp', 'doc_id': '5a76ede2a7f68e863f6da7276d0ce106'}, {'text': 'For uniformity at least, but you’re not restricted to GCP, you can use other cloud platforms like AWS if you’re comfortable with other cloud platforms, since you get every service that’s been provided by GCP in Azure and AWS or others..\\nBecause everyone has a google account, GCP has a free trial period and gives $300 in credits  to new users. Also, we are working with BigQuery, which is a part of GCP.\\nNote that to sign up for a free GCP account, you must have a valid credit card.', 'section': 'General course-related questions', 'question': 'Environment - Why are we using GCP and not other cloud providers?', 'course': 'data-engineering-zoomcamp', 'doc_id': '905a9a5de263c002c6b8b311a20f4f03'}, {'text': 'No, if you use GCP and take advantage of their free trial.', 'section': 'General course-related questions', 'question': 'Should I pay for cloud services?', 'course': 'data-engineering-zoomcamp', 'doc_id': '6cb80ff3a8a0c09a2273b073933b7311'}, {'text': 'You can do most of the course without a cloud. Almost everything we use (excluding BigQuery) can be run locally. We won’t be able to provide guidelines for some things, but most of the materials are runnable without GCP.\\nFor everything in the course, there’s a local alternative. You could even do the whole course locally.', 'section': 'General course-related questions', 'question': 'Environment - The GCP and other cloud providers are unavailable in some countries. Is it possible to provide a guide to installing a home lab?', 'course': 'data-engineering-zoomcamp', 'doc_id': 'e5a35da13d8561565696b031a6a2f7be'}, {'text': 'Yes, you can. Just remember to adapt all the information on the videos to AWS. Besides, the final capstone will be evaluated based on the task: Create a data pipeline! Develop a visualisation!\\nThe problem would be when you need help. You’d need to rely on  fellow coursemates who also use AWS (or have experience using it before), which might be in smaller numbers than those learning the course with GCP.\\nAlso see Is it possible to use x tool instead of the one tool you use?', 'section': 'General course-related questions', 'question': 'Environment - I want to use AWS. May I do that?', 'course': 'data-engineering-zoomcamp', 'doc_id': '6777d05ae24b183acd0ce819e589662c'}, {'text': 'We will probably have some calls during the Capstone period to clear some questions but it will be announced in advance if that happens.', 'section': 'General course-related questions', 'question': 'Besides the “Office Hour” which are the live zoom calls?', 'course': 'data-engineering-zoomcamp', 'doc_id': 'cd540c158b2253368bde0deecc09cfea'}, {'text': 'We will use the same data, as the project will essentially remain the same as last year’s. The data is available here', 'section': 'General course-related questions', 'question': 'Are we still using the NYC Trip data for January 2021? Or are we using the 2022 data?', 'course': 'data-engineering-zoomcamp', 'doc_id': '66ed5eeacae24a08854edfbdd2cef8c6'}, {'text': 'No, but we moved the 2022 stuff here', 'section': 'General course-related questions', 'question': 'Is the 2022 repo deleted?', 'course': 'data-engineering-zoomcamp', 'doc_id': '43f190716d165eb1f69e48c7354d84d8'}, {'text': 'Yes, you can use any tool you want for your project.', 'section': 'General course-related questions', 'question': 'Can I use Airflow instead for my final project?', 'course': 'data-engineering-zoomcamp', 'doc_id': 'd9faa54bdb2c801f4fc21fd5d8632468'}, {'text': 'Yes, this applies if you want to use Airflow or Prefect instead of Mage, AWS or Snowflake instead of GCP products or Tableau instead of Metabase or Google data studio.\\nThe course covers 2 alternative data stacks, one using GCP and one using local installation of everything. You can use one of them or use your tool of choice.\\nShould you consider it instead of the one tool you use? That we can’t support you if you choose to use a different stack, also you would need to explain the different choices of tool for the peer review of your capstone project.', 'section': 'General course-related questions', 'question': 'Is it possible to use tool “X” instead of the one tool you use in the course?', 'course': 'data-engineering-zoomcamp', 'doc_id': '0d049ccb21a1875fe936664a7738f885'}, {'text': 'Star the repo! Share it with friends if you find it useful ❣️\\nCreate a PR if you see you can improve the text or the structure of the repository.', 'section': 'General course-related questions', 'question': 'How can we contribute to the course?', 'course': 'data-engineering-zoomcamp', 'doc_id': '70bab3635e1e14c864d64ff31a7f2086'}, {'text': 'Yes! Linux is ideal but technically it should not matter. Students last year used all 3 OSes successfully', 'section': 'General course-related questions', 'question': 'Environment - Is the course [Windows/mac/Linux/...] friendly?', 'course': 'data-engineering-zoomcamp', 'doc_id': 'e38d8a50c8c70bc8ab52ee1454385d5f'}, {'text': \"Have no idea how past cohorts got past this as I haven't read old slack messages, and no FAQ entries that I can find.\\nLater modules (module-05 & RisingWave workshop) use shell scripts in *.sh files and most Windows users not using WSL would hit a wall and cannot continue, even in git bash or MINGW64. This is why WSL environment setup is recommended from the start.\", 'section': 'General course-related questions', 'question': 'Environment - Roadblock for Windows users in modules with *.sh (shell scripts).', 'course': 'data-engineering-zoomcamp', 'doc_id': 'db45cb7fc7b4b9952fac580fe960b547'}, {'text': 'Yes to both! check out this document: https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/awesome-data-engineering.md', 'section': 'General course-related questions', 'question': 'Any books or additional resources you recommend?', 'course': 'data-engineering-zoomcamp', 'doc_id': 'be3ebb503529b4378a61baf869e1283c'}, {'text': 'You will have two attempts for a project. If the first project deadline is over and you’re late or you submit the project and fail the first attempt, you have another chance to submit the project with the second attempt.', 'section': 'General course-related questions', 'question': 'Project - What is Project Attemp #1 and Project Attempt #2 exactly?', 'course': 'data-engineering-zoomcamp', 'doc_id': '7d4799d5ce9bf3ba9fbc9fb9dfb8b6b0'}, {'text': \"The first step is to try to solve the issue on your own. Get used to solving problems and reading documentation. This will be a real life skill you need when employed. [ctrl+f] is your friend, use it! It is a universal shortcut and works in all apps/browsers.\\nWhat does the error say? There will often be a description of the error or instructions on what is needed or even how to fix it. I have even seen a link to the solution. Does it reference a specific line of your code?\\nRestart app or server/pc.\\nGoogle it, use ChatGPT, Bing AI etc.\\nIt is going to be rare that you are the first to have the problem, someone out there has posted the fly issue and likely the solution.\\nSearch using: <technology> <problem statement>. Example: pgcli error column c.relhasoids does not exist.\\nThere are often different solutions for the same problem due to variation in environments.\\nCheck the tech’s documentation. Use its search if available or use the browsers search function.\\nTry uninstall (this may remove the bad actor) and reinstall of application or reimplementation of action. Remember to restart the server/pc for reinstalls.\\nSometimes reinstalling fails to resolve the issue but works if you uninstall first.\\nPost your question to Stackoverflow. Read the Stackoverflow guide on posting good questions.\\nhttps://stackoverflow.com/help/how-to-ask\\nThis will be your real life. Ask an expert in the future (in addition to coworkers).\\nAsk in Slack\\nBefore asking a question,\\nCheck Pins (where the shortcut to the repo and this FAQ is located)\\nUse the slack app’s search function\\nUse the bot @ZoomcampQABot to do the search for you\\ncheck the FAQ (this document), use search [ctrl+f]\\nWhen asking a question, include as much information as possible:\\nWhat are you coding on? What OS?\\nWhat command did you run, which video did you follow? Etc etc\\nWhat error did you get? Does it have a line number to the “offending” code and have you check it for typos?\\nWhat have you tried that did not work? This answer is crucial as without it, helpers would ask you to do the suggestions in the error log first. Or just read this FAQ document.\\nDO NOT use screenshots, especially don’t take pictures from a phone.\\nDO NOT tag instructors, it may discourage others from helping you. Copy and paste errors; if it’s long, just post it in a reply to your thread.\\nUse ``` for formatting your code.\\nUse the same thread for the conversation (that means reply to your own thread).\\nDO NOT create multiple posts to discuss the issue.\\nlearYou may create a new post if the issue reemerges down the road. Describe what has changed in the environment.\\nProvide additional information in the same thread of the steps you have taken for resolution.\\nTake a break and come back later. You will be amazed at how often you figure out the solution after letting your brain rest. Get some fresh air, workout, play a video game, watch a tv show, whatever allows your brain to not think about it for a little while or even until the next day.\\nRemember technology issues in real life sometimes take days or even weeks to resolve.\\nIf somebody helped you with your problem and it's not in the FAQ, please add it there. It will help other students.\", 'section': 'General course-related questions', 'question': 'How to troubleshoot issues', 'course': 'data-engineering-zoomcamp', 'doc_id': '27967023a6e120e0295d6ccbeea93ab3'}, {'text': 'When the troubleshooting guide above does not help resolve it and you need another pair of eyeballs to spot mistakes. When asking a question, include as much information as possible:\\nWhat are you coding on? What OS?\\nWhat command did you run, which video did you follow? Etc etc\\nWhat error did you get? Does it have a line number to the “offending” code and have you check it for typos?\\nWhat have you tried that did not work? This answer is crucial as without it, helpers would ask you to do the suggestions in the error log first. Or just read this FAQ document.', 'section': 'General course-related questions', 'question': 'How to ask questions', 'course': 'data-engineering-zoomcamp', 'doc_id': 'a4a893511970b4f7c6042d6dd3245aab'}, {'text': 'After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\\nHaving this local repository on your computer will make it easy for you to access the instructors’ code and make pull requests (if you want to add your own notes or make changes to the course content).\\nYou will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\\nRemember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\\nThis is also a great resource: https://dangitgit.com/', 'section': 'General course-related questions', 'question': 'How do I use Git / GitHub for this course?', 'course': 'data-engineering-zoomcamp', 'doc_id': 'b94eb7c49602d342b87e6fa16e0933e6'}, {'text': 'Error: Makefile:2: *** missing separator.  Stop.\\nSolution: Tabs in document should be converted to Tab instead of spaces. Follow this stack.', 'section': 'General course-related questions', 'question': 'VS Code: Tab using spaces', 'course': 'data-engineering-zoomcamp', 'doc_id': '410e2883d6fde933ca239f0561ffcdf3'}, {'text': \"If you’re running Linux on Windows Subsystem for Linux (WSL) 2, you can open HTML files from the guest (Linux) with whatever Internet Browser you have installed on the host (Windows). Just install wslu and open the page with wslview <file>, for example:\\nwslview index.html\\nYou can customise which browser to use by setting the BROWSER environment variable first. For example:\\nexport BROWSER='/mnt/c/Program Files/Firefox/firefox.exe'\", 'section': 'General course-related questions', 'question': 'Opening an HTML file with a Windows browser from Linux running on WSL', 'course': 'data-engineering-zoomcamp', 'doc_id': '4eacb446aca37f3c80e188822e941b64'}, {'text': 'This tutorial shows you how to set up the Chrome Remote Desktop service on a Debian Linux virtual machine (VM) instance on Compute Engine. Chrome Remote Desktop allows you to remotely access applications with a graphical user interface.\\nTaxi Data - Yellow Taxi Trip Records downloading error, Error no or XML error webpage\\nWhen you try to download the 2021 data from TLC website, you get this error:\\nIf you click on the link, and ERROR 403: Forbidden on the terminal.\\nWe have a backup, so use it instead: https://github.com/DataTalksClub/nyc-tlc-data\\nSo the link should be https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz\\nNote: Make sure to unzip the “gz” file (no, the “unzip” command won’t work for this.)\\n“gzip -d file.gz”g', 'section': 'Module 1: Docker and Terraform', 'question': 'Set up Chrome Remote Desktop for Linux on Compute Engine', 'course': 'data-engineering-zoomcamp', 'doc_id': '1c0bf6109b0a22dcc724356f80bdc64b'}, {'text': 'In this video, we store the data file as “output.csv”. The data file won’t store correctly if the file extension is csv.gz instead of csv. One alternative is to replace csv_name = “output.cs -v” with the file name given at the end of the URL. Notice that the URL for the yellow taxi data is: https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz where the highlighted part is the name of the file. We can parse this file name from the URL and use it as csv_name. That is, we can replace csv_name = “output.csv” with\\ncsv_name = url.split(“/”)[-1] . Then when we use csv_name to using pd.read_csv, there won’t be an issue even though the file name really has the extension csv.gz instead of csv since the pandas read_csv function can read csv.gz files directly.', 'section': 'Module 1: Docker and Terraform', 'question': 'Taxi Data - How to handle taxi data files, now that the files are available as *.csv.gz?', 'course': 'data-engineering-zoomcamp', 'doc_id': '986c62fecab40efdcdb546d45bc9eaaa'}, {'text': 'Yellow Trips: https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf\\nGreen Trips: https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_green.pdf', 'section': 'Module 1: Docker and Terraform', 'question': 'Taxi Data - Data Dictionary for NY Taxi data?', 'course': 'data-engineering-zoomcamp', 'doc_id': '45fafc4085fb6c0006de2cd36daa932a'}, {'text': 'You can unzip this downloaded parquet file, in the command line. The result is a csv file which can be imported with pandas using the pd.read_csv() shown in the videos.\\n‘’’gunzip green_tripdata_2019-09.csv.gz’’’\\nSOLUTION TO USING PARQUET FILES DIRECTLY IN PYTHON SCRIPT ingest_data.py\\nIn the def main(params) add this line\\nparquet_name= \\'output.parquet\\'\\nThen edit the code which downloads the files\\nos.system(f\"wget {url} -O {parquet_name}\")\\nConvert the download .parquet file to csv and rename as csv_name to keep it relevant to the rest of the code\\ndf = pd.read_parquet(parquet_name)\\ndf.to_csv(csv_name, index=False)', 'section': 'Module 1: Docker and Terraform', 'question': 'Taxi Data - Unzip Parquet file', 'course': 'data-engineering-zoomcamp', 'doc_id': 'f8d07d24cff4ccd512186da29f1a0f29'}, {'text': '“wget is not recognized as an internal or external command”, you need to install it.\\nOn Ubuntu, run:\\n$ sudo apt-get install wget\\nOn MacOS, the easiest way to install wget is to use Brew:\\n$ brew install wget\\nOn Windows, the easiest way to install wget is to use Chocolatey:\\n$ choco install wget\\nOr you can download a binary (https://gnuwin32.sourceforge.net/packages/wget.htm) and put it to any location in your PATH (e.g. C:/tools/)\\nAlso, you can following this step to install Wget on MS Windows\\n* Download the latest wget binary for windows from [eternallybored] (https://eternallybored.org/misc/wget/) (they are available as a zip with documentation, or just an exe)\\n* If you downloaded the zip, extract all (if windows built in zip utility gives an error, use [7-zip] (https://7-zip.org/)).\\n* Rename the file `wget64.exe` to `wget.exe` if necessary.\\n* Move wget.exe to your `Git\\\\mingw64\\\\bin\\\\`.\\nAlternatively, you can use a Python wget library, but instead of simply using “wget” you’ll need to use\\npython -m wget\\nYou need to install it with pip first:\\npip install wget\\nAlternatively, you can just paste the file URL into your web browser and download the file normally that way. You’ll want to move the resulting file into your working directory.\\nAlso recommended a look at the python library requests for the loading gz file  https://pypi.org/project/requests', 'section': 'Module 1: Docker and Terraform', 'question': 'lwget is not recognized as an internal or external command', 'course': 'data-engineering-zoomcamp', 'doc_id': '7e938b178c613e38d4ad0b60cbbd5ec5'}, {'text': 'Firstly, make sure that you add “!” before wget if you’re running your command in a Jupyter Notebook or CLI. Then, you can check one of this 2 things (from CLI):\\nUsing the Python library wget you installed with pip, try python -m wget <url>\\nWrite the usual command and add --no-check-certificate at the end. So it should be:\\n!wget <website_url> --no-check-certificate', 'section': 'Module 1: Docker and Terraform', 'question': 'wget - ERROR: cannot verify <website> certificate  (MacOS)', 'course': 'data-engineering-zoomcamp', 'doc_id': 'ee6ddd0a3149f6fb83a51da174ca36b0'}, {'text': 'For those who wish to use the backslash as an escape character in Git Bash for Windows (as Alexey normally does), type in the terminal: bash.escapeChar=\\\\ (no need to include in .bashrc)', 'section': 'Module 1: Docker and Terraform', 'question': 'Git Bash - Backslash as an escape character in Git Bash for Windows', 'course': 'data-engineering-zoomcamp', 'doc_id': 'af897dc72fc98964eb311728956ac0cf'}, {'text': 'Instruction on how to store secrets that will be avialable in GitHub  Codespaces.\\nManaging your account-specific secrets for GitHub Codespaces - GitHub Docs', 'section': 'Module 1: Docker and Terraform', 'question': 'GitHub Codespaces - How to store secrets', 'course': 'data-engineering-zoomcamp', 'doc_id': 'da30e93aa55a403a1b97ea56540179a8'}, {'text': \"Make sure you're able to start the Docker daemon, and check the issue immediately down below:\\nAnd don’t forget to update the wsl in powershell the  command is wsl –update\", 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - Cannot connect to Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?', 'course': 'data-engineering-zoomcamp', 'doc_id': '0fe88ef23e6b302f64c0306a9a4db629'}, {'text': \"As the official Docker for Windows documentation says, the Docker engine can either use the\\nHyper-V or WSL2 as its backend. However, a few constraints might apply\\nWindows 10 Pro / 11 Pro Users: \\nIn order to use Hyper-V as its back-end, you MUST have it enabled first, which you can do by following the tutorial: Enable Hyper-V Option on Windows 10 / 11\\nWindows 10 Home / 11 Home Users: \\nOn the other hand, Users of the 'Home' version do NOT have the option Hyper-V option enabled, which means, you can only get Docker up and running using the WSL2 credentials(Windows Subsystem for Linux). Url\\nYou can find the detailed instructions to do so here: rt ghttps://pureinfotech.com/install-wsl-windows-11/\\nIn case, you run into another issue while trying to install WSL2 (WslRegisterDistribution failed with error: 0x800701bc), Make sure you update the WSL2 Linux Kernel, following the guidelines here: \\n\\nhttps://github.com/microsoft/WSL/issues/5393\", 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - Error during connect: In the default daemon configuration on Windows, the docker client must be run with elevated privileges to connect.: Post: \"http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.24/containers/create\" : open //./pipe/docker_engine: The system cannot find the file specified', 'course': 'data-engineering-zoomcamp', 'doc_id': '2c141bf23500c4a0b3310bb6041cac2f'}, {'text': 'Whenever a `docker pull is performed (either manually or by `docker-compose up`), it attempts to fetch the given image name (pgadmin4, for the example above) from a repository (dbpage).\\nIF the repository is public, the fetch and download happens without any issue whatsoever.\\nFor instance:\\ndocker pull postgres:13\\ndocker pull dpage/pgadmin4\\nBE ADVISED:\\n\\nThe Docker Images we\\'ll be using throughout the Data Engineering Zoomcamp are all public (except when or if explicitly said otherwise by the instructors or co-instructors).\\n\\nMeaning: you are NOT required to perform a docker login to fetch them. \\n\\nSo if you get the message above saying \"docker login\\': denied: requested access to the resource is denied. That is most likely due to a typo in your image name:\\n\\nFor instance:\\n$ docker pull dbpage/pgadmin4\\nWill throw that exception telling you \"repository does not exist or may require \\'docker login\\'\\nError response from daemon: pull access denied for dbpage/pgadmin4, repository does not exist or \\nmay require \\'docker login\\': denied: requested access to the resource is denied\\nBut that actually happened because the actual image is dpage/pgadmin4 and NOT dbpage/pgadmin4\\nHow to fix it:\\n$ docker pull dpage/pgadmin4\\nEXTRA NOTES:\\nIn the real world, occasionally, when you\\'re working for a company or closed organisation, the Docker image you\\'re trying to fetch might be under a private repo that your DockerHub Username was granted access to.\\nFor which cases, you must first execute:\\n$ docker login\\nFill in the details of your username and password.\\nAnd only then perform the `docker pull` against that private repository\\nWhy am I encountering a \"permission denied\" error when creating a PostgreSQL Docker container for the New York Taxi Database with a mounted volume on macOS M1?\\nIssue Description:\\nWhen attempting to run a Docker command similar to the one below:\\ndocker run -it \\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"root\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n-p 5432:5432 \\\\mount\\npostgres:13\\nYou encounter the error message:\\ndocker: Error response from daemon: error while creating mount source path \\'/path/to/ny_taxi_postgres_data\\': chown /path/to/ny_taxi_postgres_data: permission denied.\\nSolution:\\n1- Stop Rancher Desktop:\\nIf you are using Rancher Desktop and face this issue, stop Rancher Desktop to resolve compatibility problems.\\n2- Install Docker Desktop:\\nInstall Docker Desktop, ensuring that it is properly configured and has the required permissions.\\n2-Retry Docker Command:\\nRun the Docker command again after switching to Docker Desktop. This step resolves compatibility issues on some systems.\\nNote: The issue occurred because Rancher Desktop was in use. Switching to Docker Desktop resolves compatibility problems and allows for the successful creation of PostgreSQL containers with mounted volumes for the New York Taxi Database on macOS M1.', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - docker pull dbpage', 'course': 'data-engineering-zoomcamp', 'doc_id': 'b1da87c134123eec20fbdba58a0b0aaa'}, {'text': 'When I runned command to create postgre in docker container it created folder on my local machine to mount it to volume inside container. It has write and read protection and owned by user 999, so I could not delete it by simply drag to trash.  My obsidian could not started due to access error, so I had to change placement of this folder and delete old folder by this command:\\nsudo rm -r -f docker_test/\\n- where `rm` - remove, `-r` - recursively, `-f` - force, `docker_test/` - folder.', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - can’t delete local folder that mounted to docker volume', 'course': 'data-engineering-zoomcamp', 'doc_id': '30313b1b331a278d08a0a4a12497cdb5'}, {'text': 'First off, make sure you\\'re running the latest version of Docker for Windows, which you can download from here. Sometimes using the menu to \"Upgrade\" doesn\\'t work (which is another clear indicator for you to uninstall, and reinstall with the latest version)\\nIf docker is stuck on starting, first try to switch containers by right clicking the docker symbol from the running programs and switch the containers from windows to linux or vice versa\\n[Windows 10 / 11 Pro Edition] The Pro Edition of Windows can run Docker either by using Hyper-V or WSL2 as its backend (Docker Engine)\\nIn order to use Hyper-V as its back-end, you MUST have it enabled first, which you can do by following the tutorial: Enable Hyper-V Option on Windows 10 / 11\\nIf you opt-in for WSL2, you can follow the same steps as detailed in the tutorial here', 'section': 'Module 1: Docker and Terraform', 'question': \"Docker - Docker won't start or is stuck in settings (Windows 10 / 11)\", 'course': 'data-engineering-zoomcamp', 'doc_id': 'd529aa12653e124c6ca7aacf8ff02a3f'}, {'text': \"It is recommended by the Docker do\\n[Windows 10 / 11 Home Edition] If you're running a Home Edition, you can still make it work with WSL2 (Windows Subsystem for Linux) by following the tutorial here\\nIf even after making sure your WSL2 (or Hyper-V) is set up accordingly, Docker remains stuck, you can try the option to Reset to Factory Defaults or do a fresh install.\", 'section': 'Module 1: Docker and Terraform', 'question': 'Should I run docker commands from the windows file system or a file system of a Linux distribution in WSL?', 'course': 'data-engineering-zoomcamp', 'doc_id': '5bcdc9b262f8cacfc146e5850803809b'}, {'text': 'More info in the Docker Docs on Best Practises', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - cs to store all code in your default Linux distro to get the best out of file system performance (since Docker runs on WSL2 backend by default for Windows 10 Home / Windows 11 Home users).', 'course': 'data-engineering-zoomcamp', 'doc_id': 'e91c73638688cb35359988653675589c'}, {'text': 'You may have this error:\\n$ docker run -it ubuntu bash\\nthe input device is not a TTY. If you are using mintty, try prefixing the command with \\'winpty\\'\\nerror:\\nSolution:\\nUse winpty before docker command (source)\\n$ winpty docker run -it ubuntu bash\\nYou also can make an alias:\\necho \"alias docker=\\'winpty docker\\'\" >> ~/.bashrc\\nOR\\necho \"alias docker=\\'winpty docker\\'\" >> ~/.bash_profile', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - The input device is not a TTY (Docker run for Windows)', 'course': 'data-engineering-zoomcamp', 'doc_id': '46d432046a6c65e5a83f8e2c6d975e7c'}, {'text': \"You may have this error:\\nRetrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.u\\nrllib3.connection.HTTPSConnection object at 0x7efe331cf790>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')':\\n/simple/pandas/\\nPossible solution might be:\\n$ winpty docker run -it --dns=8.8.8.8 --entrypoint=bash python:3.9\", 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - Cannot pip install on Docker container (Windows)', 'course': 'data-engineering-zoomcamp', 'doc_id': '330244beecd57dd10ec034a87be3b486'}, {'text': 'Even after properly running the docker script the folder is empty in the vs code  then try this (For Windows)\\nwinpty docker run -it \\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"root\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-v \"C:\\\\Users\\\\abhin\\\\dataengg\\\\DE_Project_git_connected\\\\DE_OLD\\\\week1_set_up\\\\docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data\" \\\\\\n-p 5432:5432 \\\\\\npostgres:13\\nHere quoting the absolute path in  the -v parameter is solving the issue and all the files are visible in the Vs-code ny_taxi folder as shown in the video', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - ny_taxi_postgres_data is empty', 'course': 'data-engineering-zoomcamp', 'doc_id': 'ef728ab7bd73e5885bb969c5ed448735'}, {'text': 'Check this article for details - Setting up docker in macOS\\nFrom researching it seems this method might be out of date, it seems that since docker changed their licensing model, the above is a bit hit and miss. What worked for me was to just go to the docker website and download their dmg. Haven’t had an issue with that method.', 'section': 'Module 1: Docker and Terraform', 'question': 'dasDocker - Setting up Docker on Mac', 'course': 'data-engineering-zoomcamp', 'doc_id': '01c76f428c5063a556ab487016fc8d92'}, {'text': '$ docker run -it\\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"admin\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-v \"/mnt/path/to/ny_taxi_postgres_data\":\"/var/lib/postgresql/data\" \\\\\\n-p 5432:5432 \\\\\\npostgres:13\\nCCW\\nThe files belonging to this database system will be owned by user \"postgres\".\\nThis use The database cluster will be initialized with locale \"en_US.utf8\".\\nThe default databerrorase encoding has accordingly been set to \"UTF8\".\\nxt search configuration will be set to \"english\".\\nData page checksums are disabled.\\nfixing permissions on existing directory /var/lib/postgresql/data ... initdb: f\\nerror: could not change permissions of directory \"/var/lib/postgresql/data\": Operation not permitted  volume\\nOne way to solve this issue is to create a local docker volume and map it to postgres data directory /var/lib/postgresql/data\\nThe input dtc_postgres_volume_local must match in both commands below\\n$ docker volume create --name dtc_postgres_volume_local -d local\\n$ docker run -it\\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"root\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-v dtc_postgres_volume_local:/var/lib/postgresql/data \\\\\\n-p 5432:5432\\\\\\npostgres:13\\nTo verify the above command works in (WSL2 Ubuntu 22.04, verified 2024-Jan), go to the Docker Desktop app and look under Volumes - dtc_postgres_volume_local would be listed there. The folder ny_taxi_postgres_data would however be empty, since we used an alternative config.\\nAn alternate error could be:\\ninitdb: error: directory \"/var/lib/postgresql/data\" exists but is not empty\\nIf you want to create a new database system, either remove or empthe directory \"/var/lib/postgresql/data\" or run initdb\\nwitls', 'section': 'Module 1: Docker and Terraform', 'question': '1Docker - Could not change permissions of directory \"/var/lib/postgresql/data\": Operation not permitted', 'course': 'data-engineering-zoomcamp', 'doc_id': 'baf320ea02c708552e1587f4074271cb'}, {'text': 'Mapping volumes on Windows could be tricky. The way it was done in the course video doesn’t work for everyone.\\nFirst, if yo\\nmove your data to some folder without spaces. E.g. if your code is in “C:/Users/Alexey Grigorev/git/…”, move it to “C:/git/…”\\nTry replacing the “-v” part with one of the following options:\\n-v /c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n-v //c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n-v /c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n-v //c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n--volume //driveletter/path/ny_taxi_postgres_data/:/var/lib/postgresql/data\\nwinpty docker run -it\\n-e POSTGRES_USER=\"root\"\\n-e POSTGRES_PASSWORD=\"root\"\\n-e POSTGRES_DB=\"ny_taxi\"\\n-v /c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n-p 5432:5432\\npostgres:1\\nTry adding winpty before the whole command\\n3\\nwin\\nTry adding quotes:\\n-v \"/c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\"\\n-v \"//c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\"\\n-v “/c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\"\\n-v \"//c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\"\\n-v \"c:\\\\some\\\\path\\\\ny_taxi_postgres_data\":/var/lib/postgresql/data\\nNote:  (Window) if it automatically creates a folder called “ny_taxi_postgres_data;C” suggests you have problems with volume mapping, try deleting both folders and replacing “-v” part with other options. For me “//c/” works instead of “/c/”. And it will work by automatically creating a correct folder called “ny_taxi_postgres_data”.\\nA possible solution to this error would be to use /”$(pwd)”/ny_taxi_postgres_data:/var/lib/postgresql/data (with quotes’ position varying as in the above list).\\nYes for windows use the command it works perfectly fine\\n-v /”$(pwd)”/ny_taxi_postgres_data:/var/lib/postgresql/data\\nImportant: note how the quotes are placed.\\nIf none of these options work, you can use a volume name instead of the path:\\n-v ny_taxi_postgres_data:/var/lib/postgresql/data\\nFor Mac: You can wrap $(pwd) with quotes like the highlighted.\\ndocker run -it \\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"root\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-v \"$(pwd)\"/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n-p 5432:5432 \\\\\\nPostgres:13\\ndocker run -it \\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"root\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-v \"$(pwd)\"/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n-p 5432:5432 \\\\\\npostgres:13\\nSource:https://stackoverflow.com/questions/48522615/docker-error-invalid-reference-format-repository-name-must-be-lowercase', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - invalid reference format: repository name must be lowercase (Mounting volumes with Docker on Windows)', 'course': 'data-engineering-zoomcamp', 'doc_id': 'f8af77b84b0f9ede78ea7ae3f1d88e23'}, {'text': 'Change the mounting path. Replace it with one of following:\\n-v /e/zoomcamp/...:/var/lib/postgresql/data\\n-v /c:/.../ny_taxi_postgres_data:/var/lib/postgresql/data\\\\ (leading slash in front of c:)', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - Error response from daemon: invalid mode: \\\\Program Files\\\\Git\\\\var\\\\lib\\\\postgresql\\\\data.', 'course': 'data-engineering-zoomcamp', 'doc_id': '09505439c01f8a62ddf2d893d6ffc433'}, {'text': 'When you run this command second time\\ndocker run -it \\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"root\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-v <your path>:/var/lib/postgresql/data \\\\\\n-p 5432:5432 \\\\\\npostgres:13\\nThe error message above could happen. That means you should not mount on the second run. This command helped me:\\nWhen you run this command second time\\ndocker run -it \\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"root\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-p 5432:5432 \\\\\\npostgres:13', 'section': 'Module 1: Docker and Terraform', 'question': \"Docker - Error response from daemon: error while creating buildmount source path '/run/desktop/mnt/host/c/<your path>': mkdir /run/desktop/mnt/host/c: file exists\", 'course': 'data-engineering-zoomcamp', 'doc_id': 'c4773a912a8a34adf75bf48cca50dcd4'}, {'text': 'This error appeared when running the command: docker build -t taxi_ingest:v001 .\\nWhen feeding the database with the data the user id of the directory ny_taxi_postgres_data was changed to 999, so my user couldn’t access it when running the above command. Even though this is not the problem here it helped to raise the error due to the permission issue.\\nSince at this point we only need the files Dockerfile and ingest_data.py, to fix this error one can run the docker build command on a different directory (having only these two files).\\nA more complete explanation can be found here: https://stackoverflow.com/questions/41286028/docker-build-error-checking-context-cant-stat-c-users-username-appdata\\nYou can fix the problem by changing the permission of the directory on ubuntu with following command:\\nsudo chown -R $USER dir_path\\nOn windows follow the link: https://thegeekpage.com/take-ownership-of-a-file-folder-through-command-prompt-in-windows-10/ \\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tAdded by\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tKenan Arslanbay', 'section': 'Module 1: Docker and Terraform', 'question': \"Docker - build error: error checking context: 'can't stat '/home/user/repos/data-engineering/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data''.\", 'course': 'data-engineering-zoomcamp', 'doc_id': '11534759f9dbcaaefe703778ce36a4e5'}, {'text': 'You might have installed docker via snap. Run “sudo snap status docker” to verify.\\nIf you have “error: unknown command \"status\", see \\'snap help\\'.” as a response than deinstall docker and install via the official website\\nBind for 0.0.0.0:5432 failed: port is a', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - ERRO[0000] error waiting for container: context canceled', 'course': 'data-engineering-zoomcamp', 'doc_id': 'deda982166aa674f4387114bb2964474'}, {'text': 'Found the issue in the PopOS linux. It happened because our user didn’t have authorization rights to the host folder ( which also caused folder seems empty, but it didn’t!).\\n✅Solution:\\nJust add permission for everyone to the corresponding folder\\nsudo chmod -R 777 <path_to_folder>\\nExample:\\nsudo chmod -R 777 ny_taxi_postgres_data/', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - build error checking context: can’t stat ‘/home/fhrzn/Projects/…./ny_taxi_postgres_data’', 'course': 'data-engineering-zoomcamp', 'doc_id': '3a2aef76af2ccede4862e0e153659735'}, {'text': 'This happens on Ubuntu/Linux systems when trying to run the command to build the Docker container again.\\n$ docker build -t taxi_ingest:v001 .\\nA folder is created to host the Docker files. When the build command is executed again to rebuild the pipeline or create a new one the error is raised as there are no permissions on this new folder. Grant permissions by running this comtionmand;\\n$ sudo chmod -R 755 ny_taxi_postgres_data\\nOr use 777 if you still see problems. 755 grants write access to only the owner.', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - failed to solve with frontend dockerfile.v0: failed to read dockerfile: error from sender: open ny_taxi_postgres_data: permission denied.', 'course': 'data-engineering-zoomcamp', 'doc_id': 'db1a5ca1aaf92db8951bdcb973be73b5'}, {'text': 'Get the network name via: $ docker network ls.', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - Docker network name', 'course': 'data-engineering-zoomcamp', 'doc_id': 'cc748fbd54f00462f5d629d496ce6ad8'}, {'text': 'Sometimes, when you try to restart a docker image configured with a network name, the above message appears. In this case, use the following command with the appropriate container name:\\n>>> If the container is running state, use docker stop <container_name>\\n>>> then, docker rm pg-database\\nOr use docker start instead of docker run in order to restart the docker image without removing it.', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - Error response from daemon: Conflict. The container name \"pg-database\" is already in use by container “xxx”.  You have to remove (or rename) that container to be able to reuse that name.', 'course': 'data-engineering-zoomcamp', 'doc_id': 'b37556b8c705e8232cf2d850e9752ebb'}, {'text': 'Typical error: sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name \"pgdatabase\" to address: Name or service not known\\nWhen running docker-compose up -d see which network is created and use this for the ingestions script instead of pg-network and see the name of the database to use instead of pgdatabase\\nE.g.:\\npg-network becomes 2docker_default\\nPgdatabase becomes 2docker-pgdatabase-1', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - ingestion when using docker-compose could not translate host name', 'course': 'data-engineering-zoomcamp', 'doc_id': 'c7280035dea7d06185338a83effb576f'}, {'text': 'terraformRun this command before starting your VM:\\nOn Intel CPU:\\nmodprobe -r kvm_intel\\nmodprobe kvm_intel nested=1\\nOn AMD CPU:\\nmodprobe -r kvm_amd\\nmodprobe kvm_amd nested=1', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - Cannot install docker on MacOS/Windows 11 VM running on top of Linux (due to Nested virtualization).', 'course': 'data-engineering-zoomcamp', 'doc_id': '22a55ea2c032175dac184c3df8b5addd'}, {'text': 'It’s very easy to manage your docker container, images, network and compose projects from VS Code.\\nJust install the official extension and launch it from the left side icon.\\nIt will work even if your Docker runs on WSL2, as VS Code can easily connect with your Linux.\\nDocker - How to stop a container?\\nUse the following command:\\n$ docker stop <container_id>', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - Connecting from VS Code', 'course': 'data-engineering-zoomcamp', 'doc_id': 'fb4bf292ef5fcbd94ca9c5338e4705e0'}, {'text': \"When you see this in logs, your container with postgres is not accepting any requests, so if you attempt to connect, you'll get this error:\\nconnection failed: server closed the connection unexpectedly\\nThis probably means the server terminated abnormally before or while processing the request.\\nIn this case, you need to delete the directory with data (the one you map to the container with the -v flag) and restart the container.\", 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - PostgreSQL Database directory appears to contain a database. Database system is shut down', 'course': 'data-engineering-zoomcamp', 'doc_id': 'df8ca5ad410a99111bb9b4353e020576'}, {'text': 'On few versions of Ubuntu, snap command can be used to install Docker.\\nsudo snap install docker', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker not installable on Ubuntu', 'course': 'data-engineering-zoomcamp', 'doc_id': '8cb2378ff9ca8c4020541c368f49a033'}, {'text': 'error: could not change permissions of directory \"/var/lib/postgresql/data\": Operation not permitted  volume\\nif you have used the prev answer (just before this) and have created a local docker volume, then you need to tell the compose file about the named volume:\\nvolumes:\\ndtc_postgres_volume_local:  # Define the named volume here\\n# services mentioned in the compose file auto become part of the same network!\\nservices:\\nyour remaining code here . . .\\nnow use docker volume inspect dtc_postgres_volume_local to see the location by checking the value of Mountpoint\\nIn my case, after i ran docker compose up the mounting dir created was named ‘docker_sql_dtc_postgres_volume_local’ whereas it should have used the already existing ‘dtc_postgres_volume_local’\\nAll i did to fix this is that I renamed the existing ‘dtc_postgres_volume_local’ to ‘docker_sql_dtc_postgres_volume_local’ and removed the newly created one (just be careful when doing this)\\nrun docker compose up again and check if the table is there or not!', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker-Compose - mounting error', 'course': 'data-engineering-zoomcamp', 'doc_id': '59f9a9d64404a78ca0b6793117f47b8e'}, {'text': 'Couldn’t translate host name to address\\nMake sure postgres database is running.\\n\\n\\u200b\\u200bUse the command to start containers in detached mode: docker-compose up -d\\n(data-engineering-zoomcamp) hw % docker compose up -d\\n[+] Running 2/2\\n⠿ Container pg-admin     Started                                                                                                                                                                      0.6s\\n⠿ Container pg-database  Started\\nTo view the containers use: docker ps.\\n(data-engineering-zoomcamp) hw % docker ps\\nCONTAINER ID   IMAGE            COMMAND                  CREATED          STATUS          PORTS                           NAMES\\nfaf05090972e   postgres:13      \"docker-entrypoint.s…\"   39 seconds ago   Up 37 seconds   0.0.0.0:5432->5432/tcp          pg-database\\n6344dcecd58f   dpage/pgadmin4   \"/entrypoint.sh\"         39 seconds ago   Up 37 seconds   443/tcp, 0.0.0.0:8080->80/tcp   pg-admin\\nhw\\nTo view logs for a container: docker logs <containerid>\\n(data-engineering-zoomcamp) hw % docker logs faf05090972e\\nPostgreSQL Database directory appears to contain a database; Skipping initialization\\n2022-01-25 05:58:45.948 UTC [1] LOG:  starting PostgreSQL 13.5 (Debian 13.5-1.pgdg110+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\\n2022-01-25 05:58:45.948 UTC [1] LOG:  listening on IPv4 address \"0.0.0.0\", port 5432\\n2022-01-25 05:58:45.948 UTC [1] LOG:  listening on IPv6 address \"::\", port 5432\\n2022-01-25 05:58:45.954 UTC [1] LOG:  listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5432\"\\n2022-01-25 05:58:45.984 UTC [28] LOG:  database system was interrupted; last known up at 2022-01-24 17:48:35 UTC\\n2022-01-25 05:58:48.581 UTC [28] LOG:  database system was not properly shut down; automatic recovery in\\nprogress\\n2022-01-25 05:58:48.602 UTC [28] LOG:  redo starts at 0/872A5910\\n2022-01-25 05:59:33.726 UTC [28] LOG:  invalid record length at 0/98A3C160: wanted 24, got 0\\n2022-01-25 05:59:33.726 UTC [28\\n] LOG:  redo done at 0/98A3C128\\n2022-01-25 05:59:48.051 UTC [1] LOG:  database system is ready to accept connections\\nIf docker ps doesn’t show pgdatabase running, run: docker ps -a\\nThis should show all containers, either running or stopped.\\nGet the container id for pgdatabase-1, and run', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker-Compose - Error translating host name to address', 'course': 'data-engineering-zoomcamp', 'doc_id': '6500cf47f217d6d0894d6c7e11bc3b73'}, {'text': 'After executing `docker-compose up` - if you lose database data and are unable to successfully execute your Ingestion script (to re-populate your database) but receive the following error:\\nsqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name /data_pgadmin:/var/lib/pgadmin\"pg-database\" to address: Name or service not known\\nDocker compose is creating its own default network since it is no longer specified in a docker execution command or file. Docker Compose will emit to logs the new network name. See the logs after executing `docker compose up` to find the network name and change the network name argument in your Ingestion script.\\nIf problems persist with pgcli, we can use HeidiSQL,usql\\nKrishna Anand', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker-Compose -  Data retention (could not translate host name \"pg-database\" to address: Name or service not known)', 'course': 'data-engineering-zoomcamp', 'doc_id': '8e23179495b674317d1772d9963aec43'}, {'text': 'It returns --> Error response from daemon: network 66ae65944d643fdebbc89bd0329f1409dec2c9e12248052f5f4c4be7d1bdc6a3 not found\\nTry:\\ndocker ps -a to see all the stopped & running containers\\nd to nuke all the containers\\nTry: docker-compose up -d again ports\\nOn localhost:8080 server → Unable to connect to server: could not translate host name \\'pg-database\\' to address: Name does not resolve\\nTry: new host name, best without “ - ” e.g. pgdatabase\\nAnd on docker-compose.yml, should specify docker network & specify the same network in both  containers\\nservices:\\npgdatabase:\\nimage: postgres:13\\nenvironment:\\n- POSTGRES_USER=root\\n- POSTGRES_PASSWORD=root\\n- POSTGRES_DB=ny_taxi\\nvolumes:\\n- \"./ny_taxi_postgres_data:/var/lib/postgresql/data:rw\"\\nports:\\n- \"5431:5432\"\\nnetworks:\\n- pg-network\\npgadmin:\\nimage: dpage/pgadmin4\\nenvironment:\\n- PGADMIN_DEFAULT_EMAIL=admin@admin.com\\n- PGADMIN_DEFAULT_PASSWORD=root\\nports:\\n- \"8080:80\"\\nnetworks:\\n- pg-network\\nnetworks:\\npg-network:\\nname: pg-network', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker-Compose - Hostname does not resolve', 'course': 'data-engineering-zoomcamp', 'doc_id': 'cd5f39582fb837c66f23571914f8715e'}, {'text': 'So one common issue is when you run docker-compose on GCP, postgres won’t persist it’s data to mentioned path for example:\\nservices:\\n…\\n…\\npgadmin:\\n…\\n…\\nVolumes:\\n“./pgadmin”:/var/lib/pgadmin:wr”\\nMight not work so in this use you can use Docker Volume to make it persist, by simply changing\\nservices:\\n…\\n….\\npgadmin:\\n…\\n…\\nVolumes:\\npgadmin:/var/lib/pgadmin\\nvolumes:\\nPgadmin:', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker-Compose - Persist PGAdmin docker contents on GCP', 'course': 'data-engineering-zoomcamp', 'doc_id': '0103de3a45e2a6037776195e87a4c8e4'}, {'text': 'The docker will keep on crashing continuously\\nNot working after restart\\ndocker engine stopped\\nAnd failed to fetch extensions pop ups will on screen non-stop\\nSolution :\\nTry checking if latest version of docker is installed / Try updating the docker\\nIf Problem still persist then final solution is to reinstall docker\\n(Just have to fetch images again else no issues)', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker engine stopped_failed to fetch extensions', 'course': 'data-engineering-zoomcamp', 'doc_id': '6bfc4d459578c2b5915e9657999d4fad'}, {'text': 'As per the lessons,\\nPersisting pgAdmin configuration (i.e. server name) is done by adding a “volumes” section:\\nservices:\\npgdatabase:\\n[...]\\npgadmin:\\nimage: dpage/pgadmin4\\nenvironment:\\n- PGADMIN_DEFAULT_EMAIL=admin@admin.com\\n- PGADMIN_DEFAULT_PASSWORD=root\\nvolumes:\\n- \"./pgAdmin_data:/var/lib/pgadmin/sessions:rw\"\\nports:\\n- \"8080:80\"\\nIn the example above, ”pgAdmin_data” is a folder on the host machine, and “/var/lib/pgadmin/sessions” is the session settings folder in the pgAdmin container.\\nBefore running docker-compose up on the YAML file, we also need to give the pgAdmin container access to write to the “pgAdmin_data” folder. The container runs with a username called “5050” and user group “5050”. The bash command to give access over the mounted volume is:\\nsudo chown -R 5050:5050 pgAdmin_data', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker-Compose - Persist PGAdmin configuration', 'course': 'data-engineering-zoomcamp', 'doc_id': 'a35444e7d68896d5b61ce6df2d9a47a8'}, {'text': 'This happens if you did not create the docker group and added your user. Follow these steps from the link:\\nguides/docker-without-sudo.md at main · sindresorhus/guides · GitHub\\nAnd then press ctrl+D to log-out and log-in again. pgAdmin: Maintain state so that it remembers your previous connection\\nIf you are tired of having to setup your database connection each time that you fire up the containers, all you have to do is create a volume for pgAdmin:\\nIn your docker-compose.yaml file, enter the following into your pgAdmin declaration:\\nvolumes:\\n- type: volume\\nsource: pgadmin_data\\ntarget: /var/lib/pgadmin\\nAlso add the following to the end of the file:ls\\nvolumes:\\nPgadmin_data:', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker-Compose - dial unix /var/run/docker.sock: connect: permission denied', 'course': 'data-engineering-zoomcamp', 'doc_id': 'e01623d2ae1685ee89e1960a99b997cd'}, {'text': 'This is happen to me after following 1.4.1 video where we are installing docker compose in our Google Cloud VM. In my case, the docker-compose file downloaded from github named docker-compose-linux-x86_64 while it is more convenient to use docker-compose command instead. So just change the docker-compose-linux-x86_64 into docker-compose.', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker-Compose - docker-compose still not available after changing .bashrc', 'course': 'data-engineering-zoomcamp', 'doc_id': 'd52183c3c3682727977fbfb8d5c7991c'}, {'text': 'Installing pass via ‘sudo apt install pass’ helped to solve the issue. More about this can be found here: https://github.com/moby/buildkit/issues/1078', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker-Compose - Error getting credentials after running docker-compose up -d', 'course': 'data-engineering-zoomcamp', 'doc_id': '46cf484a8e6061e243b8a6c302fe5773'}, {'text': \"For everyone who's having problem with Docker compose, getting the data in postgres and similar issues, please take care of the following:\\ncreate a new volume on docker (either using the command line or docker desktop app)\\nmake the following changes to your docker-compose.yml file (see attachment)\\nset low_memory=false when importing the csv file (df = pd.read_csv('yellow_tripdata_2021-01.csv', nrows=1000, low_memory=False))\\nuse the below function (in the upload-data.ipynb) for better tracking of your ingestion process (see attachment)\\nOrder of execution:\\n(1) open terminal in 2_docker_sql folder and run docker compose up\\n(2) ensure no other containers are running except the one you just executed (pgadmin and pgdatabase)\\n(3) open jupyter notebook and begin the data ingestion\\n(4) open pgadmin and set up a server (make sure you use the same configurations as your docker-compose.yml file like the same name (pgdatabase), port, databasename (ny_taxi) etc.\", 'section': 'Module 1: Docker and Terraform', 'question': 'Docker-Compose - Errors pertaining to docker-compose.yml and pgadmin setup', 'course': 'data-engineering-zoomcamp', 'doc_id': 'b5f20c6db58fcb09be95a2ca60a8780a'}, {'text': 'Locate config.json file for docker (check your home directory; Users/username/.docker).\\nModify credsStore to credStore\\nSave and re-run', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker Compose up -d error getting credentials - err: exec: \"docker-credential-desktop\": executable file not found in %PATH%, out: ``', 'course': 'data-engineering-zoomcamp', 'doc_id': '0abf41c5114a6dfbbae5c5670b748607'}, {'text': 'To figure out which docker-compose you need to download from https://github.com/docker/compose/releases you can check your system with these commands:\\nuname -s  -> return Linux most likely\\nuname -m -> return \"flavor\"\\nOr try this command -\\nsudo curl -L \"https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker-Compose - Which docker-compose binary to use for WSL?', 'course': 'data-engineering-zoomcamp', 'doc_id': '3806ea67375141d89d62dc0d0d1d1940'}, {'text': 'If you wrote the docker-compose.yaml file exactly like the video, you might run into an error like this:dev\\nservice \"pgdatabase\" refers to undefined volume dtc_postgres_volume_local: invalid compose project\\nIn order to make it work, you need to include the volume in your docker-compose file. Just add the following:\\nvolumes:\\ndtc_postgres_volume_local:\\n(Make sure volumes are at the same level as services.)', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker-Compose - Error undefined volume in Windows/WSL', 'course': 'data-engineering-zoomcamp', 'doc_id': '7e8ff37a5bb11dba5470c2c81f3755a3'}, {'text': 'Error:  initdb: error: could not change permissions of directory\\nIssue: WSL and Windows do not manage permissions in the same way causing conflict if using the Windows file system rather than the WSL file system.\\nSolution: Use Docker volumes.\\nWhy: Volume is used for storage of persistent data and not for use of transferring files. A local volume is unnecessary.\\nBenefit: This resolves permission issues and allows for better management of volumes.\\nNOTE: the ‘user:’ is not necessary if using docker volumes, but is if using local drive.\\n</>  docker-compose.yaml\\nservices:\\npostgres:\\nimage: postgres:15-alpine\\ncontainer_name: postgres\\nuser: \"0:0\"\\nenvironment:\\n- POSTGRES_USER=postgres\\n- POSTGRES_PASSWORD=postgres\\n- POSTGRES_DB=ny_taxi\\nvolumes:\\n- \"pg-data:/var/lib/postgresql/data\"\\nports:\\n- \"5432:5432\"\\nnetworks:\\n- pg-network\\npgadmin:\\nimage: dpage/pgadmin4\\ncontainer_name: pgadmin\\nuser: \"${UID}:${GID}\"\\nenvironment:\\n- PGADMIN_DEFAULT_EMAIL=email@some-site.com\\n- PGADMIN_DEFAULT_PASSWORD=pgadmin\\nvolumes:\\n- \"pg-admin:/var/lib/pgadmin\"\\nports:\\n- \"8080:80\"\\nnetworks:\\n- pg-network\\nnetworks:\\npg-network:\\nname: pg-network\\nvolumes:\\npg-data:\\nname: ingest_pgdata\\npg-admin:\\nname: ingest_pgadmin', 'section': 'Module 1: Docker and Terraform', 'question': 'WSL Docker directory permissions error', 'course': 'data-engineering-zoomcamp', 'doc_id': '738a0fec53294dcda0d394ab868f24d5'}, {'text': 'Cause : If Running on git bash or vm in windows pgadmin doesnt work easily LIbraries like psycopg2 and libpq ar required still the error persists.\\nSolution- I use psql instead of pgadmin totally same\\nPip install psycopg2\\ndock', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - If pgadmin is not working for Querying in Postgres Use PSQL', 'course': 'data-engineering-zoomcamp', 'doc_id': '5118357924412cfb8a903a2a9ac416e8'}, {'text': 'Cause:\\nIt happens because the apps are not updated. To be specific, search for any pending updates for Windows Terminal, WSL and Windows Security updates.\\nSolution\\nfor updating Windows terminal which worked for me:\\nGo to Microsoft Store.\\nGo to the library of apps installed in your system.\\nSearch for Windows terminal.\\nUpdate the app and restart your system to  see the changes.\\nFor updating the Windows security updates:\\nGo to Windows updates and check if there are any pending updates from Windows, especially security updates.\\nDo restart your system once the updates are downloaded and installed successfully.', 'section': 'Module 1: Docker and Terraform', 'question': 'WSL - Insufficient system resources exist to complete the requested service.', 'course': 'data-engineering-zoomcamp', 'doc_id': 'b356773f0fdfa7e80e8379ec97de86b0'}, {'text': 'Up restardoting the same issue appears. Happens out of the blue on windows.\\nSolution 1: Fixing DNS Issue (credit: reddit) this worked for me personally\\nreg add \"HKLM\\\\System\\\\CurrentControlSet\\\\Services\\\\Dnscache\" /v \"Start\" /t REG_DWORD /d \"4\" /f\\nRestart your computer and then enable it with the following\\nreg add \"HKLM\\\\System\\\\CurrentControlSet\\\\Services\\\\Dnscache\" /v \"Start\" /t REG_DWORD /d \"2\" /f\\nRestart your OS again. It should work.\\nSolution 2: right click on running Docker icon (next to clock) and chose \"Switch to Linux containers\"\\nbash: conda: command not found\\nDatabase is uninitialized and superuser password is not specified.\\nDatabase is uninitialized and superuser password is not specified.', 'section': 'Module 1: Docker and Terraform', 'question': 'WSL - WSL integration with distro Ubuntu unexpectedly stopped with exit code 1.', 'course': 'data-engineering-zoomcamp', 'doc_id': 'a05dd29d16052d5e6944af79b86471e2'}, {'text': 'Issue when trying to run the GPC VM through SSH through WSL2,  probably because WSL2 isn’t looking for .ssh keys in the correct folder. My case I was trying to run this command in the terminal and getting an error\\nPC:/mnt/c/Users/User/.ssh$ ssh -i gpc [username]@[my external IP]\\nYou can try to use sudo before the command\\nSudo .ssh$ ssh -i gpc [username]@[my external IP]\\nYou can also try to cd to your folder and change the permissions for the private key SSH file.\\nchmod 600 gpc\\nIf that doesn’t work, create a .ssh folder in the home diretory of WSL2 and copy the content of windows .ssh folder to that new folder.\\ncd ~\\nmkdir .ssh\\ncp -r /mnt/c/Users/YourUsername/.ssh/* ~/.ssh/\\nYou might need to adjust the permissions of the files and folders in the .ssh directory.', 'section': 'Module 1: Docker and Terraform', 'question': 'WSL - Permissions too open at Windows', 'course': 'data-engineering-zoomcamp', 'doc_id': 'cfa2686924e94e02180b430cb5556481'}, {'text': 'Such as the issue above, WSL2 may not be referencing the correct .ssh/config path from Windows. You can create a config file at the home directory of WSL2.\\ncd ~\\nmkdir .ssh\\nCreate a config file in this new .ssh/ folder referencing this folder:\\nHostName [GPC VM external IP]\\nUser [username]\\nIdentityFile ~/.ssh/[private key]', 'section': 'Module 1: Docker and Terraform', 'question': 'WSL - Could not resolve host name', 'course': 'data-engineering-zoomcamp', 'doc_id': '766825e6d472908d617f6287be837d22'}, {'text': 'Change TO Socket\\npgcli -h 127.0.0.1 -p 5432 -u root -d ny_taxi\\npgcli -h 127.0.0.1 -p 5432 -u root -d ny_taxi', 'section': 'Module 1: Docker and Terraform', 'question': 'PGCLI - connection failed: :1), port 5432 failed: could not receive data from server: Connection refused could not send SSL negotiation packet: Connection refused', 'course': 'data-engineering-zoomcamp', 'doc_id': '8461cae11d9a724ddd21d69fe0da75e8'}, {'text': 'probably some installation error, check out sy', 'section': 'Module 1: Docker and Terraform', 'question': 'PGCLI --help error', 'course': 'data-engineering-zoomcamp', 'doc_id': '2b529f6b4ece1e2fa1817993427da1dc'}, {'text': 'In this section of the course, the 5432 port of pgsql is mapped to your computer’s 5432 port. Which means you can access the postgres database via pgcli directly from your computer.\\nSo No, you don’t need to run it inside another container. Your local system will do.', 'section': 'Module 1: Docker and Terraform', 'question': 'PGCLI - INKhould we run pgcli inside another docker container?', 'course': 'data-engineering-zoomcamp', 'doc_id': '8657e19f6e95966969fe364abe7d3ce2'}, {'text': 'FATAL:  password authentication failed for user \"root\"\\nobservations: Below in bold do not forget the folder that was created ny_taxi_postgres_data\\nThis happens if you have a local Postgres installation in your computer. To mitigate this, use a different port, like 5431, when creating the docker container, as in: -p 5431: 5432\\nThen, we need to use this port when connecting to pgcli, as shown below:\\npgcli -h localhost -p 5431 -u root -d ny_taxi\\nThis will connect you to your postgres docker container, which is mapped to your host’s 5431 port (though you might choose any port of your liking as long as it is not occupied).\\nFor a more visual and detailed explanation, feel free to check the video 1.4.2 - Port Mapping and Networks in Docker\\nIf you want to debug: the following can help (on a MacOS)\\nTo find out if something is blocking your port (on a MacOS):\\nYou can use the lsof command to find out which application is using a specific port on your local machine. `lsof -i :5432`wi\\nOr list the running postgres services on your local machine with launchctl\\nTo unload the running service on your local machine (on a MacOS):\\nunload the launch agent for the PostgreSQL service, which will stop the service and free up the port  \\n`launchctl unload -w ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist`\\nthis one to start it again\\n`launchctl load -w ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist`\\nChanging port from 5432:5432 to 5431:5432 helped me to avoid this error.', 'section': 'Module 1: Docker and Terraform', 'question': 'PGCLI - FATAL: password authentication failed for user \"root\" (You already have Postgres)', 'course': 'data-engineering-zoomcamp', 'doc_id': '0d0995ec00a0eb51e3f38fab144c133a'}, {'text': 'I get this error\\npgcli -h localhost -p 5432 -U root -d ny_taxi\\nTraceback (most recent call last):\\nFile \"/opt/anaconda3/bin/pgcli\", line 8, in <module>\\nsys.exit(cli())\\nFile \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 1128, in __call__\\nreturn self.main(*args, **kwargs)\\nFile \"/opt/anaconda3/lib/python3.9/sitYe-packages/click/core.py\", line\\n1053, in main\\nrv = self.invoke(ctx)\\nFile \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 1395, in invoke\\nreturn ctx.invoke(self.callback, **ctx.params)\\nFile \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 754, in invoke\\nreturn __callback(*args, **kwargs)\\nFile \"/opt/anaconda3/lib/python3.9/site-packages/pgcli/main.py\", line 880, in cli\\nos.makedirs(config_dir)\\nFile \"/opt/anaconda3/lib/python3.9/os.py\", line 225, in makedirspython\\nmkdir(name, mode)PermissionError: [Errno 13] Permission denied: \\'/Users/vray/.config/pgcli\\'\\nMake sure you install pgcli without sudo.\\nThe recommended approach is to use conda/anaconda to make sure your system python is not affected.\\nIf conda install gets stuck at \"Solving environment\" try these alternatives: https://stackoverflow.com/questions/63734508/stuck-at-solving-environment-on-anaconda', 'section': 'Module 1: Docker and Terraform', 'question': \"PGCLI - PermissionError: [Errno 13] Permission denied: '/some/path/.config/pgcli'\", 'course': 'data-engineering-zoomcamp', 'doc_id': '9aeef98f6e5d4bb793b02167b1ad9b9c'}, {'text': 'ImportError: no pq wrapper available.\\nAttempts made:\\n- couldn\\'t import \\\\dt\\nopg \\'c\\' implementation: No module named \\'psycopg_c\\'\\n- couldn\\'t import psycopg \\'binary\\' implementation: No module named \\'psycopg_binary\\'\\n- couldn\\'t import psycopg \\'python\\' implementation: libpq library not found\\nSolution:\\nFirst, make sure your Python is set to 3.9, at least.\\nAnd the reason for that is we have had cases of \\'psycopg2-binary\\' failing to install because of an old version of Python (3.7.3). \\n\\n0. You can check your current python version with: \\n$ python -V(the V must be capital)\\n1. Based on the previous output, if you\\'ve got a 3.9, skip to Step #2\\n   Otherwispye better off with a new environment with 3.9\\n$ conda create –name de-zoomcamp python=3.9\\n$ conda activate de-zoomcamp\\n2. Next, you should be able to install the lib for postgres like this:\\n```\\n$ e\\n$ pip install psycopg2_binary\\n```\\n3. Finally, make sure you\\'re also installing pgcli, but use conda for that:\\n```\\n$ pgcli -h localhost -U root -d ny_taxisudo\\n```\\nThere, you should be good to go now!\\nAnother solution:\\nRun this\\npip install \"psycopg[binary,pool]\"', 'section': 'Module 1: Docker and Terraform', 'question': 'PGCLI - no pq wrapper available.', 'course': 'data-engineering-zoomcamp', 'doc_id': '9c3d1319cf17c5448fbe281beb82273c'}, {'text': 'If your Bash prompt is stuck on the password command for postgres\\nUse winpty:\\nwinpty pgcli -h localhost -p 5432 -u root -d ny_taxi\\nAlternatively, try using Windows terminal or terminal in VS code.\\nEditPGCLI -connection failed: FATAL:  password authentication failed for user \"root\"\\nThe error above was faced continually despite inputting the correct password\\nSolution\\nOption 1: Stop the PostgreSQL service on Windows\\nOption 2 (using WSL): Completely uninstall Protgres 12 from Windows and install postgresql-client on WSL (sudo apt install postgresql-client-common postgresql-client libpq-dev)\\nOption 3: Change the port of the docker container\\nNEW SOLUTION: 27/01/2024\\nPGCLI -connection failed: FATAL:  password authentication failed for user \"root\"\\nIf you’ve got the error above, it’s probably because you were just like me, closed the connection to the Postgres:13 image in the previous step of the tutorial, which is\\n\\ndocker run -it \\\\\\n-e POSTGRES_USER=root \\\\\\n-e POSTGRES_PASSWORD=root \\\\\\n-e POSTGRES_DB=ny_taxi \\\\\\n-v d:/git/data-engineering-zoomcamp/week_1/docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n-p 5432:5432 \\\\\\npostgres:13\\nSo keep the database connected and you will be able to implement all the next steps of the tutorial.', 'section': 'Module 1: Docker and Terraform', 'question': 'PGCLI -  stuck on password prompt', 'course': 'data-engineering-zoomcamp', 'doc_id': '4d8e1ce0bfa05b09f7506758f11fd97a'}, {'text': 'Problem: If you have already installed pgcli but bash doesn\\'t recognize pgcli\\nOn Git bash: bash: pgcli: command not found\\nOn Windows Terminal: pgcli: The term \\'pgcli\\' is not recognized…\\nSolution: Try adding a Python path C:\\\\Users\\\\...\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\Scripts to Windows PATH\\nFor details:\\nGet the location: pip list -v\\nCopy C:\\\\Users\\\\...\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\site-packages\\n3. Replace site-packages with Scripts: C:\\\\Users\\\\...\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\Scripts\\nIt can also be that you have Python installed elsewhere.\\nFor me it was under c:\\\\python310\\\\lib\\\\site-packages\\nSo I had to add c:\\\\python310\\\\lib\\\\Scripts to PATH, as shown below.\\nPut the above path in \"Path\" (or \"PATH\") in System Variables\\nReference: https://stackoverflow.com/a/68233660', 'section': 'Module 1: Docker and Terraform', 'question': 'PGCLI - pgcli: command not found', 'course': 'data-engineering-zoomcamp', 'doc_id': 'da8a8b98320a89ff9299c5492705d52e'}, {'text': 'In case running pgcli  locally causes issues or you do not want to install it locally you can use it running in a Docker container instead.\\nBelow the usage with values used in the videos of the course for:\\nnetwork name (docker network)\\npostgres related variables for pgcli\\nHostname\\nUsername\\nPort\\nDatabase name\\n$ docker run -it --rm --network pg-network ai2ys/dockerized-pgcli:4.0.1\\n175dd47cda07:/# pgcli -h pg-database -U root -p 5432 -d ny_taxi\\nPassword for root:\\nServer: PostgreSQL 16.1 (Debian 16.1-1.pgdg120+1)\\nVersion: 4.0.1\\nHome: http://pgcli.com\\nroot@pg-database:ny_taxi> \\\\dt\\n+--------+------------------+-------+-------+\\n| Schema | Name             | Type  | Owner |\\n|--------+------------------+-------+-------|\\n| public | yellow_taxi_data | table | root  |\\n+--------+------------------+-------+-------+\\nSELECT 1\\nTime: 0.009s\\nroot@pg-database:ny_taxi>', 'section': 'Module 1: Docker and Terraform', 'question': 'PGCLI - running in a Docker container', 'course': 'data-engineering-zoomcamp', 'doc_id': '0bf9851e18246f955c3993427b0180c4'}, {'text': 'PULocationID will not be recognized but “PULocationID” will be. This is because unquoted \"Localidentifiers are case insensitive. See docs.', 'section': 'Module 1: Docker and Terraform', 'question': 'PGCLI - case sensitive use “Quotations” around columns with capital letters', 'course': 'data-engineering-zoomcamp', 'doc_id': '474e5cb0387de8a3fa98663235ab6d0e'}, {'text': 'When using the command `\\\\d <database name>` you get the error column `c.relhasoids does not exist`.\\nResolution:\\nUninstall pgcli\\nReinstall pgclidatabase \"ny_taxi\" does not exist\\nRestart pc', 'section': 'Module 1: Docker and Terraform', 'question': 'PGCLI - error column c.relhasoids does not exist', 'course': 'data-engineering-zoomcamp', 'doc_id': '47c218676f2ca86c5714d1bd3f9120fb'}, {'text': \"This happens while uploading data via the connection in jupyter notebook\\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\\nThe port 5432 was taken by another postgres. We are not connecting to the port in docker, but to the port on our machine. Substitute 5431 or whatever port you mapped to for port 5432.\\nAlso if this error is still persistent , kindly check if you have a service in windows running postgres , Stopping that service will resolve the issue\", 'section': 'Module 1: Docker and Terraform', 'question': 'Postgres - OperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  password authentication failed for user \"root\"', 'course': 'data-engineering-zoomcamp', 'doc_id': 'd665e7a40bc0b83e407ca3c1e1c64fb0'}, {'text': 'Can happen when connecting via pgcli\\npgcli -h localhost -p 5432 -U root -d ny_taxi\\nOr while uploading data via the connection in jupyter notebook\\nengine = create_engine(\\'postgresql://root:root@localhost:5432/ny_taxi\\')\\nThis can happen when Postgres is already installed on your computer. Changing the port can resolve that (e.g. from 5432 to 5431).\\nTo check whether there even is a root user with the ability to login:\\nTry: docker exec -it <your_container_name> /bin/bash\\nAnd then run\\n???\\nAlso, you could change port from 5432:5432 to 5431:5432\\nOther solution that worked:\\nChanging `POSTGRES_USER=juroot` to `PGUSER=postgres`\\nBased on this: postgres with docker compose gives FATAL: role \"root\" does not exist error - Stack Overflow\\nAlso `docker compose down`, removing folder that had postgres volume, running `docker compose up` again.', 'section': 'Module 1: Docker and Terraform', 'question': 'Postgres - OperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  role \"root\" does not exist', 'course': 'data-engineering-zoomcamp', 'doc_id': '1976886c64619cf18931254f88831eb8'}, {'text': '~\\\\anaconda3\\\\lib\\\\site-packages\\\\psycopg2\\\\__init__.py in connect(dsn, connection_factory, cursor_factory, **kwargs)\\n120\\n121     dsn = _ext.make_dsn(dsn, **kwargs)\\n--> 122     conn = _connect(dsn, connection_factory=connection_factory, **kwasync)\\n123     if cursor_factory is not None:\\n124         conn.cursor_factory = cursor_factory\\nOperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  database \"ny_taxi\" does not exist\\nMake sure postgres is running. You can check that by running `docker ps`\\n✅Solution: If you have postgres software installed on your computer before now, build your instance on a different port like 8080 instead of 5432', 'section': 'Module 1: Docker and Terraform', 'question': 'Postgres - OperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  dodatabase \"ny_taxi\" does not exist', 'course': 'data-engineering-zoomcamp', 'doc_id': '593d1694e3f338425e3e749f9f819ae4'}, {'text': \"Issue:\\ne…\\nSolution:\\npip install psycopg2-binary\\nIf you already have it, you might need to update it:\\npip install psycopg2-binary --upgrade\\nOther methods, if the above fails:\\nif you are getting the “ ModuleNotFoundError: No module named 'psycopg2' “ error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\\nFirst uninstall the psycopg package\\nThen update conda or pip\\nThen install psycopg again using pip.\\nif you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\", 'section': 'Module 1: Docker and Terraform', 'question': \"Postgres - ModuleNotFoundError: No module named 'psycopg2'\", 'course': 'data-engineering-zoomcamp', 'doc_id': 'b3903d19ed9c8d403dab875132c243ad'}, {'text': 'In the join queries, if we mention the column name directly or enclosed in single quotes it’ll throw an error says “column does not exist”.\\n✅Solution: But if we enclose the column names in double quotes then it will work', 'section': 'Module 1: Docker and Terraform', 'question': 'Postgres - \"Column does not exist\" but it actually does (Pyscopg2 error in MacBook Pro M2)', 'course': 'data-engineering-zoomcamp', 'doc_id': '55c8c14fd799533f29123b58bdc6aef1'}, {'text': 'pgAdmin has a new version. Create server dialog may not appear. Try using register-> server instead.', 'section': 'Module 1: Docker and Terraform', 'question': 'pgAdmin - Create server dialog does not appear', 'course': 'data-engineering-zoomcamp', 'doc_id': '7cb12c69e4689e432f52fe3353d80838'}, {'text': 'Using GitHub Codespaces in the browser resulted in a blank screen after the login to pgAdmin (running in a Docker container). The terminal of the pgAdmin container was showing the following error message:\\nCSRFError: 400 Bad Request: The referrer does not match the host.\\nSolution #1:\\nAs recommended in the following issue  https://github.com/pgadmin-org/pgadmin4/issues/5432 setting the following environment variable solved it.\\nPGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\"\\nModified “docker run” command\\ndocker run --rm -it \\\\\\n-e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\\\\n-e PGADMIN_DEFAULT_PASSWORD=\"root\" \\\\\\n-e PGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\" \\\\\\n-p \"8080:80\" \\\\\\n--name pgadmin \\\\\\n--network=pg-network \\\\\\ndpage/pgadmin4:8.2\\nSolution #2:\\nUsing the local installed VSCode to display GitHub Codespaces.\\nWhen using GitHub Codespaces in the locally installed VSCode (opening a Codespace or creating/starting one) this issue did not occur.', 'section': 'Module 1: Docker and Terraform', 'question': 'pgAdmin - Blank/white screen after login (browser)', 'course': 'data-engineering-zoomcamp', 'doc_id': 'd434b352c745f11ba24099850947f8e9'}, {'text': 'I am using a Mac Pro device and connect to the GCP Compute Engine via Remote SSH - VSCode. But when I trying to run the PgAdmin container via docker run or docker compose command, I am failed to access the pgAdmin address via my browser. I have switched to another browser, but still can not access the pgAdmin address. So I modified a little bit the configuration from the previous DE Zoomcamp repository like below and can access the pgAdmin address:\\nSolution #1:\\nModified “docker run” command\\ndocker run --rm -it \\\\\\n-e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\\\\n-e PGADMIN_DEFAULT_PASSWORD=\"pgadmin\" \\\\\\n-e PGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\" \\\\\\n-e PGADMIN_LISTEN_ADDRESS=0.0.0.0 \\\\\\n-e PGADMIN_LISTEN_PORT=5050 \\\\\\n-p 5050:5050 \\\\\\n--network=de-zoomcamp-network \\\\\\n--name pgadmin-container \\\\\\n--link postgres-container \\\\\\n-t dpage/pgadmin4\\nSolution #2:\\nModified docker-compose.yaml configuration (via “docker compose up” command)\\npgadmin:\\nimage: dpage/pgadmin4\\ncontainer_name: pgadmin-conntainer\\nenvironment:\\n- PGADMIN_DEFAULT_EMAIL=admin@admin.com\\n- PGADMIN_DEFAULT_PASSWORD=pgadmin\\n- PGADMIN_CONFIG_WTF_CSRF_ENABLED=False\\n- PGADMIN_LISTEN_ADDRESS=0.0.0.0\\n- PGADMIN_LISTEN_PORT=5050\\nvolumes:\\n- \"./pgadmin_data:/var/lib/pgadmin/data\"\\nports:\\n- \"5050:5050\"\\nnetworks:\\n- de-zoomcamp-network\\ndepends_on:\\n- postgres-conntainer\\nPython - ModuleNotFoundError: No module named \\'pysqlite2\\'\\nImportError: DLL load failed while importing _sqlite3: The specified module could not be found. ModuleNotFoundError: No module named \\'pysqlite2\\'\\nThe issue seems to arise from the missing of sqlite3.dll in path \".\\\\Anaconda\\\\Dlls\\\\\".\\n✅I solved it by simply copying that .dll file from \\\\Anaconda3\\\\Library\\\\bin and put it under the path mentioned above. (if you are using anaconda)', 'section': 'Module 1: Docker and Terraform', 'question': 'pgAdmin - Can not access/open the PgAdmin address via browser', 'course': 'data-engineering-zoomcamp', 'doc_id': '4eadd1fb6a425a8d92ef1b50ca897eff'}, {'text': 'If you follow the video 1.2.2 - Ingesting NY Taxi Data to Postgres and you execute all the same\\nsteps as Alexey does, you will ingest all the data (~1.3 million rows) into the table yellow_taxi_data as expected.\\nHowever, if you try to run the whole script in the Jupyter notebook for a second time from top to bottom, you will be missing the first chunk of 100000 records. This is because there is a call to the iterator before the while loop that puts the data in the table. The while loop therefore starts by ingesting the second chunk, not the first.\\n✅Solution: remove the cell “df=next(df_iter)” that appears higher up in the notebook than the while loop. The first time w(df_iter) is called should be within the while loop.\\n📔Note: As this notebook is just used as a way to test the code, it was not intended to be run top to bottom, and the logic is tidied up in a later step when it is instead inserted into a .py file for the pipeline', 'section': 'Module 1: Docker and Terraform', 'question': 'Python - Ingestion with Jupyter notebook - missing 100000 records', 'course': 'data-engineering-zoomcamp', 'doc_id': '838d870c752e8d210b37db869c71ac61'}, {'text': '{t_end - t_start} seconds\")\\nimport pandas as pd\\ndf = pd.read_csv(\\'path/to/file.csv.gz\\', /app/ingest_data.py:1: DeprecationWarning:)\\nIf you prefer to keep the uncompressed csv (easier preview in vscode and similar), gzip files can be unzipped using gunzip (but not unzip). On a Ubuntu local or virtual machine, you may need to apt-get install gunzip first.', 'section': 'Module 1: Docker and Terraform', 'question': 'Python - Iteration csv without error', 'course': 'data-engineering-zoomcamp', 'doc_id': 'dae5f85b563f21eac8ff6d80e4979f22'}, {'text': \"Pandas can interpret “string” column values as “datetime” directly when reading the CSV file using “pd.read_csv” using the parameter “parse_dates”, which for example can contain a list of column names or column indices. Then the conversion afterwards is not required anymore.\\npandas.read_csv — pandas 2.1.4 documentation (pydata.org)\\nExample from week 1\\nimport pandas as pd\\ndf = pd.read_csv(\\n'yellow_tripdata_2021-01.csv',\\nnrows=100,\\nparse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\\ndf.info()\\nwhich will output\\n<class 'pandas.core.frame.DataFrame'>\\nRangeIndex: 100 entries, 0 to 99\\nData columns (total 18 columns):\\n#   Column                 Non-Null Count  Dtype\\n---  ------                 --------------  -----\\n0   VendorID               100 non-null    int64\\n1   tpep_pickup_datetime   100 non-null    datetime64[ns]\\n2   tpep_dropoff_datetime  100 non-null    datetime64[ns]\\n3   passenger_count        100 non-null    int64\\n4   trip_distance          100 non-null    float64\\n5   RatecodeID             100 non-null    int64\\n6   store_and_fwd_flag     100 non-null    object\\n7   PULocationID           100 non-null    int64\\n8   DOLocationID           100 non-null    int64\\n9   payment_type           100 non-null    int64\\n10  fare_amount            100 non-null    float64\\n11  extra                  100 non-null    float64\\n12  mta_tax                100 non-null    float64\\n13  tip_amount             100 non-null    float64\\n14  tolls_amount           100 non-null    float64\\n15  improvement_surcharge  100 non-null    float64\\n16  total_amount           100 non-null    float64\\n17  congestion_surcharge   100 non-null    float64\\ndtypes: datetime64[ns](2), float64(9), int64(6), object(1)\\nmemory usage: 14.2+ KB\", 'section': 'Module 1: Docker and Terraform', 'question': 'iPython - Pandas parsing dates with ‘read_csv’', 'course': 'data-engineering-zoomcamp', 'doc_id': 'cac3fadd8ba053b07a34dd33e23d47d7'}, {'text': 'os.system(f\"curl -LO {url} -o {csv_name}\")', 'section': 'Module 1: Docker and Terraform', 'question': 'Python - Python cant ingest data from the github link provided using curl', 'course': 'data-engineering-zoomcamp', 'doc_id': 'd4d3bcb0e4486b4423f29a40da25d631'}, {'text': 'When a CSV file is compressed using Gzip, it is saved with a \".csv.gz\" file extension. This file type is also known as a Gzip compressed CSV file. When you want to read a Gzip compressed CSV file using Pandas, you can use the read_csv() function, which is specifically designed to read CSV files. The read_csv() function accepts several parameters, including a file path or a file-like object. To read a Gzip compressed CSV file, you can pass the file path of the \".csv.gz\" file as an argument to the read_csv() function.\\nHere is an example of how to read a Gzip compressed CSV file using Pandas:\\ndf = pd.read_csv(\\'file.csv.gz\\'\\n, compression=\\'gzip\\'\\n, low_memory=False\\n)', 'section': 'Module 1: Docker and Terraform', 'question': 'Python - Pandas can read *.csv.gzip', 'course': 'data-engineering-zoomcamp', 'doc_id': 'd3fdf1cf741b217b53e694b3737e1839'}, {'text': \"Contrary to panda’s read_csv method there’s no such easy way to iterate through and set chunksize for parquet files. We can use PyArrow (Apache Arrow Python bindings) to resolve that.\\nimport pyarrow.parquet as pq\\noutput_name = “https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet”\\nparquet_file = pq.ParquetFile(output_name)\\nparquet_size = parquet_file.metadata.num_rows\\nengine = create_engine(f'postgresql://{user}:{password}@{host}:{port}/{db}')\\ntable_name=”yellow_taxi_schema”\\n# Clear table if exists\\npq.read_table(output_name).to_pandas().head(n=0).to_sql(name=table_name, con=engine, if_exists='replace')\\n# default (and max) batch size\\nindex = 65536\\nfor i in parquet_file.iter_batches(use_threads=True):\\nt_start = time()\\nprint(f'Ingesting {index} out of {parquet_size} rows ({index / parquet_size:.0%})')\\ni.to_pandas().to_sql(name=table_name, con=engine, if_exists='append')\\nindex += 65536\\nt_end = time()\\nprint(f'\\\\t- it took %.1f seconds' % (t_end - t_start))\", 'section': 'Module 1: Docker and Terraform', 'question': 'Python - How to iterate through and ingest parquet file', 'course': 'data-engineering-zoomcamp', 'doc_id': '7bba89ed698a8f4f950ad2506e085967'}, {'text': 'Error raised during the jupyter notebook’s cell execution:\\nfrom sqlalchemy import create_engine.\\nSolution: Version of Python module “typing_extensions” >= 4.6.0. Can be updated by Conda or pip.', 'section': 'Module 1: Docker and Terraform', 'question': \"Python - SQLAlchemy - ImportError: cannot import name 'TypeAliasType' from 'typing_extensions'.\", 'course': 'data-engineering-zoomcamp', 'doc_id': '74e89ec4c6a4118ac167c50e5e7aca03'}, {'text': 'create_engine(\\'postgresql://root:root@localhost:5432/ny_taxi\\')  I get the error \"TypeError: \\'module\\' object is not callable\"\\nSolution:\\nconn_string = \"postgresql+psycopg://root:root@localhost:5432/ny_taxi\"\\nengine = create_engine(conn_string)', 'section': 'Module 1: Docker and Terraform', 'question': \"Python - SQLALchemy - TypeError 'module' object is not callable\", 'course': 'data-engineering-zoomcamp', 'doc_id': 'f27d2763e7ca34ea23375754d4ff8942'}, {'text': \"Error raised during the jupyter notebook’s cell execution:\\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi').\\nSolution: Need to install Python module “psycopg2”. Can be installed by Conda or pip.\", 'section': 'Module 1: Docker and Terraform', 'question': \"Python - SQLAlchemy - ModuleNotFoundError: No module named 'psycopg2'.\", 'course': 'data-engineering-zoomcamp', 'doc_id': '0a696602479b26b83bc0b80851ff21ca'}, {'text': 'Unable to add Google Cloud SDK PATH to Windows\\nWindows error: The installer is unable to automatically update your system PATH. Please add  C:\\\\tools\\\\google-cloud-sdk\\\\bin\\nif you are constantly getting this feedback. Might be that you needed to add Gitbash to your Windows path:\\nOne way of doing that is to use conda: ‘If you are not already using it\\nDownload the Anaconda Navigator\\nMake sure to check the box (add conda to the path when installing navigator: although not recommended do it anyway)\\nYou might also need to install git bash if you are not already using it(or you might need to uninstall it to reinstall it properly)\\nMake sure to check the following boxes while you install Gitbash\\nAdd a GitBash to Windows Terminal\\nUse Git and optional Unix tools from the command prompt\\nNow open up git bash and type conda init bash This should modify your bash profile\\nAdditionally, you might want to use Gitbash as your default terminal.\\nOpen your Windows terminal and go to settings, on the default profile change Windows power shell to git bash', 'section': 'Module 1: Docker and Terraform', 'question': 'GCP - Unable to add Google Cloud SDK PATH to Windows', 'course': 'data-engineering-zoomcamp', 'doc_id': '1f1d169182750a5cd92d6705ffe705db'}, {'text': 'It asked me to create a project. This should be done from the cloud console. So maybe we don’t need this FAQ.\\nWARNING: Project creation failed: HttpError accessing <https://cloudresourcemanager.googleapis.com/v1/projects?alt=json>: response: <{\\'vtpep_pickup_datetimeary\\': \\'Origin, X-Origin, Referer\\', \\'content-type\\': \\'application/json; charset=UTF-8\\', \\'content-encoding\\': \\'gzip\\', \\'date\\': \\'Mon, 24 Jan 2022 19:29:12 GMT\\', \\'server\\': \\'ESF\\', \\'cache-control\\': \\'private\\', \\'x-xss-protection\\': \\'0\\', \\'x-frame-options\\': \\'SAMEORIGIN\\', \\'x-content-type-options\\': \\'nosniff\\', \\'server-timing\\': \\'gfet4t7; dur=189\\', \\'alt-svc\\': \\'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000,h3-Q050=\":443\"; ma=2592000,h3-Q046=\":443\"; ma=2592000,h3-Q043=\":443\"; ma=2592000,quic=\":443\"; ma=2592000; v=\"46,43\"\\', \\'transfer-encoding\\': \\'chunked\\', \\'status\\': 409}>, content <{\\n\"error\": {\\n\"code\": 409,\\n\"message\": \"Requested entity alreadytpep_pickup_datetime exists\",\\n\"status\": \"ALREADY_EXISTS\"\\n}\\n}\\nFrom Stackoverflow: https://stackoverflow.com/questions/52561383/gcloud-cli-cannot-create-project-the-project-id-you-specified-is-already-in-us?rq=1\\nProject IDs are unique across all projects. That means if any user ever had a project with that ID, you cannot use it. testproject is pretty common, so it\\'s not surprising it\\'s already taken.', 'section': 'Module 1: Docker and Terraform', 'question': 'GCP - Project creation failed: HttpError accessing … Requested entity alreadytpep_pickup_datetime exists', 'course': 'data-engineering-zoomcamp', 'doc_id': '4ff9c46e52a92b5ce1eebf4fd4dffd33'}, {'text': 'If you receive the error: “Error 403: The project to be billed is associated with an absent billing account., accountDisabled” It is most likely because you did not enter YOUR project ID. The snip below is from video 1.3.2\\nThe value you enter here will be unique to each student. You can find this value on your GCP Dashboard when you login.\\nAshish Agrawal\\nAnother possibility is that you have not linked your billing account to your current project', 'section': 'Module 1: Docker and Terraform', 'question': 'GCP - The project to be billed is associated with an absent billing account', 'course': 'data-engineering-zoomcamp', 'doc_id': '7eec07b94c454090c53fc445b5766c26'}, {'text': 'GCP Account Suspension Inquiry\\nIf Google refuses your credit/debit card, try another - I’ve got an issue with Kaspi (Kazakhstan) but it worked with TBC (Georgia).\\nUnfortunately, there’s small hope that support will help.\\nIt seems that Pyypl web-card should work too.', 'section': 'Module 1: Docker and Terraform', 'question': 'GCP - OR-CBAT-15 ERROR Google cloud free trial account', 'course': 'data-engineering-zoomcamp', 'doc_id': '68f546f4e94d64ebe4a55a50db686ffc'}, {'text': 'The ny-rides.json is your private file in Google Cloud Platform (GCP). \\n\\nAnd here’s the way to find it:\\nGCP -> Select project with your  instance -> IAM & Admin -> Service Accounts Keys tab -> add key, JSON as key type, then click create\\nNote: Once you go into Service Accounts Keys tab, click the email, then you can see the “KEYS” tab where you can add key as a JSON as its key type', 'section': 'Module 1: Docker and Terraform', 'question': 'GCP - Where can I find the “ny-rides.json” file?', 'course': 'data-engineering-zoomcamp', 'doc_id': 'ab1f26c64bbe505158385472c5c6dcf2'}, {'text': 'In this lecture, Alexey deleted his instance in Google Cloud. Do I have to do it?\\nNope. Do not delete your instance in Google Cloud platform. Otherwise, you have to do this twice for the week 1 readings.', 'section': 'Module 1: Docker and Terraform', 'question': 'GCP - Do I need to delete my instance in Google Cloud?', 'course': 'data-engineering-zoomcamp', 'doc_id': '3dc3b920c9043a99c2824813f71dc7be'}, {'text': 'System Resource Usage:\\ntop or htop: Shows real-time information about system resource usage, including CPU, memory, and processes.\\nfree -h: Displays information about system memory usage and availability.\\ndf -h: Shows disk space usage of file systems.\\ndu -h <directory>: Displays disk usage of a specific directory.\\nRunning Processes:\\nps aux: Lists all running processes along with detailed information.\\nNetwork:\\nifconfig or ip addr show: Shows network interface configuration.\\nnetstat -tuln: Displays active network connections and listening ports.\\nHardware Information:\\nlscpu: Displays CPU information.\\nlsblk: Lists block devices (disks and partitions).\\nlshw: Lists hardware configuration.\\nUser and Permissions:\\nwho: Shows who is logged on and their activities.\\nw: Displays information about currently logged-in users and their processes.\\nPackage Management:\\napt list --installed: Lists installed packages (for Ubuntu and Debian-based systems)', 'section': 'Module 1: Docker and Terraform', 'question': 'Commands to inspect the health of your VM:', 'course': 'data-engineering-zoomcamp', 'doc_id': 'd16b86b87ef4c3e81b5b1b5593de6e4e'}, {'text': 'if you’ve got the error\\n│ Error: Error updating Dataset \"projects/<your-project-id>/datasets/demo_dataset\": googleapi: Error 403: Billing has not been enabled for this project. Enable billing at https://console.cloud.google.com/billing. The default table expiration time must be less than 60 days, billingNotEnabled\\nbut you’ve set your billing account indeed, then try to disable billing for the project and enable it again. It worked for ME!', 'section': 'Module 1: Docker and Terraform', 'question': 'Billing account has not been enabled for this project. But you’ve done it indeed!', 'course': 'data-engineering-zoomcamp', 'doc_id': 'f9d95ffad70b2a47c66591a5aa94773d'}, {'text': 'for windows if you having trouble install SDK try follow these steps on the link, if you getting this error:\\nThese credentials will be used by any library that requests Application Default Credentials (ADC).\\nWARNING:\\nCannot find a quota project to add to ADC. You might receive a \"quota exceeded\" or \"API not enabled\" error. Run $ gcloud auth application-default set-quota-project to add a quota project.\\nFor me:\\nI reinstalled the sdk using unzip file “install.bat”,\\nafter successfully checking gcloud version,\\nrun gcloud init to set up project before\\nyou run gcloud auth application-default login\\nhttps://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/1_terraform_gcp/windows.md\\nGCP VM - I cannot get my Virtual Machine to start because GCP has no resources.\\nClick on your VM\\nCreate an image of your VM\\nOn the page of the image, tell GCP to create a new VM instance via the image\\nOn the settings page, change the location', 'section': 'Module 1: Docker and Terraform', 'question': 'GCP - Windows Google Cloud SDK install issue:gcp', 'course': 'data-engineering-zoomcamp', 'doc_id': 'fe2179cb72e3706187b171791c188dd5'}, {'text': 'The reason this video about the GCP VM exists is that many students had problems configuring their env. You can use your own env if it works for you.\\nAnd the advantage of using your own environment is that if you are working in a Github repo where you can commit, you will be able to commit the changes that you do. In the VM the repo is cloned via HTTPS so it is not possible to directly commit, even if you are the owner of the repo.', 'section': 'Module 1: Docker and Terraform', 'question': 'GCP VM - Is it necessary to use a GCP VM? When is it useful?', 'course': 'data-engineering-zoomcamp', 'doc_id': '1b3faa0c54f2a7788eb06c1b871a7ca4'}, {'text': \"I am trying to create a directory but it won't let me do it\\nUser1@DESKTOP-PD6UM8A MINGW64 /\\n$ mkdir .ssh\\nmkdir: cannot create directory ‘.ssh’: Permission denied\\nYou should do it in your home directory. Should be your home (~)\\nLocal. But it seems you're trying to do it in the root folder (/). Should be your home (~)\\nLink to Video 1.4.1\", 'section': 'Module 1: Docker and Terraform', 'question': 'GCP VM - mkdir: cannot create directory ‘.ssh’: Permission denied', 'course': 'data-engineering-zoomcamp', 'doc_id': 'cc1e34dcdf6e567b4c7ddbffd9434e62'}, {'text': \"Failed to save '<file>': Unable to write file 'vscode-remote://ssh-remote+de-zoomcamp/home/<user>/data_engineering_course/week_2/airflow/dags/<file>' (NoPermissions (FileSystemError): Error: EACCES: permission denied, open '/home/<user>/data_engineering_course/week_2/airflow/dags/<file>')\\nYou need to change the owner of the files you are trying to edit via VS Code. You can run the following command to change the ownership.\\nssh\\nsudo chown -R <user> <path to your directory>\", 'section': 'Module 1: Docker and Terraform', 'question': 'GCP VM - Error while saving the file in VM via VS Code', 'course': 'data-engineering-zoomcamp', 'doc_id': '9be68d4032955da7b706d3f0d102647f'}, {'text': 'Question: I connected to my VM perfectly fine last week (ssh) but when I tried again this week, the connection request keeps timing out.\\n✅Answer: Start your VM. Once the VM is running, copy its External IP and paste that into your config file within the ~/.ssh folder.\\ncd ~/.ssh\\ncode config ← this opens the config file in VSCode', 'section': 'Module 1: Docker and Terraform', 'question': '. GCP VM - VM connection request timeout', 'course': 'data-engineering-zoomcamp', 'doc_id': '837fe6c1dda3544931ebf521d053eb5c'}, {'text': '(reference: https://serverfault.com/questions/953290/google-compute-engine-ssh-connect-to-host-ip-port-22-operation-timed-out)Go to edit your VM.\\nGo to section Automation\\nAdd Startup script\\n```\\n#!/bin/bash\\nsudo ufw allow ssh\\n```\\nStop and Start VM.', 'section': 'Module 1: Docker and Terraform', 'question': 'GCP VM -  connect to host port 22 no route to host', 'course': 'data-engineering-zoomcamp', 'doc_id': 'b0d79af2c8720962d8a44bb18d287b64'}, {'text': 'You can easily forward the ports of pgAdmin, postgres and Jupyter Notebook using the built-in tools in Ubuntu and without any additional client:\\nFirst, in the VM machine, launch docker-compose up -d and jupyter notebook in the correct folder.\\nFrom the local machine, execute: ssh -i ~/.ssh/gcp -L 5432:localhost:5432 username@external_ip_of_vm\\nExecute the same command but with ports 8080 and 8888.\\nNow you can access pgAdmin on local machine in browser typing localhost:8080\\nFor Jupyter Notebook, type localhost:8888 in the browser of your local machine. If you have problems with the credentials, it is possible that you have to copy the link with the access token provided in the logs of the terminal of the VM machine when you launched the jupyter notebook command.\\nTo forward both pgAdmin and postgres use, ssh -i ~/.ssh/gcp -L 5432:localhost:5432 -L 8080:localhost:8080 modito@35.197.218.128', 'section': 'Module 1: Docker and Terraform', 'question': 'GCP VM - Port forwarding from GCP without using VS Code', 'course': 'data-engineering-zoomcamp', 'doc_id': '362708e7257a7e78abe1e958c98730d7'}, {'text': 'If you are using MS VS Code and running gcloud in WSL2, when you first try to login to gcp via the gcloud cli gcloud auth application-default login, you will see a message like this, and nothing will happen\\nAnd there might be a prompt to ask if you want to open it via browser, if you click on it, it will open up a page with error message\\nSolution : you should instead hover on the long link, and ctrl + click the long link\\n\\nClick configure Trusted Domains here\\n\\nPopup will appear, pick first or second entry\\nNext time you gcloud auth, the login page should popup via default browser without issues', 'section': 'Module 1: Docker and Terraform', 'question': 'GCP gcloud + MS VS Code - gcloud auth hangs', 'course': 'data-engineering-zoomcamp', 'doc_id': '8c200425dd2261cd9dc3807ce732323b'}, {'text': 'It is an internet connectivity error, terraform is somehow not able to access the online registry. Check your VPN/Firewall settings (or just clear cookies or restart your network). Try terraform init again after this, it should work.', 'section': 'Module 1: Docker and Terraform', 'question': 'Terraform - Error: Failed to query available provider packages │ Could not retrieve the list of available versions for provider hashicorp/google: could not query │ provider registry for registry.terrafogorm.io/hashicorp/google: the request failed after 2 attempts, │ please try again later', 'course': 'data-engineering-zoomcamp', 'doc_id': 'b968f1e7beb645edfc97fcf4580eb7dd'}, {'text': \"The issue was with the network. Google is not accessible in my country, I am using a VPN. And The terminal program does not automatically follow the system proxy and requires separate proxy configuration settings.I opened a Enhanced Mode in Clash, which is a VPN app, and 'terraform apply' works! So if you encounter the same issue, you can ask help for your vpn provider.\", 'section': 'Module 1: Docker and Terraform', 'question': 'Terraform - Error:Post \"https://storage.googleapis.com/storage/v1/b?alt=json&prettyPrint=false&project=coherent-ascent-379901\": oauth2: cannot fetch token: Post \"https://oauth2.googleapis.com/token\": dial tcp 172.217.163.42:443: i/o timeout', 'course': 'data-engineering-zoomcamp', 'doc_id': 'f872f9661869c9fbabed56388c86b53a'}, {'text': 'https://techcommunity.microsoft.com/t5/azure-developer-community-blog/configuring-terraform-on-windows-10-linux-sub-system/ba-p/393845', 'section': 'Module 1: Docker and Terraform', 'question': 'Terraform - Install for WSL', 'course': 'data-engineering-zoomcamp', 'doc_id': 'd56632943fea4ae66a38c17e2df1323b'}, {'text': 'https://github.com/hashicorp/terraform/issues/14513', 'section': 'Module 1: Docker and Terraform', 'question': 'Terraform - Error acquiring the state lock', 'course': 'data-engineering-zoomcamp', 'doc_id': '0886519ec09b02d126814c2da22fd892'}, {'text': 'When running\\nterraform apply\\non wsl2 I\\'ve got this error:\\n│ Error: Post \"https://storage.googleapis.com/storage/v1/b?alt=json&prettyPrint=false&project=<your-project-id>\": oauth2: cannot fetch token: 400 Bad Request\\n│ Response: {\"error\":\"invalid_grant\",\"error_description\":\"Invalid JWT: Token must be a short-lived token (60 minutes) and in a reasonable timeframe. Check your iat and exp values in the JWT claim.\"}\\nIT happens because there may be time desync on your machine which affects computing JWT\\nTo fix this, run the command\\nsudo hwclock -s\\nwhich fixes your system time.\\nReference', 'section': 'Module 1: Docker and Terraform', 'question': 'Terraform - Error 400 Bad Request.  Invalid JWT Token  on WSL.', 'course': 'data-engineering-zoomcamp', 'doc_id': '621a9647244b93237d63e108a1f43051'}, {'text': '│ Error: googleapi: Error 403: Access denied., forbidden\\nYour $GOOGLE_APPLICATION_CREDENTIALS might not be pointing to the correct file \\nrun = export GOOGLE_APPLICATION_CREDENTIALS=~/.gc/YOUR_JSON.json\\nAnd then = gcloud auth activate-service-account --key-file $GOOGLE_APPLICATION_CREDENTIALS', 'section': 'Module 1: Docker and Terraform', 'question': 'Terraform - Error 403 : Access denied', 'course': 'data-engineering-zoomcamp', 'doc_id': '057d96a4e708905c66f01563cc260c86'}, {'text': \"One service account is enough for all the services/resources you'll use in this course. After you get the file with your credentials and set your environment variable, you should be good to go.\", 'section': 'Module 1: Docker and Terraform', 'question': 'Terraform - Do I need to make another service account for terraform before I get the keys (.json file)?', 'course': 'data-engineering-zoomcamp', 'doc_id': 'aa94f799d68c5cd307b7e037b4e7691d'}, {'text': 'Here: https://releases.hashicorp.com/terraform/1.1.3/terraform_1.1.3_linux_amd64.zip', 'section': 'Module 1: Docker and Terraform', 'question': 'Terraform - Where can I find the Terraform 1.1.3 Linux (AMD 64)?', 'course': 'data-engineering-zoomcamp', 'doc_id': 'd425496ebe596e7f60ae2f4256510cdd'}, {'text': 'You get this error because I run the command terraform init outside the working directory, and this is wrong.You need first to navigate to the working directory that contains terraform configuration files, and and then run the command.', 'section': 'Module 1: Docker and Terraform', 'question': 'Terraform - Terraform initialized in an empty directory! The directory has no Terraform configuration files. You may begin working with Terraform immediately by creating Terraform configuration files.g', 'course': 'data-engineering-zoomcamp', 'doc_id': '97d4b899113daaddc4210407655e7be9'}, {'text': 'The error:\\nError: googleapi: Error 403: Access denied., forbidden\\n│\\nand\\n│ Error: Error creating Dataset: googleapi: Error 403: Request had insufficient authentication scopes.\\nFor this solution make sure to run:\\necho $GOOGLE_APPLICATION_CREDENTIALS\\necho $?\\nSolution:\\nYou have to set again the GOOGLE_APPLICATION_CREDENTIALS as Alexey did in the environment set-up video in week1:\\nexport GOOGLE_APPLICATION_CREDENTIALS=\"<path/to/your/service-account-authkeys>.json', 'section': 'Module 1: Docker and Terraform', 'question': 'Terraform - Error creating Dataset: googleapi: Error 403: Request had insufficient authentication scopes', 'course': 'data-engineering-zoomcamp', 'doc_id': '8050de712036dd9d17c6c578add83733'}, {'text': \"The error:\\nError: googleapi: Error 403: terraform-trans-campus@trans-campus-410115.iam.gserviceaccount.com does not have storage.buckets.create access to the Google Cloud project. Permission 'storage.buckets.create' denied on resource (or it may not exist)., forbidden\\nThe solution:\\nYou have to declare the project name as your Project ID, and not your Project name, available on GCP console Dashboard.\", 'section': 'Module 1: Docker and Terraform', 'question': 'Terraform - Error creating Bucket: googleapi: Error 403: Permission denied to access ‘storage.buckets.create’', 'course': 'data-engineering-zoomcamp', 'doc_id': 'db61f1180ab41ee6b823624698c289ee'}, {'text': 'provider \"google\" {\\nproject     = var.projectId\\ncredentials = file(\"${var.gcpkey}\")\\n#region      = var.region\\nzone = var.zone\\n}', 'section': 'Module 1: Docker and Terraform', 'question': 'To ensure the sensitivity of the credentials file, I had to spend lot of time to input that as a file.', 'course': 'data-engineering-zoomcamp', 'doc_id': '561430539007cc4c0b2e36880549f2ea'}, {'text': 'For the HW1 I encountered this issue. The solution is\\nSELECT * FROM zones AS z WHERE z.\"Zone\" = \\'Astoria Zone\\';\\nI think columns which start with uppercase need to go between “Column”. I ran into a lot of issues like this and “ ” made it work out.\\nAddition to the above point, for me, there is no ‘Astoria Zone’, only ‘Astoria’ is existing in the dataset.\\nSELECT * FROM zones AS z WHERE z.\"Zone\" = \\'Astoria’;', 'section': 'Module 1: Docker and Terraform', 'question': \"SQL - SELECT * FROM zones_taxi WHERE Zone='Astoria Zone'; Error Column Zone doesn't exist\", 'course': 'data-engineering-zoomcamp', 'doc_id': '45e79d2efeb36a996bc050f30f4cc4c0'}, {'text': 'It is inconvenient to use quotation marks all the time, so it is better to put the data to the database all in lowercase, so in Pandas after\\ndf = pd.read_csv(‘taxi+_zone_lookup.csv’)\\nAdd the row:\\ndf.columns = df.columns.str.lower()', 'section': 'Module 1: Docker and Terraform', 'question': \"SQL - SELECT Zone FROM taxi_zones Error Column Zone doesn't exist\", 'course': 'data-engineering-zoomcamp', 'doc_id': 'b0aa2fa601aac05f213da374cc329f30'}, {'text': 'Solution (for mac users): os.system(f\"curl {url} --output {csv_name}\")', 'section': 'Module 1: Docker and Terraform', 'question': 'CURL - curl: (6) Could not resolve host: output.csv', 'course': 'data-engineering-zoomcamp', 'doc_id': 'ff00ff8663add6a1b39fef34b7477ea2'}, {'text': 'To resolve this, ensure that your config file is in C/User/Username/.ssh/config', 'section': 'Module 1: Docker and Terraform', 'question': 'SSH Error: ssh: Could not resolve hostname linux: Name or service not known', 'course': 'data-engineering-zoomcamp', 'doc_id': 'e82d352bc0ff03e047f304319f9f8fc8'}, {'text': 'If you use Anaconda (recommended for the course), it comes with pip, so the issues is probably that the anaconda’s Python is not on the PATH.\\nAdding it to the PATH is different for each operation system.\\nFor Linux and MacOS:\\nOpen a terminal.\\nFind the path to your Anaconda installation. This is typically `~/anaconda3` or `~/opt/anaconda3`.\\nAdd Anaconda to your PATH with the command: `export PATH=\"/path/to/anaconda3/bin:$PATH\"`.\\nTo make this change permanent, add the command to your `.bashrc` (Linux) or `.bash_profile` (MacOS) file.\\nOn Windows, python and pip are in different locations (python is in the anaconda root, and pip is in Scripts). With GitBash:\\nLocate your Anaconda installation. The default path is usually `C:\\\\Users\\\\[YourUsername]\\\\Anaconda3`.\\nDetermine the correct path format for Git Bash. Paths in Git Bash follow the Unix-style, so convert the Windows path to a Unix-style path. For example, `C:\\\\Users\\\\[YourUsername]\\\\Anaconda3` becomes `/c/Users/[YourUsername]/Anaconda3`.\\nAdd Anaconda to your PATH with the command: `export PATH=\"/c/Users/[YourUsername]/Anaconda3/:/c/Users/[YourUsername]/Anaconda3/Scripts/$PATH\"`.\\nTo make this change permanent, add the command to your `.bashrc` file in your home directory.\\nRefresh your environment with the command: `source ~/.bashrc`.\\nFor Windows (without Git Bash):\\nRight-click on \\'This PC\\' or \\'My Computer\\' and select \\'Properties\\'.\\nClick on \\'Advanced system settings\\'.\\nIn the System Properties window, click on \\'Environment Variables\\'.\\nIn the Environment Variables window, select the \\'Path\\' variable in the \\'System variables\\' section and click \\'Edit\\'.\\nIn the Edit Environment Variable window, click \\'New\\' and add the path to your Anaconda installation (typically `C:\\\\Users\\\\[YourUsername]\\\\Anaconda3` and C:\\\\Users\\\\[YourUsername]\\\\Anaconda3\\\\Scripts`).\\nClick \\'OK\\' in all windows to apply the changes.\\nAfter adding Anaconda to the PATH, you should be able to use `pip` from the command line. Remember to restart your terminal (or command prompt in Windows) to apply these changes.', 'section': 'Module 1: Docker and Terraform', 'question': \"'pip' is not recognized as an internal or external command, operable program or batch file.\", 'course': 'data-engineering-zoomcamp', 'doc_id': '3984133995b8a76f1ae618199651489c'}, {'text': \"Resolution: You need to stop the services which is using the port.\\nRun the following:\\n```\\nsudo kill -9 `sudo lsof -t -i:<port>`\\n```\\n<port> being 8080 in this case. This will free up the port for use.\\n~ Abhijit Chakraborty\\nError: error response from daemon: cannot stop container: 1afaf8f7d52277318b71eef8f7a7f238c777045e769dd832426219d6c4b8dfb4: permission denied\\nResolution: In my case, I had to stop docker and restart the service to get it running properly\\nUse the following command:\\n```\\nsudo systemctl restart docker.socket docker.service\\n```\\n~ Abhijit Chakraborty\\nError: cannot import module psycopg2\\nResolution: Run the following command in linux:\\n```\\nsudo apt-get install libpq-dev\\npip install psycopg2\\n```\\n~ Abhijit Chakraborty\\nError: docker build Error checking context: 'can't stat '<path-to-file>'\\nResolution: This happens due to insufficient permission for docker to access a certain file within the directory which hosts the Dockerfile.\\n1. You can create a .dockerignore file and add the directory/file which you want Dockerfile to ignore while build.\\n2. If the above does not work, then put the dockerfile and corresponding script, `\\t1.py` in our case to a subfolder. and run `docker build ...`\\nfrom inside the new folder.\\n~ Abhijit Chakraborty\", 'section': 'Module 1: Docker and Terraform', 'question': 'Error: error starting userland proxy: listen tcp4 0.0.0.0:8080: bind: address already in use', 'course': 'data-engineering-zoomcamp', 'doc_id': '59d0236f6f5b44f8447b5c06b0761a21'}, {'text': 'To get a pip-friendly requirements.txt file file from Anaconda use\\nconda install pip then `pip list –format=freeze > requirements.txt`.\\n`conda list -d > requirements.txt` will not work and `pip freeze > requirements.txt` may give odd pathing.', 'section': 'Module 2: Workflow Orchestration', 'question': 'Anaconda to PIP', 'course': 'data-engineering-zoomcamp', 'doc_id': '9f6c7b8f81e33ec230b86a42abd27b77'}, {'text': 'Prefect: https://docs.google.com/document/d/1K_LJ9RhAORQk3z4Qf_tfGQCDbu8wUWzru62IUscgiGU/edit?usp=sharing\\nAirflow: https://docs.google.com/document/d/1-BwPAsyDH_mAsn8HH5z_eNYVyBMAtawJRjHHsjEKHyY/edit?usp=sharing', 'section': 'Module 2: Workflow Orchestration', 'question': 'Where are the FAQ questions from the previous cohorts for the orchestration module?', 'course': 'data-engineering-zoomcamp', 'doc_id': '0e1bab7b5559bcc859dffb68eccd0797'}, {'text': 'Issue : Docker containers exit instantly with code 132, upon docker compose up\\nMage documentation has it listing the cause as \"older architecture\" .\\nThis might be a hardware issue, so unless you have another computer, you can\\'t solve it without purchasing a new one, so the next best solution is a VM.\\nThis is from a student running on a VirtualBox VM, Ubuntu 22.04.3 LTS, Docker version 25.0.2. So not having the context on how the vbox was spin up with (CPU, RAM, network, etc), it’s really inconclusive at this time.', 'section': 'Module 2: Workflow Orchestration', 'question': 'Docker - 2.2.2 Configure Mage', 'course': 'data-engineering-zoomcamp', 'doc_id': '871e643f39ae68f0d0963b1c8dd81552'}, {'text': 'This issue was occurring with Windows WSL 2\\nFor me this was because WSL 2 was not dedicating enough cpu cores to Docker.The load seems to take up at least one cpu core so I recommend dedicating at least two.\\nOpen Bash and run the following code:\\n$ cd ~\\n$ ls -la\\nLook for the .wsl config file:\\n-rw-r--r-- 1 ~1049089       31 Jan 25 12:54  .wslconfig\\nUsing a text editing tool of your choice edit or create your .wslconfig file:\\n$ nano .wslconfig\\nPaste the following into the new file/ edit the existing file in this format and save:\\n*** Note - for memory– this is the RAM on your machine you can dedicate to Docker, your situation may be different than mine ***\\n[wsl2]\\nprocessors=<Number of Processors - at least 2!> example: 4\\nmemory=<memory> example:4GB\\nExample:\\nOnce you do that run:\\n$ wsl --shutdown\\nThis shuts down WSL\\nThen Restart Docker Desktop - You should now be able to load the .csv.gz file without the error into a pandas dataframe', 'section': 'Module 2: Workflow Orchestration', 'question': 'WSL - 2.2.3 Mage - Unexpected Kernel Restarts; Kernel Running out of memory:', 'course': 'data-engineering-zoomcamp', 'doc_id': 'a973039a99b7e76c247c95796eb24403'}, {'text': 'The issue and solution on the link:\\nhttps://datatalks-club.slack.com/archives/C01FABYF2RG/p1706817366764269?thread_ts=1706815324.993529&cid=C01FABYF2RG', 'section': 'Module 2: Workflow Orchestration', 'question': '2.2.3 Configuring Postgres', 'course': 'data-engineering-zoomcamp', 'doc_id': 'abb04c2a86adaed198fe2471a56cd84b'}, {'text': 'Check that the POSTGRES_PORT variable in the io_config.yml  file is set to port 5432, which is the default postgres port. The POSTGRES_PORT variable is the mage container port, not the host port. Hence, there’s no need to set the POSTGRES_PORT to 5431 just because you already have a conflicting postgres installation in your host machine.', 'section': 'Module 2: Workflow Orchestration', 'question': 'MAGE - 2.2.3 OperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5431 failed: Connection refused', 'course': 'data-engineering-zoomcamp', 'doc_id': 'e8e815bf20be8c36b32b978ff62acbbb'}, {'text': 'You forgot to select ‘dev’ profile in the dropdown menu next to where you select ‘PostgreSQL’ in the connection drop down.', 'section': 'Module 2: Workflow Orchestration', 'question': 'MAGE - 2.2.4 executing SELECT 1; results in KeyError', 'course': 'data-engineering-zoomcamp', 'doc_id': '5080b067441e5b77172a8087e63e9407'}, {'text': 'If you are getting this error. Update your mage io_config.yaml file, and specify a timeout value set to 600 like this.\\nMake sure to save your changes.\\nMAGE - 2.2.4 Testing BigQuery connection using SQL 404 error:\\nNotFound: 404 Not found: Dataset ny-rides-diegogutierrez:None was not found in location northamerica-northeast1\\nIf you get this error even with all roles/permissions given to the service account check if you have ticked the box where it says “Use raw SQL”, just like the image below.', 'section': 'Module 2: Workflow Orchestration', 'question': \"MAGE -2.2.4 ConnectionError: ('Connection aborted.', TimeoutError('The write operation timed out'))\", 'course': 'data-engineering-zoomcamp', 'doc_id': '84b7376fde34ce26b13ff0f045482124'}, {'text': 'Solution: https://stackoverflow.com/questions/48056381/google-client-invalid-jwt-token-must-be-a-short-lived-token', 'section': 'Module 2: Workflow Orchestration', 'question': \"Problem: RefreshError: ('invalid_grant: Invalid JWT: Token must be a short-lived token (60 minutes) and in a reasonable timeframe. Check your iat and exp values in the JWT claim.', {'error': 'invalid_grant', 'error_description': 'Invalid JWT: Token must be a short-lived token (60 minutes) and in a reasonable timeframe. Check your iat and exp values in the JWT claim.'})\", 'course': 'data-engineering-zoomcamp', 'doc_id': 'ad22a1c032a1ac6758cb57a483798a4c'}, {'text': \"Origin of Solution (Mage Slack-Channel): https://mageai.slack.com/archives/C03HTTWFEKE/p1706543947795599\\nProblem: This error can often be seen after solving the error mentioned in 2.2.4. The error can be found in Mage version 0.9.61 and is a side-effect of the update of the code for data-loader blocks.\\nNote: Mage 0.9.62 has been released, as of Feb 5 2024. Please recheck. Solution below may be obsolete\\nSolution: Using a “fixed” version of the docker container\\nPull updated docker image from docker-hub\\nmageai/mageaidocker pull:alpha\\nUpdate docker-compose.yaml\\nversion: '3'\\nservices:\\nmagic:\\nimage: mageai/mageai:alpha  <--- instead of “latest”-tag\\ndocker-compose up\\nThe original Error is still present, but the SQL-query will return the desired result:\\n--------------------------------------------------------------------------------------\", 'section': 'Module 2: Workflow Orchestration', 'question': 'Mage - 2.2.4 IndexError: list index out of range', 'course': 'data-engineering-zoomcamp', 'doc_id': '615dac37a3de92b207e05ae5198bb71e'}, {'text': 'Add\\nif not path.parent.is_dir():\\npath.parent.mkdir(parents=True)\\npath = Path(path).as_posix()\\nsee:\\nhttps://datatalks-club.slack.com/archives/C01FABYF2RG/p1675774214591809?thread_ts=1675768839.028879&cid=C01FABYF2RG', 'section': 'Module 2: Workflow Orchestration', 'question': '2.2.6 OSError: Cannot save file into a non-existent directory: \\'..\\\\\\\\..\\\\\\\\data\\\\\\\\yellow\\'\\\\n\")', 'course': 'data-engineering-zoomcamp', 'doc_id': 'c102b9840636e6bcea9b9d07498671a0'}, {'text': 'The video DE Zoomcamp 2.2.7 is missing  the actual deployment of Mage using Terraform to GCP. The steps for the deployment were not covered in the video.\\nI successfully deployed it and wanted to share some key points:\\nIn variables.tf, set the project_id default value to your GCP project ID.\\nEnable the Cloud Filestore API:\\nVisit the Google Cloud Console.to\\nNavigate to \"APIs & Services\" > \"Library.\"\\nSearch for \"Cloud Filestore API.\"\\nClick on the API and enable it.\\nTo perform the deployment:\\nterraform init\\nterraform apply\\nPlease note that during the terraform apply step, Terraform will prompt you to enter the PostgreSQL password. After that, it will ask for confirmation to proceed with the deployment. Review the changes, type \\'yes\\' when prompted, and press Enter.', 'section': 'Module 2: Workflow Orchestration', 'question': 'GCP - 2.2.7d Deploying Mage to GCP', 'course': 'data-engineering-zoomcamp', 'doc_id': '70e8d91bf57879d15e1be588cf3d522d'}, {'text': 'If you want to rune multiple docker containers from different directories. Then make sure to change the port mappings in the docker-compose.yml file.\\nports:\\n- 8088:6789\\nThe 8088 port in above case is hostport, where mage will run on your local machine. You can customize this as long as the port is available. If you are running on VM, make sure to forward the port too. You need to keep the container port to 6789 as this is the port where mage is running.\\nGCP - 2.2.7d Deploying Mage to Google Cloud\\nWhile terraforming all the resources inside a VM created in GCS the following error is shown.\\nError log:\\nmodule.lb-http.google_compute_backend_service.default[\"default\"]: Creating...\\n╷\\n│ Error: Error creating GlobalAddress: googleapi: Error 403: Request had insufficient authentication scopes.\\n│ Details:\\n│ [\\n│   {\\n│     \"@type\": \"type.googleapis.com/google.rpc.ErrorInfo\",\\n│     \"domain\": \"googleapis.com\",\\n│     \"metadatas\": {\\n│       \"method\": \"compute.beta.GlobalAddressesService.Insert\",\\n│       \"service\": \"compute.googleapis.com\"\\n│     },\\n│     \"reason\": \"ACCESS_TOKEN_SCOPE_INSUFFICIENT\"\\n│   }\\n│ ]\\n│\\n│ More details:\\n│ Reason: insufficientPermissions, Message: Insufficient Permission\\nThis error might happen when you are using a VM inside GCS. To use the Google APIs from a GCP virtual machine you need to add the cloud platform scope (\"https://www.googleapis.com/auth/cloud-platform\") to your VM when it is created.\\nSince ours is already created you can just stop it and change the permissions. You can do it in the console, just go to \"EDIT\", g99o all the way down until you find \"Cloud API access scopes\". There you can \"Allow full access to all Cloud APIs\". I did this and all went smoothly generating all the resources needed. Hope it helps if you encounter this same error.\\nResources: https://stackoverflow.com/questions/35928534/403-request-had-insufficient-authentication-scopes-during-gcloud-container-clu', 'section': 'Module 2: Workflow Orchestration', 'question': 'Ruuning Multiple Mage instances in Docker from different directories', 'course': 'data-engineering-zoomcamp', 'doc_id': 'd51ba2b3458a57d6413eec259a2644f3'}, {'text': 'If you are on the free trial account on GCP you will face this issue when trying to deploy the infrastructures with terraform. This service is not available for this kind of account.\\nThe solution I found was to delete the load_balancer.tf file and to comment or delete the rows that differentiate it on the main.tf file. After this just do terraform destroy to delete any infrastructure created on the fail attempts and re-run the terraform apply.\\nCode on main.tf to comment/delete:\\nLine 166, 167, 168', 'section': 'Module 2: Workflow Orchestration', 'question': 'GCP - 2.2.7d Load Balancer Problem (Security Policies quota)', 'course': 'data-engineering-zoomcamp', 'doc_id': 'fe6091cbd5e71b99a13fe981a027f62a'}, {'text': \"If you get the following error\\nYou have to edit variables.tf on the gcp folder, set your project-id and region and zones properly. Then, run terraform apply again.\\nYou can find correct regions/zones here: https://cloud.google.com/compute/docs/regions-zones\\nDeploying MAGE to GCP  with Terraform via the VM (2.2.7)\\nFYI - It can take up to 20 minutes to deploy the MAGE Terraform files if you are using a GCP Virtual Machine. It is normal, so don’t interrupt the process or think it’s taking too long. If you have, make sure you run a terraform destroy before trying again as you will have likely partially created resources which will cause errors next time you run `terraform apply`.\\n`terraform destroy` may not completely delete partial resources - go to Google Cloud Console and use the search bar at the top to search for the ‘app.name’ you declared in your variables.tf file; this will list all resources with that name - make sure you delete them all before running `terraform apply` again.\\nWhy are my GCP free credits going so fast? MAGE .tf files - Terraform Destroy not destroying all Resources\\nI checked my GCP billing last night & the MAGE Terraform IaC didn't destroy a GCP Resource called Filestore as ‘mage-data-prep- it has been costing £5.01 of my free credits each day  I now have £151 left - Alexey has assured me that This amount WILL BE SUFFICIENT funds to finish the course. Note to anyone who had issues deploying the MAGE terraform code: check your billing account to see what you're being charged for (main menu - billing) (even if it's your free credits) and run a search for 'mage-data-prep' in the top bar just to be sure that your resources have been destroyed - if any come up delete them.\", 'section': 'Module 2: Workflow Orchestration', 'question': 'GCP - 2.2.7d Part 2 - Getting error when you run terraform apply', 'course': 'data-engineering-zoomcamp', 'doc_id': '25dfa9a94fe4323b2c2883f8f88ba145'}, {'text': '```\\n│ Error: Error creating Connector: googleapi: Error 403: Permission \\'vpcaccess.connectors.create\\' denied on resource \\'//vpcaccess.googleapis.com/projects/<ommit>/locations/us-west1\\' (or it may not exist).\\n│ Details:\\n│ [\\n│   {\\n│     \"@type\": \"type.googleapis.com/google.rpc.ErrorInfo\",\\n│     \"domain\": \"vpcaccess.googleapis.com\",\\n│     \"metadata\": {\\n│       \"permission\": \"vpcaccess.connectors.create\",\\n│       \"resource\": \"projects/<ommit>/locations/us-west1\"\\n│     },\\n│     \"reason\": \"IAM_PERMISSION_DENIED\"\\n│   }\\n│ ]\\n│\\n│   with google_vpc_access_connector.connector,\\n│   on fs.tf line 19, in resource \"google_vpc_access_connector\" \"connector\":\\n│   19: resource \"google_vpc_access_connector\" \"connector\" {\\n│\\n```\\nSolution: Add Serverless VPC Access Admin to Service Account.\\nLine 148', 'section': 'Module 2: Workflow Orchestration', 'question': \"Question: Permission 'vpcaccess.connectors.create'\", 'course': 'data-engineering-zoomcamp', 'doc_id': '7bfd7e24a07483f406d28072dd0804b6'}, {'text': 'Git won’t push an empty folder to GitHub, so if you put a file in that folder and then push, then you should be good to go.\\nOr - in your code- make the folder if it doesn’t exist using Pathlib as shown here: https://stackoverflow.com/a/273227/4590385.\\nFor some reason, when using github storage, the relative path for writing locally no longer works. Try using two separate paths, one full path for the local write, and the original relative path for GCS bucket upload.', 'section': 'Module 2: Workflow Orchestration', 'question': \"File Path: Cannot save file into a non-existent directory: 'data/green'\", 'course': 'data-engineering-zoomcamp', 'doc_id': '55965256a3d83eb17799cad6a9f24404'}, {'text': 'The green dataset contains lpep_pickup_datetime while the yellow contains tpep_pickup_datetime. Modify the script(s) depending on  the dataset as required.', 'section': 'Module 2: Workflow Orchestration', 'question': 'No column name lpep_pickup_datetime / tpep_pickup_datetime', 'course': 'data-engineering-zoomcamp', 'doc_id': '35f4ad343b536b28a4f6974b20d6c0b6'}, {'text': 'pd.read_csv\\ndf_iter = pd.read_csv(dataset_url, iterator=True, chunksize=100000)\\nThe data needs to be appended to the parquet file using the fastparquet engine\\ndf.to_parquet(path, compression=\"gzip\", engine=\\'fastparquet\\', append=True)', 'section': 'Module 2: Workflow Orchestration', 'question': 'Process to download the VSC using Pandas is killed right away', 'course': 'data-engineering-zoomcamp', 'doc_id': 'f22ae696e3dbe6750d7d3c97f2276e1d'}, {'text': 'denied: requested access to the resource is denied\\nThis can happen when you\\nHaven\\'t logged in properly to Docker Desktop (use docker login -u \"myusername\")\\nHave used the wrong username when pushing to docker images. Use the same one as your username and as the one you build on\\ndocker image build -t <myusername>/<imagename>:<tag>\\ndocker image push <myusername>/<imagename>:<tag>', 'section': 'Module 2: Workflow Orchestration', 'question': 'Push to docker image failure', 'course': 'data-engineering-zoomcamp', 'doc_id': '02de1aac9f211c660302696213204326'}, {'text': \"16:21:35.607 | INFO    | Flow run 'singing-malkoha' - Executing 'write_bq-b366772c-0' immediately...\\nKilled\\nSolution:  You probably are running out of memory on your VM and need to add more.  For example, if you have 8 gigs of RAM on your VM, you may want to expand that to 16 gigs.\", 'section': 'Module 2: Workflow Orchestration', 'question': 'Flow script fails with “killed” message:', 'course': 'data-engineering-zoomcamp', 'doc_id': '3807894b51e8b031b157127e91aeefa2'}, {'text': 'After playing around with prefect for a while this can happen.\\nSsh to your VM and run sudo du -h --block-size=G | sort -n -r | head -n 30 to see which directory needs the most space.\\nMost likely it will be …/.prefect/storage, where your cached flows are stored. You can delete older flows from there. You also have to delete the corresponding flow in the UI, otherwise it will throw you an error, when you try to run your next flow.\\nSSL Certificate Verify: (I got it when trying to run flows on MAC): urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED]\\npip install certifi\\n/Applications/Python\\\\ {ver}/Install\\\\ Certificates.command\\nor\\nrunning the “Install Certificate.command” inside of the python{ver} folder', 'section': 'Module 2: Workflow Orchestration', 'question': 'GCP VM: Disk Space is full', 'course': 'data-engineering-zoomcamp', 'doc_id': '64d0c71192fdbd893ed186e46972c5b9'}, {'text': 'It means your container consumed all available RAM allocated to it. It can happen in particular when working on Question#3 in the homework as the dataset is relatively large and containers eat a lot of memory in general.\\nI would recommend restarting your computer and only starting the necessary processes to run the container. If that doesn’t work, allocate more resources to docker. If also that doesn’t work because your workstation is a potato, you can use an online compute environment service like GitPod, which is free under under 50 hours / month of use.', 'section': 'Module 2: Workflow Orchestration', 'question': 'Docker: container crashed with status code 137.', 'course': 'data-engineering-zoomcamp', 'doc_id': 'a4a592a521b05eee7232aad390d93198'}, {'text': 'In Q3 there was a task to run the etl script from web to GCS. The problem was, it wasn’t really an ETL straight from web to GCS, but it was actually a web to local storage to local memory to GCS over network ETL. Yellow data is about 100 MB each per month compressed and ~700 MB after uncompressed on memory\\nThis leads to a problem where i either got a network type error because my not so good 3rd world internet or i got my WSL2 crashed/hanged because out of memory error and/or 100% resource usage hang.\\nSolution:\\nif you have a lot of time at hand, try compressing it to parquet and writing it to GCS with the timeout argument set to a really high number (the default os 60 seconds)\\nthe yellow taxi data for feb 2019 is about 100MB as parquet file\\ngcp_cloud_storage_bucket_block.upload_from_path(\\nfrom_path=f\"{path}\",\\nto_path=path,\\ntimeout=600\\n)', 'section': 'Module 2: Workflow Orchestration', 'question': 'Timeout due to slow upload internet', 'course': 'data-engineering-zoomcamp', 'doc_id': 'a8ee017005c03aeb6747dbda613e54b6'}, {'text': 'This error occurs when you try to re-run the export block, of the transformed green_taxi data to PostgreSQL.\\nWhat you’ll need to do is to drop the table using SQL in Mage (screenshot below).\\nYou should be able to re-run the block successfully after dropping the table.', 'section': 'Module 2: Workflow Orchestration', 'question': 'UndefinedColumn: column \"ratecode_id\", \"rate_code_id\" “vendor_id”, “pu_location_id”, “do_location_id” of relation \"green_taxi\" does not exist - Export transformed green_taxi data to PostgreSQL', 'course': 'data-engineering-zoomcamp', 'doc_id': '6bfbbd7224075e2fcf16558a25f97da4'}, {'text': 'SettingWithCopyWarning:\\nA value is trying to be set on a copy of a slice from a DataFrame.\\nUse the data.loc[] = value syntax instead of df[] = value to ensure that the new column is being assigned to the original dataframe instead of a copy of a dataframe or a series.', 'section': 'Module 2: Workflow Orchestration', 'question': 'Homework - Q3 SettingWithCopyWarning Error:', 'course': 'data-engineering-zoomcamp', 'doc_id': '1ec21efeaae547cdfc85b1ca6e44d872'}, {'text': 'CSV Files are very big in nyc data, so we instead of using Pandas/Python kernel , we can try Pyspark Kernel\\nDocumentation of Mage for using pyspark kernel: https://docs.mage.ai/integrations/spark-pyspark\\n?', 'section': 'Module 2: Workflow Orchestration', 'question': 'Since I was using slow laptop, and we have so big csv files, I used pyspark kernel in mage instead of python, How to do it?', 'course': 'data-engineering-zoomcamp', 'doc_id': '7b3a838be3f9073df33993e32c59a63a'}, {'text': 'So we will first delete the connection between blocks then we can remove the connection.', 'section': 'Module 2: Workflow Orchestration', 'question': 'I got an error when I was deleting  BLOCK IN A PIPELINE', 'course': 'data-engineering-zoomcamp', 'doc_id': '8e1f1e15ee5f1e0d9ee50357837fa1eb'}, {'text': 'While Editing the Pipeline Name It throws permission denied error.\\n(Work around)In that case proceed with the work and save later on revisit it will let you edit.', 'section': 'Module 2: Workflow Orchestration', 'question': 'Mage UI won’t let you edit the Pipeline name?', 'course': 'data-engineering-zoomcamp', 'doc_id': 'f784a9be56c5971a4aae8e7c5f07cc67'}, {'text': 'Solution n°1 if you want to download everything :\\n```\\nimport pyarrow as pa\\nimport pyarrow.parquet as pq\\nfrom pyarrow.fs import GcsFileSystem\\n…\\n@data_loader\\ndef load_data(*args, **kwargs):\\n    bucket_name = YOUR_BUCKET_NAME_HERE\\'\\n    blob_prefix = \\'PATH / TO / WHERE / THE / PARTITIONS / ARE\\'\\n    root_path = f\"{bucket_name}/{blob_prefix}\"\\npa_table = pq.read_table(\\n        source=root_path,\\n        filesystem=GcsFileSystem(),        \\n    )\\n\\n    return pa_table.to_pandas()\\nSolution n°2 if you want to download only some dates :\\n@data_loader\\ndef load_data(*args, **kwargs):\\ngcs = pa.fs.GcsFileSystem()\\nbucket_name = \\'YOUR_BUCKET_NAME_HERE\\'\\nblob_prefix = \\'\\'PATH / TO / WHERE / THE / PARTITIONS / ARE\\'\\'\\nroot_path = f\"{bucket_name}/{blob_prefix}\"\\npa_dataset = pq.ParquetDataset(\\npath_or_paths=root_path,\\nfilesystem=gcs,\\nfilters=[(\\'lpep_pickup_date\\', \\'>=\\', \\'2020-10-01\\'), (\\'lpep_pickup_date\\', \\'<=\\', \\'2020-10-31\\')]\\n)\\nreturn pa_dataset.read().to_pandas()\\n# More information about the pq.Parquet.Dataset : Encapsulates details of reading a complete Parquet dataset possibly consisting of multiple files and partitions in subdirectories. Documentation here :\\nhttps://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetDataset.html#pyarrow.parquet.ParquetDataset\\nERROR: UndefinedColumn: column \"vendor_id\" of relation \"green_taxi\" does not exist\\nTwo possible solutions both of them work in the same way.\\nOpen up a Data Loader connect using SQL - RUN the command \\n`DROP TABLE mage.green_taxi`\\nElse, Open up a Data Extractor of SQL  - increase the rows to above the number of rows in the dataframe (you can find that in the bottom of the transformer block) change the Write Policy to `Replace` and run the SELECT statement', 'section': 'Module 2: Workflow Orchestration', 'question': 'How do I make Mage load the partitioned files that we created on 2.2.4, to load them into BigQuery ?', 'course': 'data-engineering-zoomcamp', 'doc_id': 'e905ac509dbed2c56dd84feed73b5518'}, {'text': \"All mage files are in your /home/src/folder where you saved your credentials.json so you should be able to access them locally. You will see a folder for ‘Pipelines’,  'data loaders', 'data transformers' & 'data exporters' - inside these will be the .py or .sql files for the blocks you created in your pipeline.\\nRight click & ‘download’ the pipeline itself to your local machine (which gives you metadata, pycache and other files)\\nAs above, download each .py/.sql file that corresponds to each block you created for the pipeline. You'll find these under 'data loaders', 'data transformers' 'data exporters'\\nMove the downloaded files to your GitHub repo folder & commit your changes.\", 'section': 'Module 2: Workflow Orchestration', 'question': 'Git - What Files Should I Submit for Homework 2 & How do I get them out of MAGE:', 'course': 'data-engineering-zoomcamp', 'doc_id': 'dd5b1118497e328fc57af3670f614f81'}, {'text': 'Assuming you downloaded the Mage repo in the week 2 folder of the Data Engineering Zoomcamp, you might want to include your mage copy, demo pipelines and homework within your personal copy of the Data Engineering Zoomcamp repo. This will not work by default, because GitHub sees them as two separate repositories, and one does not track the other. To add the Mage files to your main DE Zoomcamp repo, you will need to:\\nMove the contents of the .gitignore file in your main .gitignore.\\nUse the terminal to cd into the Mage folder and:\\nrun “git remote remove origin” to de-couple the Mage repo,\\nrun “rm -rf .git” to delete local git files,\\nrun “git add .” to add the current folder as changes to stage, commit and push.', 'section': 'Module 2: Workflow Orchestration', 'question': 'Git - How do I include the files in the Mage repo (including exercise files and homework) in a personal copy of the Data Engineering Zoomcamp repo?', 'course': 'data-engineering-zoomcamp', 'doc_id': 'c46d5dcf3e3edfe9632aedbb563a108b'}, {'text': \"When try to add three assertions:\\nvendor_id is one of the existing values in the column (currently)\\npassenger_count is greater than 0\\ntrip_distance is greater than 0\\nto test_output, I got ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). Below is my code:\\ndata_filter = (data['passenger_count'] > 0) and (data['trip_distance'] > 0)\\nAfter looking for solutions at Stackoverflow, I found great discussion about it. So I changed my code into:\\ndata_filter = (data['passenger_count'] > 0) & (data['trip_distance'] > 0)\", 'section': 'Module 2: Workflow Orchestration', 'question': 'Got ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()', 'course': 'data-engineering-zoomcamp', 'doc_id': '9040804b0502dcbd85d1c1f271c73e29'}, {'text': 'This happened when I just booted up my PC, continuing from the progress I was doing from yesterday.\\nAfter cd-ing into your directory, and running docker compose up , the web interface for the Mage shows, but the files that I had yesterday was gone.\\nIf your files are gone, go ahead and close the web interface, and properly shutting down the mage docker compose by doing Ctrl + C once. Try running it again. This worked for me more than once (yes the issue persisted with my PC twice)\\nAlso, you should check if you’re in the correct repository before doing docker compose up . This was discussed in the Slack #course-data-engineering channel', 'section': 'Module 2: Workflow Orchestration', 'question': 'Mage AI Files are Gone/disappearing', 'course': 'data-engineering-zoomcamp', 'doc_id': '4c1f7e5273e94cdca09ca47cd72aa690'}, {'text': 'The above errors due to “ at the trailing side and it need to be modified with ‘ quotes at both ends\\nKrishna Anand', 'section': 'Module 2: Workflow Orchestration', 'question': 'Mage - Errors in io.config.yaml file', 'course': 'data-engineering-zoomcamp', 'doc_id': 'dd6b76d0d707b5ac094893276ff4e363'}, {'text': \"Problem: The following error occurs when attempting to export data from Mage to a GCS bucket using pyarrow suggesting Mage doesn’t have the necessary permissions to access the specified GCP credentials .json file.\\nArrowException: Unknown error: google::cloud::Status(UNKNOWN: Permanent error GetBucketMetadata: Could not create a OAuth2 access token to authenticate the request. The request was not sent, as such an access token is required to complete the request successfully. Learn more about Google Cloud authentication at https://cloud.google.com/docs/authentication. The underlying error message was: Cannot open credentials file /home/src/...\\nSolution: Inside the Mage app:\\nCreate a credentials folder (e.g. gcp-creds) within the magic-zoomcamp folder\\nIn the credentials folder create a .json key file (e.g. mage-gcp-creds.json)\\nCopy/paste GCP service account credentials into the .json key file and save\\nUpdate code to point to this file. E.g.\\nenviron['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/src/magic-zoomcamp/gcp-creds/mage-gcp-creds.json'\", 'section': 'Module 2: Workflow Orchestration', 'question': 'Mage - ArrowException Cannot open credentials file', 'course': 'data-engineering-zoomcamp', 'doc_id': '65596cceb05c47082f0851bf26edb60b'}, {'text': \"Oserror: google::cloud::status(unavailable: retry policy exhausted getbucketmetadata: could not create a OAuth2 access token to authenticate the request. the request was not sent, as such an access token is required to complete the request successfully. learn more about google cloud authentication at https://cloud.google.com/docs/authentication. the underlying error message was: performwork() - curl error [6]=couldn't resolve host name)\", 'section': 'Module 2: Workflow Orchestration', 'question': 'Mage - OSError', 'course': 'data-engineering-zoomcamp', 'doc_id': '7572252a86d49641ef23e5b270d77581'}, {'text': \"Problem: The following error occurs when attempting to export data from Mage to a GCS bucket. Assigned service account doesn’t have the necessary permissions access Google Cloud Storage Bucket\\nPermissionError: [Errno 13] google::cloud::Status(PERMISSION_DENIED: Permanent error GetBucketMetadata:... .iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket. Permission 'storage.buckets.get' denied on resource (or it may not exist). error_info={reason=forbidden, domain=global, metadata={http_status_code=403}}). Detail: [errno 13] Permission denied\\nSolution: Add Cloud Storage Admin role to the service account:\\nGo to project in Google Cloud Console>IAM & Admin>IAM\\nClick Edit principal (pencil symbol) to the right of the service account you are using\\nClick + ADD ANOTHER ROLE\\nSelect Cloud Storage>Storage Admin\\nClick Save\", 'section': 'Module 2: Workflow Orchestration', 'question': 'Mage - PermissionError service account does not have storage.buckets.get access to the Google Cloud Storage bucket', 'course': 'data-engineering-zoomcamp', 'doc_id': 'e81c4a41477de7b40f6de2f644b17107'}, {'text': '1. Make sure your pyspark script is ready to be send to Dataproc cluster\\n2. Create a Dataproc Cluster in GCP Console\\n3. Make sure to edit the service account and add new role - Dataproc Editor\\n4. Copy the python script ./notebooks/pyspark_script.py and place it under GCS bucket path\\n5. Make sure gcloud cli is installed either in Mage manually or  via your Dockerfile and docker-compose files. This is needed to let Mage access google Dataproc and the script it needs to execute. Refer - Installing the latest gcloud CLI\\n6. Use the Bigquery/Dataproc script mentioned here - https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/05-batch/code/cloud.md . Use Mage to trigger the query', 'section': 'Module 3: Data Warehousing', 'question': 'Trigger Dataproc from Mage', 'course': 'data-engineering-zoomcamp', 'doc_id': '95a162ef411ada22f55a6aefbd54cab1'}, {'text': 'A:\\n1 solution) Add -Y flag, so that apt-get automatically agrees to install additional packages\\n2) Use python ZipFile package, which is included in all modern python distributions', 'section': 'Module 3: Data Warehousing', 'question': 'Docker-compose takes infinitely long to install zip unzip packages for linux, which are required to unpack datasets', 'course': 'data-engineering-zoomcamp', 'doc_id': 'b1271b4c8fb328b047bae36331c600e4'}, {'text': 'Make sure to use Nullable dataTypes, such as Int64 when appliable.', 'section': 'Module 3: Data Warehousing', 'question': 'GCS Bucket - error when writing data from web to GCS:', 'course': 'data-engineering-zoomcamp', 'doc_id': '2ce2553e01dc146b8ee68ae0747fbe8b'}, {'text': 'Ultimately, when trying to ingest data into a BigQuery table, all files within a given directory must have the same schema.\\nWhen dealing for example with the FHV Datasets from 2019, however (see image below), one can see that the files for \\'2019-05\\', and 2019-06, have the columns \"PUlocationID\" and \"DOlocationID\" as Integers, while for the period of \\'2019-01\\' through \\'2019-04\\', the same column is defined as FLOAT.\\nSo while importing these files as parquet to BigQuery, the first one will be used to define the schema of the table, while all files following that will be used to append data on the existing table. Which means, they must all follow the very same schema of the file that created the table.\\nSo, in order to prevent errors like that, make sure to enforce the data types for the columns on the DataFrame before you serialize/upload them to BigQuery. Like this:\\npd.read_csv(\"path_or_url\").astype({\\n\\t\"col1_name\": \"datatype\",\\t\\n\\t\"col2_name\": \"datatype\",\\t\\n\\t...\\t\\t\\t\\t\\t\\n\\t\"colN_name\": \"datatype\" \\t\\n})', 'section': 'Module 3: Data Warehousing', 'question': \"GCS Bucket - Failed to create table: Error while reading data, error message: Parquet column 'XYZ' has type INT which does not match the target cpp_type DOUBLE. File: gs://path/to/some/blob.parquet\", 'course': 'data-engineering-zoomcamp', 'doc_id': 'ce2bb8eab25b105cd61ecb3cd8ba3dab'}, {'text': \"If you receive the error gzip.BadGzipFile: Not a gzipped file (b'\\\\n\\\\n'), this is because you have specified the wrong URL to the FHV dataset. Make sure to use https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/{dataset_file}.csv.gz\\nEmphasising the ‘/releases/download’ part of the URL.\", 'section': 'Module 3: Data Warehousing', 'question': 'GCS Bucket - Fix Error when importing FHV data to GCS', 'course': 'data-engineering-zoomcamp', 'doc_id': '56e80b1d7fe0c6ac13fcf48bb94b65a1'}, {'text': 'Krishna Anand', 'section': 'Module 3: Data Warehousing', 'question': 'GCS Bucket - Load Data From URL list in to GCP Bucket', 'course': 'data-engineering-zoomcamp', 'doc_id': '4652be8ac3b1a65a4220321a96c2b76b'}, {'text': 'Check the Schema\\nYou might have a wrong formatting\\nTry to upload the CSV.GZ files without formatting or going through pandas via wget\\nSee this Slack conversation for helpful tips', 'section': 'Module 3: Data Warehousing', 'question': 'GCS Bucket - I query my dataset and get a Bad character (ASCII 0) error?', 'course': 'data-engineering-zoomcamp', 'doc_id': 'e18466e72ccb88874a9c7e696e33e7ce'}, {'text': 'Run the following command to check if “BigQuery Command Line Tool” is installed or not: gcloud components list\\nYou can also use bq.cmd instead of bq to make it work.', 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - “bq: command not found”', 'course': 'data-engineering-zoomcamp', 'doc_id': '7461ee1437f1acc68752628f4eec9c51'}, {'text': 'Use big queries carefully,\\nI created by bigquery dataset on an account where my free trial was exhausted, and got a bill of $80.\\nUse big query in free credits and destroy all the datasets after creation.\\nCheck your Billing daily! Especially if you’ve spinned up a VM.', 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - Caution in using bigquery:no', 'course': 'data-engineering-zoomcamp', 'doc_id': 'b6ed61ad03a19d2c0d58d3ccd5942e05'}, {'text': 'Be careful when you create your resources on GCP, all of them have to share the same Region in order to allow load data from GCS Bucket to BigQuery. If you forgot it when you created them, you can create a new dataset on BigQuery using the same Region which you used on your GCS Bucket.\\nThis means that your GCS Bucket and the BigQuery dataset are placed in different regions. You have to create a new dataset inside BigQuery in the same region with your GCS bucket and store the data in the newly created dataset.', 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - Cannot read and write in different locations: source: EU, destination: US - Loading data from GCS into BigQuery (different Region):', 'course': 'data-engineering-zoomcamp', 'doc_id': 'ea86ccb241434434ae5c33a185f469c8'}, {'text': \"Make sure to create the BigQuery dataset in the very same location that you've created the GCS Bucket. For instance, if your GCS Bucket was created in `us-central1`, then BigQuery dataset must be created in the same region (us-central1, in this example)\", 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - Cannot read and write in different locations: source: <REGION_HERE>, destination: <ANOTHER_REGION_HERE>', 'course': 'data-engineering-zoomcamp', 'doc_id': '028c7ec99028bed4f21ef64aca40a762'}, {'text': 'By the way, this isn’t a problem/solution, but a useful hint:\\nPlease, remember to save your progress in BigQuery SQL Editor.\\nI was almost finishing the homework, when my Chrome Tab froze and I had to reload it. Then I lost my entire SQL script.\\nSave your script from time to time. Just click on the button at the top bar. Your saved file will be available on the left panel.\\nAlternatively, you can copy paste your queries into an .sql file in your preferred editor (Notepad++, VS Code, etc.). Using the .sql extension will provide convenient color formatting.', 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - Remember to save your queries', 'course': 'data-engineering-zoomcamp', 'doc_id': '97d9cbb84690c2717488f2e5209a03f5'}, {'text': 'Ans :  While real-time analytics might not be explicitly mentioned, BigQuery has real-time data streaming capabilities, allowing for potential integration in future project iterations.', 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - Can I use BigQuery for real-time analytics in this project?', 'course': 'data-engineering-zoomcamp', 'doc_id': '22a9ae0bbe950219e56bc581b1fec5db'}, {'text': \"could not parse 'pickup_datetime' as timestamp for field pickup_datetime (position 2)\\nThis error is caused by invalid data in the timestamp column. A way to identify the problem is to define the schema from the external table using string datatype. This enables the queries to work at which point we can filter out the invalid rows from the import to the materialised table and insert the fields with the timestamp data type.\", 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - Unable to load data from external tables into a materialized table in BigQuery due to an invalid timestamp error that are added while appending data to the file in Google Cloud Storage', 'course': 'data-engineering-zoomcamp', 'doc_id': 'e258d7d874a417e373e721df5f292757'}, {'text': 'Background:\\n`pd.read_parquet`\\n`pd.to_datetime`\\n`pq.write_to_dataset`\\nReference:\\nhttps://stackoverflow.com/questions/48314880/are-parquet-file-created-with-pyarrow-vs-pyspark-compatible\\nhttps://stackoverflow.com/questions/57798479/editing-parquet-files-with-python-causes-errors-to-datetime-format\\nhttps://www.reddit.com/r/bigquery/comments/16aoq0u/parquet_timestamp_to_bq_coming_across_as_int/?share_id=YXqCs5Jl6hQcw-kg6-VgF&utm_content=1&utm_medium=ios_app&utm_name=ioscss&utm_source=share&utm_term=1\\nSolution:\\nAdd `use_deprecated_int96_timestamps=True` to `pq.write_to_dataset` function, like below\\npq.write_to_dataset(\\ntable,\\nroot_path=root_path,\\nfilesystem=gcs,\\nuse_deprecated_int96_timestamps=True\\n# Write timestamps to INT96 Parquet format\\n)', 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - Error Message in BigQuery: annotated as a valid Timestamp, please annotate it as TimestampType(MICROS) or TimestampType(MILLIS)', 'course': 'data-engineering-zoomcamp', 'doc_id': 'ad18b5172ca2ab722e148b476eb89107'}, {'text': 'Solution:\\nIf you’re using Mage, in the last Data Exporter that writes to Google Cloud Storage use PyArrow to generate the Parquet file with the correct logical type for the datetime columns, otherwise they won\\'t be converted to timestamp when loaded by BigQuery later on.\\nimport pyarrow as pa\\nimport pyarrow.parquet as pq\\nimport os\\nif \\'data_exporter\\' not in globals():\\nfrom mage_ai.data_preparation.decorators import data_exporter\\n# Replace with the location of your service account key JSON file.\\nos.environ[\\'GOOGLE_APPLICATION_CREDENTIALS\\'] = \\'/home/src/personal-gcp.json\\'\\nbucket_name = \"<YOUR_BUCKET_NAME>\"\\nobject_key = \\'nyc_taxi_data_2022.parquet\\'\\nwhere = f\\'{bucket_name}/{object_key}\\'\\n@data_exporter\\ndef export_data(data, *args, **kwargs):\\ntable = pa.Table.from_pandas(data, preserve_index=False)\\ngcs = pa.fs.GcsFileSystem()\\npq.write_table(\\ntable,\\nwhere,\\n# Convert integer columns in Epoch milliseconds\\n# to Timestamp columns in microseconds (\\'us\\') so\\n# they can be loaded into BigQuery with the right\\n# data type\\ncoerce_timestamps=\\'us\\',\\nfilesystem=gcs\\n)\\nSolution 2:\\nIf you’re using Mage, in the last Data Exporter that writes to Google Cloud Storage, provide PyArrow with explicit schema to generate the Parquet file with the correct logical type for the datetime columns, otherwise they won\\'t be converted to timestamp when loaded by BigQuery later on.\\nschema = pa.schema([\\n(\\'vendor_id\\', pa.int64()),\\n(\\'lpep_pickup_datetime\\', pa.timestamp(\\'ns\\')),\\n(\\'lpep_dropoff_datetime\\', pa.timestamp(\\'ns\\')),\\n(\\'store_and_fwd_flag\\', pa.string()),\\n(\\'ratecode_id\\', pa.int64()),\\n(\\'pu_location_id\\', pa.int64()),\\n(\\'do_location_id\\', pa.int64()),\\n(\\'passenger_count\\', pa.int64()),\\n(\\'trip_distance\\', pa.float64()),\\n(\\'fare_amount\\', pa.float64()),\\n(\\'extra\\', pa.float64()),\\n(\\'mta_tax\\', pa.float64()),\\n(\\'tip_amount\\', pa.float64()),\\n(\\'tolls_amount\\', pa.float64()),\\n(\\'improvement_surcharge\\', pa.float64()),\\n(\\'total_amount\\', pa.float64()),\\n(\\'payment_type\\', pa.int64()),\\n(\\'trip_type\\', pa.int64()),\\n(\\'congestion_surcharge\\', pa.float64()),\\n(\\'lpep_pickup_month\\', pa.int64())\\n])\\ntable = pa.Table.from_pandas(data, schema=schema)', 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - Datetime columns in Parquet files created from Pandas show up as integer columns in BigQuery', 'course': 'data-engineering-zoomcamp', 'doc_id': '1e8b11f4057c788ea3597c05a5ae5038'}, {'text': 'Reference:\\nhttps://cloud.google.com/bigquery/docs/external-data-cloud-storage\\nSolution:\\nfrom google.cloud import bigquery\\n# Set table_id to the ID of the table to create\\ntable_id = f\"{project_id}.{dataset_name}.{table_name}\"\\n# Construct a BigQuery client object\\nclient = bigquery.Client()\\n# Set the external source format of your table\\nexternal_source_format = \"PARQUET\"\\n# Set the source_uris to point to your data in Google Cloud\\nsource_uris = [ f\\'gs://{bucket_name}/{object_key}/*\\']\\n# Create ExternalConfig object with external source format\\nexternal_config = bigquery.ExternalConfig(external_source_format)\\n# Set source_uris that point to your data in Google Cloud\\nexternal_config.source_uris = source_uris\\nexternal_config.autodetect = True\\ntable = bigquery.Table(table_id)\\n# Set the external data configuration of the table\\ntable.external_data_configuration = external_config\\ntable = client.create_table(table)  # Make an API request.\\nprint(f\\'Created table with external source: {table_id}\\')\\nprint(f\\'Format: {table.external_data_configuration.source_format}\\')', 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - Create External Table using Python', 'course': 'data-engineering-zoomcamp', 'doc_id': '15a20156271f360b11536bb3d6a98fb6'}, {'text': 'Reference:\\nhttps://stackoverflow.com/questions/60941726/can-bigquery-api-overwrite-existing-table-view-with-create-table-tables-inser\\nSolution:\\nCombine with “Create External Table using Python”, use it before “client.create_table” function.\\ndef tableExists(tableID, client):\\n\"\"\"\\nCheck if a table already exists using the tableID.\\nreturn : (Boolean)\\n\"\"\"\\ntry:\\ntable = client.get_table(tableID)\\nreturn True\\nexcept Exception as e: # NotFound:\\nreturn False', 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - Check BigQuery Table Exist And Delete', 'course': 'data-engineering-zoomcamp', 'doc_id': 'b947c4b9daf651d7856de5dddbf8ec78'}, {'text': 'To avoid this error you can upload data from Google Cloud Storage to BigQuery through BigQuery Cloud Shell using the command:\\n$ bq load  --autodetect --allow_quoted_newlines --source_format=CSV dataset_name.table_name \"gs://dtc-data-lake-bucketname/fhv/fhv_tripdata_2019-*.csv.gz\"', 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - Error: Missing close double quote (\") character', 'course': 'data-engineering-zoomcamp', 'doc_id': 'e0e5f221898f70204919ae5ac3f80077'}, {'text': 'Solution: This problem arises if your gcs and bigquery storage is in different regions.\\nOne potential way to solve it:\\nGo to your google cloud bucket and check the region in field named “Location”\\nNow in bigquery, click on three dot icon near your project name and select create dataset.\\nIn region filed choose the same regions as you saw in your google cloud bucket', 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - Cannot read and write in different locations: source: asia-south2, destination: US', 'course': 'data-engineering-zoomcamp', 'doc_id': 'e95f5ac7f88f301844a5dd045a125114'}, {'text': 'There are multiple benefits of using Cloud Functions to automate tasks in Google Cloud.\\nUse below Cloud Function python script to load files directly to BigQuery. Use your project id, dataset id & table id as defined by you.\\nimport tempfile\\nimport requests\\nimport logging\\nfrom google.cloud import bigquery\\ndef hello_world(request):\\n# table_id = <project_id.dataset_id.table_id>\\ntable_id = \\'de-zoomcap-project.dezoomcamp.fhv-2019\\'\\n# Create a new BigQuery client\\nclient = bigquery.Client()\\nfor month in range(4, 13):\\n# Define the schema for the data in the CSV.gz files\\nurl = \\'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-{:02d}.csv.gz\\'.format(month)\\n# Download the CSV.gz file from Github\\nresponse = requests.get(url)\\n# Create new table if loading first month data else append\\nwrite_disposition_string = \"WRITE_APPEND\" if month > 1 else \"WRITE_TRUNCATE\"\\n# Defining LoadJobConfig with schema of table to prevent it from changing with every table\\njob_config = bigquery.LoadJobConfig(\\nschema=[\\nbigquery.SchemaField(\"dispatching_base_num\", \"STRING\"),\\nbigquery.SchemaField(\"pickup_datetime\", \"TIMESTAMP\"),\\nbigquery.SchemaField(\"dropOff_datetime\", \"TIMESTAMP\"),\\nbigquery.SchemaField(\"PUlocationID\", \"STRING\"),\\nbigquery.SchemaField(\"DOlocationID\", \"STRING\"),\\nbigquery.SchemaField(\"SR_Flag\", \"STRING\"),\\nbigquery.SchemaField(\"Affiliated_base_number\", \"STRING\"),\\n],\\nskip_leading_rows=1,\\nwrite_disposition=write_disposition_string,\\nautodetect=True,\\nsource_format=\"CSV\",\\n)\\n# Load the data into BigQuery\\n# Create a temporary file to prevent the exception- AttributeError: \\'bytes\\' object has no attribute \\'tell\\'\"\\nwith tempfile.NamedTemporaryFile() as f:\\nf.write(response.content)\\nf.seek(0)\\njob = client.load_table_from_file(\\nf,\\ntable_id,\\nlocation=\"US\",\\njob_config=job_config,\\n)\\njob.result()\\nlogging.info(\"Data for month %d successfully loaded into table %s.\", month, table_id)\\nreturn \\'Data loaded into table {}.\\'.format(table_id)', 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - Tip: Using Cloud Function to read csv.gz files from github directly to BigQuery in Google Cloud:', 'course': 'data-engineering-zoomcamp', 'doc_id': 'ac0606c0724c06bb318b337a55ca3b2c'}, {'text': 'You need to uncheck cache preferences in query settings', 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - When querying two different tables external and materialized you get the same result when count(distinct(*))', 'course': 'data-engineering-zoomcamp', 'doc_id': 'a51285691d043314c28f7453912a8a33'}, {'text': 'Problem: When you inject data into GCS using Pandas, there is a chance that some dataset has missing values on  DOlocationID and PUlocationID. Pandas by default will cast these columns as float data type, causing inconsistent data type between parquet in GCS and schema defined in big query. You will see something like this:\\nSolution:\\nFix the data type issue in data pipeline\\nBefore injecting data into GCS, use astype and Int64 (which is different from int64 and accept both missing value and integer exist in the column) to cast the columns.\\nSomething like:\\ndf[\"PUlocationID\"] = df.PUlocationID.astype(\"Int64\")\\ndf[\"DOlocationID\"] = df.DOlocationID.astype(\"Int64\")\\nNOTE: It is best to define the data type of all the columns in the Transformation section of the ETL pipeline before loading to BigQuery', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'GCP BQ - How to handle type error from big query and parquet data?', 'course': 'data-engineering-zoomcamp', 'doc_id': '993efca5e0227b9178433f34b6d531b7'}, {'text': 'Problem occurs when misplacing content after fro``m clause in BigQuery SQLs.\\nCheck to remove any extra apaces or any other symbols, keep in lowercases, digits and dashes only', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'GCP BQ - Invalid project ID . Project IDs must contain 6-63 lowercase letters, digits, or dashes. Some project', 'course': 'data-engineering-zoomcamp', 'doc_id': 'f20fa7c9dd754957e78e4df64d35a7be'}, {'text': 'No. Based on the documentation for Bigquery, it does not support more than 1 column to be partitioned.\\n[source]', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'GCP BQ - Does BigQuery support multiple columns partition?', 'course': 'data-engineering-zoomcamp', 'doc_id': 'cf854d6d177758f5d3aa4b32c47ece9b'}, {'text': 'Error Message:\\nPARTITION BY expression must be DATE(<timestamp_column>), DATE(<datetime_column>), DATETIME_TRUNC(<datetime_column>, DAY/HOUR/MONTH/YEAR), a DATE column, TIMESTAMP_TRUNC(<timestamp_column>, DAY/HOUR/MONTH/YEAR), DATE_TRUNC(<date_column>, MONTH/YEAR), or RANGE_BUCKET(<int64_column>, GENERATE_ARRAY(<int64_value>, <int64_value>[, <int64_value>]))\\nSolution:\\nConvert the column to datetime first.\\ndf[\"pickup_datetime\"] = pd.to_datetime(df[\"pickup_datetime\"])\\ndf[\"dropOff_datetime\"] = pd.to_datetime(df[\"dropOff_datetime\"])', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'GCP BQ - DATE() Error in BigQuery', 'course': 'data-engineering-zoomcamp', 'doc_id': '404d1a954630b06f1522027ed80f3b08'}, {'text': 'Native tables are tables where the data is stored in BigQuery.  External tables store the data outside BigQuery, with BigQuery storing metadata about that external table.\\nResources:\\nhttps://cloud.google.com/bigquery/docs/external-tables\\nhttps://cloud.google.com/bigquery/docs/tables-intro', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'GCP BQ - Native tables vs External tables in BigQuery?', 'course': 'data-engineering-zoomcamp', 'doc_id': '23e7a284a81def51bd78b07b0fe680fa'}, {'text': 'Issue: Tried running command to export ML model from BQ to GCS from Week 3\\nbq --project_id taxi-rides-ny extract -m nytaxi.tip_model gs://taxi_ml_model/tip_model\\nIt is failing on following error:\\nBigQuery error in extract operation: Error processing job Not found: Dataset was not found in location US\\nI verified the BQ data set and gcs bucket are in the same region- us-west1. Not sure how it gets location US. I couldn’t find the solution yet.\\nSolution:  Please enter correct project_id and gcs_bucket folder address. My gcs_bucket folder address is\\ngs://dtc_data_lake_optimum-airfoil-376815/tip_model', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'GCP BQ ML - Unable to run command (shown in video) to export ML model from BQ to GCS', 'course': 'data-engineering-zoomcamp', 'doc_id': 'd216ae3d1f7df0e131f9b65557c70849'}, {'text': \"To solve this error mention the location = US when creating the dim_zones table\\n{{ config(\\nmaterialized='table',\\nlocation='US'\\n) }}\\nJust Update this part to solve the issue and run the dim_zones again and then run the fact_trips\", 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'Dim_zones.sql Dataset was not found in location US When Running fact_trips.sql', 'course': 'data-engineering-zoomcamp', 'doc_id': '58a6ccecbef224117d090ea90fb9f221'}, {'text': 'Solution: proceed with setting up serving_dir on your computer as in the extract_model.md file. Then instead of\\ndocker pull tensorflow/serving\\nuse\\ndocker pull emacski/tensorflow-serving\\nThen\\ndocker run -p 8500:8500 -p 8501:8501 --mount type=bind,source=`pwd`/serving_dir/tip_model,target=/models/tip_model -e MODEL_NAME=tip_model -t emacski/tensorflow-serving\\nThen run the curl command as written, and you should get a prediction.', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'GCP BQ ML - Export ML model to make predictions does not work for MacBook with Apple M1 chip (arm architecture).', 'course': 'data-engineering-zoomcamp', 'doc_id': 'd8106c1b647455454cc129d60338e4d7'}, {'text': 'Try deleting data you’ve saved to your VM locally during ETLs\\nKill processes related to deleted files\\nDownload ncdu and look for large files (pay particular attention to files related to Prefect)\\nIf you delete any files related to Prefect, eliminate caching from your flow code', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'VMs - What do I do if my VM runs out of space?', 'course': 'data-engineering-zoomcamp', 'doc_id': 'ada54c16175f63fe0d0ab287c1873993'}, {'text': \"Ans: What they mean is that they don't want you to do anything more than that. You should load the files into the bucket and create an external table based on those files (but nothing like cleaning the data and putting it in parquet format)\", 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': \"Homework - What does it mean “Stop with loading the files into a bucket.' Stop with loading the files into a bucket?”\", 'course': 'data-engineering-zoomcamp', 'doc_id': '83aa9cfa3a68b5b70b705d49518e160e'}, {'text': 'If for whatever reason you try to read parquets directly from nyc.gov’s cloudfront into pandas, you might run into this error:\\npyarrow.lib.ArrowInvalid: Casting from timestamp[us] to timestamp[ns] would result in out of bounds\\nCause:\\nthere is one errant data record where the dropOff_datetime was set to year 3019 instead of 2019.\\npandas uses “timestamp[ns]” (as noted above), and int64 only allows a ~580 year range, centered on 2000. See `pd.Timestamp.max` and `pd.Timestamp.min`\\nThis becomes out of bounds when pandas tries to read it because 3019 > 2300 (approx value of pd.Timestamp.Max\\nFix:\\nUse pyarrow to read it:\\nimport pyarrow.parquet as pq df = pq.read_table(\\'fhv_tripdata_2019-02.parquet\\').to_pandas(safe=False)\\nHowever this results in weird timestamps for the offending record\\nRead the datetime columns separately using pq.read_table\\n\\ntable = pq.read_table(‘taxi.parquet’)\\ndatetimes = [‘list of datetime column names’]\\ndf_dts = pd.DataFrame()\\nfor col in datetimes:\\ndf_dts[col] = pd.to_datetime(table .column(col), errors=\\'coerce\\')\\n\\nThe `errors=’coerce’` parameter will convert the out of bounds timestamps into either the max or the min\\nUse parquet.compute.filter to remove the offending rows\\n\\nimport pyarrow.compute as pc\\ntable = pq.read_table(\"‘taxi.parquet\")\\ndf = table.filter(\\npc.less_equal(table[\"dropOff_datetime\"], pa.scalar(pd.Timestamp.max))\\n).to_pandas()', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'Homework - Reading parquets from nyc.gov directly into pandas returns Out of bounds error', 'course': 'data-engineering-zoomcamp', 'doc_id': '7ac17b5474d555fc2542dbfb4ed97dcd'}, {'text': 'Answer: The 2022 NYC taxi data parquet files are available for each month separately. Therefore, you need to add all 12 files to your GCS bucket and then refer to them using the URIs option when creating an external table in BigQuery. You can use the wildcard \"*\" to refer to all 12 files using a single string.', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'Question: for homework 3 , we need all 12 parquet files for green taxi 2022 right ?', 'course': 'data-engineering-zoomcamp', 'doc_id': '4af3391cfed0c4153f92c937dd22d178'}, {'text': 'This can help avoid schema issues in the homework. \\nDownload files locally and use the ‘upload files’ button in GCS at the desired path. You can upload many files at once. You can also choose to upload a folder.', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'Homework - Uploading files to GCS via GUI', 'course': 'data-engineering-zoomcamp', 'doc_id': '0aa7dbd4143a5324f7f6e85589eace61'}, {'text': 'Ans: Take a careful look at the format of the dates in the question.', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'Homework - Qn 5: The partitioned/clustered table isn’t giving me the prediction I expected', 'course': 'data-engineering-zoomcamp', 'doc_id': '0ca9323fe927c9bffb032fea7ed68f6f'}, {'text': 'Many people aren’t getting an exact match, but are very close to one of the options. As per Alexey said to choose the closest option.', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'Homework - Qn 6: Did anyone get an exact match for one of the options given in Module 3 homework Q6?', 'course': 'data-engineering-zoomcamp', 'doc_id': '562f8b274f9bcaa51c8b21e4168e2850'}, {'text': 'UnicodeDecodeError: \\'utf-8\\' codec can\\'t decode byte 0xa0 in position 41721: invalid start byte\\nSolution:\\nStep 1: When reading the data from the web into the pandas dataframe mention the encoding as follows:\\npd.read_csv(dataset_url, low_memory=False, encoding=\\'latin1\\')\\nStep 2: When writing the dataframe from the local system to GCS as a csv mention the encoding as follows:\\ndf.to_csv(path_on_gsc, compression=\"gzip\", encoding=\\'utf-8\\')\\nAlternative: use pd.read_parquet(url)', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'Python - invalid start byte Error Message', 'course': 'data-engineering-zoomcamp', 'doc_id': '7720f64f62d3a05334c70e58c32c8397'}, {'text': 'A generator is a function in python that returns an iterator using the yield keyword.\\nA generator is a special type of iterable, similar to a list or a tuple, but with a crucial difference. Instead of creating and storing all the values in memory at once, a generator generates values on-the-fly as you iterate over it. This makes generators memory-efficient, particularly when dealing with large datasets.', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'Python - Generators in python', 'course': 'data-engineering-zoomcamp', 'doc_id': '5305ee09e1f8844a37eb47f92e7a8ed2'}, {'text': 'The read_parquet function supports a list of files as an argument. The list of files will be merged into a single result table.', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'Python - Easiest way to read multiple files at the same time?', 'course': 'data-engineering-zoomcamp', 'doc_id': '8e46b4e5c2af0bf2358d3277a56c4bb4'}, {'text': \"Incorrect:\\ndf['DOlocationID'] = pd.to_numeric(df['DOlocationID'], downcast=integer) or\\ndf['DOlocationID'] = df['DOlocationID'].astype(int)\\nCorrect:\\ndf['DOlocationID'] = df['DOlocationID'].astype('Int64')\", 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': \"Python - These won't work. You need to make sure you use Int64:\", 'course': 'data-engineering-zoomcamp', 'doc_id': '98f0115a5f41cdb68b2bb4e6a98eaa0c'}, {'text': \"ValueError: Path /Users/kt/.prefect/storage/44ccce0813ed4f24ab2d3783de7a9c3a does not exist.\\nRemove ```cache_key_fn=task_input_hash ``` as it’s in argument in your function & run your flow again.\\nNote: catche key is beneficial if you happen to run the code multiple times, it won't repeat the process which you have finished running in the previous run.  That means, if you have this ```cache_key``` in your initial run, this might cause the error.\", 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'Prefect - Error on Running Prefect Flow to Load data to GCS', 'course': 'data-engineering-zoomcamp', 'doc_id': 'f53995481a9a9e025455a8a1f3847819'}, {'text': '@task\\ndef download_file(url: str, file_path: str):\\nresponse = requests.get(url)\\nopen(file_path, \"wb\").write(response.content)\\nreturn file_path\\n@flow\\ndef extract_from_web() -> None:\\nfile_path = download_file(url=f\\'{url-filename}.csv.gz\\',file_path=f\\'{filename}.csv.gz\\')', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Prefect - Tip: Downloading csv.gz from a url in a prefect environment (sample snippet).', 'course': 'data-engineering-zoomcamp', 'doc_id': '4e4a3188963f28721840645b19c38c9b'}, {'text': 'Update the seed column types in the dbt_project.yaml file\\nfor using double : float\\nfor using int : numeric\\nDBT Cloud production error: prod dataset not available in location EU\\nProblem: I am trying to deploy my DBT  models to production, using DBT Cloud. The data should live in BigQuery.  The dataset location is EU.  However, when I am running the model in production, a prod dataset is being create in BigQuery with a location US and the dbt invoke build is failing giving me \"ERROR 404: porject.dataset:prod not available in location EU\". I tried different ways to fix this. I am not sure if there is a more simple solution then creating my project or buckets in location US. Hope anyone can help here.\\nNote: Everything is working fine in development mode, the issue is just happening when scheduling and running job in production\\nSolution: I created the prod dataset manually in BQ and specified EU, then I ran the job.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'If you are getting not found in location us error.', 'course': 'data-engineering-zoomcamp', 'doc_id': 'bc902576bcb529437abb2fd323aa1b15'}, {'text': 'Error: This project does not have a development environment configured. Please create a development environment and configure your development credentials to use the dbt IDE.\\nThe error itself tells us how to solve this issue, the guide is here. And from videos @1:42 and also slack chat', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Setup - No development environment', 'course': 'data-engineering-zoomcamp', 'doc_id': 'b837581146fa27bb174cd5fe7b6cf752'}, {'text': \"Runtime Error\\ndbt was unable to connect to the specified database.\\nThe database returned the following error:\\n>Database Error\\nAccess Denied: Project <project_name>: User does not have bigquery.jobs.create permission in project <project_name>.\\nCheck your database credentials and try again. For more information, visit:\\nhttps://docs.getdbt.com/docs/configure-your-profile\\nSteps to resolve error in Google Cloud:\\n1. Navigate to IAM & Admin and select IAM\\n2. Click Grant Access if your newly created dbt service account isn't listed\\n3. In New principals field, add your service account\\n4. Select a Role and search for BigQuery Job User to add\\n5. Go back to dbt cloud project setup and Test your connection\\n6. Note: Also add BigQuery Data Owner, Storage Object Admin, & Storage Admin to prevent permission issues later in the course\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'Setup - Connecting dbt Cloud with BigQuery Error', 'course': 'data-engineering-zoomcamp', 'doc_id': 'e438b8a5df11bdae2b11e27c549a2f11'}, {'text': 'error: This dbt Cloud run was cancelled because a valid dbt project was not found. Please check that the repository contains a proper dbt_project.yml config file. If your dbt project is located in a subdirectory of the connected repository, be sure to specify its location on the Project settings page in dbt Cloud', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Dbt build error', 'course': 'data-engineering-zoomcamp', 'doc_id': '664afcccf5f2d4a3132bb6e1333e4eb2'}, {'text': \"Error: Failed to clone repository.\\ngit clone git@github.com:DataTalksClub/data-engineering-zoomcamp.git /usr/src/develop/…\\nCloning into '/usr/src/develop/...\\nWarning: Permanently added 'github.com,140.82.114.4' (ECDSA) to the list of known hosts.\\ngit@github.com: Permission denied (publickey).\\nfatal: Could not read from remote repository.\\nIssue: You don’t have permissions to write to DataTalksClub/data-engineering-zoomcamp.git\\nSolution 1: Clone the repository and use this forked repo, which contains your github username. Then, proceed to specify the path, as in:\\n[your github username]/data-engineering-zoomcamp.git\\nSolution 2: create a fresh repo for dbt-lessons. We’d need to do branching and PRs in this lesson, so it might be a good idea to also not mess up your whole other repo. Then you don’t have to create a subfolder for the dbt project files\\nSolution 3: Use https link\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'Setup - Failed to clone repository.', 'course': 'data-engineering-zoomcamp', 'doc_id': '57f9e473194ac44ff5381544ac0febad'}, {'text': \"Solution:\\nCheck if you’re on the Developer Plan. As per the prerequisites, you'll need to be enrolled in the Team Plan or Enterprise Plan to set up a CI Job in dbt Cloud.\\nSo If you're on the Developer Plan, you'll need to upgrade to utilise CI Jobs.\\nNote from another user: I’m in the Team Plan (trial period) but the option is still disabled. What worked for me instead was this. It works for the Developer (free) plan.\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'dbt job - Triggered by pull requests is disabled when I try to create a new Continuous Integration job in dbt cloud.', 'course': 'data-engineering-zoomcamp', 'doc_id': 'a417cd2fea0f4a266393cf118752c7c0'}, {'text': 'Issue: If the DBT cloud IDE loading indefinitely then giving you this error\\nSolution: check the dbt_cloud_setup.md  file and make a SSH Key and use gitclone to import repo into dbt project, copy and paste deploy key back in your repo setting.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Setup - Your IDE session was unable to start. Please contact support.', 'course': 'data-engineering-zoomcamp', 'doc_id': '9bca93be39d55769af52c5d99f89ec7c'}, {'text': 'Issue: If you don’t define the column format while converting from csv to parquet Python will “choose” based on the first rows.\\n✅Solution: Defined the schema while running web_to_gcp.py pipeline.\\nSebastian adapted the script:\\nhttps://github.com/sebastian2296/data-engineering-zoomcamp/blob/main/week_4_analytics_engineering/web_to_gcs.py\\nNeed a quick change to make the file work with gz files, added the following lines (and don’t forget to delete the file at the end of each iteration of the loop to avoid any problem of disk space)\\nfile_name_gz = f\"{service}_tripdata_{year}-{month}.csv.gz\"\\nopen(file_name_gz, \\'wb\\').write(r.content)\\nos.system(f\"gzip -d {file_name_gz}\")\\nos.system(f\"rm {file_name_init}.*\")\\nSame ERROR - When running dbt run for fact_trips.sql, the task failed with error:\\n“Parquet column \\'ehail_fee\\' has type DOUBLE which does not match the target cpp_type INT64”\\n开启屏幕阅读器支持\\n要启用屏幕阅读器支持，请按Ctrl+Alt+Z。要了解键盘快捷键，请按Ctrl+斜杠。\\n查找和替换\\nReason: Parquet files have their own schema. Some parquet files for green data have records with decimals in ehail_fee column.\\nThere are some possible fixes:\\nDrop ehail_feel column since it is not really used. For instance when creating a partitioned table from the external table in BigQuery\\nSELECT * EXCEPT (ehail_fee) FROM…\\nModify stg_green_tripdata.sql model using this line cast(0 as numeric) as ehail_fee.\\nModify Airflow dag to make the conversion and avoid the error.\\npv.read_csv(src_file, convert_options=pv.ConvertOptions(column_types = {\\'ehail_fee\\': \\'float64\\'}))\\nSame type of ERROR - parquet files with different data types - Fix it with pandas\\nHere is another possibility that could be interesting:\\nYou can specify the dtypes when importing the file from csv to a dataframe with pandas\\npd.from_csv(..., dtype=type_dict)\\nOne obstacle is that the regular int64 pandas use (I think this is from the numpy library) does not accept null values (NaN, not a number). But you can use the pandas Int64 instead, notice capital ‘I’. The type_dict is a python dictionary mapping the column names to the dtypes.\\nSources:\\nhttps://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\\nNullable integer data type — pandas 1.5.3 documentation', 'section': 'Module 4: analytics engineering with dbt', 'question': 'DBT - I am having problems with columns datatype while running DBT/BigQuery', 'course': 'data-engineering-zoomcamp', 'doc_id': '1189db657523527c5ef3f9d32a14ffed'}, {'text': 'If the provided URL isn’t working for you (https://nyc-tlc.s3.amazonaws.com/trip+data/):\\nWe can use the GitHub CLI to easily download the needed trip data from https://github.com/DataTalksClub/nyc-tlc-data, and manually upload to a GCS bucket.\\nInstructions on how to download the CLI here: https://github.com/cli/cli\\nCommands to use:\\ngh auth login\\ngh release list -R DataTalksClub/nyc-tlc-data\\ngh release download yellow -R DataTalksClub/nyc-tlc-data\\ngh release download green -R DataTalksClub/nyc-tlc-data\\netc.\\nNow you can upload the files to a GCS bucket using the GUI.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Ingestion: When attempting to use the provided quick script to load trip data into GCS, you receive error Access Denied from the S3 bucket', 'course': 'data-engineering-zoomcamp', 'doc_id': '33f4dbd02b63cbc4859ae58692aa3814'}, {'text': \"R: This conversion is needed for the question 3 of homework, in order to process files for fhv data. The error is:\\npyarrow.lib.ArrowInvalid: CSV parse error: Expected 7 columns, got 1: B02765\\nCause: Some random line breaks in this particular file.\\nFixed by opening a bash in the container executing the dag and manually running the following command that deletes all \\\\n not preceded by \\\\r.\\nperl -i -pe 's/(?<!\\\\r)\\\\n/\\\\1/g' fhv_tripdata_2020-01.csv\\nAfter that, clear the failed task in Airflow to force re-execution.\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'Ingestion - Error thrown by format_to_parquet_task when converting fhv_tripdata_2020-01.csv using Airflow', 'course': 'data-engineering-zoomcamp', 'doc_id': 'c381de406ce99a7983abe013b35a8f7f'}, {'text': 'I initially followed data-engineering-zoomcamp/03-data-warehouse/extras/web_to_gcs.py at main · DataTalksClub/data-engineering-bootcamp (github.com)\\nBut it was taking forever for the yellow trip data and when I tried to download and upload the parquet files directly to GCS, that works fine but when creating the Bigquery table, there was a schema inconsistency issue\\nThen I found another hack shared in the slack which was suggested by Victoria.\\n[Optional] Hack for loading data to BigQuery for Week 4 - YouTube\\nPlease watch until the end as there is few schema changes required to be done', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Hack to load yellow and green trip data for 2019 and 2020', 'course': 'data-engineering-zoomcamp', 'doc_id': 'f6230b62b5e6576973f62814abbf0c46'}, {'text': '“gs\\\\storage_link\\\\*.parquet” need to be added in destination folder', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Move many files (more than one) from Google cloud storage bucket to Big query', 'course': 'data-engineering-zoomcamp', 'doc_id': '409a0d08dcbd219fdd2a340d081555e0'}, {'text': 'One common cause experienced is lack of space after running prefect several times. When running prefect, check the folder ‘.prefect/storage’ and delete the logs now and then to avoid the problem.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'GCP VM - All of sudden ssh stopped working for my VM after my last restart', 'course': 'data-engineering-zoomcamp', 'doc_id': '6979d254c9a326d21dafd5d662d44a3b'}, {'text': 'You can try to do this steps:', 'section': 'Module 4: analytics engineering with dbt', 'question': 'GCP VM - If you have lost SSH access to your machine due to lack of space. Permission denied (publickey)', 'course': 'data-engineering-zoomcamp', 'doc_id': 'e203ba14d396ab76cca94f2eafcf801f'}, {'text': 'R: Go to BigQuery, and check the location of BOTH\\nThe source dataset (trips_data_all), and\\nThe schema you’re trying to write to (name should be \\tdbt_<first initial><last name> (if you didn’t change the default settings at the end when setting up your project))\\nLikely, your source data will be in your region, but the write location will be a multi-regional location (US in this example). Delete these datasets, and recreate them with your specified region and the correct naming format.\\nAlternatively, instead of removing datasets, you can specify the single-region location you are using. E.g. instead of ‘location: US’, specify the region, so ‘location: US-east1’. See this Github comment for more detail. Additionally please see this post of Sandy\\nIn DBT cloud you can actually specify the location using the following steps:\\nGPo to your profile page (top right drop-down --> profile)\\nThen go to under Credentials --> Analytics (you may have customised this name)\\nClick on Bigquery >\\nHit Edit\\nUpdate your location, you may need to re-upload your service account JSON to re-fetch your private key, and save. (NOTE: be sure to exactly copy the region BigQuery specifies your dataset is in.)', 'section': 'Module 4: analytics engineering with dbt', 'question': '404 Not found: Dataset eighth-zenith-372015:trip_data_all was not found in location us-west1', 'course': 'data-engineering-zoomcamp', 'doc_id': '1d7707cfd71be984b4016abf81be12eb'}, {'text': 'Error: `dbt_utils.surrogate_key` has been replaced by `dbt_utils.generate_surrogate_key`\\nFix:\\nReplace dbt_utils.surrogate_key  with dbt_utils.generate_surrogate_key in stg_green_tripdata.sql\\nWhen executing dbt run after fact_trips.sql has been created, the task failed with error:\\nR: “Access Denied: BigQuery BigQuery: Permission denied while globbing file pattern.”\\n1. Fixed by adding the Storage Object Viewer role to the service account in use in BigQuery.\\n2. Add the related roles to the service account in use in GCS.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'When executing dbt run after installing dbt-utils latest version i.e., 1.0.0 warning has generated', 'course': 'data-engineering-zoomcamp', 'doc_id': 'eea31aad3d0762ceeeebf2149bceba48'}, {'text': 'You need to create packages.yml file in main project directory and add packages’ meta data:\\npackages:\\n- package: dbt-labs/dbt_utils\\nversion: 0.8.0\\nAfter creating file run:\\nAnd hit enter.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'When You are getting error dbt_utils not found', 'course': 'data-engineering-zoomcamp', 'doc_id': 'a538abbb25dab6c8353ebf0fee4a73f3'}, {'text': \"Ensure you properly format your yml file. Check the build logs if the run was completed successfully. You can expand the command history console (where you type the --vars '{'is_test_run': 'false'}')  and click on any stage’s logs to expand and read errors messages or warnings.\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'Lineage is currently unavailable. Check that your project does not contain compilation errors or contact support if this error persists.', 'course': 'data-engineering-zoomcamp', 'doc_id': 'd17c30f5bbb71d60b08cf066dea9c999'}, {'text': \"Make sure you use:\\ndbt run --var ‘is_test_run: false’ or\\ndbt build --var ‘is_test_run: false’\\n(watch out for formatted text from this document: re-type the single quotes). If that does not work, use --vars '{'is_test_run': 'false'}' with each phrase separately quoted.\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'Build - Why do my Fact_trips only contain a few days of data?', 'course': 'data-engineering-zoomcamp', 'doc_id': '7e5a325f9fc3fd371b05838cda828eaf'}, {'text': 'Check if you specified if_exists argument correctly when writing data from GCS to BigQuery. When I wrote my automated flow for each month of the years 2019 and 2020 for green and yellow data I had specified if_exists=\"replace\" while I was experimenting with the flow setup. Once you want to run the flow for all months in 2019 and 2020 make sure to set if_exists=\"append\"\\nif_exists=\"replace\" will replace the whole table with only the month data that you are writing into BigQuery in that one iteration -> you end up with only one month in BigQuery (the last one you inserted)\\nif_exists=\"append\" will append the new monthly data -> you end up with data from all months', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Build - Why do my fact_trips only contain one month of data?', 'course': 'data-engineering-zoomcamp', 'doc_id': 'd5ae6d3d038eb0bed69732d073071ae9'}, {'text': \"R: After the second SELECT, change this line:\\ndate_trunc('month', pickup_datetime) as revenue_month,\\nTo this line:\\ndate_trunc(pickup_datetime, month) as revenue_month,\\nMake sure that “month” isn’t surrounded by quotes!\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'BigQuery returns an error when I try to run the dm_monthly_zone_revenue.sql model.', 'course': 'data-engineering-zoomcamp', 'doc_id': '9210f659302de46887a6765f4aac8df2'}, {'text': 'For this instead:\\n{{ dbt_utils.generate_surrogate_key([ \\n     field_a, \\n     field_b, \\n     field_c,\\n     …,\\n     field_z\\n]) }}', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Replace: \\n{{ dbt_utils.surrogate_key([ \\n     field_a, \\n     field_b, \\n     field_c,\\n     …,\\n     field_z     \\n]) }}', 'course': 'data-engineering-zoomcamp', 'doc_id': '9f5ad916ce1a1c9d892d050fb8c7c6e5'}, {'text': 'Remove the dataset from BigQuery which was created by dbt and run dbt run again so that it will recreate the dataset in BigQuery with the correct location', 'section': 'Module 4: analytics engineering with dbt', 'question': 'I changed location in dbt, but dbt run still gives me an error', 'course': 'data-engineering-zoomcamp', 'doc_id': '410d421e59dd5fc6c077374237b9e691'}, {'text': 'Remove the dataset from BigQuery created by dbt and run again (with test disabled) to ensure the dataset created has all the rows.\\nDBT - Why am I getting a new dataset after running my CI/CD Job? / What is this new dbt dataset in BigQuery?\\nAnswer: when you create the CI/CD job, under ‘Compare Changes against an environment (Deferral) make sure that you select ‘ No; do not defer to another environment’ - otherwise dbt won’t merge your dev models into production models; it will create a new environment called ‘dbt_cloud_pr_number of pull request’', 'section': 'Module 4: analytics engineering with dbt', 'question': 'I ran dbt run without specifying variable which gave me a table of 100 rows. I ran again with the variable value specified but my table still has 100 rows in BQ.', 'course': 'data-engineering-zoomcamp', 'doc_id': '2801500c05ebd8c6edc453c7f0fe4bcd'}, {'text': \"Vic created three different datasets in the videos.. dbt_<name> was used for development and you used a production dataset for the production environment. What was the use for the staging dataset?\\nR: Staging, as the name suggests, is like an intermediate between the raw datasets and the fact and dim tables, which are the finished product, so to speak. You'll notice that the datasets in staging are materialised as views and not tables.\\nVic didn't use it for the project, you just need to create production and dbt_name + trips_data_all that you had already.\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'Why do we need the Staging dataset?', 'course': 'data-engineering-zoomcamp', 'doc_id': '225313d7c5664c939ea5870af4835a77'}, {'text': 'Try removing the “network: host” line in docker-compose.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'DBT Docs Served but Not Accessible via Browser', 'course': 'data-engineering-zoomcamp', 'doc_id': '2d8e2ef15c0b90f1cbadc3042f57d84e'}, {'text': 'Go to Account settings >> Project >> Analytics >> Click on your connection >> go all the way down to Location and type in the GCP location just as displayed in GCP (e.g. europe-west6). You might need to reupload your GCP key.\\nDelete your dataset in GBQ\\nRebuild project: dbt build\\nNewly built dataset should be in the correct location', 'section': 'Module 4: analytics engineering with dbt', 'question': 'BigQuery adapter: 404 Not found: Dataset was not found in location europe-west6', 'course': 'data-engineering-zoomcamp', 'doc_id': 'f6a6512b657392e9385c51f6ed803f2b'}, {'text': 'Create a new branch to edit. More on this can be found here in the dbt docs.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Dbt+git - Main branch is “read-only”', 'course': 'data-engineering-zoomcamp', 'doc_id': '76668af37d135d410c0e639cf2e64d63'}, {'text': 'Create a new branch for development, then you can merge it to the main branch\\nCreate a new branch and switch to this branch. It allows you to make changes. Then you can commit and push the changes to the “main” branch.', 'section': 'Module 4: analytics engineering with dbt', 'question': \"Dbt+git - It appears that I can't edit the files because I'm in read-only mode. Does anyone know how I can change that?\", 'course': 'data-engineering-zoomcamp', 'doc_id': '9b1199c201587cc4349e6c5dcda015f4'}, {'text': \"Error:\\nTriggered by pull requests\\nThis feature is only available for dbt repositories connected through dbt Cloud's native integration with Github, Gitlab, or Azure DevOps\\nSolution: Contrary to the guide on DTC repo, don’t use the Git Clone option. Use the Github one instead. Step-by-step guide to UN-LINK Git Clone and RE-LINK with Github in the next entry below\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'Dbt deploy + Git CI - cannot create CI checks job for deployment to Production. See more discussion in slack chat', 'course': 'data-engineering-zoomcamp', 'doc_id': 'e8bb94f8325b7fc04997732c7563325e'}, {'text': 'If you’re trying to configure CI with Github and on the job’s options you can’t see Run on Pull Requests? on triggers, you have to reconnect with Github using native connection instead clone by SSH. Follow these steps:\\nOn Profile Settings > Linked Accounts connect your Github account with dbt project allowing the permissions asked. More info at https://docs.getdbt.com/docs/collaborate/git/connect-gith\\nDisconnect your current Github’s configuration from Account Settings > Projects (analytics) > Github connection. At the bottom left appears the button Disconnect, press it.\\nOnce we have confirmed the change, we can configure it again. This time, choose Github and it will appear in all repositories which you have allowed to work with dbt. Select your repository and it’s ready.\\nGo to the Deploy > job configuration’s page and go down until Triggers and now you can see the option Run on Pull Requests:', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Dbt deploy + Git CI - Unable to configure Continuous Integration (CI) with Github', 'course': 'data-engineering-zoomcamp', 'doc_id': 'a35fac43ba748e61d3e794f5f746b880'}, {'text': \"If you're following video DE Zoomcamp 4.3.1 - Building the First DBT Models, you may have encountered an issue at 14:25 where the Lineage graph isn't displayed and a Compilation Error occurs, as shown in the attached image. Don't worry - a quick fix for this is to simply save your schema.yml file. Once you've done this, you should be able to view your Lineage graph without any further issues.\", 'section': 'Module 4: analytics engineering with dbt', 'question': \"Compilation Error (Model 'model.my_new_project.stg_green_tripdata' (models/staging/stg_green_tripdata.sql) depends on a source named 'staging.green_trip_external' which was not found)\", 'course': 'data-engineering-zoomcamp', 'doc_id': 'b290dbc9a4a8dd9b07cc4907fc6af297'}, {'text': '> in macro test_accepted_values (tests/generic/builtin.sql)\\n> called by test accepted_values_stg_green_tripdata_Payment_type__False___var_payment_type_values_ (models/staging/schema.yml)\\nRemember that you have to add to dbt_project.yml the vars:\\nvars:\\npayment_type_values: [1, 2, 3, 4, 5, 6]', 'section': 'Module 4: analytics engineering with dbt', 'question': \"'NoneType' object is not iterable\", 'course': 'data-engineering-zoomcamp', 'doc_id': '1239fa510881a1f5a631c0431acacd33'}, {'text': \"You will face this issue if you copied and pasted the exact macro directly from data-engineering-zoomcamp repo.\\nBigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for operator CASE for argument types: STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, NULL at [35:5]; reason: invalidQuery, location: query, message: No matching signature for operator CASE for argument types: STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, NULL at [35:5]')\\nWhat you’d have to do is to change the data type of the numbers (1, 2, 3 etc.) to text by inserting ‘’, as the initial ‘payment_type’ data type should be string (Note: I extracted and loaded the green trips data using Google BQ Marketplace)\\n{#\\nThis macro returns the description of the payment_type\\n#}\\n{% macro get_payment_type_description(payment_type) -%}\\ncase {{ payment_type }}\\nwhen '1' then 'Credit card'\\nwhen '2' then 'Cash'\\nwhen '3' then 'No charge'\\nwhen '4' then 'Dispute'\\nwhen '5' then 'Unknown'\\nwhen '6' then 'Voided trip'\\nend\\n{%- endmacro %}\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'dbt macro errors with get_payment_type_description(payment_type)', 'course': 'data-engineering-zoomcamp', 'doc_id': '4860112b4167caa1e502ac33e05746a0'}, {'text': 'The dbt error  log contains a link to BigQuery. When you follow it you will see your query and the problematic line will be highlighted.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Troubleshooting in dbt:', 'course': 'data-engineering-zoomcamp', 'doc_id': '933a90b803fe78752fe43f338f2e0bf7'}, {'text': 'It is a default behaviour of dbt to append custom schema to initial schema. To override this behaviour simply create a macro named “generate_schema_name.sql”:\\n{% macro generate_schema_name(custom_schema_name, node) -%}\\n{%- set default_schema = target.schema -%}\\n{%- if custom_schema_name is none -%}\\n{{ default_schema }}\\n{%- else -%}\\n{{ custom_schema_name | trim }}\\n{%- endif -%}\\n{%- endmacro %}\\nNow you can override default custom schema in “dbt_project.yml”:', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Why changing the target schema to “marts” actually creates a schema named “dbt_marts” instead?', 'course': 'data-engineering-zoomcamp', 'doc_id': '2588b789b16c5a507a85c3d82bd1bb9a'}, {'text': 'There is a project setting which allows you to set `Project subdirectory` in dbt cloud:', 'section': 'Module 4: analytics engineering with dbt', 'question': 'How to set subdirectory of the github repository as the dbt project root', 'course': 'data-engineering-zoomcamp', 'doc_id': 'a224104dbd0cb17795551f0a8da8437b'}, {'text': \"Remember that you should modify accordingly your .sql models, to read from existing table names in BigQuery/postgres db\\nExample: select * from {{ source('staging',<your table name in the database>') }}\", 'section': 'Module 4: analytics engineering with dbt', 'question': \"Compilation Error : Model 'model.XXX' (models/<model_path>/XXX.sql) depends on a source named '<a table name>' which was not found\", 'course': 'data-engineering-zoomcamp', 'doc_id': '7f65b4ec80e16bc0b24a5fb8787757e8'}, {'text': 'Make sure that you create a pull request from your Development branch to the Production branch (main by default). After that, check in your ‘seeds’ folder if the seed file is inside it.\\nAnother thing to check is your .gitignore file. Make sure that the .csv extension is not included.', 'section': 'Module 4: analytics engineering with dbt', 'question': \"Compilation Error : Model '<model_name>' (<model_path>) depends on a node named '<seed_name>' which was not found   (Production Environment)\", 'course': 'data-engineering-zoomcamp', 'doc_id': 'c1618ee0fcadc3c77318502b9390e60e'}, {'text': '1. Go to your dbt cloud service account\\n1. Adding the  [Storage Object Admin,Storage Admin] role in addition tco BigQuery Admin.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'When executing dbt run after using fhv_tripdata as an external table: you get “Access Denied: BigQuery BigQuery: Permission denied”', 'course': 'data-engineering-zoomcamp', 'doc_id': '21e809f69a098e5193f2cb81fcc18336'}, {'text': 'Problem: when injecting data to bigquery, you may face the type error. This is because pandas by default will parse integer columns with missing value as float type.\\nSolution:\\nOne way to solve this problem is to specify/ cast data type Int64 during the data transformation stage.\\nHowever, you may be lazy to type all the int columns. If that is the case, you can simply use convert_dtypes to infer the data type\\n# Make pandas to infer correct data type (as pandas parse int with missing as float)\\ndf.fillna(-999999, inplace=True)\\ndf = df.convert_dtypes()\\ndf = df.replace(-999999, None)', 'section': 'Module 4: analytics engineering with dbt', 'question': 'How to automatically infer the column data type (pandas missing value issues)?', 'course': 'data-engineering-zoomcamp', 'doc_id': '63576af395086fb8d1a3855d78b625a4'}, {'text': 'Seed files loaded from directory with name ‘seed’, that’s why you should rename dir with name ‘data’ to ‘seed’', 'section': 'Module 4: analytics engineering with dbt', 'question': 'When loading github repo raise exception that ‘taxi_zone_lookup’ not found', 'course': 'data-engineering-zoomcamp', 'doc_id': '7a80a0966fcbaf52d51c57a0218e63f1'}, {'text': 'Check the .gitignore file and make sure you don’t have *.csv in it\\n\\nDbt error 404 was not found in location\\nMy specific error:\\nRuntime Error in rpc request (from remote system.sql) 404 Not found: Table dtc-de-0315:trips_data_all.green_tripdata_partitioned was not found in location europe-west6 Location: europe-west6 Job ID: 168ee9bd-07cd-4ca4-9ee0-4f6b0f33897c\\nMake sure all of your datasets have the correct region and not a generalised region:\\nEurope-west6 as opposed to EU\\n\\nMatch this in dbt settings:\\ndbt -> projects -> optional settings -> manually set location to match', 'section': 'Module 4: analytics engineering with dbt', 'question': '‘taxi_zone_lookup’ not found', 'course': 'data-engineering-zoomcamp', 'doc_id': '545d5b92fb14b2dd9a129234423caa43'}, {'text': \"The easiest way to avoid these errors is by ingesting the relevant data in a .csv.gz file type. Then, do:\\nCREATE OR REPLACE EXTERNAL TABLE `dtc-de.trips_data_all.fhv_tripdata`\\nOPTIONS (\\nformat = 'CSV',\\nuris = ['gs://dtc_data_lake_dtc-de-updated/data/fhv_all/fhv_tripdata_2019-*.csv.gz']\\n);\\nAs an example. You should no longer have any data type issues for week 4.\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'Data type errors when ingesting with parquet files', 'course': 'data-engineering-zoomcamp', 'doc_id': 'da8518e178713c457fea9ae3d8c4959a'}, {'text': 'This is due to the way the deduplication is done in the two staging files.\\nSolution: add order by in the partition by part of both staging files. Keep adding columns to order by until the number of rows in the fact_trips table is consistent when re-running the fact_trips model.\\nExplanation (a bit convoluted, feel free to clarify, correct etc.)\\nWe partition by vendor id and pickup_datetime and choose the first row (rn=1) from all these partitions. These partitions are not ordered, so every time we run this, the first row might be a different one. Since the first row is different between runs, it might or might not contain an unknown borough. Then, in the fact_trips model we will discard a different number of rows when we discard all values with an unknown borough.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Inconsistent number of rows when re-running fact_trips model', 'course': 'data-engineering-zoomcamp', 'doc_id': 'b9063449e12981e3117b1870d2aa12c1'}, {'text': 'If you encounter data type error on trip_type column, it may due to some nan values that isn’t null in bigquery.\\nSolution: try casting it to FLOAT datatype instead of NUMERIC', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Data Type Error when running fact table', 'course': 'data-engineering-zoomcamp', 'doc_id': 'b2e2c5135204ae8635d73daa7781b06f'}, {'text': \"This error could result if you are using some select * query without mentioning the name of table for ex:\\nwith dim_zones as (\\nselect * from `engaged-cosine-374921`.`dbt_victoria_mola`.`dim_zones`\\nwhere borough != 'Unknown'\\n),\\nfhv as (\\nselect * from `engaged-cosine-374921`.`dbt_victoria_mola`.`stg_fhv_tripdata`\\n)\\nselect * from fhv\\ninner join dim_zones as pickup_zone\\non fhv.PUlocationID = pickup_zone.locationid\\ninner join dim_zones as dropoff_zone\\non fhv.DOlocationID = dropoff_zone.locationid\\n);\\nTo resolve just replace use : select fhv.* from fhv\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'CREATE TABLE has columns with duplicate name locationid.', 'course': 'data-engineering-zoomcamp', 'doc_id': 'c297422bcae4c71b2373ff8e975b017b'}, {'text': 'Some ehail fees are null and casting them to integer gives Bad int64 value: 0.0 error,\\nSolution:\\nUsing safe_cast returns NULL instead of throwing an error. So use safe_cast from dbt_utils function in the jinja code for casting into integer as follows:\\n{{ dbt_utils.safe_cast(\\'ehail_fee\\',  api.Column.translate_type(\"integer\"))}} as ehail_fee,\\nCan also just use safe_cast(ehail_fee as integer) without relying on dbt_utils.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Bad int64 value: 0.0 error', 'course': 'data-engineering-zoomcamp', 'doc_id': '909e6035e92539225be4e762bd56bf5d'}, {'text': \"You might encounter this when building the fact_trips.sql model. The issue may be with the payment_type_description field.\\nUsing safe_cast as above, would cause the entire field to become null. A better approach is to drop the offending decimal place, then cast to integer.\\ncast(replace({{ payment_type }},'.0','') as integer)\\nBad int64 value: 1.0 error (again)\\n\\nI found that there are more columns causing the bad INT64: ratecodeid and trip_type on Green_tripdata table.\\nYou can use the queries below to address them:\\nCAST(\\nREGEXP_REPLACE(CAST(rate_code AS STRING), r'\\\\.0', '') AS INT64\\n) AS ratecodeid,\\nCAST(\\nCASE\\nWHEN REGEXP_CONTAINS(CAST(trip_type AS STRING), r'\\\\.\\\\d+') THEN NULL\\nELSE CAST(trip_type AS INT64)\\nEND AS INT64\\n) AS trip_type,\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'Bad int64 value: 2.0/1.0 error', 'course': 'data-engineering-zoomcamp', 'doc_id': '9cdcd2e604172fccf755305d85740b28'}, {'text': 'The two solution above don’t work for me - I used the line below in `stg_green_trips.sql` to replace the original ehail_fee line:\\n`{{ dbt.safe_cast(\\'ehail_fee\\',  api.Column.translate_type(\"numeric\"))}} as ehail_fee,`', 'section': 'Module 4: analytics engineering with dbt', 'question': 'DBT - Error on building fact_trips.sql: Parquet column \\'ehail_fee\\' has type DOUBLE which does not match the target cpp_type INT64. File: gs://<gcs bucket>/<table>/green_taxi_2019-01.parquet\")', 'course': 'data-engineering-zoomcamp', 'doc_id': '5279d6df98a59765af30892bd7bdf6be'}, {'text': \"Remember to add a space between the variable and the value. Otherwise, it won't be interpreted as a dictionary.\\nIt should be:\\ndbt run --var 'is_test_run: false'\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'The - vars argument must be a YAML dictionary, but was of type str', 'course': 'data-engineering-zoomcamp', 'doc_id': '06dd4008f15205b5e36b6ca8f23cae95'}, {'text': \"You don't need to change the environment type. If you are following the videos, you are creating a Production Deployment, so the only available option is the correct one.'\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'Not able to change Environment Type as it is greyed out and inaccessible', 'course': 'data-engineering-zoomcamp', 'doc_id': 'c49872e18a267f489352bd22d26ad439'}, {'text': 'Database Error in model stg_yellow_tripdata (models/staging/stg_yellow_tripdata.sql)\\nAccess Denied: Table taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata: User does not have permission to query table taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata, or perhaps it does not exist in location US.\\ncompiled Code at target/run/taxi_rides_ny/models/staging/stg_yellow_tripdata.sql\\nIn my case, I was set up in a different branch, so always check the branch you are working on. Change the 04-analytics-engineering/taxi_rides_ny/models/staging/schema.yml file in the\\nsources:\\n- name: staging\\ndatabase: your_database_name\\nIf this error will continue when running dbt job, As for changing the branch for your job, you can use the ‘Custom Branch’ settings in your dbt Cloud environment. This allows you to run your job on a different branch than the default one (usually main). To do this, you need to:\\nGo to an environment and select Settings to edit it\\nSelect Only run on a custom branch in General settings\\nEnter the name of your custom branch (e.g. HW)\\nClick Save\\nCould not parse the dbt project. please check that the repository contains a valid dbt project\\nRunning the Environment on the master branch causes this error, you must activate “Only run on a custom branch” checkbox and specify the branch you are  working when Environment is setup.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Access Denied: Table taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata: User does not have permission to query table taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata, or perhaps it does not exist in location US.', 'course': 'data-engineering-zoomcamp', 'doc_id': '3fac0fd50cd1cb4805ae561765209bf0'}, {'text': 'Change to main branch, make a pull request from the development branch.\\nNote: this will take you to github.\\nApprove the merging and rerun you job, it would work as planned now', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Made change to your modelling files and commit the your development branch, but Job still runs on old file?', 'course': 'data-engineering-zoomcamp', 'doc_id': 'e282b01e152500e83026969d5eee51ab'}, {'text': 'Before you can develop some data model on dbt, you should create development environment and set some parameter on it. After the model being developed, we should also create deployment environment to create and run some jobs.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Setup - I’ve set Github and Bigquery to dbt successfully. Why nothing showed in my Develop tab?', 'course': 'data-engineering-zoomcamp', 'doc_id': '54c1e0cc66a54e4dad1fa57279dee056'}, {'text': 'Error Message:\\nInvestigate Sentry error: ProtocolError \"Invalid input ConnectionInputs.SEND_HEADERS in state ConnectionState.CLOSED\"\\nSolution:\\nreference\\nRun it again because it happens sometimes. Or wait a few minutes, it will continue.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Prefect Agent retrieving runs from queue sometimes fails with httpx.LocalProtocolError', 'course': 'data-engineering-zoomcamp', 'doc_id': 'cb85c129af7193e9bf3758e6b8834f0a'}, {'text': \"My taxi data was loaded into gcs with etl_web_to_gcs.py script that converts csv data into parquet. Then I placed raw data trips into external tables and when I executed dbt run I got an error message: Parquet column 'passenger_count' has type INT64 which does not match the target cpp_type DOUBLE. It is because several columns in files have different formats of data.\\nWhen I added df[col] = df[col].astype('Int64') transformation to the columns: passenger_count, payment_type, RatecodeID, VendorID, trip_type it went ok. Several people also faced this error and more about it you can read on the slack channel.\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'BigQuery returns an error when i try to run ‘dbt run’:', 'course': 'data-engineering-zoomcamp', 'doc_id': 'cb6a876346f80f5626a79ac306a93f9d'}, {'text': 'Use the syntax below instead if the code in the tutorial is not working.\\ndbt run --select stg_green_tripdata --vars \\'{\"is_test_run\": false}\\'', 'section': 'Module 4: analytics engineering with dbt', 'question': \"Running dbt run --models stg_green_tripdata --var 'is_test_run: false' is not returning anything:\", 'course': 'data-engineering-zoomcamp', 'doc_id': '258ca6922e3cb23db89885dd68cd2034'}, {'text': \"Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\\nSolution:\\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\", 'section': 'Module 4: analytics engineering with dbt', 'question': \"DBT - Error: No module named 'pytz' while setting up dbt with docker\", 'course': 'data-engineering-zoomcamp', 'doc_id': 'c837ec46a05dcca407039cca40248b92'}, {'text': \"If you have problems editing dbt_project.yml when using Docker after ‘docker-compose run dbt-bq-dtc init’, to change profile ‘taxi_rides_ny’ to 'bq-dbt-workshop’, just run:\\nsudo chown -R username path\\nDBT - Internal Error: Profile should not be None if loading is completed\\nWhen  running dbt debug, change the directory to the newly created subdirectory (e.g: the newly created `taxi_rides_ny` directory, which contains the dbt project).\", 'section': 'Module 4: analytics engineering with dbt', 'question': '\\u200b\\u200bVS Code: NoPermissions (FileSystemError): Error: EACCES: permission denied (linux)', 'course': 'data-engineering-zoomcamp', 'doc_id': '3a107db446cbe2be15a991e7bd544025'}, {'text': 'When running a query on BigQuery sometimes could appear a this table is not on the specified location error.\\nFor this problem there is not a straightforward solution, you need to dig a little, but the problem could be one of these:\\nCheck the locations of your bucket, datasets and tables. Make sure they are all on the same one.\\nChange the query settings to the location you are in: on the query window select more -> query settings -> select the location\\nCheck if all the paths you are using in your query to your tables are correct: you can click on the table -> details -> and copy the path.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Google Cloud BigQuery Location Problems', 'course': 'data-engineering-zoomcamp', 'doc_id': '27b9cdc0054421c76c7afe7650e87c4b'}, {'text': 'This happens because we have moved the dbt project to another directory on our repo.\\nOr might be that you’re on a different branch than is expected to be merged from / to.\\nSolution:\\nGo to the projects window on dbt cloud -> settings -> edit -> and add directory (the extra path to the dbt project)\\nFor example:\\n/week5/taxi_rides_ny\\nMake sure your file explorer path and this Project settings path matches and there’s no files waiting to be committed to github if you’re running the job to deploy to PROD.\\nAnd that you had setup the PROD environment to check in the main branch, or whichever you specified.\\nIn the picture below, I had set it to ella2024 to be checked as “production-ready” by the “freshness” check mark at the PROD environment settings. So each time I merge a branch from something else into ella2024 and then trigger the PR, the CI check job would kick-in. But we still do need to Merge and close the PR manually, I believe, that part is not automated.\\nYou set up the PROD custom branch (if not default main) in the Environment setup screen.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'DBT Deploy - This dbt Cloud run was cancelled because a valid dbt project was not found.', 'course': 'data-engineering-zoomcamp', 'doc_id': '5e501c52e258fa527cc2dbdaa31e5de1'}, {'text': 'When you are creating the pull request and running the CI, dbt is creating a new schema on BIgQuery. By default that new schema will be created on ‘US’ location, if you have your dataset, schemas and tables on ‘EU’ that will generate an error and the pull request will not be accepted. To change that location to ‘EU’ on the connection to BigQuery from dbt we need to add the location ‘EU’ on the connection optional settings:\\nDbt -> project -> settings -> connection BIgQuery -> OPtional Settings -> Location -> EU', 'section': 'Module 4: analytics engineering with dbt', 'question': 'DBT Deploy + CI - Location Problems on BigQuery', 'course': 'data-engineering-zoomcamp', 'doc_id': 'ed2f4845500df713b2fcb7240f2fed4e'}, {'text': 'When running trying to run the dbt project on prod there is some things you need to do and check on your own:\\nFirst Make the pull request and Merge the branch into the main.\\nMake sure you have the latest version, if you made changes to the repo in another place.\\nCheck if the dbt_project.yml file is accessible to the project, if not check this solution (Dbt: This dbt Cloud run was cancelled because a valid dbt project was not found.).\\nCheck if the name you gave to the dataset on BigQuery is the same you put on the dataset spot on the production environment created on dbt cloud.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'DBT Deploy - Error When trying to run the dbt project on Prod', 'course': 'data-engineering-zoomcamp', 'doc_id': '8f42ae073c1622984691f9e1d0fc4295'}, {'text': 'In the step in this video (DE Zoomcamp 4.3.1 - Build the First dbt Models), after creating `stg_green_tripdata.sql` and clicking `build`, I encountered an error saying dataset not found in location EU. The default location for dbt Bigquery is the US, so when generating the new Bigquery schema for dbt, unless specified, the schema locates in the US.\\nSolution:\\nTurns out I forgot to specify Location to be `EU` when adding connection details.\\nDevelop -> Configure Cloud CLI -> Projects -> taxi_rides_ny -> (connection) Bigquery -> Edit -> Location (Optional) -> type `EU` -> Save', 'section': 'Module 4: analytics engineering with dbt', 'question': 'DBT - Error: “404 Not found: Dataset <dataset_name>:<dbt_schema_name> was not found in location EU” after building from stg_green_tripdata.sql', 'course': 'data-engineering-zoomcamp', 'doc_id': '20f5e60a9e9dbc583ceb803ffc3f65f8'}, {'text': 'Issue: If you’re having problems loading the FHV_20?? data from the github repo into GCS and then into BQ (input file not of type parquet), you need to do two things. First, append the URL Template link with ‘?raw=true’ like so:\\nURL_TEMPLATE = URL_PREFIX + \"/fhv_tripdata_{{ execution_date.strftime(\\\\\\'%Y-%m\\\\\\') }}.parquet?raw=true\"\\nSecond, update make sure the URL_PREFIX is set to the following value:\\n\\nURL_PREFIX = \"https://github.com/alexeygrigorev/datasets/blob/master/nyc-tlc/fhv\"\\nIt is critical that you use this link with the keyword blob. If your link has ‘tree’ here, replace it. Everything else can stay the same, including the curl -sSLf command. ‘', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Homework - Ingesting FHV_20?? data', 'course': 'data-engineering-zoomcamp', 'doc_id': '3110bd8bd12ecb22a6cfee6cf094d31f'}, {'text': 'I found out that the easies way to upload datasets form github for the homework is utilising this script git_csv_to_gcs.py. Thank you Lidia!!\\nIt is similar to a script that Alexey provided us in 03-data-warehouse/extras/web_to_gcs.py', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Homework - Ingesting NYC TLC Data', 'course': 'data-engineering-zoomcamp', 'doc_id': 'a74fb416e1b26797eda99742659cf25e'}, {'text': 'If you have to securely put your credentials for a project and, probably, push it to a git repository then the best option is to use an environment variable\\nFor example for web_to_gcs.py or git_csv_to_gcs.py we have to set these variables:\\nGOOGLE_APPLICATION_CREDENTIALS\\nGCP_GCS_BUCKET\\nThe easises option to do it  is to use .env  (dotenv).\\nInstall it and add a few lines of code that inject these variables for your project\\npip install python-dotenv\\nfrom dotenv import load_dotenv\\nimport os\\n# Load environment variables from .env file\\nload_dotenv()\\n# Now you can access environment variables like GCP_GCS_BUCKET and GOOGLE_APPLICATION_CREDENTIALS\\ncredentials_path = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\\nBUCKET = os.environ.get(\"GCP_GCS_BUCKET\")', 'section': 'Module 4: analytics engineering with dbt', 'question': 'How to set environment variable easily for any credentials', 'course': 'data-engineering-zoomcamp', 'doc_id': 'b965ed9defe63caaf66013e3b5deeed4'}, {'text': \"If you uploaded manually the fvh 2019 csv files, you may face errors regarding date types. Try to create an the external table in bigquery but define the pickup_datetime and dropoff_datetime to be strings\\nCREATE OR REPLACE EXTERNAL TABLE `gcp_project.trips_data_all.fhv_tripdata`  (\\ndispatching_base_num STRING,\\npickup_datetime STRING,\\ndropoff_datetime STRING,\\nPUlocationID STRING,\\nDOlocationID STRING,\\nSR_Flag STRING,\\nAffiliated_base_number STRING\\n)\\nOPTIONS (\\nformat = 'csv',\\nuris = ['gs://bucket/*.csv']\\n);\\nThen when creating the fhv core model in dbt, use TIMESTAMP(CAST(()) to ensure it first parses as a string and then convert it to timestamp.\\nwith fhv_tripdata as (\\nselect * from {{ ref('stg_fhv_tripdata') }}\\n),\\ndim_zones as (\\nselect * from {{ ref('dim_zones') }}\\nwhere borough != 'Unknown'\\n)\\nselect fhv_tripdata.dispatching_base_num,\\nTIMESTAMP(CAST(fhv_tripdata.pickup_datetime AS STRING)) AS pickup_datetime,\\nTIMESTAMP(CAST(fhv_tripdata.dropoff_datetime AS STRING)) AS dropoff_datetime,\", 'section': 'Module 4: analytics engineering with dbt', 'question': \"Invalid date types after Ingesting FHV data through CSV files: Could not parse 'pickup_datetime' as a timestamp\", 'course': 'data-engineering-zoomcamp', 'doc_id': '081e0f956a3c95d2cd13c70cf7df36a7'}, {'text': \"If you uploaded manually the fvh 2019 parquet files manually after downloading from https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-*.parquet you may face errors regarding date types while loading the data in a landing table (say fhv_tripdata). Try to create an the external table with the schema defines as following and load each month in a loop.\\n-----Correct load with schema defination----will not throw error----------------------\\nCREATE OR REPLACE EXTERNAL TABLE `dw-bigquery-week-3.trips_data_all.external_tlc_fhv_trips_2019` (\\ndispatching_base_num STRING,\\npickup_datetime TIMESTAMP,\\ndropoff_datetime TIMESTAMP,\\nPUlocationID FLOAT64,\\nDOlocationID FLOAT64,\\nSR_Flag FLOAT64,\\nAffiliated_base_number STRING\\n)\\nOPTIONS (\\nformat = 'PARQUET',\\nuris = ['gs://project id/fhv_2019_8.parquet']\\n);\\nCan Also USE  uris = ['gs://project id/fhv_2019_*.parquet'] (THIS WILL remove the need for the loop and can be done for all month in single RUN )\\n– THANKYOU FOR THIS –\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'Invalid data types after Ingesting FHV data through parquet files: Could not parse SR_Flag as Float64,Couldn’t parse datetime column as timestamp,couldn’t handle NULL values in PULocationID,DOLocationID', 'course': 'data-engineering-zoomcamp', 'doc_id': '96ab602ab730326f4e5ade751c685fdc'}, {'text': 'When accessing Looker Studio through the Google Cloud Project console, you may be prompted to subscribe to the Pro version and receive the following errors:\\nInstead, navigate to https://lookerstudio.google.com/navigation/reporting which will take you to the free version.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Google Looker Studio - you have used up your 30-day trial', 'course': 'data-engineering-zoomcamp', 'doc_id': 'fcc4b084c2720a56e63fc7e44accfea3'}, {'text': 'Ans: Dbt provides a mechanism called \"ref\" to manage dependencies between models. By referencing other models using the \"ref\" keyword in SQL, dbt automatically understands the dependencies and ensures the correct execution order.\\nLoading FHV Data goes into slumber using Mage?\\nTry loading the data using jupyter notebooks in a local environment. There might be bandwidth issues with Mage.\\nLoad the data into a pandas dataframe using the urls, make necessary transformations, upload the gcp bucket / alternatively download the parquet/csv files locally and then upload to GCP manually.\\nRegion Mismatch in DBT and BigQuery\\nIf you are using the datasets copied into BigQuery from BigQuery public datasets, the region will be set as US by default and hence it is much easier to set your dbt profile location as US while transforming the tables and views. \\nYou can change the location as follows:', 'section': 'Module 4: analytics engineering with dbt', 'question': 'How does dbt handle dependencies between models?', 'course': 'data-engineering-zoomcamp', 'doc_id': '5ce328e89269d0ded5ca7fbcd5d5e5f0'}, {'text': \"Use the PostgreSQL COPY FROM feature that is compatible with csv files\\nCOPY table_name [ ( column_name [, ...] ) ]\\nFROM { 'filename' | PROGRAM 'command' | STDIN }\\n[ [ WITH ] ( option [, ...] ) ]\\n[ WHERE condition ]\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'What is the fastest way to upload taxi data to dbt-postgres?', 'course': 'data-engineering-zoomcamp', 'doc_id': 'f9ecb6dac7f8a60e9d54d3ce54e98698'}, {'text': 'Update the line:\\nWith:', 'section': 'Module 5: pyspark', 'question': 'When configuring the profiles.yml file for dbt-postgres with jinja templates with environment variables, I\\'m getting \"Credentials in profile \"PROFILE_NAME\", target: \\'dev\\', invalid: \\'5432\\'is not of type \\'integer\\'', 'course': 'data-engineering-zoomcamp', 'doc_id': '763e9f356cdaf083a0e9276b43ae29bf'}, {'text': 'Install SDKMAN:\\ncurl -s \"https://get.sdkman.io\" | bash\\nsource \"$HOME/.sdkman/bin/sdkman-init.sh\"\\nUsing SDKMAN, install Java 11 and Spark 3.3.2:\\nsdk install java 11.0.22-tem\\nsdk install spark 3.3.2\\nOpen a new terminal or run the following in the same shell:\\nsource \"$HOME/.sdkman/bin/sdkman-init.sh\"\\nVerify the locations and versions of Java and Spark that were installed:\\necho $JAVA_HOME\\njava -version\\necho $SPARK_HOME\\nspark-submit --version', 'section': 'Module 5: pyspark', 'question': 'Setting up Java and Spark (with PySpark) on Linux (Alternative option using SDKMAN)', 'course': 'data-engineering-zoomcamp', 'doc_id': 'a59dfa025c6c7266cc363f6bf5d7f268'}, {'text': 'If you’re seriously struggling to set things up \"locally\" (here locally meaning non/partly-managed environment like own laptop, a VM or Codespaces) you can use the following guide to use Spark in Google Colab:\\nhttps://medium.com/gitconnected/launch-spark-on-google-colab-and-connect-to-sparkui-342cad19b304\\nStarter notebook:\\nhttps://github.com/aaalexlit/medium_articles/blob/main/Spark_in_Colab.ipynb\\nIt’s advisable to spend some time setting things up locally rather than jumping right into this solution.', 'section': 'Module 5: pyspark', 'question': 'PySpark - Setting Spark up in Google Colab', 'course': 'data-engineering-zoomcamp', 'doc_id': '99524554b94b97bcc83feda49bb64162'}, {'text': 'If after installing Java (either jdk or openjdk), Hadoop and Spark, and setting the corresponding environment variables you find the following error when spark-shell is run at CMD:\\njava.lang.IllegalAccessError: class org.apache.spark.storage.StorageUtils$ (in unnamed module @0x3c947bc5) cannot access class sun.nio.ch.DirectBuffer (in module java.base) because module java.base does not export sun.nio.ch to unnamed\\nmodule @0x3c947bc5\\nSolution: Java 17 or 19 is not supported by Spark. Spark 3.x: requires Java 8/11/16. Install Java 11 from the website provided in the windows.md setup file.', 'section': 'Module 5: pyspark', 'question': 'Spark-shell: unable to load native-hadoop library for platform - Windows', 'course': 'data-engineering-zoomcamp', 'doc_id': '8f0544f83301e18cd1ccaaab11756c5f'}, {'text': 'I found this error while executing the user defined function in Spark (crazy_stuff_udf). I am working on Windows and using conda. After following the setup instructions, I found that the PYSPARK_PYTHON environment variable was not set correctly, given that conda has different python paths for each environment.\\nSolution:\\npip install findspark on the command line inside proper environment\\nAdd to the top of the script\\nimport findspark\\nfindspark.init()', 'section': 'Module 5: pyspark', 'question': 'PySpark - Python was not found; run without arguments to install from the Microsoft Store, or disable this shortcut from Settings > Manage App Execution Aliases.', 'course': 'data-engineering-zoomcamp', 'doc_id': '3cb00ba3380932fdb5674799e483345f'}, {'text': 'This is because Python 3.11 has some inconsistencies with such an old version of Spark. The solution is a downgrade in the Python version. Python 3.9 using a conda environment takes care of it. Or install newer PySpark >= 3.5.1 works for me (Ella) [source].', 'section': 'Module 5: pyspark', 'question': 'PySpark - TypeError: code() argument 13 must be str, not int  , while executing `import pyspark`  (Windows/ Spark 3.0.3 - Python 3.11)', 'course': 'data-engineering-zoomcamp', 'doc_id': '8e5e165f434bd27c129793409c4883ff'}, {'text': 'If anyone is a Pythonista or becoming one (which you will essentially be one along this journey), and desires to have all python dependencies under same virtual environment (e.g. conda) as done with prefect and previous exercises, simply follow these steps\\nInstall OpenJDK 11,\\non MacOS: $ brew install java11\\nAdd export PATH=\"/opt/homebrew/opt/openjdk@11/bin:$PATH\"\\nto ~/.bashrc or ~/zshrc\\nActivate working environment (by pipenv / poetry / conda)\\nRun $ pip install pyspark\\nWork with exercises as normal\\nAll default commands of spark will be also available at shell session under activated enviroment.\\nHope this can help!\\nP.s. you won’t need findspark to firstly initialize.\\nPy4J - Py4JJavaError: An error occurred while calling (...)  java.net.ConnectException: Connection refused: no further information;\\nIf you\\'re getting `Py4JavaError` with a generic root cause, such as the described above (Connection refused: no further information). You\\'re most likely using incompatible versions of the JDK or Python with Spark.\\nAs of the current latest Spark version (3.5.0), it supports JDK 8 / 11 / 17. All of which can be easily installed with SDKMan! on macOS or Linux environments\\n\\n$ sdk install java 17.0.10-librca\\n$ sdk install spark 3.5.0\\n$ sdk install hadoop 3.3.5\\nAs PySpark 3.5.0 supports Python 3.8+ make sure you\\'re setting up your virtualenv with either 3.8 / 3.9 / 3.10 / 3.11 (Most importantly avoid using 3.12 for now as not all libs in the data-science/engineering ecosystem are fully package for that)\\n\\n\\n$ conda create -n ENV_NAME python=3.11\\n$ conda activate ENV_NAME\\n$ pip install pyspark==3.5.0\\nThis setup makes installing `findspark` and the likes of it unnecessary. Happy coding.\\nPy4J - Py4JJavaError: An error occurred while calling o54.parquet. Or any kind of Py4JJavaError that show up after run df.write.parquet(\\'zones\\')(On window)\\nThis assume you already correctly set up the PATH in the nano ~/.bashrc\\nHere my\\nexport JAVA_HOME=\"/c/tools/jdk-11.0.21\"\\nexport PATH=\"${JAVA_HOME}/bin:${PATH}\"\\nexport HADOOP_HOME=\"/c/tools/hadoop-3.2.0\"\\nexport PATH=\"${HADOOP_HOME}/bin:${PATH}\"\\nexport SPARK_HOME=\"/c/tools/spark-3.3.2-bin-hadoop3\"\\nexport PATH=\"${SPARK_HOME}/bin:${PATH}\"\\nexport PYTHONPATH=\"${SPARK_HOME}/python/:$PYTHONPATH\"\\nexport PYTHONPATH=\"${SPARK_HOME}spark-3.5.1-bin-hadoop3py4j-0.10.9.5-src.zip:$PYTHONPATH\"\\nYou also need to add environment variables correctly which paths to java jdk, spark and hadoop through\\nGo to Stephenlaye2/winutils3.3.0: winutils.exe hadoop.dll and hdfs.dll binaries for hadoop windows (github.com), download the right winutils for hadoop-3.2.0. Then create a new folder,bin and put every thing in side to make a /c/tools/hadoop-3.2.0/bin(You might not need to do this, but after testing it without the /bin I could not make it to work)\\nThen follow the solution in this video: How To Resolve Issue with Writing DataFrame to Local File | winutils | msvcp100.dll (youtube.com)\\nRemember to restart IDE and computer, After the error An error occurred while calling o54.parquet.  is fixed but new errors like o31.parquet. Or o35.parquet. appear.', 'section': 'Module 5: pyspark', 'question': 'Java+Spark - Easy setup with miniconda env (worked on MacOS)', 'course': 'data-engineering-zoomcamp', 'doc_id': 'b137acf26aeadaf2a4bcc9a8153f439a'}, {'text': 'After installing all including pyspark (and it is successfully imported), but then running this script on the jupyter notebook\\nimport pyspark\\nfrom pyspark.sql import SparkSession\\nspark = SparkSession.builder \\\\\\n.master(\"local[*]\") \\\\\\n.appName(\\'test\\') \\\\\\n.getOrCreate()\\ndf = spark.read \\\\\\n.option(\"header\", \"true\") \\\\\\n.csv(\\'taxi+_zone_lookup.csv\\')\\ndf.show()\\nit gives the error:\\nRuntimeError: Java gateway process exited before sending its port number\\n✅The solution (for me) was:\\npip install findspark on the command line and then\\nAdd\\nimport findspark\\nfindspark.init()\\nto the top of the script.\\nAnother possible solution is:\\nCheck that pyspark is pointing to the correct location.\\nRun pyspark.__file__. It should be list /home/<your user name>/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/__init__.py if you followed the videos.\\nIf it is pointing to your python site-packages remove the pyspark directory there and check that you have added the correct exports to you .bashrc file and that there are not any other exports which might supersede the ones provided in the course content.\\nTo add to the solution above, if the errors persist in regards to setting the correct path for spark,  an alternative solution for permanent path setting solve the error is  to set environment variables on system and user environment variables following this tutorial: Install Apache PySpark on Windows PC | Apache Spark Installation Guide\\nOnce everything is installed, skip to 7:14 to set up environment variables. This allows for the environment variables to be set permanently.', 'section': 'Module 5: pyspark', 'question': 'lsRuntimeError: Java gateway process exited before sending its port number', 'course': 'data-engineering-zoomcamp', 'doc_id': 'a19590a5a1c7fd33a7fc784c61b09875'}, {'text': 'Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\\nThe solution which worked for me(use following in jupyter notebook) :\\n!pip install findspark\\nimport findspark\\nfindspark.init()\\nThereafter , import pyspark and create spark contex<<t as usual\\nNone of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\\nFilter based on conditions based on multiple columns\\nfrom pyspark.sql.functions import col\\nnew_final.filter((new_final.a_zone==\"Murray Hill\") & (new_final.b_zone==\"Midwood\")).show()\\nKrishna Anand', 'section': 'Module 5: pyspark', 'question': 'Module Not Found Error in Jupyter Notebook .', 'course': 'data-engineering-zoomcamp', 'doc_id': '1afc8e18f9f068c525de42599f0d814a'}, {'text': 'You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\\n` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\\nexport PYTHONPATH=”${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}”\\nMake sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named \\'py4j\\'` while executing `import pyspark`.\\nFor instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\\nThen the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\"` appropriately.\\nAdditionally, you can check for the version of ‘py4j’ of the spark you’re using from here and update as mentioned above.\\n~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.', 'section': 'Module 5: pyspark', 'question': \"Py4JJavaError - ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\", 'course': 'data-engineering-zoomcamp', 'doc_id': 'e49f125d1310abe1a8b1d8455b6891a7'}, {'text': 'If below does not work, then download the latest available py4j version with\\nconda install -c conda-forge py4j\\nTake care of the latest version number in the website to replace appropriately.\\nNow add\\nexport PYTHONPATH=\"${SPARK_HOME}/python/:$PYTHONPATH\"\\nexport PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH\"\\nin your  .bashrc file.', 'section': 'Module 5: pyspark', 'question': \"Py4J Error - ModuleNotFoundError: No module named 'py4j' (Solve with latest version)\", 'course': 'data-engineering-zoomcamp', 'doc_id': 'ac127fff86fe89f5bc25f8285899126a'}, {'text': 'Even after we have exported our paths correctly you may find that  even though Jupyter is installed you might not have Jupyter Noteboopgak for one reason or another. Full instructions are found here (for my walkthrough) or here (where I got the original instructions from) but are included below. These instructions include setting up a virtual environment (handy if you are on your own machine doing this and not a VM):\\nFull steps:\\nUpdate and upgrade packages:\\nsudo apt update && sudo apt -y upgrade\\nInstall Python:\\nsudo apt install python3-pip python3-dev\\nInstall Python virtualenv:\\nsudo -H pip3 install --upgrade pip\\nsudo -H pip3 install virtualenv\\nCreate a Python Virtual Environment:\\nmkdir notebook\\ncd notebook\\nvirtualenv jupyterenv\\nsource jupyterenv/bin/activate\\nInstall Jupyter Notebook:\\npip install jupyter\\nRun Jupyter Notebook:\\njupyter notebook', 'section': 'Module 5: pyspark', 'question': 'Exception: Jupyter command `jupyter-notebook` not found.', 'course': 'data-engineering-zoomcamp', 'doc_id': '7cc3ea352b4c376c1b1339c235ae2b5d'}, {'text': 'Code executed:\\ndf = spark.read.parquet(pq_path)\\n… some operations on df …\\ndf.write.parquet(pq_path, mode=\"overwrite\")\\njava.io.FileNotFoundException: File file:/home/xxx/code/data/pq/fhvhv/2021/02/part-00021-523f9ad5-14af-4332-9434-bdcb0831f2b7-c000.snappy.parquet does not exist\\nThe problem is that Sparks performs lazy transformations, so the actual action that trigger the job is df.write, which does delete the parquet files that is trying to read (mode=”overwrite”)\\n✅Solution: Write to a different directorydf\\ndf.write.parquet(pq_path_temp, mode=\"overwrite\")', 'section': 'Module 5: pyspark', 'question': 'Error java.io.FileNotFoundException', 'course': 'data-engineering-zoomcamp', 'doc_id': '6aa300af5d0e6cfac152847861edd4b8'}, {'text': 'You need to create the Hadoop /bin directory manually and add the downloaded files in there, since the shell script provided for Windows installation just puts them in /c/tools/hadoop-3.2.0/ .', 'section': 'Module 5: pyspark', 'question': 'Hadoop - FileNotFoundException: Hadoop bin directory does not exist , when trying to write (Windows)', 'course': 'data-engineering-zoomcamp', 'doc_id': 'c4a2e1885760193839c02d8a5202733a'}, {'text': 'Actually Spark SQL is one independent “type” of SQL - Spark SQL.\\nThe several SQL providers are very similar:\\nSELECT [attributes]\\nFROM [table]\\nWHERE [filter]\\nGROUP BY [grouping attributes]\\nHAVING [filtering the groups]\\nORDER BY [attribute to order]\\n(INNER/FULL/LEFT/RIGHT) JOIN [table2]\\nON [attributes table joining table2] (...)\\nWhat differs the most between several SQL providers are built-in functions.\\nFor Built-in Spark SQL function check this link: https://spark.apache.org/docs/latest/api/sql/index.html\\nExtra information on SPARK SQL :\\nhttps://databricks.com/glossary/what-is-spark-sql#:~:text=Spark%20SQL%20is%20a%20Spark,on%20existing%20deployments%20and%20data.', 'section': 'Module 5: pyspark', 'question': 'Which type of SQL is used in Spark? Postgres? MySQL? SQL Server?', 'course': 'data-engineering-zoomcamp', 'doc_id': 'd014331085eae2d80c1140b93c7e91ce'}, {'text': \"✅Solution: I had two notebooks running, and the one I wanted to look at had opened a port on localhost:4041.\\nIf a port is in use, then Spark uses the next available port number. It can be even 4044. Clean up after yourself when a port does not work or a container does not run.\\nYou can run spark.sparkContext.uiWebUrl\\nand result will be some like\\n'http://172.19.10.61:4041'\", 'section': 'Module 5: pyspark', 'question': 'The spark viewer on localhost:4040 was not showing the current run', 'course': 'data-engineering-zoomcamp', 'doc_id': 'a5f8891568cba573c020ec37fd15dbba'}, {'text': '✅Solution: replace Java Developer Kit 11 with Java Developer Kit 8.\\nJava - RuntimeError: Java gateway process exited before sending its port number\\nShows java_home is not set on the notebook log\\nhttps://sparkbyexamples.com/pyspark/pyspark-exception-java-gateway-process-exited-before-sending-the-driver-its-port-number/\\nhttps://twitter.com/drkrishnaanand/status/1765423415878463839', 'section': 'Module 5: pyspark', 'question': 'Java - java.lang.NoSuchMethodError: sun.nio.ch.DirectBuffer.cleaner()Lsun/misc/Cleaner Error during repartition call (conda pyspark installation)', 'course': 'data-engineering-zoomcamp', 'doc_id': 'bb45bbfc5afb32200bb09242c1ce21f6'}, {'text': '✅I got it working using `gcs-connector-hadoop-2.2.5-shaded.jar` and Spark 3.1\\nI also added the google_credentials.json and .p12 to auth with gcs. These files are downloadable from GCP Service account.\\nTo create the SparkSession:\\nspark = SparkSession.builder.master(\\'local[*]\\') \\\\\\n.appName(\\'spark-read-from-bigquery\\') \\\\\\n.config(\\'BigQueryProjectId\\',\\'razor-project-xxxxxxx) \\\\\\n.config(\\'BigQueryDatasetLocation\\',\\'de_final_data\\') \\\\\\n.config(\\'parentProject\\',\\'razor-project-xxxxxxx) \\\\\\n.config(\"google.cloud.auth.service.account.enable\", \"true\") \\\\\\n.config(\"credentialsFile\", \"google_credentials.json\") \\\\\\n.config(\"GcpJsonKeyFile\", \"google_credentials.json\") \\\\\\n.config(\"spark.driver.memory\", \"4g\") \\\\\\n.config(\"spark.executor.memory\", \"2g\") \\\\\\n.config(\"spark.memory.offHeap.enabled\",True) \\\\\\n.config(\"spark.memory.offHeap.size\",\"5g\") \\\\\\n.config(\\'google.cloud.auth.service.account.json.keyfile\\', \"google_credentials.json\") \\\\\\n.config(\"fs.gs.project.id\", \"razor-project-xxxxxxx\") \\\\\\n.config(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\\\\n.config(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\") \\\\\\n.getOrCreate()', 'section': 'Module 5: pyspark', 'question': 'Spark fails when reading from BigQuery and using `.show()` on `SELECT` queries', 'course': 'data-engineering-zoomcamp', 'doc_id': '603340328200d8f6beacde6c42ee5ab2'}, {'text': 'While creating a SparkSession using the config spark.jars.packages as com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.23.2\\nspark = SparkSession.builder.master(\\'local\\').appName(\\'bq\\').config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.23.2\").getOrCreate()\\nautomatically downloads the required dependency jars and configures the connector, removing the need to manage this dependency. More details available here', 'section': 'Module 5: pyspark', 'question': 'Spark BigQuery connector Automatic configuration', 'course': 'data-engineering-zoomcamp', 'doc_id': '3746748ccd5b32d49bb2d279f4a7cda6'}, {'text': 'Link to Slack Thread : has anyone figured out how to read from GCP data lake instead of downloading all the taxi data again?\\nThere’s a few extra steps to go into reading from GCS with PySpark\\n1.)  IMPORTANT: Download the Cloud Storage connector for Hadoop here: https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage#clusters\\nAs the name implies, this .jar file is what essentially connects PySpark with your GCS\\n2.) Move the .jar file to your Spark file directory. I installed Spark using homebrew on my MacOS machine and I had to create a /jars directory under \"/opt/homebrew/Cellar/apache-spark/3.2.1/ (where my spark dir is located)\\n3.) In your Python script, there are a few extra classes you’ll have to import:\\nimport pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.conf import SparkConf\\nfrom pyspark.context import SparkContext\\n4.) You must set up your configurations before building your SparkSession. Here’s my code snippet:\\nconf = SparkConf() \\\\\\n.setMaster(\\'local[*]\\') \\\\\\n.setAppName(\\'test\\') \\\\\\n.set(\"spark.jars\", \"/opt/homebrew/Cellar/apache-spark/3.2.1/jars/gcs-connector-hadoop3-latest.jar\") \\\\\\n.set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\\\\n.set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"path/to/google_credentials.json\")\\nsc = SparkContext(conf=conf)\\nsc._jsc.hadoopConfiguration().set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\\nsc._jsc.hadoopConfiguration().set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\\nsc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.json.keyfile\", \"path/to/google_credentials.json\")\\nsc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.enable\", \"true\")\\n5.) Once you run that, build your SparkSession with the new parameters we’d just instantiated in the previous step:\\nspark = SparkSession.builder \\\\\\n.config(conf=sc.getConf()) \\\\\\n.getOrCreate()\\n6.) Finally, you’re able to read your files straight from GCS!\\ndf_green = spark.read.parquet(\"gs://{BUCKET}/green/202*/\")', 'section': 'Module 5: pyspark', 'question': 'Spark Cloud Storage connector', 'course': 'data-engineering-zoomcamp', 'doc_id': 'ef69af8d2b3bebcd836a7f5ba146b2c3'}, {'text': 'from pyarrow.parquet import ParquetFile\\npf = ParquetFile(\\'fhvhv_tripdata_2021-01.parquet\\')\\n#pyarrow builds tables, not dataframes\\ntbl_small = next(pf.iter_batches(batch_size = 1000))\\n#this function converts the table to a dataframe of manageable size\\ndf = tbl_small.to_pandas()\\nAlternatively without PyArrow:\\ndf = spark.read.parquet(\\'fhvhv_tripdata_2021-01.parquet\\')\\ndf1 = df.sort(\\'DOLocationID\\').limit(1000)\\npdf = df1.select(\"*\").toPandas()\\ngcsu', 'section': 'Module 5: pyspark', 'question': 'How can I read a small number of rows from the parquet file directly?', 'course': 'data-engineering-zoomcamp', 'doc_id': '3cfa199e26be92db5fe0e82278678ae0'}, {'text': 'Probably you’ll encounter this if you followed the video ‘5.3.1 - First Look at Spark/PySpark’ and used the parquet file from the TLC website (csv was used in the video).\\nWhen defining the schema, the PULocation and DOLocationID are defined as IntegerType. This will cause an error because the Parquet file is INT64 and you’ll get an error like:\\nParquet column cannot be converted in file [...] Column [...] Expected: int, Found: INT64\\nChange the schema definition from IntegerType to LongType and it should work', 'section': 'Module 5: pyspark', 'question': 'DataType error when creating Spark DataFrame with a specified schema?', 'course': 'data-engineering-zoomcamp', 'doc_id': '0e232e0a013a67c9a634bb8a91abf459'}, {'text': 'df_finalx=df_finalw.select([col(x).alias(x.replace(\" \",\"\")) for x in df_finalw.columns])\\nKrishna Anand', 'section': 'Module 5: pyspark', 'question': 'Remove white spaces from column names in Pyspark', 'course': 'data-engineering-zoomcamp', 'doc_id': 'ee03fe6273414518d98ba680a60c541f'}, {'text': 'This error comes up on the Spark video 5.3.1 - First Look at Spark/PySpark,\\nbecause as at the creation of the video, 2021 data was the most recent which utilised csv files but as at now its parquet.\\nSo when you run the command spark.createDataFrame(df1_pandas).show(),\\nYou get the Attribute error. This is caused by the pandas version 2.0.0 which seems incompatible with Spark 3.3.2, so to fix it you have to downgrade pandas to 1.5.3 using the command pip install -U pandas==1.5.3\\nAnother option is adding the following after importing pandas, if one does not want to downgrade pandas version (source) :\\npd.DataFrame.iteritems = pd.DataFrame.items\\nNote that this problem is solved with Spark versions from 3.4.1', 'section': 'Module 5: pyspark', 'question': \"AttributeError: 'DataFrame' object has no attribute 'iteritems'\", 'course': 'data-engineering-zoomcamp', 'doc_id': 'aab10b7d86d0bddbdeddfa92e1063349'}, {'text': 'Another alternative is to install pandas 2.0.1 (it worked well as at the time of writing this), and it is compatible with Pyspark 3.5.1. Make sure to add or edit your environment variable like this:\\nexport SPARK_HOME=\"${HOME}/spark/spark-3.5.1-bin-hadoop3\"\\nexport PATH=\"${SPARK_HOME}/bin:${PATH}\"', 'section': 'Module 5: pyspark', 'question': \"AttributeError: 'DataFrame' object has no attribute 'iteritems'\", 'course': 'data-engineering-zoomcamp', 'doc_id': '256b5291e16db15e8bae86d25f5df694'}, {'text': 'Open a CMD terminal in administrator mode\\ncd %SPARK_HOME%\\nStart a master node: bin\\\\spark-class org.apache.spark.deploy.master.Master\\nStart a worker node: bin\\\\spark-class org.apache.spark.deploy.worker.Worker spark://<master_ip>:<port> --host <IP_ADDR>\\nbin/spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077 --host <IP_ADDR>\\nspark://<master_ip>:<port>: copy the address from the previous command, in my case it was spark://localhost:7077\\nUse --host <IP_ADDR> if you want to run the worker on a different machine. For now leave it empty.\\nNow you can access Spark UI through localhost:8080\\nHomework for Module 5:\\nDo not refer to the homework file located under /05-batch/code/. The correct file is located under\\nhttps://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/cohorts/2024/05-batch/homework.md', 'section': 'Module 5: pyspark', 'question': 'Spark Standalone Mode on Windows', 'course': 'data-engineering-zoomcamp', 'doc_id': 'a3a8e3e93ee8783eaa5c01e2ee8038ba'}, {'text': 'You can either type the export command every time you run a new session, add it to the .bashrc/ which you can find in /home or run this command at the beginning of your homebook:\\nimport findspark\\nfindspark.init()', 'section': 'Module 5: pyspark', 'question': 'Export PYTHONPATH command in linux is temporary', 'course': 'data-engineering-zoomcamp', 'doc_id': 'cf9d9351dab5911ebc9151af387d3238'}, {'text': 'I solved this issue: unzip the file with:\\nf\\nbefore creating head.csv', 'section': 'Module 5: pyspark', 'question': 'Compressed file ended before the end-of-stream marker was reached', 'course': 'data-engineering-zoomcamp', 'doc_id': '1be833a1c124f03d12bd3ed9461bac6a'}, {'text': 'In the code along from Video 5.3.3 Alexey downloads the CSV files from the NYT website and gzips them in their bash script. If we now (2023) follow along but download the data from the GH course Repo, it will already be zippes as csv.gz files. Therefore we zip it again if we follow the code from the video exactly. This then leads to gibberish outcome when we then try to cat the contents or count the lines with zcat, because the file is zipped twitch and zcat only unzips it once.\\n✅solution: do not gzip the files downloaded from the course repo. Just wget them and save them as they are as csv.gz files. Then the zcat command and the showSchema command will also work\\nURL=\"${URL_PREFIX}/${TAXI_TYPE}/${TAXI_TYPE}_tripdata_${YEAR}-${FMONTH}.csv.gz\"\\nLOCAL_PREFIX=\"data/raw/${TAXI_TYPE}/${YEAR}/${FMONTH}\"\\nLOCAL_FILE=\"${TAXI_TYPE}_tripdata_${YEAR}_${FMONTH}.csv.gz\"\\nLOCAL_PATH=\"${LOCAL_PREFIX}/${LOCAL_FILE}\"\\necho \"downloading ${URL} to ${LOCAL_PATH}\"\\nmkdir -p ${LOCAL_PREFIX}\\nwget ${URL} -O ${LOCAL_PATH}\\necho \"compressing ${LOCAL_PATH}\"\\n# gzip ${LOCAL_PATH} <- uncomment this line', 'section': 'Module 5: pyspark', 'question': 'Compression Error: zcat output is gibberish, seems like still compressed', 'course': 'data-engineering-zoomcamp', 'doc_id': 'f4e0f5879ea0de15e6bd1bdc10074ec1'}, {'text': 'Occurred while running : spark.createDataFrame(df_pandas).show()\\nThis error is usually due to the python version, since spark till date of 2 march 2023 doesn’t support python 3.11, try creating a new env with python version 3.8 and then run this command.\\nOn the virtual machine, you can create a conda environment (here called myenv) with python 3.10 installed:\\nconda create -n myenv python=3.10 anaconda\\nThen you must run conda activate myenv to run python 3.10. Otherwise you’ll still be running version 3.11. You can deactivate by typing conda deactivate.', 'section': 'Module 5: pyspark', 'question': 'PicklingError: Could not serialise object: IndexError: tuple index out of range.', 'course': 'data-engineering-zoomcamp', 'doc_id': '035917ef6e885d7ee8eaf6da5d8c676b'}, {'text': 'Make sure you have your credentials of your GCP in your VM under the location defined in the script.', 'section': 'Module 5: pyspark', 'question': 'Connecting from local Spark to GCS - Spark does not find my google credentials as shown in the video?', 'course': 'data-engineering-zoomcamp', 'doc_id': 'c3fa79ba562f0fd5b0fc29020aa45848'}, {'text': 'To run spark in docker setup\\n1. Build bitnami spark docker\\na. clone bitnami repo using command\\ngit clone https://github.com/bitnami/containers.git\\n(tested on commit 9cef8b892d29c04f8a271a644341c8222790c992)\\nb. edit file `bitnami/spark/3.3/debian-11/Dockerfile` and update java and spark version as following\\n\"python-3.10.10-2-linux-${OS_ARCH}-debian-11\" \\\\\\n\"java-17.0.5-8-3-linux-${OS_ARCH}-debian-11\" \\\\\\nreference: https://github.com/bitnami/containers/issues/13409\\nc. build docker image by navigating to above directory and running docker build command\\nnavigate cd bitnami/spark/3.3/debian-11/\\nbuild command docker build -t spark:3.3-java-17 .\\n2. run docker compose\\nusing following file\\n```yaml docker-compose.yml\\nversion: \\'2\\'\\nservices:\\nspark:\\nimage: spark:3.3-java-17\\nenvironment:\\n- SPARK_MODE=master\\n- SPARK_RPC_AUTHENTICATION_ENABLED=no\\n- SPARK_RPC_ENCRYPTION_ENABLED=no\\n- SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\\n- SPARK_SSL_ENABLED=no\\nvolumes:\\n- \"./:/home/jovyan/work:rw\"\\nports:\\n- \\'8080:8080\\'\\n- \\'7077:7077\\'\\nspark-worker:\\nimage: spark:3.3-java-17\\nenvironment:\\n- SPARK_MODE=worker\\n- SPARK_MASTER_URL=spark://spark:7077\\n- SPARK_WORKER_MEMORY=1G\\n- SPARK_WORKER_CORES=1\\n- SPARK_RPC_AUTHENTICATION_ENABLED=no\\n- SPARK_RPC_ENCRYPTION_ENABLED=no\\n- SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\\n- SPARK_SSL_ENABLED=no\\nvolumes:\\n- \"./:/home/jovyan/work:rw\"\\nports:\\n- \\'8081:8081\\'\\nspark-nb:\\nimage: jupyter/pyspark-notebook:java-17.0.5\\nenvironment:\\n- SPARK_MASTER_URL=spark://spark:7077\\nvolumes:\\n- \"./:/home/jovyan/work:rw\"\\nports:\\n- \\'8888:8888\\'\\n- \\'4040:4040\\'\\n```\\nrun command to deploy docker compose\\ndocker-compose up\\nAccess jupyter notebook using link logged in docker compose logs\\nSpark master url is spark://spark:7077', 'section': 'Module 5: pyspark', 'question': 'Spark docker-compose setup', 'course': 'data-engineering-zoomcamp', 'doc_id': '80f7ce009b2369b4d56932d6fae4b176'}, {'text': 'To do this\\npip install gcsfs,\\nThereafter copy the uri path to the file and use \\ndf = pandas.read_csc(gs://path)', 'section': 'Module 5: pyspark', 'question': 'How do you read data stored in gcs on pandas with your local computer?', 'course': 'data-engineering-zoomcamp', 'doc_id': 'b999c095b22d7d507de677ff90744029'}, {'text': 'Error:\\nspark.createDataFrame(df_pandas).schema\\nTypeError: field Affiliated_base_number: Can not merge type <class \\'pyspark.sql.types.StringType\\'> and <class \\'pyspark.sql.types.DoubleType\\'>\\nSolution:\\nAffiliated_base_number is a mix of letters and numbers (you can check this with a preview of the table), so it cannot be set to DoubleType (only for double-precision numbers). The suitable type would be StringType. Spark  inferSchema is more accurate than Pandas infer type method in this case. You can set it to  true  while reading the csv, so you don’t have to take out any data from your dataset. Something like this can help:\\ndf = spark.read \\\\\\n.options(\\nheader = \"true\", \\\\\\ninferSchema = \"true\", \\\\\\n) \\\\\\n.csv(\\'path/to/your/csv/file/\\')\\nSolution B:\\nIt\\'s because some rows in the affiliated_base_number are null and therefore it is assigned the datatype String and this cannot be converted to type Double. So if you really want to convert this pandas df to a pyspark df only take the  rows from the pandas df that are not null in the \\'Affiliated_base_number\\' column. Then you will be able to apply the pyspark function createDataFrame.\\n# Only take rows that have no null values\\npandas_df= pandas_df[pandas_df.notnull().all(1)]', 'section': 'Module 5: pyspark', 'question': 'TypeError when using spark.createDataFrame function on a pandas df', 'course': 'data-engineering-zoomcamp', 'doc_id': '06c6ca3b4be4246efe928c469ab43bad'}, {'text': 'Default executor memory is 1gb. This error appeared when working with the homework dataset.\\nError: MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\\nScaling row group sizes to 95.00% for 8 writers\\nSolution:\\nIncrease the memory of the executor when creating the Spark session like this:\\nRemember to restart the Jupyter session (ie. close the Spark session) or the config won’t take effect.', 'section': 'Module 5: pyspark', 'question': 'MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory', 'course': 'data-engineering-zoomcamp', 'doc_id': '72ad729b22d445dbb927ec884b50b898'}, {'text': 'Change the working directory to the spark directory:\\nif you have setup up your SPARK_HOME variable, use the following;\\ncd %SPARK_HOME%\\nif not, use the following;\\ncd <path to spark installation>\\nCreating a Local Spark Cluster\\nTo start Spark Master:\\nbin\\\\spark-class org.apache.spark.deploy.master.Master --host localhost\\nStarting up a cluster:\\nbin\\\\spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077 --host localhost', 'section': 'Module 5: pyspark', 'question': 'How to spark standalone cluster is run on windows OS', 'course': 'data-engineering-zoomcamp', 'doc_id': 'd81a49dfa4ba62959242f4240f77d7f9'}, {'text': 'I added PYTHONPATH, JAVA_HOME and SPARK_HOME to ~/.bashrc, import pyspark worked ok in iPython in terminal, but couldn’t be found in .ipynb opened in VS Code\\nAfter adding new lines to ~/.bashrc, need to restart the shell to activate the new lines, do either\\nsource ~/.bashrc\\nexec bash\\nInstead of configuring paths in ~/.bashrc, I created .env file in the root of my workspace:', 'section': 'Module 5: pyspark', 'question': 'Env variables set in ~/.bashrc are not loaded to Jupyter in VS Code', 'course': 'data-engineering-zoomcamp', 'doc_id': 'c20435830845e10f30e49d717687cd5c'}, {'text': 'I don’t use visual studio, so I did it the old fashioned way: ssh -L 8888:localhost:8888 <my user>@<VM IP> (replace user and IP with the ones used by the GCP VM, e.g. : ssh -L 8888:localhost:8888 myuser@34.140.188.1', 'section': 'Module 5: pyspark', 'question': 'How to port forward outside VS Code', 'course': 'data-engineering-zoomcamp', 'doc_id': '519addda0a15aba3f0c6c788360ed46e'}, {'text': 'If you are doing wc -l fhvhv_tripdata_2021-01.csv.gz  with the gzip file as the file argument, you will get a different result, obviously! Since the file is compressed.\\nUnzip the file and then do wc -l fhvhv_tripdata_2021-01.csv to get the right results.', 'section': 'Module 5: pyspark', 'question': '“wc -l” is giving a different result then shown in the video', 'course': 'data-engineering-zoomcamp', 'doc_id': '3d8c043971825a76ef02c7226a38f90a'}, {'text': 'when trying to:\\nURL=\"spark://$HOSTNAME:7077\"\\nspark-submit \\\\\\n--master=\"{$URL}\" \\\\\\n06_spark_sql.py \\\\\\n--input_green=data/pq/green/2021/*/ \\\\\\n--input_yellow=data/pq/yellow/2021/*/ \\\\\\n--output=data/report-2021\\nand you get errors like the following (SUMMARIZED):\\nWARN Utils: Your hostname, <HOSTNAME> resolves to a loopback address..\\nWARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address Setting default log level to \"WARN\".\\nException in thread \"main\" org.apache.spark.SparkException: Master must either be yarn or start with spark, mesos, k8s, or local at …\\nTry replacing --master=\"{$URL}\"\\nwith --master=$URL (edited)\\nExtra edit for spark version 3.4.2 - if encountering:\\n`Error: Unrecognized option: --master=`\\n→ Replace `--master=\"{$URL}\"` with  `--master \"${URL}\"`', 'section': 'Module 5: pyspark', 'question': '`spark-submit` errors', 'course': 'data-engineering-zoomcamp', 'doc_id': '13a2c593633508013c6aa4bd5c2acede'}, {'text': 'If you are seeing this (or similar) error when attempting to write to parquet, it is likely an issue with your path variables.\\nFor Windows, create a new User Variable “HADOOP_HOME” that points to your Hadoop directory. Then add “%HADOOP_HOME%\\\\bin” to the PATH variable.\\nAdditional tips can be found here: https://stackoverflow.com/questions/41851066/exception-in-thread-main-java-lang-unsatisfiedlinkerror-org-apache-hadoop-io', 'section': 'Module 5: pyspark', 'question': 'Hadoop - Exception in thread \"main\" java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z', 'course': 'data-engineering-zoomcamp', 'doc_id': '695c6359e5f9f242896d00a9abd27e16'}, {'text': \"Change the hadoop version to 3.0.1.Replace all the files in the local hadoop bin folder with the files in this repo:  winutils/hadoop-3.0.1/bin at master · cdarlint/winutils (github.com)\\nIf this does not work try to change other versions found in this repository.\\nFor more information please see this link: This version of %1 is not compatible with the version of Windows you're running · Issue #20 · cdarlint/winutils (github.com)\", 'section': 'Module 5: pyspark', 'question': 'Java.io.IOException. Cannot run program “C:\\\\hadoop\\\\bin\\\\winutils.exe”. CreateProcess error=216, This version of 1% is not compatible with the version of Windows you are using.', 'course': 'data-engineering-zoomcamp', 'doc_id': 'f0d13fb4e53f55eb357b8955e071a0b7'}, {'text': 'Fix is to set the flag like the error states. Get your project ID from your dashboard and set it like so:\\ngcloud dataproc jobs submit pyspark \\\\\\n--cluster=my_cluster \\\\\\n--region=us-central1 \\\\\\n--project=my-dtc-project-1010101 \\\\\\ngs://my-dtc-bucket-id/code/06_spark_sql.py\\n-- \\\\\\n…', 'section': 'Module 5: pyspark', 'question': 'Dataproc - ERROR: (gcloud.dataproc.jobs.submit.pyspark) The required property [project] is not currently set. It can be set on a per-command basis by re-running your command with the [--project] flag.', 'course': 'data-engineering-zoomcamp', 'doc_id': '06ab85e4257d34390ded3fd2bc80642d'}, {'text': 'Go to %SPARK_HOME%\\\\bin\\nRun spark-class org.apache.spark.deploy.master.Master to run the master. This will give you a URL of the form spark://ip:port\\nRun spark-class org.apache.spark.deploy.worker.Worker spark://ip:port to run the worker. Make sure you use the URL you obtained in step 2.\\nCreate a new Jupyter notebook:\\nspark = SparkSession.builder \\\\\\n.master(\"spark://{ip}:7077\") \\\\\\n.appName(\\'test\\') \\\\\\n.getOrCreate()\\nCheck on Spark UI the master, worker and app.', 'section': 'Module 5: pyspark', 'question': 'Run Local Cluster Spark in Windows 10 with CMD', 'course': 'data-engineering-zoomcamp', 'doc_id': '676f6d5ab7b75bfa7864f72489efaf3b'}, {'text': 'This occurs because you are not logged in “gcloud auth login” and maybe the project id is not settled. Then type in a terminal:\\ngcloud auth login\\nThis will open a tab in the browser, accept the terms, after that close the tab if you want. Then set the project is like:\\ngcloud config set project <YOUR PROJECT_ID>\\nThen you can run the command to upload the pq dir to a GCS Bucket:\\ngsutil -m cp -r pq/ <YOUR URI from gsutil>/pq', 'section': 'Module 5: pyspark', 'question': \"lServiceException: 401 Anonymous caller does not have storage.objects.list access to the Google Cloud Storage bucket. Permission 'storage.objects.list' denied on resource (or it may not exist).\", 'course': 'data-engineering-zoomcamp', 'doc_id': 'a58ff4a93ac26c573c7b78e7ca974746'}, {'text': \"When submit a job, it might throw an error about Java in log panel within Dataproc. I changed the Versioning Control when I created a cluster, so it means that I delete the cluster and created a new one, and instead of choosing Debian-Hadoop-Spark, I switch to Ubuntu 20.02-Hadoop3.3-Spark3.3 for Versioning Control feature, the main reason to choose this is because I have the same Ubuntu version in mi laptop, I tried to find documentation to sustent this but unfortunately I couldn't nevertheless it works for me.\", 'section': 'Module 5: pyspark', 'question': 'py4j.protocol.Py4JJavaError  GCP', 'course': 'data-engineering-zoomcamp', 'doc_id': 'de3e16ac06292bc6aa63d3150ff2e32e'}, {'text': \"Use both repartition and coalesce, like so:\\ndf = df.repartition(6)\\ndf = df.coalesce(6)\\ndf.write.parquet('fhv/2019/10', mode='overwrite')\", 'section': 'Module 5: pyspark', 'question': 'Repartition the Dataframe to 6 partitions using df.repartition(6) - got 8 partitions instead', 'course': 'data-engineering-zoomcamp', 'doc_id': '2bf0b694bd7ffff3d5872171929e62d1'}, {'text': \"Possible solution - Try to forward the port using ssh cli instead of vs code.\\nRun > “ssh -L <local port>:<VM host/ip>:<VM port> <ssh hostname>”\\nssh hostname is the name you specified in the ~/.ssh/config file\\nIn case of Jupyter Notebook run\\n“ssh -L 8888:localhost:8888 gcp-vm”\\nfrom your local machine’s cli.\\nNOTE: If you logout from the session, the connection would break. Also while creating the spark session notice the block's log because sometimes it fails to run at 4040 and then switches to 4041.\\n~Abhijit Chakrborty: If you are having trouble accessing localhost ports from GCP VM consider adding the forwarding instructions to .ssh/config file as following:\\n```\\nHost <hostname>\\nHostname <external-gcp-ip>\\nUser xxxx\\nIdentityFile yyyy\\nLocalForward 8888 localhost:8888\\nLocalForward 8080 localhost:8080\\nLocalForward 5432 localhost:5432\\nLocalForward 4040 localhost:4040\\n```\\nThis should automatically forward all ports and will enable accessing localhost ports.\", 'section': 'Module 5: pyspark', 'question': 'Jupyter Notebook or SparkUI not loading properly at localhost after port forwarding from VS code?', 'course': 'data-engineering-zoomcamp', 'doc_id': 'fc79af0540172bab1d5c6f3551084578'}, {'text': '~ Abhijit Chakraborty\\n`sdk list java`  to check for available java sdk versions.\\n`sdk install java 11.0.22-amzn`  as  java-11.0.22-amzn was available for my codespace.\\nclick on Y if prompted to change the default java version.\\nCheck for java version using `java -version `.\\nIf working fine great, else `sdk default java 11.0.22-amzn` or whatever version you have installed.', 'section': 'Module 5: pyspark', 'question': 'Installing Java 11 on codespaces', 'course': 'data-engineering-zoomcamp', 'doc_id': '832f7a1e7dba2516aa4f96ba00b8e25e'}, {'text': 'Sometimes while creating a dataproc cluster on GCP, the following error is encountered.\\nSolution: As mentioned here, sometimes there might not be enough resources in the given region to allocate the request. Usually, gets freed up in a bit and one can create a cluster. – abhirup ghosh\\nSolution 2:  Changing the type of boot-disk from PD-Balanced to PD-Standard, in terraform, helped solve the problem.- Sundara Kumar Padmanabhan', 'section': 'Module 5: pyspark', 'question': \"Error: Insufficient 'SSD_TOTAL_GB' quota. Requested 500.0, available 470.0.\", 'course': 'data-engineering-zoomcamp', 'doc_id': '6774f22277396a3e83c94161fc8c148a'}, {'text': \"Pyspark converts the difference of two TimestampType values to Python's native datetime.timedelta object. The timedelta object only stores the duration in terms of days, seconds, and microseconds. Each of the three units of time must be manually converted into hours in order to express the total duration between the two timestamps using only hours.\\nAnother way for achieving this is using the datediff (sql function). It receives this parameters\\nUpper Date: the closest date you have. For example dropoff_datetime\\nLower Date: the farthest date you have.  For example pickup_datetime\\nAnd the result is returned in terms of days, so you could multiply the result for 24 in order to get the hours.\", 'section': 'Module 5: pyspark', 'question': 'Homework - how to convert the time difference of two timestamps to hours', 'course': 'data-engineering-zoomcamp', 'doc_id': 'cc9fa4185dec182eb7d9812fea9286d6'}, {'text': 'This version combination worked for me:\\nPySpark = 3.3.2\\nPandas = 1.5.3\\n\\nIf it still has an error,', 'section': 'Module 5: pyspark', 'question': 'PicklingError: Could not serialize object: IndexError: tuple index out of range', 'course': 'data-engineering-zoomcamp', 'doc_id': '9116035e365bcc72d9cb2ed4bda7f46c'}, {'text': \"Run this before SparkSession\\nimport os\\nimport sys\\nos.environ['PYSPARK_PYTHON'] = sys.executable\\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\", 'section': 'Module 5: pyspark', 'question': 'Py4JJavaError: An error occurred while calling o180.showString. : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 6) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.', 'course': 'data-engineering-zoomcamp', 'doc_id': '18e0e2369c993ca3cce46a9fa1ffe064'}, {'text': \"import os\\nimport sys\\nos.environ['PYSPARK_PYTHON'] = sys.executable\\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\\nDataproc Pricing: https://cloud.google.com/dataproc/pricing#on_gke_pricing\", 'section': 'Module 5: pyspark', 'question': 'RuntimeError: Python in worker has different version 3.11 than that in driver 3.10, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.', 'course': 'data-engineering-zoomcamp', 'doc_id': '5ae366189ebef7af23ee05f65bd4ebee'}, {'text': 'Ans: No, you can submit a job to DataProc from your local computer by installing gsutil (https://cloud.google.com/storage/docs/gsutil_install) and configuring it. Then, you can execute the following command from your local computer.\\ngcloud dataproc jobs submit pyspark \\\\\\n--cluster=de-zoomcamp-cluster \\\\\\n--region=europe-west6 \\\\\\ngs://dtc_data_lake_de-zoomcamp-nytaxi/code/06_spark_sql.py \\\\\\n-- \\\\\\n--input_green=gs://dtc_data_lake_de-zoomcamp-nytaxi/pq/green/2020/*/ \\\\\\n--input_yellow=gs://dtc_data_lake_de-zoomcamp-nytaxi/pq/yellow/2020/*/ \\\\\\n--output=gs://dtc_data_lake_de-zoomcamp-nytaxi/report-2020 (edited)', 'section': 'Module 5: pyspark', 'question': 'Dataproc Qn: Is it essential to have a VM on GCP for running Dataproc and submitting jobs ?', 'course': 'data-engineering-zoomcamp', 'doc_id': 'abd9de5b5e98af387767b0628afce923'}, {'text': \"AttributeError: 'DataFrame' object has no attribute 'iteritems'\\nthis is because the method inside the pyspark refers to a package that has been already deprecated\\n(https://stackoverflow.com/questions/76404811/attributeerror-dataframe-object-has-no-attribute-iteritems)\\nYou can do this code below, which is mentioned in the stackoverflow link above:\\nQ: DE Zoomcamp 5.6.3 - Setting up a Dataproc Cluster I cannot create a cluster and get this message. I tried many times as the FAQ said, but it didn't work. What can I do?\\nError\\nInsufficient 'SSD_TOTAL_GB' quota. Requested 500.0, available 250.0.\\nRequest ID: 17942272465025572271\\nA: The master and worker nodes are allocated a maximum of 250 GB of memory combined. In the configuration section, adhere to the following specifications:\\nMaster Node:\\nMachine type: n2-standard-2\\nPrimary disk size: 85 GB\\nWorker Node:\\nNumber of worker nodes: 2\\nMachine type: n2-standard-2\\nPrimary disk size: 80 GB\\nYou can allocate up to 82.5 GB memory for worker nodes, keeping in mind that the total memory allocated across all nodes cannot exceed 250 GB.\", 'section': 'Module 5: pyspark', 'question': 'In module 5.3.1, trying to run spark.createDataFrame(df_pandas).show() returns error', 'course': 'data-engineering-zoomcamp', 'doc_id': 'befb5094bf6f3a77b3431045a07d1dd3'}, {'text': 'The MacOS setup instruction (https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/05-batch/setup/macos.md#installing-java) for setting the JAVA_HOME environment variable is for Intel-based Macs which have a default install location at /usr/local/. If you have an Apple Silicon mac, you will have to set JAVA_HOME to /opt/homebrew/, specifically in your .bashrc or .zshrc:\\nexport JAVA_HOME=\"/opt/homebrew/opt/openjdk/bin\"\\nexport PATH=\"$JAVA_HOME:$PATH\"\\nConfirm that your path was correctly set by running the command: which java\\nYou should expect to see the output:\\n/opt/homebrew/opt/openjdk/bin/java\\nReference: https://docs.brew.sh/Installation', 'section': 'Module 6: streaming with kafka', 'question': 'Setting JAVA_HOME with Homebrew on Apple Silicon', 'course': 'data-engineering-zoomcamp', 'doc_id': '3d4e700944839df22eef5139a35e303d'}, {'text': 'Check Docker Compose File:\\nEnsure that your docker-compose.yaml file is correctly configured with the necessary details for the \"control-center\" service. Check the service name, image name, ports, volumes, environment variables, and any other configurations required for the container to start.\\nOn Mac OSX 12.2.1 (Monterey) I could not start the kafka control center. I opened Docker Desktop and saw docker images still running from week 4, which I did not see when I typed “docker ps.” I deleted them in docker desktop and then had no problem starting up the kafka environment.', 'section': 'Module 6: streaming with kafka', 'question': 'Could not start docker image “control-center” from the docker-compose.yaml file.', 'course': 'data-engineering-zoomcamp', 'doc_id': 'b555f0b62071b2e95b13c8533cf0e92d'}, {'text': \"Solution from Alexey: create a virtual environment and run requirements.txt and the python files in that environment.\\nTo create a virtual env and install packages (run only once)\\npython -m venv env\\nsource env/bin/activate\\npip install -r ../requirements.txt\\nTo activate it (you'll need to run it every time you need the virtual env):\\nsource env/bin/activate\\nTo deactivate it:\\ndeactivate\\nThis works on MacOS, Linux and Windows - but for Windows the path is slightly different (it's env/Scripts/activate)\\nAlso the virtual environment should be created only to run the python file. Docker images should first all be up and running.\", 'section': 'Module 6: streaming with kafka', 'question': 'Module “kafka” not found when trying to run producer.py', 'course': 'data-engineering-zoomcamp', 'doc_id': 'e1f9867f4f46565dce33b85c589478b1'}, {'text': 'ImportError: DLL load failed while importing cimpl: The specified module could not be found\\nVerify Python Version:\\nMake sure you are using a compatible version of Python with the Avro library. Check the Python version and compatibility requirements specified by the Avro library documentation.\\n... you may have to load librdkafka-5d2e2910.dll in the code. Add this before importing avro:\\nfrom ctypes import CDLL\\nCDLL(\"C:\\\\\\\\Users\\\\\\\\YOUR_USER_NAME\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\dtcde\\\\\\\\Lib\\\\\\\\site-packages\\\\\\\\confluent_kafka.libs\\\\librdkafka-5d2e2910.dll\")\\nIt seems that the error may occur depending on the OS and python version installed.\\nALTERNATIVE:\\nImportError: DLL load failed while importing cimpl\\n✅SOLUTION: $env:CONDA_DLL_SEARCH_MODIFICATION_ENABLE=1 in Powershell.\\nYou need to set this DLL manually in Conda Env.\\nSource: https://githubhot.com/repo/confluentinc/confluent-kafka-python/issues/1186?page=2', 'section': 'Module 6: streaming with kafka', 'question': 'Error importing cimpl dll when running avro examples', 'course': 'data-engineering-zoomcamp', 'doc_id': '49f2a5695e4021660a2ca19d21fc09ec'}, {'text': \"✅SOLUTION: pip install confluent-kafka[avro].\\nFor some reason, Conda also doesn't include this when installing confluent-kafka via pip.\\nMore sources on Anaconda and confluent-kafka issues:\\nhttps://github.com/confluentinc/confluent-kafka-python/issues/590\\nhttps://github.com/confluentinc/confluent-kafka-python/issues/1221\\nhttps://stackoverflow.com/questions/69085157/cannot-import-producer-from-confluent-kafka\", 'section': 'Module 6: streaming with kafka', 'question': \"ModuleNotFoundError: No module named 'avro'\", 'course': 'data-engineering-zoomcamp', 'doc_id': '41d1a1e9024e2f9b969cc0931e91c0e1'}, {'text': 'If you get an error while running the command python3 stream.py worker\\nRun pip uninstall kafka-python\\nThen run pip install kafka-python==1.4.6\\nWhat is the use of  Redpanda ?\\nRedpanda: Redpanda is built on top of the Raft consensus algorithm and is designed as a high-performance, low-latency alternative to Kafka. It uses a log-centric architecture similar to Kafka but with different underlying principles.\\nRedpanda is a powerful, yet simple, and cost-efficient streaming data platform that is compatible with Kafka® APIs while eliminating Kafka complexity.', 'section': 'Module 6: streaming with kafka', 'question': 'Error while running python3 stream.py worker', 'course': 'data-engineering-zoomcamp', 'doc_id': '6ae94a0148b4a03e49eb5aa8fb7ff3b3'}, {'text': 'Got this error because the docker container memory was exhausted. The dta file was upto 800MB but my docker container does not have enough memory to handle that.\\nSolution was to load the file in chunks with Pandas, then create multiple parquet files for each dat file I was processing. This worked smoothly and the issue was resolved.', 'section': 'Module 6: streaming with kafka', 'question': 'Negsignal:SIGKILL while converting dta files to parquet format', 'course': 'data-engineering-zoomcamp', 'doc_id': '32b3f865b191a53c8dc1a8003470c007'}, {'text': 'Copy the file found in the Java example: data-engineering-zoomcamp/week_6_stream_processing/java/kafka_examples/src/main/resources/rides.csv', 'section': 'Module 6: streaming with kafka', 'question': 'data-engineering-zoomcamp/week_6_stream_processing/python/resources/rides.csv is missing', 'course': 'data-engineering-zoomcamp', 'doc_id': 'c411665e99458d26806a509c901493de'}, {'text': 'tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.\\nKafka Python Videos - Rides.csv\\nThere is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.', 'section': 'Module 6: streaming with kafka', 'question': 'Kafka- python videos have low audio and hard to follow up', 'course': 'data-engineering-zoomcamp', 'doc_id': '9832e366499256e280005967d71b9586'}, {'text': 'If you have this error, it most likely that your kafka broker docker container is not working.\\nUse docker ps to confirm\\nThen in the docker compose yaml file folder, run docker compose up -d to start all the instances.', 'section': 'Module 6: streaming with kafka', 'question': 'kafka.errors.NoBrokersAvailable: NoBrokersAvailable', 'course': 'data-engineering-zoomcamp', 'doc_id': '8ca42bdb0a4ec0b17a9e73af4536dbc6'}, {'text': 'Ankush said we can focus on horizontal scaling option.\\n“think of scaling in terms of scaling from consumer end. Or consuming message via horizontal scaling”', 'section': 'Module 6: streaming with kafka', 'question': 'Kafka homwork Q3, there are options that support scaling concept more than the others:', 'course': 'data-engineering-zoomcamp', 'doc_id': 'c8b2f0bbadf1355c878887d16ebbf0b1'}, {'text': 'If you get this error, know that you have not built your sparks and juypter images. This images aren’t readily available on dockerHub.\\nIn the spark folder, run ./build.sh from a bash cli to to build all images before running docker compose', 'section': 'Module 6: streaming with kafka', 'question': \"How to fix docker compose error: Error response from daemon: pull access denied for spark-3.3.1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", 'course': 'data-engineering-zoomcamp', 'doc_id': '8239bd33320791fbff1b2223156b2eb3'}, {'text': 'Run this command in terminal in the same directory (/docker/spark):\\nchmod +x build.sh', 'section': 'Module 6: streaming with kafka', 'question': 'Python Kafka: ./build.sh: Permission denied Error', 'course': 'data-engineering-zoomcamp', 'doc_id': 'd6b9b507f661e0ec320f8d11e338c0ce'}, {'text': 'Restarting all services worked for me:\\ndocker-compose down\\ndocker-compose up', 'section': 'Module 6: streaming with kafka', 'question': 'Python Kafka: ‘KafkaTimeoutError: Failed to update metadata after 60.0 secs.’ when running stream-example/producer.py', 'course': 'data-engineering-zoomcamp', 'doc_id': '563333ceaf1725c8d9aaf3c68e8703d0'}, {'text': 'While following tutorial 13.2 , when running ./spark-submit.sh streaming.py, encountered the following error:\\n…\\n24/03/11 09:48:36 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\\n24/03/11 09:48:36 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:7077 after 10 ms (0 ms spent in bootstraps)\\n24/03/11 09:48:54 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\\n24/03/11 09:48:56 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077…\\n24/03/11 09:49:16 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\\n24/03/11 09:49:36 WARN StandaloneSchedulerBackend: Application ID is not initialized yet.\\n24/03/11 09:49:36 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\\n…\\npy4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.sql.SparkSession.\\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\\n…\\nSolution:\\nDowngrade your local PySpark to 3.3.1 (same as Dockerfile)\\nThe reason for the failed connection in my case was the mismatch of PySpark versions. You can see that from the logs of spark-master in the docker container.\\nSolution 2:\\nCheck what Spark version your local machine has\\npyspark –version\\nspark-submit –version\\nAdd your version to SPARK_VERSION in build.sh', 'section': 'Module 6: streaming with kafka', 'question': 'Python Kafka: ./spark-submit.sh streaming.py - ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.', 'course': 'data-engineering-zoomcamp', 'doc_id': 'cc0d5667f5f6101d3f199fe7fb5864a3'}, {'text': 'Start a new terminal\\nRun: docker ps\\nCopy the CONTAINER ID of the spark-master container\\nRun: docker exec -it <spark_master_container_id> bash\\nRun: cat logs/spark-master.out\\nCheck for the log when the error happened\\nGoogle the error message from there', 'section': 'Module 6: streaming with kafka', 'question': 'Python Kafka: ./spark-submit.sh streaming.py - How to check why Spark master connection fails', 'course': 'data-engineering-zoomcamp', 'doc_id': 'e2003657cff057b9b11fa88708601532'}, {'text': 'Make sure your java version is 11 or 8.\\nCheck your version by:\\njava --version\\nCheck all your versions by:\\n/usr/libexec/java_home -V\\nIf you already have got java 11 but just not selected as default, select the specific version by:\\nexport JAVA_HOME=$(/usr/libexec/java_home -v 11.0.22)\\n(or other version of 11)', 'section': 'Module 6: streaming with kafka', 'question': 'Python Kafka: ./spark-submit.sh streaming.py Error: py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.', 'course': 'data-engineering-zoomcamp', 'doc_id': '3545aa543db98bab5ed0dd317248d690'}, {'text': 'In my set up, all of the dependencies listed in gradle.build were not installed in <project_name>-1.0-SNAPSHOT.jar.\\nSolution:\\nIn build.gradle file, I added the following at the end:\\nshadowJar {\\narchiveBaseName = \"java-kafka-rides\"\\narchiveClassifier = \\'\\'\\n}\\nAnd then in the command line ran ‘gradle shadowjar’, and run the script from java-kafka-rides-1.0-SNAPSHOT.jar created by the shadowjar', 'section': 'Module 6: streaming with kafka', 'question': 'Java Kafka: <project_name>-1.0-SNAPSHOT.jar errors: package xxx does not exist even after gradle build', 'course': 'data-engineering-zoomcamp', 'doc_id': 'b00d735fd96ccfeae9bf11b81ade28cc'}, {'text': 'confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\\nfastavro: pip install fastavro\\nAbhirup Ghosh\\nCan install Faust Library for Module 6 Python Version due to dependency conflicts?\\nThe Faust repository and library is no longer maintained - https://github.com/robinhood/faust\\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.', 'section': 'Module 6: streaming with kafka', 'question': 'Python Kafka: Installing dependencies for python3 06-streaming/python/avro_example/producer.py', 'course': 'data-engineering-zoomcamp', 'doc_id': '2f3d8121460e42bfb02905897a28263d'}, {'text': 'In the project directory, run:\\njava -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java', 'section': 'Module 6: streaming with kafka', 'question': 'Java Kafka: How to run producer/consumer/kstreams/etc in terminal', 'course': 'data-engineering-zoomcamp', 'doc_id': '3be81666e9ee97d3888e971491790f22'}, {'text': 'For example, when running JsonConsumer.java, got:\\nConsuming form kafka started\\nRESULTS:::0\\nRESULTS:::0\\nRESULTS:::0\\nOr when running JsonProducer.java, got:\\nException in thread \"main\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\\nSolution:\\nMake sure in the scripts in src/main/java/org/example/ that you are running (e.g. JsonConsumer.java, JsonProducer.java), the StreamsConfig.BOOTSTRAP_SERVERS_CONFIG is the correct server url (e.g. europe-west3 from example vs europe-west2)\\nMake sure cluster key and secrets are updated in src/main/java/org/example/Secrets.java (KAFKA_CLUSTER_KEY and KAFKA_CLUSTER_SECRET)', 'section': 'Module 6: streaming with kafka', 'question': 'Java Kafka: When running the producer/consumer/etc java scripts, no results retrieved or no message sent', 'course': 'data-engineering-zoomcamp', 'doc_id': '0775e143c04555f51e577b6e38a0dcd6'}, {'text': 'Situation: in VS Code, usually there will be a triangle icon next to each test. I couldn’t see it at first and had to do some fixes.\\nSolution:\\n(Source)\\nVS Code\\n→ Explorer (first icon on the left navigation bar)\\n→ JAVA PROJECTS (bottom collapsable)\\n→  icon next in the rightmost position to JAVA PROJECTS\\n→  clean Workspace\\n→ Confirm by clicking Reload and Delete\\nNow you will be able to see the triangle icon next to each test like what you normally see in python tests.\\nE.g.:\\nYou can also add classes and packages in this window instead of creating files in the project directory', 'section': 'Module 6: streaming with kafka', 'question': 'Java Kafka: Tests are not picked up in VSCode', 'course': 'data-engineering-zoomcamp', 'doc_id': '113e318ef9a1564f5e5e109147db7b1d'}, {'text': 'In Confluent Cloud:\\nEnvironment → default (or whatever you named your environment as) → The right navigation bar →  “Stream Governance API” →  The URL under “Endpoint”\\nAnd create credentials from Credentials section below it', 'section': 'Module 6: streaming with kafka', 'question': 'Confluent Kafka: Where can I find schema registry URL?', 'course': 'data-engineering-zoomcamp', 'doc_id': 'f165bbc5ea0c6fd9b2c1edd81f1cfb47'}, {'text': 'You can check the version of your local spark using spark-submit --version. In the build.sh file of the Python folder, make sure that SPARK_VERSION matches your local version. Similarly, make sure the pyspark you pip installed also matches this version.', 'section': 'Module 6: streaming with kafka', 'question': 'How do I check compatibility of local and container Spark versions?', 'course': 'data-engineering-zoomcamp', 'doc_id': '110ee05c84a8da330c0073e457ad036f'}, {'text': 'According to https://github.com/dpkp/kafka-python/\\n“DUE TO ISSUES WITH RELEASES, IT IS SUGGESTED TO USE https://github.com/wbarnha/kafka-python-ng FOR THE TIME BEING”\\nUse pip install kafka-python-ng instead', 'section': 'Project', 'question': 'How to fix the error \"ModuleNotFoundError: No module named \\'kafka.vendor.six.moves\\'\"?', 'course': 'data-engineering-zoomcamp', 'doc_id': '4fd4cee0d329ac68c3c8aa1e11f6d66e'}, {'text': 'Each submitted project will be evaluated by 3 (three) randomly assigned students that have also submitted the project.\\nYou will also be responsible for grading the projects from 3 fellow students yourself. Please be aware that: not complying to this rule also implies you failing to achieve the Certificate at the end of the course.\\nThe final grade you get will be the median score of the grades you get from the peer reviewers.\\nAnd of course, the peer review criteria for evaluating or being evaluated must follow the guidelines defined here.', 'section': 'Project', 'question': 'How is my capstone project going to be evaluated?', 'course': 'data-engineering-zoomcamp', 'doc_id': 'c4e16ffcdfa8407ad9b936b04c248830'}, {'text': 'There is only ONE project for this Zoomcamp. You do not need to submit or create two projects. There are simply TWO chances to pass the course. You can use the Second Attempt if you a) fail the first attempt b) do not have the time due to other engagements such as holiday or sickness etc. to enter your project into the first attempt.', 'section': 'Project', 'question': 'Project 1 & Project 2', 'course': 'data-engineering-zoomcamp', 'doc_id': 'aa0f6dc880990aaefc2c9aa58b829278'}, {'text': 'See a list of datasets here: https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_7_project/datasets.md', 'section': 'Project', 'question': 'Does anyone know nice and relatively large datasets?', 'course': 'data-engineering-zoomcamp', 'doc_id': 'e7269e67dfbe1ec4a64436fc02f02373'}, {'text': 'You need to redefine the python environment variable to that of your user account', 'section': 'Project', 'question': 'How to run python as start up script?', 'course': 'data-engineering-zoomcamp', 'doc_id': 'e285789a6f8e83a09bbf3d971b9e8d61'}, {'text': 'Initiate a Spark Session\\nspark = (SparkSession\\n.builder\\n.appName(app_name)\\n.master(master=master)\\n.getOrCreate())\\nspark.streams.resetTerminated()\\nquery1 = spark\\n.readStream\\n…\\n…\\n.load()\\nquery2 = spark\\n.readStream\\n…\\n…\\n.load()\\nquery3 = spark\\n.readStream\\n…\\n…\\n.load()\\nquery1.start()\\nquery2.start()\\nquery3.start()\\nspark.streams.awaitAnyTermination() #waits for any one of the query to receive kill signal or error failure. This is asynchronous\\n# On the contrary query3.start().awaitTermination() is a blocking ex call. Works well when we are reading only from one topic.', 'section': 'Project', 'question': 'Spark Streaming - How do I read from multiple topics in the same Spark Session', 'course': 'data-engineering-zoomcamp', 'doc_id': '291322bd208f0a494b4a5e10d8e4ce6e'}, {'text': 'Transformed data can be moved in to azure blob storage and then it can be moved in to azure SQL DB, instead of moving directly from databricks to Azure SQL DB.', 'section': 'Project', 'question': 'Data Transformation from Databricks to Azure SQL DB', 'course': 'data-engineering-zoomcamp', 'doc_id': '84f46a2b0875c3b521703113d51b4090'}, {'text': 'The trial dbt account provides access to dbt API. Job will still be needed to be added manually. Airflow will run the job using a python operator calling the API. You will need to provide api key, job id, etc. (be careful not committing it to Github).\\nDetailed explanation here: https://docs.getdbt.com/blog/dbt-airflow-spiritual-alignment\\nSource code example here: https://github.com/sungchun12/airflow-toolkit/blob/95d40ac76122de337e1b1cdc8eed35ba1c3051ed/dags/examples/dbt_cloud_example.py', 'section': 'Project', 'question': 'Orchestrating dbt with Airflow', 'course': 'data-engineering-zoomcamp', 'doc_id': 'f8ab07612ed2f947bea125d808d43533'}, {'text': 'https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_api/airflow/providers/google/cloud/operators/dataproc/index.html\\nhttps://airflow.apache.org/docs/apache-airflow-providers-google/stable/_modules/airflow/providers/google/cloud/operators/dataproc.html\\nGive the following roles to you service account:\\nDataProc Administrator\\nService Account User (explanation here)\\nUse DataprocSubmitPySparkJobOperator, DataprocDeleteClusterOperator and  DataprocCreateClusterOperator.\\nWhen using  DataprocSubmitPySparkJobOperator, do not forget to add:\\ndataproc_jars = [\"gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.24.0.jar\"]\\nBecause DataProc does not already have the BigQuery Connector.', 'section': 'Project', 'question': 'Orchestrating DataProc with Airflow', 'course': 'data-engineering-zoomcamp', 'doc_id': '073de3f77432f2f8e0f7fb0c9a7940e0'}, {'text': 'You can trigger your dbt job in Mage pipeline. For this get your dbt cloud api key under settings/Api tokens/personal tokens. Add it safely to  your .env\\nFor example\\ndbt_api_trigger=dbt_**\\nNavigate to job page and find api trigger  link\\nThen create a custom mage Python block with a simple http request like here\\nfrom dotenv import load_dotenv\\nfrom pathlib import Path\\ndotenv_path = Path(\\'/home/src/.env\\')\\nload_dotenv(dotenv_path=dotenv_path)\\ndbt_api_trigger= os.getenv(dbt_api_trigger)\\nurl = f\"https://cloud.getdbt.com/api/v2/accounts/{dbt_account_id}/jobs/<job_id>/run/\"\\nheaders = {\\n        \"Authorization\": f\"Token {dbt_api_trigger}\",\\n        \"Content-Type\": \"application/json\" }\\nbody = {\\n        \"cause\": \"Triggered via API\"\\n    }\\n    response = requests.post(url, headers=headers, json=body)\\nvoila! You triggered dbt job form your mage pipeline.', 'section': 'Project', 'question': 'Orchestrating dbt cloud with Mage', 'course': 'data-engineering-zoomcamp', 'doc_id': '726d0b05be7abd9ed3bda797dc6d32fb'}, {'text': \"The slack thread : thttps://datatalks-club.slack.com/archives/C01FABYF2RG/p1677678161866999\\nThe question is that sometimes even if you take plenty of effort to document every single step, and we can't even sure if the person doing the peer review will be able to follow-up, so how this criteria will be evaluated?\\nAlex clarifies: “Ideally yes, you should try to re-run everything. But I understand that not everyone has time to do it, so if you check the code by looking at it and try to spot errors, places with missing instructions and so on - then it's already great”\", 'section': 'Project', 'question': 'Project evaluation - Reproducibility', 'course': 'data-engineering-zoomcamp', 'doc_id': 'c82295cf44f53a841db3718d7250d274'}, {'text': 'The key valut in Azure cloud is used to store credentials or passwords or secrets of different tech stack used in Azure. For example if u do not want to expose the password in SQL database, then we can save the password under a given name and use them in other Azure stack.', 'section': 'Project', 'question': 'Key Vault in Azure cloud stack', 'course': 'data-engineering-zoomcamp', 'doc_id': '91218468ec989e5b8e50d4ef82cfde73'}, {'text': 'You can get the version of py4j from inside docker using this command\\ndocker exec -it --user airflow airflow-airflow-scheduler-1 bash -c \"ls /opt/spark/python/lib\"', 'section': 'Project', 'question': \"Spark docker - `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\", 'course': 'data-engineering-zoomcamp', 'doc_id': '43d25ae151481e5321b5634562400801'}, {'text': 'Either use conda or pip for managing venv, using both of them together will cause incompatibility.\\nIf you’re using conda, install psycopg2 using the conda-forge channel, which may handle the architecture compatibility automatically\\nconda install -c conda-forge psycopg2\\nIf pip, do the normal install\\npip install psycopg2', 'section': 'Project', 'question': 'psycopg2 complains of incompatible environment e.g x86 instead of amd', 'course': 'data-engineering-zoomcamp', 'doc_id': '3b57283d8324a140421a765c1df5824c'}, {'text': 'This is not a FAQ but more of an advice if you want to set up dbt locally, I did it in the following way:\\nI had the postgres instance from week 2 (year 2024) up (the docker-compose)\\nmkdir dbt\\nvi dbt/profiles.yml\\nAnd here I attached this content (only the required fields) and replaced them with the proper values (for instance mine where in the .env file of the folder of week 2 docker stuff)\\ncd dbt && git clone https://github.com/dbt-labs/dbt-starter-project\\nmkdir project && cd project && mv dbt-starter-project/* .\\nMake sure that you align the profile name in profiles.yml with the dbt_project.yml file\\nAdd this line anywhere on the dbt_project.yml file:\\nconfig-version: 2\\ndocker run --network=mage-zoomcamp_default --mount type=bind,source=/<your-path>/dbt/project,target=/usr/app --mount type=bind,source=/<your-path>/profiles.yml,target=/root/.dbt/profiles.yml ghcr.io/dbt-labs/dbt-postgres ls\\nIf you have trouble run\\ndocker run --network=mage-zoomcamp_default --mount type=bind,source=/<your-path>/dbt/project,target=/usr/app --mount type=bind,source=/<your-path>/profiles.yml,target=/root/.dbt/profiles.yml ghcr.io/dbt-labs/dbt-postgres debug', 'section': 'Project', 'question': 'Setting up dbt locally with Docker and Postgres', 'course': 'data-engineering-zoomcamp', 'doc_id': '2fc7f3a21cf2a85642b4fd30cdfb5b7b'}, {'text': 'The following line should be included in pyspark configuration\\n# Example initialization of SparkSession variable\\nspark = (SparkSession.builder\\n.master(...)\\n.appName(...)\\n# Add the following configuration\\n.config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-3.5-bigquery:0.37.0\")\\n)', 'section': 'Project', 'question': 'How to connect Pyspark with BigQuery?', 'course': 'data-engineering-zoomcamp', 'doc_id': 'a7b8a576a477f57cfd696e8e7824bcd7'}, {'text': 'Install the astronomer-cosmos package as a dependency. (see Terraform example).\\nMake a new folder, dbt/, inside the dags/ folder of your Composer GCP bucket and copy paste your dbt-core project there. (see example)\\nEnsure your profiles.yml is configured to authenticate with a service account key. (see BigQuery example)\\nCreate a new DAG using the DbtTaskGroup class and a ProfileConfig specifying a profiles_yml_filepath that points to the location of your JSON key file. (see example)\\nYour dbt lineage graph should now appear as tasks inside a task group like this:', 'section': 'Course Management Form for Homeworks', 'question': 'How to run a dbt-core project as an Airflow Task Group on Google Cloud Composer using a service account JSON key', 'course': 'data-engineering-zoomcamp', 'doc_id': '8227bb1c36d53ac76c6838d997e21f45'}, {'text': 'The display name listed on the leaderboard is an auto-generated randomized name. You can edit it to be a nickname, or your real name, if you prefer. Your entry on the Leaderboard is the one highlighted in teal(?) / light green (?).\\nThe Certificate name should be your actual name that you want to appear on your certificate after completing the course.\\nThe \"Display on Leaderboard\" option indicates whether you want your name to be listed on the course leaderboard.\\nQuestion: Is it possible to create external tables in BigQuery using URLs, such as those from the NY Taxi data website?\\nAnswer: Not really, only Bigtable, Cloud Storage, and Google Drive are supported data stores.', 'section': 'Workshop 1 - dlthub', 'question': 'Edit Course Profile.', 'course': 'data-engineering-zoomcamp', 'doc_id': '1cb02764cfdce5825411683a4c861447'}, {'text': \"Answer: To run the provided code, ensure that the 'dlt[duckdb]' package is installed. You can do this by executing the provided installation command: !pip install dlt[duckdb]. If you’re doing it locally, be sure to also have duckdb pip installed (even before the duckdb package is loaded).\", 'section': 'Workshop 1 - dlthub', 'question': 'How do I install the necessary dependencies to run the code?', 'course': 'data-engineering-zoomcamp', 'doc_id': 'd9b792f6da4b7d1c51784ba2fb9b8f4a'}, {'text': 'If you are running Jupyter Notebook on a fresh new Codespace or in local machine with a new Virtual Environment, you will need this package to run the starter Jupyter Notebook offered by the teacher. Execute this:\\npip install jupyter', 'section': 'Workshop 1 - dlthub', 'question': 'Other packages needed but not listed', 'course': 'data-engineering-zoomcamp', 'doc_id': '458839e84ebff673b6f17ed9a9f03300'}, {'text': 'Alternatively, you can switch to in-file storage with:', 'section': 'Workshop 1 - dlthub', 'question': 'How can I use DuckDB In-Memory database with dlt ?', 'course': 'data-engineering-zoomcamp', 'doc_id': '09f9484093eb6cbdf414b0247bd07044'}, {'text': 'After loading, you should have a total of 8 records, and ID 3 should have age 33\\nQuestion: Calculate the sum of ages of all the people loaded as described above\\nThe sum of all eight records\\' respective ages is too big to be in the choices. You need to first filter out the people whose occupation is equal to None in order to get an answer that is close to or present in the given choices. 😃\\n----------------------------------------------------------------------------------------\\nFIXED = use a raw string and keep the file:/// at the start of your file path\\nI\\'m having an issue with the dlt workshop notebook. The \\'Load to Parquet file\\' section specifically. No matter what I change the file path to, it\\'s still saving the dlt files directly to my C drive.\\n# Set the bucket_url. We can also use a local folder\\nos.environ[\\'DESTINATION__FILESYSTEM__BUCKET_URL\\'] = r\\'file:///content/.dlt/my_folder\\'\\nurl = \"https://storage.googleapis.com/dtc_zoomcamp_api/yellow_tripdata_2009-06.jsonl\"\\n# Define your pipeline\\npipeline = dlt.pipeline(\\npipeline_name=\\'my_pipeline\\',\\ndestination=\\'filesystem\\',\\ndataset_name=\\'mydata\\'\\n)\\n# Run the pipeline with the generator we created earlier.\\nload_info = pipeline.run(stream_download_jsonl(url), table_name=\"users\", loader_file_format=\"parquet\")\\nprint(load_info)\\n# Get a list of all Parquet files in the specified folder\\nparquet_files = glob.glob(\\'/content/.dlt/my_folder/mydata/users/*.parquet\\')\\n# show parquet files\\nfor file in parquet_files:\\nprint(file)', 'section': 'Workshop 2 - RisingWave', 'question': 'Homework - dlt Exercise 3 - Merge a generator concerns', 'course': 'data-engineering-zoomcamp', 'doc_id': '88febe0cf787aa6e960fe7dce1a74ff1'}, {'text': 'Check the contents of the repository with ls - the command.sh file should be in the root folder\\nIf it is not, verify that you had cloned the correct repository - https://github.com/risingwavelabs/risingwave-data-talks-workshop-2024-03-04', 'section': 'Workshop 2 - RisingWave', 'question': 'command.sh Error - source: no such file or directory: command.sh', 'course': 'data-engineering-zoomcamp', 'doc_id': 'ea3d3a960ba159bf98491eded27989f0'}, {'text': \"psql is a command line tool that is installed alongside PostgreSQL DB, but since we've always been running PostgreSQL in a container, you've only got `pgcli`, which lacks the feature to run a sql script into the DB. Besides, having a command line for each database flavor you'll have to deal with as a Data Professional is far from ideal.\\nSo, instead, you can use usql. Check the docs for details on how to install for your OS. On macOS, it supports `homebrew`, and on Windows, it supports scoop.\\nSo, to run the taxi_trips.sql script with usql:\", 'section': 'Workshop 2 - RisingWave', 'question': 'psql - command not found: psql (alternative install)', 'course': 'data-engineering-zoomcamp', 'doc_id': '2cf36f471360b3a13014a0597fbbdc0d'}, {'text': 'If you encounter this error and are certain that you have docker compose installed, but typically run it as docker compose without the hyphen, then consider editing command.sh file by removing the hyphen from ‘docker-compose’. Example:\\nstart-cluster() {\\ndocker compose -f docker/docker-compose.yml up -d\\n}', 'section': 'Workshop 2 - RisingWave', 'question': 'Setup - source command.sh - error: “docker-compose” not found', 'course': 'data-engineering-zoomcamp', 'doc_id': 'fb51819cf3bcf5c13d3a406c96e00432'}, {'text': 'ERROR: The Compose file \\'./docker/docker-compose.yml\\' is invalid because:\\nInvalid top-level property \"x-image\". Valid top-level sections for this Compose file are: version, services, networks, volumes, secrets, configs, and extensions starting with \"x-\".\\nYou might be seeing this error because you\\'re using the wrong Compose file version. Either specify a supported version (e.g \"2.2\" or \"3.3\") and place your service definitions under the `services` key, or omit the `version` key and place your service definitions at the root of the file to use version 1.\\nFor more on the Compose file format versions, see https://docs.docker.com/compose/compose-file/\\nIf you encounter the above error and have docker-compose installed, try updating your version of docker-compose. At the time of reporting this issue (March 17 2024), Ubuntu does not seem to support a docker-compose version high enough to run the required docker images. If you have this error and are on a Ubuntu machine, consider starting a VM with a Debian machine or look for an alternative way to download docker-compose at the latest version on your machine.', 'section': 'Workshop 2 - RisingWave', 'question': 'Setup - start-cluster error: Invalid top-level property x-image', 'course': 'data-engineering-zoomcamp', 'doc_id': '6a5f8d387e22fc62af84cc3333074df7'}, {'text': 'Ans: [source] Yes, it is so that we can observe the changes as we’re working on the queries in real-time. The script is changing the date timestamp to the current time, so our queries with the now()filter would work. Open another terminal tab to copy+paste the queries while the stream-kafka script is running in the background.\\nNoel: I have recently increased this up to 100 at a time, you may pull the latest changes from the repository.', 'section': 'Workshop 2 - RisingWave', 'question': 'stream-kafka Qn: Is it expected that the records are being ingested 10 at a time?', 'course': 'data-engineering-zoomcamp', 'doc_id': '00174d4104af17a4165a9a1ed514ceba'}, {'text': 'Ans: No, it is not.', 'section': 'Workshop 2 - RisingWave', 'question': 'Setup - Qn: Is kafka install required for the RisingWave workshop? [source]', 'course': 'data-engineering-zoomcamp', 'doc_id': '0845bdd1a52cab95f456cc06f803bc9a'}, {'text': 'Ans: about 7GB free for all the containers to be provisioned and then the psql still needs to run and ingest the taxi data, so maybe 10gb in total?', 'section': 'Workshop 2 - RisingWave', 'question': 'Setup - Qn: How much free disk space should we have? [source]', 'course': 'data-engineering-zoomcamp', 'doc_id': '6b6dbecd61063dfb0cb08890d69d1040'}, {'text': 'Replace psycopg2==2.9.9 with psycopg2-binary in the requirements.txt file [source] [another]\\nWhen you open another terminal to run the psql, remember to do the source command.sh step for each terminal session\\n---------------------------------------------------------------------------------------------', 'section': 'Workshop 2 - RisingWave', 'question': 'Psycopg2 - issues when running stream-kafka script', 'course': 'data-engineering-zoomcamp', 'doc_id': 'a5d38913b987d5f42c35fbd193e57b5d'}, {'text': \"If you’re using an Anaconda installation:\\nCd home/\\nConda install gcc\\nSource back to your RisingWave Venv - source .venv/bin/activate\\nPip install psycopg2-binary\\nPip install -r requirements.txt\\nFor some reason this worked - the Conda base doesn’t have the GCC installed - (GNU Compiler Collection) a compiler system that supports various programming languages. Without this the it fails to install pyproject.toml-based projects\\n“It's possible that in your specific environment, the gcc installation was required at the system level rather than within the virtual environment. This can happen if the build process for psycopg2 tries to access system-level dependencies during installation.\\nInstalling gcc in your main Python installation (Conda) would make it available system-wide, allowing any Python environment to access it when necessary for building packages.”\\ngcc stands for GNU Compiler Collection. It is a compiler system developed by the GNU Project that supports various programming languages, including C, C++, Objective-C, and Fortran.\\nGCC is widely used for compiling source code written in these languages into executable programs or libraries. It's a key tool in the software development process, particularly in the compilation stage where source code is translated into machine code that can be executed by a computer's processor.\\nIn addition to compiling source code, GCC also provides various optimization options, debugging support, and extensive documentation, making it a powerful and versatile tool for developers across different platforms and architectures.\\n—-----------------------------------------------------------------------------------\", 'section': 'Workshop 2 - RisingWave', 'question': 'Psycopg2 - `Could not build wheels for psycopg2, which is required to install pyproject.toml-based projects`', 'course': 'data-engineering-zoomcamp', 'doc_id': '70d1f8b1d282f556e48fefe11165fdad'}, {'text': \"Below I have listed some steps I took to rectify this and potentially other minor errors, in Windows:\\nUse the git bash terminal in windows.\\nActivate python venv from git bash: source .venv/Scripts/activate\\nModify the seed_kafka.py file: in the first line, replace python3 with python.\\nNow from git bash, run the seed-kafka cmd. It should work now.\\nAdditional Notes:\\nYou can connect to the RisingWave cluster from Powershell with the command psql -h localhost -p 4566 -d dev -U root , otherwise it asks for a password.\\nThe equivalent of source commands.sh  in Powershell is . .\\\\commands.sh from the workshop directory.\\nHope this can save you from some trouble in case you're doing this workshop on Windows like I am.\\n—--------------------------------------------------------------------------------------\", 'section': 'Workshop 2 - RisingWave', 'question': 'Psycopg2 InternalError: Failed to run the query - when running the seed-kafka command after initial setup.', 'course': 'data-engineering-zoomcamp', 'doc_id': 'ea8c784ab439dbdb3087011c1eee4761'}, {'text': 'In case the script gets stuck on\\n%3|1709652240.100|FAIL|rdkafka#producer-2| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connect to ipv4#127.0.0.1:9092 failed: Connection refused (after 0ms in state CONNECT)gre\\nafter trying to load the trip data, check the logs of the message_queue container in docker. If it keeps restarting with Could not initialize seastar: std::runtime_error (insufficient physical memory: needed 4294967296 available 4067422208)  as the last message, then go to the docker-compose file in the docker folder of the project and change the ‘memory’ command for the message_queue service for some lower value.\\nSolution: lower the memory allocation of the service “message_queue” in your docker-compose file from 4GB. If you have the “insufficient physical memory” error message (try 3GB)\\nIssue: Running psql -f risingwave-sql/table/trip_data.sql after starting services with ‘default’ values using docker-compose up gives the error  “psql:risingwave-sql/table/trip_data.sql:61: ERROR:  syntax error at or near \".\" LINE 60:       properties.bootstrap.server=\\'message_queue:29092\\'”\\nSolution: Make sure you have run source commands.sh in each terminal window', 'section': 'Workshop 2 - RisingWave', 'question': 'Running stream-kafka script gets stuck on a loop with Connection Refused', 'course': 'data-engineering-zoomcamp', 'doc_id': '10181da938d84f637e137f0226f4eb68'}, {'text': 'Use seed-kafka instead of stream-kafka to get a static set of results.', 'section': 'Workshop 2 - RisingWave', 'question': 'For the homework questions is there a specific number of records that have to be processed to obtain the final answer?', 'course': 'data-engineering-zoomcamp', 'doc_id': 'f6543d6ee50cbeee6c7d710a9a0e238c'}, {'text': 'It is best to use the order by and limit clause on the query to the materialized view instead of the materialized view creation in order to guarantee consistent results\\nHomework - The answers in the homework do not match the provided options: You must follow the following steps: 1. clean-cluster 2. docker volume prune and use seed-kafka instead of stream-kafka. Ensure that the number of records is 100K.', 'section': 'Workshop 2 - RisingWave', 'question': 'Homework - Materialized view does not guarantee order by warning', 'course': 'data-engineering-zoomcamp', 'doc_id': '33305d774c403f3582e5876726447e06'}, {'text': 'For this workshop, and if you are following the view from Noel (2024) this requires you to install postgres to use it on your terminal. Found this steps (commands) to get it done [source]:\\nwget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\\nsudo sh -c \\'echo \"deb http://apt.postgresql.org/pub/repos/apt/ $(lsb_release -cs)-pgdg main\" >> /etc/apt/sources.list.d/pgdg.list\\'\\nsudo apt update\\napt install postgresql postgresql-contrib\\n(comment): now let’s check the service for postgresql\\nservice postgresql status\\n(comment) If down: use the next command\\nservice postgresql start\\n(comment) And your are done', 'section': 'Workshop 2 - RisingWave', 'question': 'How to install postgress on Linux like OS', 'course': 'data-engineering-zoomcamp', 'doc_id': '3b60fc98d5bfbb0f84337f26a177275f'}, {'text': 'Refer to the solution given in the first solution here:\\nhttps://stackoverflow.com/questions/24683221/xdg-open-no-method-available-even-after-installing-xdg-utils\\nInstead of w3m use any other browser of your choice.\\nIt is just trying to open the index.html file. Which you can do from your File Explorer/Finder. If you’re on wsl try using explorer.exe index.html', 'section': 'Workshop 2 - RisingWave', 'question': 'Unable to Open Dashboard as xdg-open doesn’t open any browser', 'course': 'data-engineering-zoomcamp', 'doc_id': '826b6f92b5fdcb2c55178cd9c690e04e'}, {'text': 'Example Error:\\nWhen attempting to execute a Python script named seed-kafka.py or server.py with the following shebang line specifying Python 3 as the interpreter:\\nUsers may encounter the following error in a Unix-like environment:\\nThis error indicates that there is a problem with the Python interpreter path specified in the shebang line. The presence of the \\\\r character suggests that the script was edited or created in a Windows environment, causing the interpreter path to be incorrect when executed in Unix-like environments.\\n2 Solutions:\\nEither one or the other\\nUpdate Shebang Line:\\nVerify Python Interpreter Path: Use the which python3 command to determine the path to the Python 3 interpreter available in the current environment.\\nUpdate Shebang Line: Open the script file in a text editor. Modify the shebang line to point to the correct Python interpreter path found in the previous step. Ensure that the shebang line is consistent with the Python interpreter path in the execution environment.\\nExample Shebang Line:\\nReplace /usr/bin/env python3 with the correct Python interpreter path found using which python3.\\nConvert Line Endings:\\nUse the dos2unix command-line tool to convert the line endings of the script from Windows-style to Unix-style.\\nThis removes the extraneous carriage return characters (\\\\r), resolving issues related to unexpected tokens and ensuring compatibility with Unix-like environments.\\nExample Command:', 'section': 'Workshop 2 - RisingWave', 'question': 'Resolving Python Interpreter Path Inconsistencies in Unix-like Environments', 'course': 'data-engineering-zoomcamp', 'doc_id': 'dd77d96cf763ebd55127e444ee040829'}, {'text': 'Ans : Windowing in streaming SQL involves defining a time-based or row-based boundary for data processing. It allows you to analyze and aggregate data over specific time intervals or based on the number of events received, providing a way to manage and organize streaming data for analysis.', 'section': 'Workshop 2 - RisingWave', 'question': 'How does windowing work in Sql?', 'course': 'data-engineering-zoomcamp', 'doc_id': '7dd07e55a5a7a82dd25a24dcc51e54f1'}, {'text': 'Python 3.12.1, is not compatible with kafka-python-2.0.2. Therefore, instead of running \"pip install kafka-python\", you can resolve the issue by using \"pip install git+https://github.com/dpkp/kafka-python.git\". If you have already installed kafka-python, you need to run \"pip uninstall kafka-python\" before executing \"pip install git+https://github.com/dpkp/kafka-python.git\" to resolve the compatibility issue.\\nQ:In the Mage pipeline, individual blocks run successfully. However, when executing the pipeline as a whole, some blocks fail.\\nA: I have the following key-value pair in io_config.yaml file configured but still Mage blocks failed to generate OAuth and authenticate with GCP: GOOGLE_SERVICE_ACC_KEY_FILEPATH: \"{{ env_var(\\'GCP_CREDENTIALS\\') }}\". The GCP_CREDENTIALS variable holds the full path to the service account key\\'s JSON file. Adding the following line within the failed code block resolved the issue: os.environ[\\'GOOGLE_APPLICATION_CREDENTIALS\\'] = os.environ.get(\\'GCP_CREDENTIALS\\').\\nThis occurs because the path to profiles.yml is not correctly specified. You can rectify this by:\\n“export DBT_PROFILES_DBT=path/to/profiles.yml”\\nEg., /home/src/magic-zoomcamp/dbt/project_name/\\nDo the similar for DBT_PROJECT_DIR if getting similar issue with dbt_project.yml.\\nOnce DIRs are set,:\\n“dbt debug –config-dir”\\nThis would update your paths. To maintain same path across sessions, use the path variables in your .env file.\\nTo add triggers in mage pipelines via CLI, you can create a trigger of type API, and copy the API links.\\nEg. link: http://localhost:6789/api/pipeline_schedules/10/pipeline_runs/f3a1a4228fc64cfd85295b668c93f3b2\\nThen create a trigger.py as such:\\nimport os\\nimport requests\\nclass MageTrigger:\\nOPTIONS = {\\n\"<pipeline_name>\": {\\n\"trigger_id\": 10,\\n\"key\": \"f3a1a4228fc64cfd85295b668c93f3b2\"\\n}\\n}\\n@staticmethod\\ndef trigger_pipeline(pipeline_name, variables=None):\\ntrigger_id = MageTrigger.OPTIONS[pipeline_name][\"trigger_id\"]\\nkey = MageTrigger.OPTIONS[pipeline_name][\"key\"]\\nendpoint = f\"http://localhost:6789/api/pipeline_schedules/{trigger_id}/pipeline_runs/{key}\"\\nheaders = {\\'Content-Type\\': \\'application/json\\'}\\npayload = {}\\nif variables is not None:\\npayload[\\'pipeline_run\\'] = {\\'variables\\': variables}\\nresponse = requests.post(endpoint, headers=headers, json=payload)\\nreturn response\\nMageTrigger.trigger_pipeline(\"<pipeline_name>\")\\nFinally, after the mage server is up an running, simply this command:\\npython trigger.py from mage directory in terminal.\\nCan I do data partitioning & clustering run by dbt pipeline, or I would need to do this manually in BigQuery afterwards?\\nYou can use this configuration in your DBT model:\\n{\\n\"field\": \"<field name>\",\\n\"data_type\": \"<timestamp | date | datetime | int64>\",\\n\"granularity\": \"<hour | day | month | year>\"\\n# Only required if data_type is \"int64\"\\n\"range\": {\\n\"start\": <int>,\\n\"end\": <int>,\\n\"interval\": <int>\\n}\\n}\\nand for clustering\\n{{\\nconfig(\\nmaterialized = \"table\",\\ncluster_by = \"order_id\",\\n)\\n}}\\nmore details in: https://docs.getdbt.com/reference/resource-configs/bigquery-configs', 'section': 'Triggers in Mage via CLI', 'question': 'Encountering the error \"ModuleNotFoundError: No module named \\'kafka.vendor.six.moves\\'\" when running \"from kafka import KafkaProducer\" in Jupyter Notebook for Module 6 Homework?', 'course': 'data-engineering-zoomcamp', 'doc_id': 'c01c4a327793171e1e6b8acbab0afd26'}, {'text': 'Docker Commands\\n# Create a Docker Image from a base image\\nDocker run -it ubuntu bash\\n#List docker images\\nDocker images list\\n#List  Running containers\\nDocker ps -a\\n#List with full container ids\\nDocker ps -a --no-trunc\\n#Add onto existing image to create new image\\nDocker commit -a <User_Name> -m \"Message\" container_id New_Image_Name\\n# Create a Docker Image with an entrypoint from a base image\\nDocker run -it --entry_point=bash python:3.11\\n#Attach to a stopped container\\nDocker start -ai <Container_Name>\\n#Attach to a running container\\ndocker exec -it <Container_ID> bash\\n#copying from host to container\\nDocker cp <SRC_PATH/file> <containerid>:<dest_path>\\n#copying from container to host\\nDocker cp <containerid>:<Srct_path> <Dest Path on host/file>\\n#Create an image from a docker file\\nDocker build -t <Image_Name> <Location of Dockerfile>\\n#DockerFile Options and best practices\\nhttps://devopscube.com/build-docker-image/\\n#Docker delete all images forcefully\\ndocker rmi -f $(docker images -aq)\\n#Docker delete all containers forcefully\\ndocker rm -f $(docker ps -qa)\\n#docker compose creation\\nhttps://www.composerize.com/\\nGCP Commands\\n1.     Create SSH Keys\\n2.     Added to the Settings of Compute Engine VM Instance\\n3.     SSH-ed into the VM Instance with a config similar to following\\nHost my-website.com\\nHostName my-website.com\\nUser my-user\\nIdentityFile ~/.ssh/id_rsa\\n4.     Installed Anaconda by installing the sh file through bash <Anaconda.sh>\\n5.     Install Docker after\\na.     Sudo apt-get update\\nb.     Sudo apt-get docker\\n6.     To run Docker without SUDO permissions\\na.     https://github.com/sindresorhus/guides/blob/main/docker-without-sudo.md\\n7.     Google cloud remote copy\\na.     gcloud compute scp LOCAL_FILE_PATHVM_NAME:REMOTE_DIR\\nInstall GCP Cloud SDK on Docker Machine\\nhttps://stackoverflow.com/questions/23247943/trouble-installing-google-cloud-sdk-in-ubuntu\\nsudo apt-get install apt-transport-https ca-certificates gnupg && echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main\"| sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list&& curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add - && sudo apt-get update && sudo apt-get install google-cloud-sdk && sudo apt-get install google-cloud-sdk-app-engine-java && sudo apt-get install google-cloud-sdk-app-engine-python && gcloud init\\nAnaconda Commands\\n#Activate environment\\nConda Activate <environment_name>\\n#DeActivate environment\\nConda DeActivate <environment_name>\\n#Start iterm without conda environment\\nconda config --set auto_activate_base false\\n# Using Conda forge as default (Community driven packaging recipes and solutions)\\nhttps://conda-forge.org/docs/user/introduction.html\\nconda --version\\nconda update conda\\nconda config --add channels conda-forge\\nconda config --set channel_priority strict\\n#Using Libmamba as Solver\\nconda install pgcli  --solver=libmamba\\nLinux/MAC Commands\\nStarting and Stopping Services on Linux\\n●  \\tsudo systemctl start postgresql\\n●  \\tsudo systemctl stop postgresql\\nStarting and Stopping Services on MAC\\n●      launchctl start postgresql\\n●      launchctl stop postgresql\\nIdentifying processes listening to a Port across MAC/Linux\\nsudo lsof -i -P -n | grep LISTEN\\n$ sudo netstat -tulpn | grep LISTEN\\n$ sudo ss -tulpn | grep LISTEN\\n$ sudo lsof -i:22 ## see a specific port such as 22 ##\\n$ sudo nmap -sTU -O IP-address-Here\\nInstalling a package on Debian\\nsudo apt install <packagename>\\nListing all package on Debian\\nDpkg -l | grep <packagename>\\nUnInstalling a package on Debian\\nSudo apt remove <packagename>\\nSudo apt autoclean  && sudo apt autoremove\\nList all Processes on Debian/Ubuntu\\nPs -aux\\napt-get update && apt-get install procps\\napt-get install iproute2 for ss -tulpn\\n#Postgres Install\\nsudo sh -c \\'echo \"deb https://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main\" > /etc/apt/sources.list.d/pgdg.list\\'\\nwget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\\nsudo apt-get update\\nsudo apt-get -y install postgresql\\n#Changing Postgresql port to 5432\\n- sudo service postgresql stop - sed -e \\'s/^port.*/port = 5432/\\' /etc/postgresql/10/main/postgresql.conf > postgresql.conf\\n- sudo chown postgres postgresql.conf\\n- sudo mv postgresql.conf /etc/postgresql/10/main\\n- sudo systemctl restart postgresql', 'section': 'Triggers in Mage via CLI', 'question': 'Basic Commands', 'course': 'data-engineering-zoomcamp', 'doc_id': '3cf2153faa38890af19cc530f3ebe09b'}, {'text': 'Machine Learning Zoomcamp FAQ\\nThe purpose of this document is to capture frequently asked technical questions.\\nWe did this for our data engineering course and it worked quite well. Check this document for inspiration on how to structure your questions and answers:\\nData Engineering Zoomcamp FAQ\\nIn the course GitHub repository there’s a link. Here it is: https://airtable.com/shryxwLd0COOEaqXo\\nwork', 'section': 'General course-related questions', 'question': 'How do I sign up?', 'course': 'machine-learning-zoomcamp', 'doc_id': '7244bc052ce8176085f4b05d6029162f'}, {'text': 'The course videos are pre-recorded, you can start watching the course right now.\\nWe will also occasionally have office hours - live sessions where we will answer your questions. The office hours sessions are recorded too.\\nYou can see the office hours as well as the pre-recorded course videos in the course playlist on YouTube.', 'section': 'General course-related questions', 'question': 'Is it going to be live? When?', 'course': 'machine-learning-zoomcamp', 'doc_id': '4e09802e6092fd04a5f7cf0d2f6635a4'}, {'text': 'Everything is recorded, so you won’t miss anything. You will be able to ask your questions for office hours in advance and we will cover them during the live stream. Also, you can always ask questions in Slack.', 'section': 'General course-related questions', 'question': 'What if I miss a session?', 'course': 'machine-learning-zoomcamp', 'doc_id': 'c168d158841ef3e7dee19ebf2291b467'}, {'text': \"The bare minimum. The focus is more on practice, and we'll cover the theory only on the intuitive level.: https://mlbookcamp.com/article/python\\nFor example, we won't derive the gradient update rule for logistic regression (there are other great courses for that), but we'll cover how to use logistic regression and make sense of the results.\", 'section': 'General course-related questions', 'question': 'How much theory will you cover?', 'course': 'machine-learning-zoomcamp', 'doc_id': '065698c11cf3a55a7ad1a51a55678e36'}, {'text': \"Yes! We'll cover some linear algebra in the course, but in general, there will be very few formulas, mostly code.\\nHere are some interesting videos covering linear algebra that you can already watch: ML Zoomcamp 1.8 - Linear Algebra Refresher from Alexey Grigorev or the excellent playlist from 3Blue1Brown Vectors | Chapter 1, Essence of linear algebra. Never hesitate to ask the community for help if you have any question.\\n(Mélanie Fouesnard)\", 'section': 'General course-related questions', 'question': \"I don't know math. Can I take the course?\", 'course': 'machine-learning-zoomcamp', 'doc_id': 'e504ab92e0a32ffa91ed79299a100ae2'}, {'text': \"The process is automated now, so you should receive the email eventually. If you haven’t, check your promotions tab in Gmail as well as spam.\\nIf you unsubscribed from our newsletter, you won't get course related updates too.\\nBut don't worry, it’s not a problem. To make sure you don’t miss anything, join the #course-ml-zoomcamp channel in Slack and our telegram channel with announcements. This is enough to follow the course.\", 'section': 'General course-related questions', 'question': \"I filled the form, but haven't received a confirmation email. Is it normal?\", 'course': 'machine-learning-zoomcamp', 'doc_id': '5854fd9855f800f031d0c0b14105bb81'}, {'text': 'Approximately 4 months, but may take more if you want to do some extra activities (an extra project, an article, etc)', 'section': 'General course-related questions', 'question': 'How long is the course?', 'course': 'machine-learning-zoomcamp', 'doc_id': '3a19c31d89c34a3745e118e47ddec9c3'}, {'text': 'Around ~10 hours per week. Timur Kamaliev did a detailed analysis of how much time students of the previous cohort needed to spend on different modules and projects. Full article', 'section': 'General course-related questions', 'question': 'How much time do I need for this course?', 'course': 'machine-learning-zoomcamp', 'doc_id': '55e044b2713c1bb49818df4613b45545'}, {'text': 'Yes, if you finish at least 2 out of 3 projects and review 3 peers’ Projects by the deadline, you will get a certificate. This is what it looks like: link. There’s also a version without a robot: link.', 'section': 'General course-related questions', 'question': 'Will I get a certificate?', 'course': 'machine-learning-zoomcamp', 'doc_id': '85e4e1d751919a66bc80400d34e01a77'}, {'text': \"Yes, it's possible. See the previous answer.\", 'section': 'General course-related questions', 'question': 'Will I get a certificate if I missed the midterm project?', 'course': 'machine-learning-zoomcamp', 'doc_id': 'b93f158736ea7a144e931919d9996508'}, {'text': 'Check this article. If you know everything in this article, you know enough. If you don’t, read the article and join the coursIntroduction to Pythone too :)\\nIntroduction to Python – Machine Learning Bookcamp\\nYou can follow this English course from the OpenClassrooms e-learning platform, which is free and covers the python basics for data analysis: Learn Python Basics for Data Analysis - OpenClassrooms . It is important to know some basics such as: how to run a Jupyter notebook, how to import libraries (and what libraries are), how to declare a variable (and what variables are) and some important operations regarding data analysis.\\n(Mélanie Fouesnard)', 'section': 'General course-related questions', 'question': 'How much Python should I know?', 'course': 'machine-learning-zoomcamp', 'doc_id': 'ab25b906c3ad4e99bfd2e97c166cd101'}, {'text': 'For the Machine Learning part, all you need is a working laptop with an internet connection. The Deep Learning part is more resource intensive, but for that you can use a cloud (we use Saturn cloud but can be anything else).\\n(Rileen Sinha; based on response by Alexey on Slack)', 'section': 'General course-related questions', 'question': \"Any particular hardware requirements for the course or everything is mostly cloud? TIA! Couldn't really find this in the FAQ.\", 'course': 'machine-learning-zoomcamp', 'doc_id': '53af797a68ee2940b86fa469363c9e85'}, {'text': 'Here is an article that worked for me: https://knowmledge.com/2023/12/07/ml-zoomcamp-2023-project/', 'section': 'General course-related questions', 'question': 'How to setup TensorFlow with GPU support on Ubuntu?', 'course': 'machine-learning-zoomcamp', 'doc_id': '6947c5f1004b1113235bb43c680e4a64'}, {'text': \"Here’s how you join a in Slack: https://slack.com/help/articles/205239967-Join-a-channel\\nClick “All channels” at the top of your left sidebar. If you don't see this option, click “More” to find it.\\nBrowse the list of public channels in your workspace, or use the search bar to search by channel name or description.\\nSelect a channel from the list to view it.\\nClick Join Channel.\\nDo we need to provide the GitHub link to only our code corresponding to the homework questions?\\nYes. You are required to provide the URL to your repo in order to receive a grade\", 'section': 'General course-related questions', 'question': 'I’m new to Slack and can’t find the course channel. Where is it?', 'course': 'machine-learning-zoomcamp', 'doc_id': 'ba72d5fa09d37e69971e4c04f13789af'}, {'text': 'Yes, you can. You won’t be able to submit some of the homeworks, but you can still take part in the course.\\nIn order to get a certificate, you need to submit 2 out of 3 course projects and review 3 peers’ Projects by the deadline. It means that if you join the course at the end of November and manage to work on two projects, you will still be eligible for a certificate.', 'section': 'General course-related questions', 'question': 'The course has already started. Can I still join it?', 'course': 'machine-learning-zoomcamp', 'doc_id': '76b2cfa6443b8bf034146208c79d1bd6'}, {'text': 'The course is available in the self-paced mode too, so you can go through the materials at any time. But if you want to do it as a cohort with other students, the next iterations will happen in September 2023, September 2024 (and potentially other Septembers as well).', 'section': 'General course-related questions', 'question': 'When does the next iteration start?', 'course': 'machine-learning-zoomcamp', 'doc_id': '83a6c7d555f0b7200d4ff58a112a94ec'}, {'text': 'No, it’s not possible. The form is closed after the due date. But don’t worry, homework is not mandatory for finishing the course.', 'section': 'General course-related questions', 'question': 'Can I submit the homework after the due date?', 'course': 'machine-learning-zoomcamp', 'doc_id': '1b6d80a4ee79f28d8077abd62e6d4f7e'}, {'text': 'Welcome to the course! Go to the course page (http://mlzoomcamp.com/), scroll down and start going through the course materials. Then read everything in the cohort folder for your cohort’s year.\\nClick on the links and start watching the videos. Also watch office hours from previous cohorts. Go to DTC youtube channel and click on Playlists and search for {course yyyy}. ML Zoomcamp was first launched in 2021.\\nOr you can just use this link: http://mlzoomcamp.com/#syllabus', 'section': 'General course-related questions', 'question': 'I just joined. What should I do next? How can I access course materials?', 'course': 'machine-learning-zoomcamp', 'doc_id': '7909413e274d6f41bce99fb496c42db6'}, {'text': 'For the 2023 cohort, you can see the deadlines here (it’s taken from the 2023 cohort page)', 'section': 'General course-related questions', 'question': 'What are the deadlines in this course?', 'course': 'machine-learning-zoomcamp', 'doc_id': '22dc3e5404a2dd02e0ed9359a6a69221'}, {'text': 'There’s not much difference. There was one special module (BentoML) in the previous iteration of the course, but the rest of the modules are the same as in 2022. The homework this year is different.', 'section': 'General course-related questions', 'question': 'What’s the difference between the previous iteration of the course (2022) and this one (2023)?', 'course': 'machine-learning-zoomcamp', 'doc_id': '52e422ebd01108b373a0570b5ffc638e'}, {'text': 'We won’t re-record the course videos. The focus of the course and the skills we want to teach remained the same, and the videos are still up-to-date.\\nIf you haven’t taken part in the previous iteration, you can start watching the videos. It’ll be useful for you and you will learn new things. However, we recommend using Python 3.10 now instead of Python 3.8.', 'section': 'General course-related questions', 'question': 'The course videos are from the previous iteration. Will you release new ones or we’ll use the videos from 2021?', 'course': 'machine-learning-zoomcamp', 'doc_id': '2ec616a8ebb9ab48551c2929b20c8ddd'}, {'text': 'When you post about what you learned from the course on your social media pages, use the tag #mlzoomcamp. When you submit your homework, there’s a section in the form for putting the links there. Separate multiple links by any whitespace character (linebreak, space, tab, etc).\\nFor posting the learning in public links, you get extra scores. But the number of scores is limited to 7 points: if you put more than 7 links in your homework form, you’ll get only 7 points.\\nThe same content can be posted to 7 different social sites and still earn you 7 points if you add 7 URLs per week, see Alexey’s reply. (~ ellacharmed)\\nFor midterms/capstones, the awarded points are doubled as the duration is longer. So for projects the points are capped at 14 for 14 URLs.', 'section': 'General course-related questions', 'question': 'Submitting learning in public links', 'course': 'machine-learning-zoomcamp', 'doc_id': '26691c7bec9cf4b93271649d4c0a25e6'}, {'text': \"You can create your own github repository for the course with your notes, homework, projects, etc.\\nThen fork the original course repo and add a link under the 'Community Notes' section to the notes that are in your own repo.\\nAfter that's done, create a pull request to sync your fork with the original course repo.\\n(By Wesley Barreto)\", 'section': 'General course-related questions', 'question': 'Adding community notes', 'course': 'machine-learning-zoomcamp', 'doc_id': 'bade674918646e25a056585b7977c25d'}, {'text': \"Leaderboard Links:\\n2023 - https://docs.google.com/spreadsheets/d/e/2PACX-1vSNK_yGtELX1RJK1SSRl4xiUbD0XZMYS6uwHnybc7Mql-WMnMgO7hHSu59w-1cE7FeFZjkopbh684UE/pubhtml\\n2022 - https://docs.google.com/spreadsheets/d/e/2PACX-1vQzLGpva63gb2rIilFnpZMRSb-buyr5oGh8jmDtIb8DANo4n6hDalra_WRCl4EZwO1JvaC4UIS62n5h/pubhtml\\nPython Code:\\nfrom hashlib import sha1\\ndef compute_hash(email):\\nreturn sha1(email.lower().encode('utf-8')).hexdigest()\\nYou need to call the function as follows:\\nprint(compute_hash('YOUR_EMAIL_HERE'))\\nThe quotes are required to denote that your email is a string.\\n(By Wesley Barreto)\\nYou can also use this website directly by entering your email: http://www.sha1-online.com. Then, you just have to copy and paste your hashed email in the “research” bar of the leaderboard to get your scores.\\n(Mélanie Fouesnard)\", 'section': '1. Introduction to Machine Learning', 'question': 'Computing the hash for the leaderboard and project review', 'course': 'machine-learning-zoomcamp', 'doc_id': 'fbc42b7dab9cd1a9c28d3b724afa500a'}, {'text': 'If you get “wget is not recognized as an internal or external command”, you need to install it.\\nOn Ubuntu, run\\nsudo apt-get install wget\\nOn Windows, the easiest way to install wget is to use Chocolatey:\\nchoco install wget\\nOr you can download a binary from here and put it to any location in your PATH (e.g. C:/tools/)\\nOn Mac, the easiest way to install wget is to use brew.\\nBrew install wget\\nAlternatively, you can use a Python wget library, but instead of simply using “wget” you’ll need eeeto use\\npython -m wget\\nYou need to install it with pip first:\\npip install wget\\nAnd then in your python code, for example in your jupyter notebook, use:\\nimport wget\\nwget.download(\"URL\")\\nThis should download whatever is at the URL in the same directory as your code.\\n(Memoona Tahira)\\nAlternatively, you can read a CSV file from a URL directly with pandas:\\nurl = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\ndf = pd.read_csv(url)\\nValid URL schemes include http, ftp, s3, gs, and file.\\nIn some cases you might need to bypass https checks:\\nimport ssl\\nssl._create_default_https_context = ssl._create_unverified_context\\nOr you can use the built-in Python functionality for downloading the files:\\nimport urllib.request\\nurl = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\nurllib.request.urlretrieve(url, \"housing.csv\")\\nUrllib.request.urlretrieve() is a standard Python library function available on all devices and platforms. URL requests and URL data retrieval are done with the urllib.request module.\\nThe urlretrieve() function allows you to download files from URLs and save them locally. Python programs use it to download files from the internet.\\nOn any Python-enabled device or platform, you can use the urllib.request.urlretrieve() function to download the file.\\n(Mohammad Emad Sharifi)', 'section': '1. Introduction to Machine Learning', 'question': 'wget is not recognized as an internal or external command', 'course': 'machine-learning-zoomcamp', 'doc_id': '9f9dc2283b11f5ca51c5499c744a4426'}, {'text': 'You can use\\n!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nTo download the data too. The exclamation mark !, lets you execute shell commands inside your notebooks. This works generally for shell commands such as ls, cp, mkdir, mv etc . . .\\nFor instance, if you then want to move your data into a data directory alongside your notebook-containing directory, you could execute the following:\\n!mkdir -p ../data/\\n!mv housing.csv ../data/', 'section': '1. Introduction to Machine Learning', 'question': 'Retrieving csv inside notebook', 'course': 'machine-learning-zoomcamp', 'doc_id': '7befb81549f486a729a302e8cfaf06bb'}, {'text': '(Tyler Simpson)', 'section': '1. Introduction to Machine Learning', 'question': 'Windows WSL and VS Code\\nIf you have a Windows 11 device and would like to use the built in WSL to access linux you can use the Microsoft Learn link Set up a WSL development environment | Microsoft Learn. To connect this to VS Code download the Microsoft verified VS Code extension ‘WSL’ this will allow you to remotely connect to your WSL Ubuntu instance as if it was a virtual machine.', 'course': 'machine-learning-zoomcamp', 'doc_id': 'f646e1c25f4182ea0eb8126ea35d4490'}, {'text': 'This is my first time using Github to upload a code. I was getting the below error message when I type\\ngit push -u origin master:\\nerror: src refspec master does not match any\\nerror: failed to push some refs to \\'https://github.com/XXXXXX/1st-Homework.git\\'\\nSolution:\\nThe error message got fixed by running below commands:\\ngit commit -m \"initial commit\"\\ngit push origin main\\nIf this is your first time to use Github, you will find a great & straightforward tutorial in this link https://dennisivy.com/github-quickstart\\n(Asia Saeed)\\nYou can also use the “upload file” functionality from GitHub for that\\nIf you write your code on Google colab you can also directly share it on your Github.\\n(By Pranab Sarma)', 'section': '1. Introduction to Machine Learning', 'question': 'Uploading the homework to Github', 'course': 'machine-learning-zoomcamp', 'doc_id': '9b5955bbeaabe6174ad037a3d922d013'}, {'text': \"I'm trying to invert the matrix but I got error that the matrix is singular matrix\\nThe singular matrix error is caused by the fact that not every matrix can be inverted. In particular, in the homework it happens because you have to pay close attention when dealing with multiplication (the method .dot) since multiplication is not commutative! X.dot(Y) is not necessarily equal to Y.dot(X), so respect the order otherwise you get the wrong matrix.\", 'section': '1. Introduction to Machine Learning', 'question': 'Singular Matrix Error', 'course': 'machine-learning-zoomcamp', 'doc_id': 'c17ec2ceecce49e934923be85d8216ab'}, {'text': 'I have a problem with my terminal. Command\\nconda create -n ml-zoomcamp python=3.9\\ndoesn’t work. Any of 3.8/ 3.9 / 3.10 should be all fine\\nIf you’re on Windows and just installed Anaconda, you can use Anaconda’s own terminal called “Anaconda Prompt”.\\nIf you don’t have Anaconda or Miniconda, you should install it first\\n(Tatyana Mardvilko)', 'section': '1. Introduction to Machine Learning', 'question': 'Conda is not an internal command', 'course': 'machine-learning-zoomcamp', 'doc_id': '2bfd5a9384fe183960be4a03007ded18'}, {'text': 'How do I read the dataset with Pandas in Windows?\\nI used the code below but not working\\ndf = pd.read_csv(\\'C:\\\\Users\\\\username\\\\Downloads\\\\data.csv\\')\\nUnlike Linux/Mac OS, Windows uses the backslash (\\\\) to navigate the files that cause the conflict with Python. The problem with using the backslash is that in Python, the \\'\\\\\\' has a purpose known as an escape sequence. Escape sequences allow us to include special characters in strings, for example, \"\\\\n\" to add a new line or \"\\\\t\" to add spaces, etc. To avoid the issue we just need to add \"r\" before the file path and Python will treat it as a literal string (not an escape sequence).\\nHere’s how we should be loading the file instead:\\ndf = pd.read_csv(r\\'C:\\\\Users\\\\username\\\\Downloads\\\\data.csv\\')\\n(Muhammad Awon)', 'section': '1. Introduction to Machine Learning', 'question': 'Read-in the File in Windows OS', 'course': 'machine-learning-zoomcamp', 'doc_id': '0bd43356dd61d4ae2a3321d2deba034c'}, {'text': 'Type the following command:\\ngit config -l | grep url\\nThe output should look like this:\\nremote.origin.url=https://github.com/github-username/github-repository-name.git\\nChange this to the following format and make sure the change is reflected using command in step 1:\\ngit remote set-url origin \"https://github-username@github.com/github-username/github-repository-name.git\"\\n(Added by Dheeraj Karra)', 'section': '1. Introduction to Machine Learning', 'question': \"'403 Forbidden' error message when you try to push to a GitHub repository\", 'course': 'machine-learning-zoomcamp', 'doc_id': '9604ee6ba9ca95ca675bffb758b2aba5'}, {'text': \"I had a problem when I tried to push my code from Git Bash:\\nremote: Support for password authentication was removed on August 13, 2021.\\nremote: Please see https://docs.github.com/en/get-started/getting-started-with-git/about-remote-repositories#cloning-with-https-urls for information on currently recommended modes of authentication.\\nfatal: Authentication failed for 'https://github.com/username\\nSolution:\\nCreate a personal access token from your github account and use it when you make a push of your last changes.\\nhttps://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent\\nBruno Bedón\", 'section': '1. Introduction to Machine Learning', 'question': \"Fatal: Authentication failed for 'https://github.com/username\", 'course': 'machine-learning-zoomcamp', 'doc_id': '3364e783c6f4bed7f4f19429cb92c34f'}, {'text': \"In Kaggle, when you are trying to !wget a dataset from github (or any other public repository/location), you get the following error:\\nGetting  this error while trying to import data- !wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\n--2022-09-17 16:55:24--  https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... failed: Temporary failure in name resolution.\\nwget: unable to resolve host address 'raw.githubusercontent.com'\\nSolution:\\nIn your Kaggle notebook settings, turn on the Internet for your session. It's on the settings panel, on the right hand side of the Kaggle screen. You'll be asked to verify your phone number so Kaggle knows you are not a bot.\", 'section': '1. Introduction to Machine Learning', 'question': \"wget: unable to resolve host address 'raw.githubusercontent.com'\", 'course': 'machine-learning-zoomcamp', 'doc_id': 'bb35b7b7c896a1b6199a124628c0047a'}, {'text': 'I found this video quite helpful: Creating Virtual Environment for Python from VS Code\\n[Native Jupiter Notebooks support in VS Code] In VS Code you can also have a native Jupiter Notebooks support, i.e. you do not need to open a web browser to code in a Notebook. If you have port forwarding enabled + run a ‘jupyter notebook ‘ command from a remote machine + have a remote connection configured in .ssh/config (as Alexey’s video suggests) - VS Code can execute remote Jupyter Notebooks files on a remote server from your local machine: https://code.visualstudio.com/docs/datascience/jupyter-notebooks.\\n[Git support from VS Code] You can work with Github from VSCode - staging and commits are easy from the VS Code’s UI:  https://code.visualstudio.com/docs/sourcecontrol/overview\\n(Added by Ivan Brigida)', 'section': '1. Introduction to Machine Learning', 'question': 'Setting up an environment using VS Code', 'course': 'machine-learning-zoomcamp', 'doc_id': 'f9ebc1e7fafb8f3ca928d2abdf3f332e'}, {'text': 'With regards to creating an environment for the project, do we need to run the command \"conda create -n .......\" and \"conda activate ml-zoomcamp\" everytime we open vs code to work on the project?\\nAnswer:\\n\"conda create -n ....\" is just run the first time to create the environment. Once created, you just need to run \"conda activate ml-zoomcamp\" whenever you want to use it.\\n(Added by Wesley Barreto)\\nconda env export > environment.yml will also allow you to reproduce your existing environment in a YAML file.  You can then recreate it with conda env create -f environment.yml', 'section': '1. Introduction to Machine Learning', 'question': 'Conda Environment Setup', 'course': 'machine-learning-zoomcamp', 'doc_id': 'ff1be685e151dfb2edbcd9dc26921f83'}, {'text': \"I was doing Question 7 from Week1 Homework and with step6: Invert XTX, I created the inverse. Now, an inverse when multiplied by the original matrix should return in an Identity matrix. But when I multiplied the inverse with the original matrix, it gave a matrix like this:\\nInverse * Original:\\n[[ 1.00000000e+00 -1.38777878e-16]\\n[ 3.16968674e-13  1.00000000e+00]]\\nSolution:\\nIt's because floating point math doesn't work well on computers as shown here: https://stackoverflow.com/questions/588004/is-floating-point-math-broken\\n(Added by Wesley Barreto)\", 'section': '1. Introduction to Machine Learning', 'question': 'Floating Point Precision', 'course': 'machine-learning-zoomcamp', 'doc_id': 'b7ec7282f866bdc5e0fd65b9c73239d9'}, {'text': 'Answer:\\nIt prints the information about the dataset like:\\nIndex datatype\\nNo. of entries\\nColumn information with not-null count and datatype\\nMemory usage by dataset\\nWe use it as:\\ndf.info()\\n(Added by Aadarsha Shrestha & Emoghena Itakpe)', 'section': '1. Introduction to Machine Learning', 'question': 'What does pandas.DataFrame.info() do?', 'course': 'machine-learning-zoomcamp', 'doc_id': 'ec86334b1ea53a6a0979afc871818227'}, {'text': \"Pandas and numpy libraries are not being imported\\nNameError: name 'np' is not defined\\nNameError: name 'pd' is not defined\\nIf you're using numpy or pandas, make sure you use the first few lines before anything else.\\nimport pandas as pd\\nimport numpy as np\\nAdded by Manuel Alejandro Aponte\", 'section': '1. Introduction to Machine Learning', 'question': \"NameError: name 'np' is not defined\", 'course': 'machine-learning-zoomcamp', 'doc_id': 'c1047363f7b00426ff5ee9c3ffcefc5b'}, {'text': \"What if there were hundreds of columns? How do you get the columns only with numeric or object data in a more concise way?\\ndf.select_dtypes(include=np.number).columns.tolist()\\ndf.select_dtypes(include='object').columns.tolist()\\nAdded by Gregory Morris\", 'section': '1. Introduction to Machine Learning', 'question': 'How to select column by dtype', 'course': 'machine-learning-zoomcamp', 'doc_id': '500489499b2be17ee9afd089a127e8d6'}, {'text': 'There are many ways to identify the shape of dataset, one of them is using .shape attribute!\\ndf.shape\\ndf.shape[0] # for identify the number of rows\\ndf.shape[1] # for identify the number of columns\\nAdded by Radikal Lukafiardi', 'section': '1. Introduction to Machine Learning', 'question': 'How to identify the shape of dataset in Pandas', 'course': 'machine-learning-zoomcamp', 'doc_id': 'cfad7ba25e59e35011d7dc11ed891b98'}, {'text': 'First of all use np.dot for matrix multiplication. When you compute matrix-matrix multiplication you should understand that order of multiplying is crucial and affects the result of the multiplication!\\nDimension Mismatch\\nTo perform matrix multiplication, the number of columns in the 1st matrix should match the number of rows in the 2nd matrix. You can rearrange the order to make sure that this satisfies the condition.\\nAdded by Leah Gotladera', 'section': '1. Introduction to Machine Learning', 'question': 'How to avoid Value errors with array shapes in homework?', 'course': 'machine-learning-zoomcamp', 'doc_id': 'ee5f5b300844d59accea03846067497c'}, {'text': 'You would first get the average of the column and save it to a variable, then replace the NaN values with the average variable.\\nThis method is called imputing - when you have NaN/ null values in a column, but you do not want to get rid of the row because it has valuable information contributing to other columns.\\nAdded by Anneysha Sarkar', 'section': '1. Introduction to Machine Learning', 'question': 'Question 5: How and why do we replace the NaN values with average of the column?', 'course': 'machine-learning-zoomcamp', 'doc_id': 'eb2c416dbe00003af019404345cba699'}, {'text': 'In Question 7 we are asked to calculate\\nThe initial problem  can be solved by this, where a Matrix X is multiplied by some unknown weights w resulting in the target y.\\nAdditional reading and videos:\\nOrdinary least squares\\nMultiple Linear Regression in Matrix Form\\nPseudoinverse Solution to OLS\\nAdded by Sylvia Schmitt\\nwith commends from Dmytro Durach', 'section': '1. Introduction to Machine Learning', 'question': 'Question 7: Mathematical formula for linear regression', 'course': 'machine-learning-zoomcamp', 'doc_id': 'cc9536ffdda9c796b1006b1a0ee999ab'}, {'text': 'This is most likely that you interchanged the first step of the multiplication\\nYou used  instead of\\nAdded by Emmanuel Ikpesu', 'section': '1. Introduction to Machine Learning', 'question': 'Question 7: FINAL MULTIPLICATION not having 5 column', 'course': 'machine-learning-zoomcamp', 'doc_id': 'a345f42c06c620f19902c0fc71ff4963'}, {'text': 'Note, that matrix multiplication (matrix-matrix, matrix-vector multiplication) can be written as * operator in some sources, but performed as @ operator or np.matmul() via numpy. * operator performs element-wise multiplication (Hadamard product).\\nnumpy.dot() or ndarray.dot() can be used, but for matrix-matrix multiplication @ or np.matmul() is preferred (as per numpy doc).\\nIf multiplying by a scalar numpy.multiply() or * is preferred.\\nAdded by Andrii Larkin', 'section': '1. Introduction to Machine Learning', 'question': 'Question 7: Multiplication operators.', 'course': 'machine-learning-zoomcamp', 'doc_id': 'b9648c6b49f00c7a769f79d7aaa538e8'}, {'text': 'If you face an error kind of ImportError: cannot import name \\'contextfilter\\' from \\'jinja2\\' (anaconda\\\\lib\\\\site-packages\\\\jinja2\\\\__init__.py) when launching a new notebook for a brand new environment.\\nSwitch to the main environment and run \"pip install nbconvert --upgrade\".\\nAdded by George Chizhmak', 'section': '1. Introduction to Machine Learning', 'question': 'Error launching Jupyter notebook', 'course': 'machine-learning-zoomcamp', 'doc_id': '2446b53cc24d3d7e92ccafa05271fbf6'}, {'text': 'If you face this situation and see IPv6 addresses in the terminal, go to your System Settings > Network > your network connection > Details > Configure IPv6 > set to Manually > OK. Then try again', 'section': '1. Introduction to Machine Learning', 'question': 'wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv hangs on MacOS Ventura M1', 'course': 'machine-learning-zoomcamp', 'doc_id': '692b0a2bdf52dc30d67a7eff9d208cf4'}, {'text': \"Wget doesn't ship with macOS, so there are other alternatives to use.\\nNo worries, we got curl:\\nexample:\\ncurl -o ./housing.csv https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nExplanations:\\ncurl: a utility for retrieving information from the internet.\\n-o: Tell it to store the result as a file.\\nfilename: You choose the file's name.\\nLinks: Put the web address (URL) here, and cURL will extract data from it and save it under the name you provide.\\nMore about it at:\\nCurl Documentation\\nAdded by David Espejo\", 'section': '1. Introduction to Machine Learning', 'question': 'In case you are using mac os and having trouble with WGET', 'course': 'machine-learning-zoomcamp', 'doc_id': '929c10279f11813bf53bb3f6c85679f0'}, {'text': \"You can use round() function or f-strings\\nround(number, 4)  - this will round number up to 4 decimal places\\nprint(f'Average mark for the Homework is {avg:.3f}') - using F string\\nAlso there is pandas.Series. round idf you need to round values in the whole Series\\nPlease check the documentation\\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.round.html#pandas.Series.round\\nAdded by Olga Rudakova\", 'section': '2. Machine Learning for Regression', 'question': 'How to output only a certain number of decimal places', 'course': 'machine-learning-zoomcamp', 'doc_id': 'e9fcd0bc674ddc8246bb38addb7cd3d4'}, {'text': 'Here are the crucial links for this Week 2 that starts September 18, 2023\\nAsk questions for Live Sessions: https://app.sli.do/event/vsUpjYsayZ8A875Hq8dpUa/live/questions\\nCalendar for weekly meetings: https://calendar.google.com/calendar/u/0/r?cid=cGtjZ2tkbGc1OG9yb2lxa2Vwc2g4YXMzMmNAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ&pli=1\\nWeek 2 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/02-regression/homework.md\\nSubmit HW Week 2: https://docs.google.com/forms/d/e/1FAIpQLSf8eMtnErPFqzzFsEdLap_GZ2sMih-H-Y7F_IuPGqt4fOmOJw/viewform (also available at the bottom of the above link)\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYoutube Link: 2.X --- https://www.youtube.com/watch?v=vM3SqPNlStE&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=12\\nFAQs: https://docs.google.com/document/d/1LpPanc33QJJ6BSsyxVg-pWNMplal84TdZtq10naIhD8/edit#heading=h.lpz96zg7l47j\\n~~Nukta Bhatia~~', 'section': '2. Machine Learning for Regression', 'question': 'How do I get started with Week 2?', 'course': 'machine-learning-zoomcamp', 'doc_id': '265b8599867fb8f9dd036cdc4434d98b'}, {'text': 'We can use histogram:\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n# Load the data\\nurl = \\'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\'\\ndf = pd.read_csv(url)\\n# EDA\\nsns.histplot(df[\\'median_house_value\\'], kde=False)\\nplt.show()\\nOR ceck skewness and describe:\\nprint(df[\\'median_house_value\\'].describe())\\n# Calculate the skewness of the \\'median_house_value\\' variable\\nskewness = df[\\'median_house_value\\'].skew()\\n# Print the skewness value\\nprint(\"Skewness of \\'median_house_value\\':\", skewness)\\n(Mohammad Emad Sharifi)', 'section': '2. Machine Learning for Regression', 'question': 'Checking long tail of data', 'course': 'machine-learning-zoomcamp', 'doc_id': '56a61daa23e66bd43ea433db7f20a267'}, {'text': 'It’s possible that when you follow the videos, you’ll get a Singular Matrix error. We will explain why it happens in the Regularization video. Don’t worry, it’s normal that you have it.\\nYou can also have an error because you did the inverse of X once in your code and you’re doing it a second time.\\n(Added by Cécile Guillot)', 'section': '2. Machine Learning for Regression', 'question': 'LinAlgError: Singular matrix', 'course': 'machine-learning-zoomcamp', 'doc_id': 'efed81d393751fe56ecc21d576687061'}, {'text': 'You can find a detailed description of the dataset ere https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html\\nKS', 'section': '2. Machine Learning for Regression', 'question': 'California housing dataset', 'course': 'machine-learning-zoomcamp', 'doc_id': '4c1c901393fce63334991fad68ef1137'}, {'text': 'I was using for loops to apply rmse to list of y_val and y_pred. But the resulting rmse is all nan.\\nI found out that the problem was when my data reached the mean step after squaring the error in the rmse function. Turned out there were nan in the array, then I traced the problem back to where I first started to split the data: I had only use fillna(0) on the train data, not on the validation and test data. So the problem was fixed after I applied fillna(0) to all the dataset (train, val, test). Voila, my for loops to get rmse from all the seed values work now.\\nAdded by Sasmito Yudha Husada', 'section': '2. Machine Learning for Regression', 'question': 'Getting NaNs after applying .mean()', 'course': 'machine-learning-zoomcamp', 'doc_id': 'a4979b0f48774fd78cd0d8221c05a5d2'}, {'text': 'Why should we transform the target variable to logarithm distribution? Do we do this for all machine learning projects?\\nOnly if you see that your target is highly skewed. The easiest way to evaluate this is by plotting the distribution of the target variable.\\nThis can help to understand skewness and how it can be applied to the distribution of your data set.\\nhttps://en.wikipedia.org/wiki/Skewness\\nPastor Soto', 'section': '2. Machine Learning for Regression', 'question': 'Target variable transformation', 'course': 'machine-learning-zoomcamp', 'doc_id': '5f8b7ba5c6b3584bcdffb7a5a31049b6'}, {'text': 'The dataset can be read directly to pandas dataframe from the github link using the technique shown below\\ndfh=pd.read_csv(\"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\")\\nKrishna Anand', 'section': '2. Machine Learning for Regression', 'question': 'Reading the dataset directly from github', 'course': 'machine-learning-zoomcamp', 'doc_id': '53164980b53ff5dd5dea98999ce8eaed'}, {'text': \"For users of kaggle notebooks, the dataset can be loaded through widget using the below command. Please remember that ! before wget is essential\\n!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nOnce the dataset is loaded to the kaggle notebook server, it can be read through the below pandas command\\ndf = pd.read_csv('housing.csv')\\nHarish Balasundaram\", 'section': '2. Machine Learning for Regression', 'question': 'Loading the dataset directly through Kaggle Notebooks', 'course': 'machine-learning-zoomcamp', 'doc_id': '95a2c0cc815c7d87f01d7833d7e72939'}, {'text': 'We can filter a dataset by using its values as below.\\ndf = df[(df[\"ocean_proximity\"] == \"<1H OCEAN\") | (df[\"ocean_proximity\"] == \"INLAND\")]\\nYou can use | for ‘OR’, and & for ‘AND’\\nAlternative:\\ndf = df[df[\\'ocean_proximity\\'].isin([\\'<1H OCEAN\\', \\'INLAND\\'])]\\nRadikal Lukafiardi', 'section': '2. Machine Learning for Regression', 'question': 'Filter a dataset by using its values', 'course': 'machine-learning-zoomcamp', 'doc_id': 'feeebcbcdbfec16fe15cc4b3e0264f8d'}, {'text': 'Above users showed how to load the dataset directly from github. Here is another useful way of doing this using the `requests` library:\\n# Get data for homework\\nimport requests\\nurl = \\'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\'\\nresponse = requests.get(url)\\nif response.status_code == 200:\\nwith open(\\'housing.csv\\', \\'wb\\') as file:\\nfile.write(response.content)\\nelse:\\nprint(\"Download failed.\")\\nTyler Simpson', 'section': '2. Machine Learning for Regression', 'question': 'Alternative way to load the data using requests', 'course': 'machine-learning-zoomcamp', 'doc_id': 'c8e84ce581b5f06a2f58c810960f92bb'}, {'text': 'When creating a duplicate of your dataframe by doing the following:\\nX_train = df_train\\nX_val = df_val\\nYou’re still referencing the original variable, this is called a shallow copy. You can make sure that no references are attaching both variables and still keep the copy of the data do the following to create a deep copy:\\nX_train = df_train.copy()\\nX_val = df_val.copy()\\nAdded by Ixchel García', 'section': '2. Machine Learning for Regression', 'question': 'Null column is appearing even if I applied .fillna()', 'course': 'machine-learning-zoomcamp', 'doc_id': '79b4c7ec45962be67144b3ca01f04b91'}, {'text': 'Yes, you can. Here we implement it ourselves to better understand how it works, but later we will only rely on Scikit-Learn’s functions. If you want to start using it earlier — feel free to do it', 'section': '2. Machine Learning for Regression', 'question': 'Can I use Scikit-Learn’s train_test_split for this week?', 'course': 'machine-learning-zoomcamp', 'doc_id': '33ca0c080d9a2ac5db09e1c8f84de6fc'}, {'text': 'Yes, you can. We will also do that next week, so don’t worry, you will learn how to do it.', 'section': '2. Machine Learning for Regression', 'question': 'Can I use LinearRegression from Scikit-Learn for this week?', 'course': 'machine-learning-zoomcamp', 'doc_id': '380d799d4c6d38443e1da1d87c4ac79b'}, {'text': 'What are equivalents in Scikit-Learn for the linear regression with and without regularization used in week 2.\\nCorresponding function for model without regularization:\\nsklearn.linear_model.LinearRegression\\nCorresponding function for model with regularization:\\nsklearn.linear_model.Ridge\\nThe linear model from Scikit-Learn are explained  here:\\nhttps://scikit-learn.org/stable/modules/linear_model.html\\nAdded by Sylvia Schmitt', 'section': '2. Machine Learning for Regression', 'question': 'Corresponding Scikit-Learn functions for Linear Regression (with and without Regularization)', 'course': 'machine-learning-zoomcamp', 'doc_id': '453ba8cec7ef7438223ac37e390b3eb1'}, {'text': '`r` is a regularization parameter.\\nIt’s similar to `alpha` in sklearn.Ridge(), as both control the \"strength\" of regularization (increasing both will lead to stronger regularization), but mathematically not quite, here\\'s how both are used:\\nsklearn.Ridge()\\n||y - Xw||^2_2 + alpha * ||w||^2_2\\nlesson’s notebook (`train_linear_regression_reg` function)\\nXTX = XTX + r * np.eye(XTX.shape[0])\\n`r` adds “noise” to the main diagonal to prevent multicollinearity, which “breaks” finding inverse matrix.', 'section': '2. Machine Learning for Regression', 'question': 'Question 4: what is `r`, is it the same as `alpha` in sklearn.Ridge()?', 'course': 'machine-learning-zoomcamp', 'doc_id': '2040f7ffb048200bcb1ba6a60474f44b'}, {'text': 'Q: “In lesson 2.8 why is y_pred different from y? After all, we trained X_train to get the weights that when multiplied by X_train should give exactly y, or?”\\nA: linear regression is a pretty simple model, it neither can nor should fit 100% (nor any other model, as this would be the sign of overfitting). This picture might illustrate some intuition behind this, imagine X is a single feature:\\nAs our model is linear, how would you draw a line to fit all the \"dots\"?\\nYou could \"fit\" all the \"dots\" on this pic using something like scipy.optimize.curve_fit (non-linear least squares) if you wanted to, but imagine how it would perform on previously unseen data.\\nAdded by Andrii Larkin', 'section': '2. Machine Learning for Regression', 'question': 'Why linear regression doesn’t provide a “perfect” fit?', 'course': 'machine-learning-zoomcamp', 'doc_id': '67d417889e9a9b84847635441332ab5c'}, {'text': 'One of the questions on the homework calls for using a random seed of 42. When using 42, all my missing values ended up in my training dataframe and not my validation or test dataframes, why is that?\\nThe purpose of the seed value is to randomly generate the proportion split. Using a seed of 42 ensures that all learners are on the same page by getting the same behavior (in this case, all missing values ending up in the training dataframe). If using a different seed value (e.g. 9), missing values will then appear in all other dataframes.', 'section': '2. Machine Learning for Regression', 'question': 'Random seed 42', 'course': 'machine-learning-zoomcamp', 'doc_id': '948a552dc33d7582f13928d487389b01'}, {'text': 'It is possible to do the shuffling of the dataset with the pandas built-in function pandas.DataFrame.sample.The complete dataset can be shuffled including resetting the index with the following commands:\\nSetting frac=1 will result in returning a shuffled version of the complete Dataset.\\nSetting random_state=seed will result in the same randomization as used in the course resources.\\ndf_shuffled = df.sample(frac=1, random_state=seed)\\ndf_shuffled.reset_index(drop=True, inplace=True)\\nAdded by Sylvia Schmitt', 'section': '2. Machine Learning for Regression', 'question': 'Shuffling the initial dataset using pandas built-in function', 'course': 'machine-learning-zoomcamp', 'doc_id': '86dae10a3e601b27f767a0b6a511e3f3'}, {'text': 'That’s normal. We all have different environments: our computers have different versions of OS and different versions of libraries — even different versions of Python.\\nIf it’s the case, just select the option that’s closest to your answer', 'section': '2. Machine Learning for Regression', 'question': \"The answer I get for one of the homework questions doesn't match any of the options. What should I do?\", 'course': 'machine-learning-zoomcamp', 'doc_id': '3e589eef6e09d2dec9783ec476d0ed6c'}, {'text': \"In question 3 of HW02 it is mentioned: ‘For computing the mean, use the training only’. What does that mean?\\nIt means that you should use only the training data set for computing the mean, not validation or  test data set. This is how you can calculate the mean\\ndf_train['column_name'].mean( )\\nAnother option:\\ndf_train[‘column_name’].describe()\\n(Bhaskar Sarma)\", 'section': '2. Machine Learning for Regression', 'question': 'Meaning of mean in homework 2, question 3', 'course': 'machine-learning-zoomcamp', 'doc_id': 'f512202c894b1fb92a574f6cf4c15256'}, {'text': 'When the target variable has a long tail distribution, like in prices, with a wide range, you can transform the target variable with np.log1p() method, but be aware if your target variable has negative values, this method will not work', 'section': '2. Machine Learning for Regression', 'question': 'When should we transform the target variable to logarithm distribution?', 'course': 'machine-learning-zoomcamp', 'doc_id': '81127530e91aed3c306066aed52ad6fc'}, {'text': 'If we try to perform an arithmetic operation between 2 arrays of different shapes or different dimensions, it throws an error like operands could not be broadcast together with shapes. There are some scenarios when broadcasting can occur and when it fails.\\nIf this happens sometimes we can use * operator instead of dot() method to solve the issue. So that the error is solved and also we get the dot product.\\n(Santhosh Kumar)', 'section': '2. Machine Learning for Regression', 'question': 'ValueError: shapes not aligned', 'course': 'machine-learning-zoomcamp', 'doc_id': '74c27f64ac62387edea312c198cb0add'}, {'text': 'Copy of a dataframe is made with X_copy = X.copy().\\nThis is called creating a deep copy.  Otherwise it will keep changing the original dataframe if used like this: X_copy = X.\\nAny changes to X_copy will reflect back to X. This is not a real copy, instead it is a “view”.\\n(Memoona Tahira)', 'section': '2. Machine Learning for Regression', 'question': 'How to copy a dataframe without changing the original dataframe?', 'course': 'machine-learning-zoomcamp', 'doc_id': '161a73673dcf8fa7d602fdf881460749'}, {'text': 'One of the most important characteristics of the normal distribution is that mean=median=mode, this means that the most popular value, the mean of the distribution and 50% of the sample are under the same value, this is equivalent to say that the area under the curve (black) is the same on the left and on the right. The long tail (red curve) is the result of having a few observations with high values, now the behaviour of the distribution changes, first of all, the area is different on each side and now the mean, median and mode are different. As a consequence, the mean is no longer representative, the range is larger than before and the probability of being on the left or on the right is not the same.\\n(Tatiana Dávila)', 'section': '2. Machine Learning for Regression', 'question': 'What does ‘long tail’ mean?', 'course': 'machine-learning-zoomcamp', 'doc_id': '42bf88049d0ad5b9e7687f21adc4fe77'}, {'text': 'In statistics, the standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range. [Wikipedia] The formula to calculate standard deviation is:\\n(Aadarsha Shrestha)', 'section': '2. Machine Learning for Regression', 'question': 'What is standard deviation?', 'course': 'machine-learning-zoomcamp', 'doc_id': 'c9608149a7f792b457fbe11c1191ade2'}, {'text': 'The application of regularization depends on the specific situation and problem. It is recommended to consider it when training machine learning models, especially with small datasets or complex models, to prevent overfitting. However, its necessity varies depending on the data quality and size. Evaluate each case individually to determine if it is needed.\\n(Daniel Muñoz Viveros)', 'section': '2. Machine Learning for Regression', 'question': 'Do we need to apply regularization techniques always? Or only in certain scenarios?', 'course': 'machine-learning-zoomcamp', 'doc_id': '7df2789f666327ea20c50e0467ad7eaf'}, {'text': 'As it speeds up the development:\\nprepare_df(initial_df, seed, fill_na_type)  - that prepared all 3 dataframes and 3 y_vectors. Fillna() can be done before the initial_df is split.\\nOf course, you can reuse other functions: rmse() and train_linear_regression(X,y,r) from the class notebook\\n(Ivan Brigida)', 'section': '2. Machine Learning for Regression', 'question': 'Shortcut: define functions for faster execution', 'course': 'machine-learning-zoomcamp', 'doc_id': '73f53619ef43dd934b9cdda867198d34'}, {'text': 'If we have a list or series of data for example x = [1,2,3,4,5]. We can use pandas to find the standard deviation. We can pass our list into panda series and call standard deviation directly on the series pandas.Series(x).std().\\n(Quinn Avila)', 'section': '2. Machine Learning for Regression', 'question': 'How to use pandas to find standard deviation', 'course': 'machine-learning-zoomcamp', 'doc_id': '5a2eac9aa1ea41af88b0f5ab91caf495'}, {'text': 'Numpy and Pandas packages use different equations to compute the standard deviation. Numpy uses  population standard deviation, whereas pandas uses sample standard deviation by default.\\nNumpy\\nPandas\\npandas default standard deviation is computed using one degree of freedom. You can change degree in of freedom in NumPy to change this to unbiased estimator by using ddof parameter:\\nimport numpy as np\\nnp.std(df.weight, ddof=1)\\nThe result will be similar if we change the dof = 1 in numpy\\n(Harish Balasundaram)', 'section': '2. Machine Learning for Regression', 'question': 'Standard Deviation Differences in Numpy and Pandas', 'course': 'machine-learning-zoomcamp', 'doc_id': 'abddb96e0e1c6d92684c9e7939e27651'}, {'text': \"In pandas you can use built in Pandas function names std() to get standard deviation. For example\\ndf['column_name'].std() to get standard deviation of that column.\\ndf[['column_1', 'column_2']].std() to get standard deviation of multiple columns.\\n(Khurram Majeed)\", 'section': '2. Machine Learning for Regression', 'question': 'Standard deviation using Pandas built in Function', 'course': 'machine-learning-zoomcamp', 'doc_id': '50ca61b1500ebcda644cbacf51c56f43'}, {'text': 'Use ‘pandas.concat’ function (https://pandas.pydata.org/docs/reference/api/pandas.concat.html) to combine two dataframes. To combine two numpy arrays use numpy.concatenate (https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html) function. So the code would be as follows:\\ndf_train_combined = pd.concat([df_train, df_val])\\ny_train = np.concatenate((y_train, y_val), axis=0)\\n(George Chizhmak)', 'section': '2. Machine Learning for Regression', 'question': 'How to combine train and validation datasets', 'course': 'machine-learning-zoomcamp', 'doc_id': 'cc4e75564b7e5cc61cef326c061ba61c'}, {'text': 'The Root Mean Squared Error (RMSE) is one of the primary metrics to evaluate the performance of a regression model. It calculates the average deviation between the model\\'s predicted values and the actual observed values, offering insight into the model\\'s ability to accurately forecast the target variable. To calculate RMSE score:\\nLibraries needed\\nimport numpy as np\\nfrom sklearn.metrics import mean_squared_error\\nmse = mean_squared_error(actual_values, predicted_values)\\nrmse = np.sqrt(mse)\\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\\n(Aminat Abolade)', 'section': '2. Machine Learning for Regression', 'question': 'Understanding RMSE and how to calculate RMSE score', 'course': 'machine-learning-zoomcamp', 'doc_id': '037867dbb09b27c75e757671aef80011'}, {'text': 'If you would like to use multiple conditions as an example below you will get the error. The correct syntax for OR is |, and for AND is &\\n(Olga Rudakova)\\n–', 'section': '2. Machine Learning for Regression', 'question': 'What syntax use in Pandas for multiple conditions using logical AND and OR', 'course': 'machine-learning-zoomcamp', 'doc_id': 'c92e11ece3b6faea3ffc6db7531dd115'}, {'text': 'I found this video pretty usual for understanding how we got the normal form with linear regression Normal Equation Derivation for Regression', 'section': '2. Machine Learning for Regression', 'question': 'Deep dive into normal equation for regression', 'course': 'machine-learning-zoomcamp', 'doc_id': '115967ddfdf1aad7cf7779509422cea2'}, {'text': '(Hrithik Kumar Advani)', 'section': '2. Machine Learning for Regression', 'question': 'Useful Resource for Missing Data Treatment\\nhttps://www.kaggle.com/code/parulpandey/a-guide-to-handling-missing-values-in-python/notebook', 'course': 'machine-learning-zoomcamp', 'doc_id': '87912ccaf1b8446a3d14e78022f16df4'}, {'text': 'The instruction for applying log transformation to the ‘median_house_value’ variable is provided before Q3 in the homework for Week-2 under the ‘Prepare and split the dataset’ heading.\\nHowever, this instruction is absent in the subsequent questions of the homework, and I got stuck with Q5 for a long time, trying to figure out why my RMSE was so huge, when it clicked to me that I forgot to apply log transformation to the target variable. Please remember to apply log transformation to the target variable for each question.\\n(Added by Soham Mundhada)', 'section': '2. Machine Learning for Regression', 'question': 'Caution for applying log transformation in Week-2 2023 cohort homework', 'course': 'machine-learning-zoomcamp', 'doc_id': '0696a2ee44dd4f20d6372ab53797fbb2'}, {'text': 'Version 0.24.2 and Python 3.8.11\\n(Added by Diego Giraldo)', 'section': '3. Machine Learning for Classification', 'question': 'What sklearn version is Alexey using in the youtube videos?', 'course': 'machine-learning-zoomcamp', 'doc_id': '191c10a9df384a42ba4f9460c7875a15'}, {'text': 'Week 3 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/03-classification/homework.md\\nSubmit HW Week 3: https://docs.google.com/forms/d/e/1FAIpQLSeXS3pqsv_smRkYmVx-7g6KIZDnG29g2s7pdHo-ASKNqtfRFQ/viewform\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nEvaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYoutube Link: 3.X --- https://www.youtube.com/watch?v=0Zw04wdeTQo&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=29\\n~~Nukta Bhatia~~', 'section': '3. Machine Learning for Classification', 'question': 'How do I get started with Week 3?', 'course': 'machine-learning-zoomcamp', 'doc_id': '6e7eb498c10ac417dc21baa2a04dcbdc'}, {'text': \"The error message “could not convert string to float: ‘Nissan’” typically occurs when a machine learning model or function is expecting numerical input, but receives a string instead. In this case, it seems like the model is trying to convert the car brand ‘Nissan’ into a numerical value, which isn’t possible.\\nTo resolve this issue, you can encode categorical variables like car brands into numerical values. One common method is one-hot encoding, which creates new binary columns for each category/label present in the original column.\\nHere’s an example of how you can perform one-hot encoding using pandas:\\nimport pandas as pd\\n# Assuming 'data' is your DataFrame and 'brand' is the column with car brands\\ndata_encoded = pd.get_dummies(data, columns=['brand'])\\nIn this code, pd.get_dummies() creates a new DataFrame where the ‘brand’ column is replaced with binary columns for each brand (e.g., ‘brand_Nissan’, ‘brand_Toyota’, etc.). Each row in the DataFrame has a 1 in the column that corresponds to its brand and 0 in all other brand columns.\\n-Mohammad Emad Sharifi-\", 'section': '3. Machine Learning for Classification', 'question': \"Could not convert string to float:’Nissan’rt string to float: 'Nissan'\", 'course': 'machine-learning-zoomcamp', 'doc_id': 'bc3f720b1f424ace2d3348e198ee909f'}, {'text': 'Solution: Mutual Information score calculates the relationship between categorical variables or discrete variables. So in the homework, because the target which is median_house_value is continuous, we had to change it to binary format which in other words, makes its values discrete as either 0 or 1. If we allowed it to remain in the continuous variable format, the mutual information score could be calculated, but the algorithm would have to divide the continuous variables into bins and that would be highly subjective. That is why continuous variables are not used for mutual information score calculation.\\n—Odimegwu David—-', 'section': '3. Machine Learning for Classification', 'question': 'Why did we change the targets to binary format when calculating mutual information score in the homework?', 'course': 'machine-learning-zoomcamp', 'doc_id': '94b63fcda02773814ae66aa2b5d033aa'}, {'text': \"Q2 asks about correlation matrix and converting median_house_value from numeric to binary. Just to make sure here we are only dealing with df_train not df_train_full, right? As the question explicitly mentions the train dataset.\\nYes. I think it is only on df_train. The reason behind this is that df_train_full also contains the validation dataset, so at this stage we don't want to make conclusions based on the validation data, since we want to test how we did without using that portion of the data.\\nPastor Soto\", 'section': '3. Machine Learning for Classification', 'question': 'What data should we use for correlation matrix', 'course': 'machine-learning-zoomcamp', 'doc_id': 'fc186d32c9accb9ee86a339acecf7858'}, {'text': \"The background of any dataframe can be colored (not only the correlation matrix) based on the numerical values the dataframe contains by using the method pandas.io.formats.style.Styler.background_graident.\\nHere an example on how to color the correlation matrix. A color map of choice can get passed, here ‘viridis’ is used.\\n# ensure to have only numerical values in the dataframe before calling 'corr'\\ncorr_mat = df_numerical_only.corr()\\ncorr_mat.style.background_gradient(cmap='viridis')\\nHere is an example of how the coloring will look like using a dataframe containing random values and applying “background_gradient” to it.\\nnp.random.seed = 3\\ndf_random = pd.DataFrame(data=np.random.random(3*3).reshape(3,3))\\ndf_random.style.background_gradient(cmap='viridis')\\nAdded by Sylvia Schmitt\", 'section': '3. Machine Learning for Classification', 'question': 'Coloring the background of the pandas.DataFrame.corr correlation matrix directly', 'course': 'machine-learning-zoomcamp', 'doc_id': '4db7b9b7f392545dd2f1a9d9571ad54f'}, {'text': 'data_corr = pd.DataFrame(data_num.corr().round(3).abs().unstack().sort_values(ascending=False))\\ndata_corr.head(10)\\nAdded by Harish Balasundaram\\nYou can also use seaborn to create a heatmap with the correlation. The code for doing that:\\nsns.heatmap(df[numerical_features].corr(),\\nannot=True,\\nsquare=True,\\nfmt=\".2g\",\\ncmap=\"crest\")\\nAdded by Cecile Guillot\\nYou can refine your heatmap and plot only a triangle, with a blue to red color gradient, that will show every correlation between your numerical variables without redundant information with this function:\\nWhich outputs, in the case of churn dataset:\\n(Mélanie Fouesnard)', 'section': '3. Machine Learning for Classification', 'question': 'Identifying highly correlated feature pairs easily through unstack', 'course': 'machine-learning-zoomcamp', 'doc_id': '8f452775534dadd43f8c87a83aaff908'}, {'text': \"Should we perform EDA on the base of train or train+validation or train+validation+test dataset?\\nIt's indeed good practice to only rely on the train dataset for EDA. Including validation might be okay. But we aren't supposed to touch the test dataset, even just looking at it isn't a good idea. We indeed pretend that this is the future unseen data\\nAlena Kniazeva\", 'section': '3. Machine Learning for Classification', 'question': 'What data should be used for EDA?', 'course': 'machine-learning-zoomcamp', 'doc_id': '607da6002f3b38137c0e3e8655b2719b'}, {'text': 'Validation dataset helps to validate models and prediction on unseen data. This helps get an estimate on its performance on fresh data. It helps optimize the model.\\nEdidiong Esu\\nBelow is an extract of Alexey\\'s book explaining this point. Hope is useful\\nWhen we apply the fit method, this method is looking at the content of the df_train dictionaries we are passing to the DictVectorizer instance, and fit is figuring out (training) how to map the values of these dictionaries. If categorical, applies one-hot encoding, if numerical it will leave it as it is.\\nWith this context, if we apply the fit to the validation model, we are \"giving the answers\" and we are not letting the \"fit\" do its job for data that we haven\\'t seen. By not applying the fit to the validation model we can know how well it was trained.\\nBelow is an extract of Alexey\\'s book explaining this point.\\nHumberto Rodriguez\\nThere is no need to initialize another instance of dictvectorizer after fitting it on the train set as it will overwrite what it learnt from being fit on the train data.\\nThe correct way is to fit_transform the train set, and only transform the validation and test sets.\\nMemoona Tahira', 'section': '3. Machine Learning for Classification', 'question': 'Fitting DictVectorizer on validation', 'course': 'machine-learning-zoomcamp', 'doc_id': 'c88e1b73fe62e97d3d0c2c39ba95d7da'}, {'text': 'For Q5 in homework, should we calculate the smallest difference in accuracy in real values (i.e. -0.001 is less than -0.0002) or in absolute values (i.e. 0.0002 is less than 0.001)?\\nWe should select the “smallest” difference, and not the “lowest”, meaning we should reason in absolute values.\\nIf the difference is negative, it means that the model actually became better when we removed the feature.', 'section': '3. Machine Learning for Classification', 'question': 'Feature elimination', 'course': 'machine-learning-zoomcamp', 'doc_id': '56ac050051357b58949378ce66d75e7f'}, {'text': \"Instead use the method “.get_feature_names_out()” from DictVectorizer function and the warning will be resolved , but we need not worry about the waning as there won't be any warning\\nSanthosh Kumar\", 'section': '3. Machine Learning for Classification', 'question': 'FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2', 'course': 'machine-learning-zoomcamp', 'doc_id': '6a9df331310f6af680bc8107db1695d3'}, {'text': 'Fitting the logistic regression takes a long time / kernel crashes when calling predict() with the fitted model.\\nMake sure that the target variable for the logistic regression is binary.\\nKonrad Muehlberg', 'section': '3. Machine Learning for Classification', 'question': 'Logistic regression crashing Jupyter kernel', 'course': 'machine-learning-zoomcamp', 'doc_id': '6cbe8c488fcb873eedc584c23663e13f'}, {'text': 'Ridge regression is a linear regression technique used to mitigate the problem of multicollinearity (when independent variables are highly correlated) and prevent overfitting in predictive modeling. It adds a regularization term to the linear regression cost function, penalizing large coefficients.\\nsag Solver: The sag solver stands for \"Stochastic Average Gradient.\" It\\'s particularly suitable for large datasets, as it optimizes the regularization term using stochastic gradient descent (SGD). sag can be faster than some other solvers for large datasets.\\nAlpha: The alpha parameter  controls the strength of the regularization in Ridge regression. A higher alpha value leads to stronger regularization, which means the model will have smaller coefficient values, reducing the risk of overfitting.\\nfrom sklearn.linear_model import Ridge\\nridge = Ridge(alpha=alpha, solver=\\'sag\\', random_state=42)\\nridge.fit(X_train, y_train)\\nAminat Abolade', 'section': '3. Machine Learning for Classification', 'question': 'Understanding Ridge', 'course': 'machine-learning-zoomcamp', 'doc_id': '0e3605e80cb743093adf01f06f0ff376'}, {'text': 'DictVectorizer(sparse=True) produces CSR format, which is both more memory efficient and converges better during fit(). Basically it stores non-zero values and indices instead of adding a column for each class of each feature (models of cars produced 900+ columns alone in the current task).\\nUsing “sparse” format like on the picture above, both via pandas.get_dummies() and DictVectorizer(sparse=False) - is slower (around 6-8min for Q6 task - Linear/Ridge Regression) for high amount of classes (like models of cars for eg) and gives a bit “worse” results in both Logistic and Linear/Ridge Regression, while also producing convergence warnings for Linear/Ridge Regression.\\nLarkin Andrii', 'section': '3. Machine Learning for Classification', 'question': 'pandas.get_dummies() and DictVectorizer(sparse=False) produce the same type of one-hot encodings:', 'course': 'machine-learning-zoomcamp', 'doc_id': 'b50cd609a1ec135bbab2a1b1e072fd76'}, {'text': 'Ridge with sag solver requires feature to be of the same scale. You may get the following warning: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\\nPlay with different scalers. See notebook-scaling-ohe.ipynb\\nDmytro Durach\\n(Oscar Garcia)  Use a StandardScaler for the numeric fields and OneHotEncoder (sparce = False) for the categorical features.  This help with the warning. Separate the features (num/cat) without using the encoder first and see if that helps.', 'section': '3. Machine Learning for Classification', 'question': 'Convergence Problems in W3Q6', 'course': 'machine-learning-zoomcamp', 'doc_id': '193819cbd2779cb7a4abbc89d99e8737'}, {'text': \"When encountering convergence errors during the training of a Ridge regression model, consider the following steps:\\nFeature Normalization: Normalize your numerical features using techniques like MinMaxScaler or StandardScaler. This ensures that numerical features are on a \\tsimilar scale, preventing convergence issues.\\nCategorical Feature Encoding: If your dataset includes categorical features, apply \\tcategorical encoding techniques such as OneHotEncoder (OHE) to \\tconvert them into a numerical format. OHE is commonly used to represent categorical variables as binary vectors, making them compatible with regression models like Ridge.\\nCombine Features: After \\tnormalizing numerical features and encoding categorical features using OneHotEncoder, combine them to form a single feature matrix (X_train). This combined dataset serves as the input for training the Ridge regression model.\\nBy following these steps, you can address convergence errors and enhance the stability of your Ridge model training process. It's important to note that the choice of encoding method, such as OneHotEncoder, is appropriate for handling categorical features in this context.\\nYou can find an example here.\\n \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tOsman Ali\", 'section': '3. Machine Learning for Classification', 'question': 'Dealing with Convergence in Week 3 q6', 'course': 'machine-learning-zoomcamp', 'doc_id': 'bf11b5026fd5cfe9cb5cc0a446de77a3'}, {'text': 'A sparse matrix is more memory-efficient because it only stores the non-zero values and their positions in memory. This is particularly useful when working with large datasets with many zero or missing values.\\nThe default DictVectorizer configuration is a sparse matrix. For week3 Q6 using the default sparse is an interesting option because of the size of the matrix. Training the model was also more performant and didn’t give an error message like dense mode.\\n \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tQuinn Avila', 'section': '3. Machine Learning for Classification', 'question': 'Sparse matrix compared dense matrix', 'course': 'machine-learning-zoomcamp', 'doc_id': 'f04e49953daf8c152f8da7426c2e3888'}, {'text': 'The warnings on the jupyter notebooks can be disabled/ avoided with the following comments:\\nImport warnings\\nwarnings.filterwarnings(“ignore”)\\nKrishna Anand', 'section': '3. Machine Learning for Classification', 'question': 'How  to Disable/avoid Warnings in Jupyter Notebooks', 'course': 'machine-learning-zoomcamp', 'doc_id': '9285d6ee0481e63bba89a2c057d4a918'}, {'text': 'Question: Regarding RMSE, how do we decide on the correct score to choose? In the study group discussion    about week two homework, all of us got it wrong and one person had the lowest score selected as well.\\nAnswer: You need to find RMSE for each alpha. If RMSE scores  are equal, you will select the lowest alpha.\\nAsia Saeed', 'section': '3. Machine Learning for Classification', 'question': 'How to select the alpha parameter in Q6', 'course': 'machine-learning-zoomcamp', 'doc_id': 'ff9825d481a61b29aeb2807d816d71dd'}, {'text': 'Question: Could you please help me with HW3 Q3: \"Calculate the mutual information score with the (binarized) price for the categorical variable that we have. Use the training set only.\" What is the second variable that we need to use to calculate the mutual information score?\\nAnswer: You need to calculate the mutual info score between the binarized price (above_average) variable & ocean_proximity, the only original categorical variable in the dataset.\\nAsia Saeed', 'section': '3. Machine Learning for Classification', 'question': 'Second variable that we need to use to calculate the mutual information score', 'course': 'machine-learning-zoomcamp', 'doc_id': 'b0f581d2368ce6df550addf64f3d61e5'}, {'text': 'Do we need to train the model only with the features: total_rooms, total_bedrooms, population and households? or with all the available features and then pop once at a time each of the previous features and train the model to make the accuracy comparison?\\nYou need to create a list of all features in this question and evaluate the model one time to obtain the accuracy, this will be the original accuracy, and then remove one feature each time, and in each time, train the model, find the accuracy and the difference between the original accuracy and the found accuracy. Finally, find out which feature has the smallest absolute accuracy difference.\\nWhile calculating differences between accuracy scores while training on the whole model, versus dropping one feature at a time and comparing its accuracy to the model to judge impact of the feature on the accuracy of the model, do we take the smallest difference or smallest absolute difference?\\nSince order of subtraction between the two accuracy scores can result in a negative number, we will take its absolute value as we are interested in the smallest value difference, not the lowest difference value. Case in point, if difference is -4 and -2, the smallest difference is abs(-2), and not abs(-4)', 'section': '3. Machine Learning for Classification', 'question': 'Features for homework Q5', 'course': 'machine-learning-zoomcamp', 'doc_id': '2005f14d3667d528a34e7c5244e59a43'}, {'text': 'Both work in similar ways. That is, to convert categorical features to numerical variables for use in training the model. But the difference lies in the input. OneHotEncoder uses an array as input while DictVectorizer uses a dictionary.\\nBoth will produce the same result. But when we use OneHotEncoder, features are sorted alphabetically. When you use DictVectorizer you stack features that you want.\\nTanya Mard', 'section': '3. Machine Learning for Classification', 'question': 'What is the difference between OneHotEncoder and DictVectorizer?', 'course': 'machine-learning-zoomcamp', 'doc_id': '2ae0bc90c250ed885d419501d47db31e'}, {'text': 'They are basically the same. There are some key differences with regards to their input/output types, handling of missing values, etc, but they are both techniques to one-hot-encode categorical variables with identical results. The biggest difference is get_dummies are a convenient choice when you are working with Pandas Dataframes, while if you are building a scikit-learn-based machine learning pipeline and need to handle categorical data as part of that pipeline, OneHotEncoder is a more suitable choice. [Abhirup Ghosh]', 'section': '3. Machine Learning for Classification', 'question': 'What is the difference between pandas get_dummies and sklearn OnehotEncoder?', 'course': 'machine-learning-zoomcamp', 'doc_id': 'ae6b731ddb6e79e7cee85eff22fe82ee'}, {'text': \"For the test_train_split question on week 3's homework, are we supposed to use 42 as the random_state in both splits or only the 1st one?\\nAnswer: for both splits random_state = 42 should be used\\n(Bhaskar Sarma)\", 'section': '3. Machine Learning for Classification', 'question': 'Use of random seed in HW3', 'course': 'machine-learning-zoomcamp', 'doc_id': '8cbf71d4c5dadeda1075f7be24d32b5c'}, {'text': 'Should correlation be calculated after splitting or before splitting. And lastly I know how to find the correlation but how do i find the two most correlated features.\\nAnswer: Correlation matrix of your train dataset. Thus, after splitting. Two most correlated features are the ones having the highest correlation coefficient in terms of absolute values.', 'section': '3. Machine Learning for Classification', 'question': 'Correlation before or after splitting the data', 'course': 'machine-learning-zoomcamp', 'doc_id': 'ce0b1f229531da0792373107e088f5ca'}, {'text': 'Make sure that the features used in ridge regression model are only NUMERICAL ones not categorical.\\nDrop all categorical features first before proceeding.\\n(Aileah Gotladera)\\nWhile it is True that ridge regression accepts only numerical values, the categorical ones can be useful for your model. You have to transform them using one-hot encoding before training the model. To avoid the error of non convergence, put sparse=True when doing so.\\n(Erjon)', 'section': '3. Machine Learning for Classification', 'question': 'Features in Ridge Regression Model', 'course': 'machine-learning-zoomcamp', 'doc_id': 'ef6cd7f33d82c3534be809f0c54bf1cf'}, {'text': \"You need to use all features. and price for target. Don't include the average variable we created before.\\nIf you use DictVectorizer then make sure to use sparce=True to avoid convergence errors\\nI also used StandardScalar for numerical variable you can try running with or without this\\n(Peter Pan)\", 'section': '3. Machine Learning for Classification', 'question': 'Handling Column Information for Homework 3 Question 6', 'course': 'machine-learning-zoomcamp', 'doc_id': '4886d226be093ed37b95435b451bddd6'}, {'text': 'Use sklearn.preprocessing encoders and scalers, e.g. OneHotEncoder, OrdinalEncoder, and StandardScaler.', 'section': '3. Machine Learning for Classification', 'question': 'Transforming Non-Numerical Columns into Numerical Columns', 'course': 'machine-learning-zoomcamp', 'doc_id': '185d8a6316cca23bbbd979ffe67e0b72'}, {'text': 'These both methods receive the dictionary as an input. While the DictVectorizer will store the big vocabulary and takes more memory. FeatureHasher create a vectors with predefined length. They are both used for categorical features.\\nWhen you have a high cardinality for categorical features better to use FeatureHasher. If you want to preserve feature names in transformed data and have a small number of unique values is DictVectorizer. But your choice will dependence on your data.\\nYou can read more by follow the link https://scikit-learn.org/stable/auto_examples/text/plot_hashing_vs_dict_vectorizer.html\\nOlga Rudakova', 'section': '3. Machine Learning for Classification', 'question': 'What is the better option FeatureHasher or DictVectorizer', 'course': 'machine-learning-zoomcamp', 'doc_id': '264a4e8ccf8964aea1605f13d376896f'}, {'text': '(Question by Connie S.)\\nThe reason it\\'s good/recommended practice to do it after splitting is to avoid data leakage - you don\\'t want any data from the test set influencing the training stage (similarly from the validation stage in the initial training). See e.g. scikit-learn documentation on \"Common pitfalls and recommended practices\": https://scikit-learn.org/stable/common_pitfalls.html\\nAnswered/added by Rileen Sinha', 'section': '3. Machine Learning for Classification', 'question': \"Isn't it easier to use DictVertorizer or get dummies before splitting the data into train/val/test? Is there a reason we wouldn't do this? Or is it the same either way?\", 'course': 'machine-learning-zoomcamp', 'doc_id': '99371da31d2fe95526ed39b6f6fff0e2'}, {'text': 'If you are getting 1.0 as accuracy then there is a possibility you have overfitted the model. Dropping the column msrp/price can help you solve this issue.\\nAdded by Akshar Goyal', 'section': '3. Machine Learning for Classification', 'question': 'HW3Q4 I am getting 1.0 as accuracy. Should I use the closest option?', 'course': 'machine-learning-zoomcamp', 'doc_id': '5defbfd0459fcbdd00b737193ac4c11e'}, {'text': 'We can use sklearn & numpy packages to calculate Root Mean Squared Error\\nfrom sklearn.metrics import mean_squared_error\\nimport numpy as np\\nRmse = np.sqrt(mean_squared_error(y_pred, y_val/ytest)\\nAdded by Radikal Lukafiardi\\nYou can also refer to Alexey’s notebook for Week 2:\\nhttps://github.com/alexeygrigorev/mlbookcamp-code/blob/master/chapter-02-car-price/02-carprice.ipynb\\nwhich includes the following code:\\ndef rmse(y, y_pred):\\nerror = y_pred - y\\nmse = (error ** 2).mean()\\nreturn np.sqrt(mse)\\n(added by Rileen Sinha)', 'section': '3. Machine Learning for Classification', 'question': 'How to calculate Root Mean Squared Error?', 'course': 'machine-learning-zoomcamp', 'doc_id': 'efa750046cb16c49c9e31d81fcbb21fe'}, {'text': 'The solution is to use “get_feature_names_out” instead. See details: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html\\nGeorge Chizhmak', 'section': '3. Machine Learning for Classification', 'question': \"AttributeError: 'DictVectorizer' object has no attribute 'get_feature_names'\", 'course': 'machine-learning-zoomcamp', 'doc_id': '84dda2be2026ba7f0d0305d100572c2c'}, {'text': 'To use RMSE without math or numpy, ‘sklearn.metrics’ has a mean_squared_error function with a squared kwarg (defaults to True). Setting squared to False will return the RMSE.\\nfrom sklearn.metrics import mean_squared_error\\nrms = mean_squared_error(y_actual, y_predicted, squared=False)\\nSee details: https://stackoverflow.com/questions/17197492/is-there-a-library-function-for-root-mean-square-error-rmse-in-python\\nAhmed Okka', 'section': '3. Machine Learning for Classification', 'question': 'Root Mean Squared Error', 'course': 'machine-learning-zoomcamp', 'doc_id': '565ea1d0fe1d49918f7ce7b27540426f'}, {'text': 'This article explains different encoding techniques used https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02\\nHrithik Kumar Advani', 'section': '3. Machine Learning for Classification', 'question': 'Encoding Techniques', 'course': 'machine-learning-zoomcamp', 'doc_id': '8843a79b2c1a41adc0ee6e7d24c4cecb'}, {'text': \"I got this error multiple times here is the code:\\n“accuracy_score(y_val, y_pred >= 0.5)”\\nTypeError: 'numpy.float64' object is not callable\\nI solve it using\\nfrom sklearn import metrics\\nmetrics.accuracy_score(y_train, y_pred>= 0.5)\\nOMAR Wael\", 'section': '4. Evaluation Metrics for Classification', 'question': 'Error in use of accuracy_score from sklearn in jupyter (sometimes)', 'course': 'machine-learning-zoomcamp', 'doc_id': 'd108a7fdd1cd21bdff85da49d1efc5e6'}, {'text': 'Week 4 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/04-evaluation/homework.md\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nEvaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYouTube Link: 4.X --- https://www.youtube.com/watch?v=gmg5jw1bM8A&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=40\\nSci-Kit Learn on Evaluation:\\nhttps://scikit-learn.org/stable/model_selection.html\\n~~Nukta Bhatia~~', 'section': '4. Evaluation Metrics for Classification', 'question': 'How do I get started with Week 4?', 'course': 'machine-learning-zoomcamp', 'doc_id': '38f82e48a25a51073b147593b36161cc'}, {'text': 'https://datatalks-club.slack.com/archives/C0288NJ5XSA/p1696475675887119\\nMetrics can be used on a series or a dataframe\\n~~Ella Sahnan~~', 'section': '4. Evaluation Metrics for Classification', 'question': 'Using a variable to score', 'course': 'machine-learning-zoomcamp', 'doc_id': '8bb1eefd0b17fc14b53c8c3751e321ed'}, {'text': 'Ie particularly in module-04 homework Qn2 vs Qn5. https://datatalks-club.slack.com/archives/C0288NJ5XSA/p1696760905214979\\nRefer to the sklearn docs, random_state is to ensure the “randomness” that is used to shuffle dataset is reproducible, and it usually requires both random_state and shuffle params to be set accordingly.\\n~~Ella Sahnan~~', 'section': '4. Evaluation Metrics for Classification', 'question': 'Why do we sometimes use random_state and not at other times?', 'course': 'machine-learning-zoomcamp', 'doc_id': '6c474e0fac88190e328bdc1fbd9ee57e'}, {'text': 'How to get classification metrics - precision, recall, f1 score, accuracy simultaneously\\nUse classification_report from sklearn. For more info check here.\\nAbhishek N', 'section': '4. Evaluation Metrics for Classification', 'question': 'How to get all classification metrics?', 'course': 'machine-learning-zoomcamp', 'doc_id': 'deac6ed567c687ca8461c779db9f0a5e'}, {'text': 'I am getting multiple thresholds with the same F1 score, does this indicate I am doing something wrong or is there a method for choosing? I would assume just pick the lowest?\\nChoose the one closest to any of the options\\nAdded by Azeez Enitan Edunwale\\nYou can always use scikit-learn (or other standard libraries/packages) to verify results obtained using your own code, e.g. you can use  “classification_report” (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) to obtain precision, recall and F1-score.\\nAdded by Rileen Sinha', 'section': '4. Evaluation Metrics for Classification', 'question': 'Multiple thresholds for Q4', 'course': 'machine-learning-zoomcamp', 'doc_id': '9ecfe1ad05495eeff16ec98e5ec6842e'}, {'text': \"Solution description: duplicating the\\ndf.churn = (df.churn == 'yes').astype(int)\\nThis is causing you to have only 0's in your churn column. In fact, match with the error you are getting:  ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0.\\nIt is telling us that it only contains 0's.\\nDelete one of the below cells and you will get the accuracy\\nHumberto Rodriguez\", 'section': '4. Evaluation Metrics for Classification', 'question': 'ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0', 'course': 'machine-learning-zoomcamp', 'doc_id': 'cc6b86ba6b9ecc380c2dffef8f95b673'}, {'text': 'Use Yellowbrick. Yellowbrick in a library that combines scikit-learn with matplotlib to produce visualizations for your models. It produces colorful classification reports.\\nKrishna Annad', 'section': '4. Evaluation Metrics for Classification', 'question': 'Method to get beautiful classification report', 'course': 'machine-learning-zoomcamp', 'doc_id': 'fd7b6f4378621548db034a2d5e1ff20b'}, {'text': 'That’s fine, use the closest option', 'section': '4. Evaluation Metrics for Classification', 'question': 'I’m not getting the exact result in homework', 'course': 'machine-learning-zoomcamp', 'doc_id': '1aac108535145469ec3aad9b89f12c12'}, {'text': 'Check the solutions from the 2021 iteration of the course. You should use roc_auc_score.', 'section': '4. Evaluation Metrics for Classification', 'question': 'Use AUC to evaluate feature importance of numerical variables', 'course': 'machine-learning-zoomcamp', 'doc_id': 'a6e27afffa1708122b15a86c9fc6a94a'}, {'text': 'When calculating the ROC AUC score using sklearn.metrics.roc_auc_score the function expects two parameters “y_true” and “y_score”. So for each numerical value in the dataframe it will be passed as the “y_score” to the function and the target variable will get passed a “y_true” each time.\\nSylvia Schmitt', 'section': '4. Evaluation Metrics for Classification', 'question': 'Help with understanding: “For each numerical value, use it as score and compute AUC”', 'course': 'machine-learning-zoomcamp', 'doc_id': '11656496eed7b6a0230a95ce86b8864e'}, {'text': 'You must use the `dt_val` dataset to compute the metrics asked in Question 3 and onwards, as you did in Question 2.\\nDiego Giraldo', 'section': '4. Evaluation Metrics for Classification', 'question': 'What dataset should I use to compute the metrics in Question 3', 'course': 'machine-learning-zoomcamp', 'doc_id': '5bbf3a9ac33f92e9acbf64fbabab5ca2'}, {'text': \"What does this line do?\\nKFold(n_splits=n_splits, shuffle=True, random_state=1)\\nIf I do it inside the loop [0.01, 0.1, 1, 10] or outside the loop in Q6, HW04 it doesn't make any difference to my answers. I am wondering why and what is the right way, although it doesn't make a difference!\\nDid you try using a different random_state? From my understanding, KFold just makes N (which is equal to n_splits) separate pairs of datasets (train+val).\\nhttps://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\\nIn my case changing random state changed results\\n(Arthur Minakhmetov)\\nChanging the random state makes a difference in my case too, but not whether it is inside or outside the for loop. I think I have got the answer. kFold = KFold(n_splits=n_splits, shuffle = True, random_state = 1)  is just a generator object and it contains only the information n_splits, shuffle and random_state. The k-fold splitting actually happens in the next for loop for train_idx, val_idx in kFold.split(df_full_train): . So it doesn't matter where we generate the object, before or after the first loop. It will generate the same information. But from the programming point of view, it is better to do it before the loop. No point doing it again and again inside the loop\\n(Bhaskar Sarma)\\nIn case of KFold(n_splits=n_splits, shuffle=True, random_state=1) and C= [0.01, 0.1, 1, 10], it is better to loop through the different values of Cs as the video explained. I had separate train() and predict() functions, which were reused after dividing the dataset via KFold. The model ran about 10 minutes and provided a good score.\\n(Ani Mkrtumyan)\", 'section': '4. Evaluation Metrics for Classification', 'question': 'What does KFold do?', 'course': 'machine-learning-zoomcamp', 'doc_id': 'ded1ec3f6b87f390edaa70ae30a59744'}, {'text': \"I’m getting “ValueError: multi_class must be in ('ovo', 'ovr')” when using roc_auc_score to evaluate feature importance of numerical variables in question 1.\\nI was getting this error because I was passing the parameters to roc_auc_score incorrectly (df_train[col] , y_train) . The correct way is to pass the parameters in this way: roc_auc_score(y_train, df_train[col])\\nAsia Saeed\", 'section': '4. Evaluation Metrics for Classification', 'question': \"ValueError: multi_class must be in ('ovo', 'ovr')\", 'course': 'machine-learning-zoomcamp', 'doc_id': 'a98f89e6d0b56eb328c20b3b54570f5f'}, {'text': 'from tqdm.auto import tqdm\\nTqdm - terminal progress bar\\nKrishna Anand', 'section': '4. Evaluation Metrics for Classification', 'question': 'Monitoring Wait times and progress of the code execution can be done with:', 'course': 'machine-learning-zoomcamp', 'doc_id': 'efe6dabef6b46fde18b1651049a05e71'}, {'text': 'Inverting or negating variables with ROC AUC scores less than the threshold is a valuable technique to improve feature importance and model performance when dealing with negatively correlated features. It helps ensure that the direction of the correlation aligns with the expectations of most machine learning algorithms.\\nAileah Gotladera', 'section': '4. Evaluation Metrics for Classification', 'question': 'What is the use of inverting or negating the variables less than the threshold?', 'course': 'machine-learning-zoomcamp', 'doc_id': '06c365650094a70d3945a1cb23e5a64b'}, {'text': 'In case of using predict(X) for this task we are getting the binary classification predictions which are 0 and 1. This may lead to incorrect evaluation values.\\nThe solution is to use predict_proba(X)[:,1], where we get the probability that the value belongs to one of the classes.\\nVladimir Yesipov\\nPredict_proba shows probailites per class.\\nAni Mkrtumyan', 'section': '4. Evaluation Metrics for Classification', 'question': 'Difference between predict(X) and predict_proba(X)[:, 1]', 'course': 'machine-learning-zoomcamp', 'doc_id': '48878bd932f685d909627ffbae7a7244'}, {'text': 'For churn/not churn predictions, I need help to interpret the following scenario please, what is happening when:\\nThe threshold is 1.0\\nFPR is 0.0\\nAnd TPR is 0.0\\nWhen the threshold is 1.0, the condition for belonging to the positive class (churn class) is g(x)>=1.0 But g(x) is a sigmoid function for a binary classification problem. It has values between 0 and 1. This function  never becomes equal to outermost values, i.e. 0 and 1.\\nThat is why there is no object, for which churn-condition could be satisfied. And that is why there is no any positive  (churn) predicted value (neither true positive, nor false positive), if threshold is equal to 1.0\\nAlena Kniazeva', 'section': '4. Evaluation Metrics for Classification', 'question': 'Why are FPR and TPR equal to 0.0, when threshold = 1.0?', 'course': 'machine-learning-zoomcamp', 'doc_id': '4b2aa348c70eb0d3846a2fad82f54446'}, {'text': \"Matplotlib has a cool method to annotate where you could provide an X,Y point and annotate with an arrow and text. For example this will show an arrow pointing to the x,y point optimal threshold.\\nplt.annotate(f'Optimal Threshold: {optimal_threshold:.2f}\\\\nOptimal F1 Score: {optimal_f1_score:.2f}',\\nxy=(optimal_threshold, optimal_f1_score),\\nxytext=(0.3, 0.5),\\ntextcoords='axes fraction',\\narrowprops=dict(facecolor='black', shrink=0.05))\\nQuinn Avila\", 'section': '4. Evaluation Metrics for Classification', 'question': 'How can I annotate a graph?', 'course': 'machine-learning-zoomcamp', 'doc_id': '24bd0704cf5d7de5416aced916cfa18d'}, {'text': \"It's a complex and abstract topic and it requires some time to understand. You can move on without fully understanding the concept.\\nNonetheless, it might be useful for you to rewatch the video, or even watch videos/lectures/notes by other people on this topic, as the ROC AUC is one of the most important metrics used in Binary Classification models.\", 'section': '4. Evaluation Metrics for Classification', 'question': 'I didn’t fully understand the ROC curve. Can I move on?', 'course': 'machine-learning-zoomcamp', 'doc_id': '18876cd801be054ee5f20b437aa3ac11'}, {'text': 'One main reason behind that, is the way of splitting data. For example, we want to split data into train/validation/test with the ratios 60%/20%/20% respectively.\\nAlthough the following two options end up with the same ratio, the data itself is a bit different and not 100% matching in each case.\\n1)\\ndf_train, df_temp = train_test_split(df, test_size=0.4, random_state=42)\\ndf_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\\n2)\\ndf_full_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\\ndf_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=42)\\nTherefore, I would recommend using the second method which is more consistent with the lessons and thus the homeworks.\\nIbraheem Taha', 'section': '4. Evaluation Metrics for Classification', 'question': 'Why do I have different values of accuracy than the options in the homework?', 'course': 'machine-learning-zoomcamp', 'doc_id': 'cdf6ad94a5cc7f9210f1d67a98034582'}, {'text': 'You can find the intercept between these two curves using numpy diff (https://numpy.org/doc/stable/reference/generated/numpy.diff.html ) and sign (https://numpy.org/doc/stable/reference/generated/numpy.sign.html):\\nI suppose here that you have your df_scores ready with your three columns ‘threshold’, ‘precision’ and ‘recall’:\\nYou want to know at which index (or indices) you have your intercept between precision and recall (namely: where the sign of the difference between precision and recall changes):\\nidx = np.argwhere(\\nnp.diff(\\nnp.sign(np.array(df_scores[\"precision\"]) - np.array(df_scores[\"recall\"]))\\n)\\n).flatten()\\nYou can print the result to easily read it:\\nprint(\\nf\"The precision and recall curves intersect at a threshold equal to {df_scores.loc[idx][\\'threshold\\']}.\"\\n)\\n(Mélanie Fouesnard)', 'section': '4. Evaluation Metrics for Classification', 'question': 'How to find the intercept between precision and recall curves by using numpy?', 'course': 'machine-learning-zoomcamp', 'doc_id': 'a6a516391147d3f73984d78d3274c086'}, {'text': \"In the demonstration video, we are shown how to calculate the precision and recall manually. You can use the Scikit Learn library to calculate the confusion matrix. precision, recall, f1_score without having to first define true positive, true negative, false positive, and false negative.\\nfrom sklearn.metrics import precision_score, recall_score, f1_score\\nprecision_score(y_true, y_pred, average='binary')\\nrecall_score(y_true, y_pred, average='binary')\\nf1_score(y_true, y_pred, average='binary')\\nRadikal Lukafiardi\", 'section': '4. Evaluation Metrics for Classification', 'question': 'Compute Recall, Precision, and F1 Score using scikit-learn library', 'course': 'machine-learning-zoomcamp', 'doc_id': '25ad560e0b1afeeab14274692a6a9c7c'}, {'text': 'Cross-validation evaluates the performance of a model and chooses the best hyperparameters. Cross-validation does this by splitting the dataset into multiple parts (folds), typically 5 or 10. It then trains and evaluates your model multiple times, each time using a different fold as the validation set and the remaining folds as the training set.\\n\"C\" is a hyperparameter that is typically associated with regularization in models like Support Vector Machines (SVM) and logistic regression.\\nSmaller \"C\" values: They introduce more regularization, which means the model will try to find a simpler decision boundary, potentially underfitting the data. This is because it penalizes the misclassification of training examples more severely.\\nLarger \"C\" values: They reduce the regularization effect, allowing the model to fit the training data more closely, potentially overfitting. This is because it penalizes misclassification less severely, allowing the model to prioritize getting training examples correct.\\nAminat Abolade', 'section': '4. Evaluation Metrics for Classification', 'question': 'Why do we use cross validation?', 'course': 'machine-learning-zoomcamp', 'doc_id': '6db5fd7b10504fcce8b510ff7f203306'}, {'text': \"Model evaluation metrics can be easily computed using off the shelf calculations available in scikit learn library. This saves a lot of time and more precise compared to our own calculations from the scratch using numpy and pandas libraries.\\nfrom sklearn.metrics import (accuracy_score,\\nprecision_score,\\nrecall_score,\\nf1_score,\\nroc_auc_score\\n)\\naccuracy = accuracy_score(y_val, y_pred)\\nprecision = precision_score(y_val, y_pred)\\nrecall = recall_score(y_val, y_pred)\\nf1 = f1_score(y_val, y_pred)\\nroc_auc = roc_auc_score(y_val, y_pred)\\nprint(f'Accuracy: {accuracy}')\\nprint(f'Precision: {precision}')\\nprint(f'Recall: {recall}')\\nprint(f'F1-Score: {f1}')\\nprint(f'ROC AUC: {roc_auc}')\\n(Harish Balasundaram)\", 'section': '4. Evaluation Metrics for Classification', 'question': 'Evaluate the Model using scikit learn metrics', 'course': 'machine-learning-zoomcamp', 'doc_id': '37ab06961acdb2efa1e7e62434e75959'}, {'text': 'Scikit-learn offers another way: precision_recall_fscore_support\\nExample:\\nfrom sklearn.metrics import precision_recall_fscore_support\\nprecision, recall, fscore, support = precision_recall_fscore_support(y_val, y_val_pred, zero_division=0)\\n(Gopakumar Gopinathan)', 'section': '4. Evaluation Metrics for Classification', 'question': 'Are there other ways to compute Precision, Recall and F1 score?', 'course': 'machine-learning-zoomcamp', 'doc_id': 'dbf3a73e6b6e0f698ca58f5e3b3adffe'}, {'text': '- ROC curves are appropriate when the observations are balanced between each class, whereas precision-recall curves are appropriate for imbalanced datasets.\\n- The reason for this recommendation is that ROC curves present an optimistic picture of the model on datasets with a class imbalance.\\n-This is because of the use of true negatives in the False Positive Rate in the ROC Curve and the careful avoidance of this rate in the Precision-Recall curve.\\n- If the proportion of positive to negative instances changes in a test set, the ROC curves will not change. Metrics such as accuracy, precision, lift and F scores use values from both columns of the confusion matrix. As a class distribution changes these measures will change as well, even if the fundamental classifier performance does not. ROC graphs are based upon TP rate and FP rate, in which each dimension is a strict columnar ratio, so cannot give an accurate picture of performance when there is class imbalance.\\n(Anudeep Vanjavakam)', 'section': '4. Evaluation Metrics for Classification', 'question': 'When do I use ROC vs Precision-Recall curves?', 'course': 'machine-learning-zoomcamp', 'doc_id': '155490b1ed5b75e96684788363cf3573'}, {'text': 'You can use roc_auc_score function from sklearn.metrics module and pass the vector of the target variable (‘above_average’) as the first argument and the vector of feature values as the second one. This function will return AUC score for the feature that was passed as a second argument.\\n(Denys Soloviov)', 'section': '4. Evaluation Metrics for Classification', 'question': 'How to evaluate feature importance for numerical variables with AUC?', 'course': 'machine-learning-zoomcamp', 'doc_id': '803e6fb0a12a99c4fedd2bfed62d2af3'}, {'text': 'Precision-recall curve, and thus the score, explicitly depends on the ratio  of positive to negative test cases. This means that comparison of the F-score across different problems with differing class ratios is problematic. One way to address this issue is to use a standard class ratio  when making such comparisons.\\n(George Chizhmak)', 'section': '4. Evaluation Metrics for Classification', 'question': 'Dependence of the F-score on class imbalance', 'course': 'machine-learning-zoomcamp', 'doc_id': 'f5d50dd68ab7579102300c848c3e4547'}, {'text': \"We can import precision_recall_curve from scikit-learn and plot the graph as follows:\\nfrom sklearn.metrics import precision_recall_curve\\nprecision, recall, thresholds = precision_recall_curve(y_val, y_predict)\\nplt.plot(thresholds, precision[:-1], label='Precision')\\nplt.plot(thresholds, recall[:-1], label='Recall')\\nplt.legend()\\nHrithik Kumar Advani\", 'section': '4. Evaluation Metrics for Classification', 'question': 'Quick way to plot Precision-Recall Curve', 'course': 'machine-learning-zoomcamp', 'doc_id': 'fa33b290538a559cbdfb0567656a8db1'}, {'text': 'For multiclass classification it is important to keep class balance when you split the data set. In this case Stratified k-fold returns folds that contains approximately the sme percentage of samples of each classes.\\nPlease check the realisation in sk-learn library:\\nhttps://scikit-learn.org/stable/modules/cross_validation.html#stratified-k-fold\\nOlga Rudakova', 'section': '5. Deploying Machine Learning Models', 'question': 'What is Stratified k-fold?', 'course': 'machine-learning-zoomcamp', 'doc_id': 'ca52cb39a8c7fe4e61e5ddacc38c6ce9'}, {'text': 'Week 5 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/05-deployment/homework.md\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nHW 3 Solution: https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/cohorts/2022/03-classification/homework_3.ipynb\\nEvaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYouTube Link: 5.X --- https://www.youtube.com/watch?v=agIFak9A3m8&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=49\\n~~~ Nukta Bhatia ~~~', 'section': '5. Deploying Machine Learning Models', 'question': 'How do I get started with Week 5?', 'course': 'machine-learning-zoomcamp', 'doc_id': '6d246a3c374185ecba2faa2df8aa3f35'}, {'text': 'While weeks 1-4 can relatively easily be followed and the associated homework completed with just about any default environment / local setup, week 5 introduces several layers of abstraction and dependencies.\\nIt is advised to prepare your “homework environment” with a cloud provider of your choice. A thorough step-by-step guide for doing so for an AWS EC2 instance is provided in an introductory video taken from the MLOPS course here:\\nhttps://www.youtube.com/watch?v=IXSiYkP23zo\\nNote that (only) small AWS instances can be run for free, and that larger ones will be billed hourly based on usage (but can and should be stopped when not in use).\\nAlternative ways are sketched here:\\nhttps://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/01-intro/06-environment.md', 'section': '5. Deploying Machine Learning Models', 'question': 'Errors related to the default environment: WSL, Ubuntu, proper Python version, installing pipenv etc.', 'course': 'machine-learning-zoomcamp', 'doc_id': '6a145bb6b034831ea166fe7f5b8b1695'}, {'text': \"You’ll need a kaggle account\\nGo to settings, API and click `Create New Token`. This will download a `kaggle.json` file which contains your `username` and `key` information\\nIn the same location as your Jupyter NB, place the `kaggle.json` file\\nRun `!chmod 600 <ENTER YOUR FILEPATH>/kaggle.json`\\nMake sure to import os via `import os` and then run:\\nos.environ['KAGGLE_CONFIG_DIR'] = <STRING OF YOUR FILE PATH>\\nFinally you can run directly in your NB: `!kaggle datasets download -d kapturovalexander/bank-credit-scoring`\\nAnd then you can unzip the file and access the CSV via: `!unzip -o bank-credit-scoring.zip`\\n>>> Michael Fronda <<<\", 'section': '5. Deploying Machine Learning Models', 'question': 'How to download CSV data via Jupyter NB and the Kaggle API, for one seamless experience', 'course': 'machine-learning-zoomcamp', 'doc_id': '9f2cc906abb3b908f97d0870450a64d3'}, {'text': 'Cd .. (go back)\\nLs (see current folders)\\nCd ‘path’/ (go to this path)\\nPwd (home)\\nCat “file name’ --edit txt file in ubuntu\\nAileah Gotladera', 'section': '5. Deploying Machine Learning Models', 'question': 'Basic Ubuntu Commands:', 'course': 'machine-learning-zoomcamp', 'doc_id': '207171c5bdef404a3dae2db4d6881db5'}, {'text': 'Open terminal and type the code below to check the version on your laptop\\npython3 --version\\nFor windows,\\nVisit the official python website at  https://www.python.org/downloads/ to download the python version you need for installation\\nRun the installer and  ensure to check the box that says “Add Python to PATH” during installation and complete the installation by following the prompts\\nOr\\nFor Python 3,\\nOpen your command prompt or terminal and run the following command:\\npip install --upgrade python\\nAminat Abolade', 'section': '5. Deploying Machine Learning Models', 'question': 'Installing and updating to the python version 3.10 and higher', 'course': 'machine-learning-zoomcamp', 'doc_id': 'a828b5624b2758d4633d3948a0cbc9a2'}, {'text': 'It is quite simple, and you can follow these instructions here:\\nhttps://www.youtube.com/watch?v=qYlgUDKKK5A&ab_channel=NeuralNine\\nMake sure that you have “Virtual Machine Platform” feature activated in your Windows “Features”. To do that, search “features” in the research bar and see if the checkbox is selected. You also need to make sure that your system (in the bios) is able to virtualize. This is usually the case.\\nIn the Microsoft Store: look for ‘Ubuntu’ or ‘Debian’ (or any linux distribution you want) and install it\\nOnce it is downloaded, open the app and choose a username and a password (secured one). When you type your password, nothing will show in the window, which is normal: the writing is invisible.\\nYou are now inside of your linux system. You can test some commands such as “pwd”. You are not in your Windows system.\\nTo go to your windows system: you need to go back two times with cd ../.. And then go to the “mnt” directory with cd mnt. If you list here your files, you will see your disks. You can move to the desired folder, for example here I moved to the ML_Zoomcamp folder:\\nPython should be already installed but you can check it by running sudo apt install python3 command.\\nYou can make your actual folder your default folder when you open your Ubuntu terminal with this command : echo \"cd ../../mnt/your/folder/path\" >> ~/.bashrc\\nYou can disable bell sounds (when you type something that does not exist for example) by modifying the inputrc file with this command: sudo vim /etc/inputrc\\nYou have to uncomment the set bell-style none line -> to do that, press the “i” keyboard letter (for insert) and go with your keyboard to this line. Delete the # and then press the Escape keyboard touch and finally press “:wq” to write (it saves your modifications) then quit.\\nYou can check that your modifications are taken into account by opening a new terminal (you can pin it to your task bar so you do not have to go to the Microsoft app each time).\\nYou will need to install pip by running this command sudo apt install python3-pip\\nNB: I had this error message when trying to install pipenv (https://github.com/microsoft/WSL/issues/5663):\\n/sbin/ldconfig.real: Can\\'t link /usr/lib/wsl/lib/libnvoptix_loader.so.1 to libnvoptix.so.1\\n/sbin/ldconfig.real: /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link\\nSo I had to create the following symbolic link:\\nsudo ln -s /usr/lib/wsl/lib/libcuda.so.1 /usr/lib64/libcuda.so\\n(Mélanie Fouesnard)', 'section': '5. Deploying Machine Learning Models', 'question': 'How to install WSL on Windows 10 and 11 ?', 'course': 'machine-learning-zoomcamp', 'doc_id': 'b8a614d2fe2742140eaf1886b120ef84'}, {'text': \"Do you get errors building the Docker image on the Mac M1 chipset?\\nThe error I was getting was:\\nCould not open '/lib64/ld-linux-x86-64.so.2': No such file or directory\\nThe fix (from here): vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\\nOpen mlbookcamp-code/course-zoomcamp/01-intro/environment/Dockerfile\\nReplace line 1 with\\nFROM --platform=linux/amd64 ubuntu:latest\\nNow build the image as specified. In the end it took over 2 hours to build the image but it did complete in the end.\\nDavid Colton\", 'section': '5. Deploying Machine Learning Models', 'question': 'Error building Docker images on Mac with M1 silicon', 'course': 'machine-learning-zoomcamp', 'doc_id': '154d4fb88393be4eab281ecae1e4dfc6'}, {'text': 'Import waitress\\nprint(waitress.__version__)\\nKrishna Anand', 'section': '5. Deploying Machine Learning Models', 'question': 'Method to find the version of any install python libraries in jupyter notebook', 'course': 'machine-learning-zoomcamp', 'doc_id': 'd808bb99fbf16a866ead50d5630aff0a'}, {'text': 'Working on getting Docker installed - when I try running hello-world I am getting the error.\\nDocker: Cannot connect to the docker daemon at unix:///var/run/docker.sock. Is the Docker daemon running ?\\nSolution description\\nIf you’re getting this error on WSL, re-install your docker: remove the docker installation from WSL and install Docker Desktop on your host machine (Windows).\\nOn Linux, start the docker daemon with either of these commands:\\nsudo dockerd\\nsudo service docker start\\nAdded by Ugochukwu Onyebuchi', 'section': '5. Deploying Machine Learning Models', 'question': 'Cannot connect to the docker daemon. Is the Docker daemon running?', 'course': 'machine-learning-zoomcamp', 'doc_id': '5e19a4b2ddf84f43a32336e694d84326'}, {'text': 'After using the command “docker build -t churn-prediction .” to build the Docker image, the above error is raised and the image is not created.\\nIn your Dockerfile, change the Python version in the first line the Python version installed in your system:\\nFROM python:3.7.5-slim\\nTo find your python version, use the command python --version. For example:\\npython --version\\n>> Python 3.9.7\\nThen, change it on your Dockerfile:\\nFROM python:3.9.7-slim\\nAdded by Filipe Melo', 'section': '5. Deploying Machine Learning Models', 'question': \"The command '/bin/sh -c pipenv install --deploy --system &&  rm -rf /root/.cache' returned a non-zero code: 1\", 'course': 'machine-learning-zoomcamp', 'doc_id': '844707d4d50276428a66087068291a13'}, {'text': 'When the facilitator was adding sklearn to the virtual environment in the lectures, he used sklearn==0.24.1 and it ran smoothly. But while doing the homework and you are asked to use the 1.0.2 version of sklearn, it gives errors.\\nThe solution is to use the full name of sklearn. That is, run it as “pipenv install scikit-learn==1.0.2” and the error will go away, allowing you to install sklearn for the version in your virtual environment.\\nOdimegwu David\\nHomework asks you to install 1.3.1\\nPipenv install scikit-learn==1.3.1\\nUse Pipenv to install Scikit-Learn version 1.3.1\\nGopakumar Gopinathan', 'section': '5. Deploying Machine Learning Models', 'question': 'Running “pipenv install sklearn==1.0.2” gives errors. What should I do?', 'course': 'machine-learning-zoomcamp', 'doc_id': '792e926546302ecf2272fe3b3aecbfca'}, {'text': 'What is the reason we don’t want to keep the docker image in our system and why do we need to run docker containers with `--rm` flag?\\nFor best practice, you don’t want to have a lot of abandoned docker images in your system. You just update it in your folder and trigger the build one more time.\\nThey consume extra space on your disk. Unless you don’t want to re-run the previously existing containers, it is better to use the `--rm` option.\\nThe right way to say: “Why do we remove the docker container in our system?”. Well the docker image is still kept; it is the container that is not kept. Upon execution, images are not modified; only containers are.\\nThe option `--rm` is for removing containers. The images remain until you remove them manually. If you don’t specify a version when building an image, it will always rebuild and replace the latest tag. `docker images` shows you all the image you have pulled or build so far.\\nDuring development and testing you usually specify `--rm` to get the containers auto removed upon exit. Otherwise they get accumulated in a stopped state, taking up space. `docker ps -a` shows you all the containers you have in your host. Each time you change Pipfile (or any file you baked into the container), you rebuild the image under the same tag or a new tag. It’s important to understand the difference between the term “docker image” and “docker container”. Image is what we build with all the resources baked in. You can move it around, maintain it in a repository, share it. Then we use the image to spin up instances of it and they are called containers.\\nAdded by Muhammad Awon', 'section': '5. Deploying Machine Learning Models', 'question': 'Why do we need the --rm flag', 'course': 'machine-learning-zoomcamp', 'doc_id': 'c5fe16723eb15047f9a26ac547718f1a'}, {'text': 'When you create the dockerfile the name should be dockerfile and needs to be without extension. One of the problems we can get at this point is to create the dockerfile as a dockerfile extension Dockerfile.dockerfile which creates an error when we build the docker image. Instead we just need to create the file without extension: Dockerfile and will run perfectly.\\nAdded by Pastor Soto', 'section': '5. Deploying Machine Learning Models', 'question': 'Failed to read Dockerfile', 'course': 'machine-learning-zoomcamp', 'doc_id': 'be097c735f03c1a1b208e72b8af212f1'}, {'text': 'Refer to the page https://docs.docker.com/desktop/install/mac-install/ remember to check if you have apple chip or intel chip.', 'section': '5. Deploying Machine Learning Models', 'question': 'Install docker on MacOS', 'course': 'machine-learning-zoomcamp', 'doc_id': 'bb81a7bcb52a844074a8d6415a35f392'}, {'text': 'Problem: When I am trying to pull the image with the docker pull svizor/zoomcamp-model command I am getting an error:\\nUsing default tag: latest\\nError response from daemon: manifest for svizor/zoomcamp-model:latest not found: manifest unknown: manifest unknown\\nSolution: The docker by default uses the latest tag to avoid this use the correct tag from image description. In our case use command:\\ndocker pull svizor/zoomcamp-model:3.10.12-slim\\nAdded by Vladimir Yesipov', 'section': '5. Deploying Machine Learning Models', 'question': 'I cannot pull the image with docker pull command', 'course': 'machine-learning-zoomcamp', 'doc_id': '9a1c26f6bae6bec492ee2ec374ef05d1'}, {'text': 'Using the command docker images or docker image ls will dump all information for all local Docker images. It is possible to dump the information only for a specified image by using:\\ndocker image ls <image name>\\nOr alternatively:\\ndocker images <image name>\\nIn action to that it is possible to only dump specific information provided using the option --format which will dump only the size for the specified image name when using the command below:\\ndocker image ls --format \"{{.Size}}\" <image name>\\nOr alternatively:\\ndocker images --format \"{{.Size}}\" <image name>\\nSylvia Schmitt', 'section': '5. Deploying Machine Learning Models', 'question': 'Dumping/Retrieving only the size of for a specific Docker image', 'course': 'machine-learning-zoomcamp', 'doc_id': '2cea86e22c08413e2a7c93ecc4da009d'}, {'text': \"It creates them in\\nOSX/Linux: ~/.local/share/virtualenvs/folder-name_cyrptic-hash\\nWindows: C:\\\\Users\\\\<USERNAME>\\\\.virtualenvs\\\\folder-name_cyrptic-hash\\nEg: C:\\\\Users\\\\Ella\\\\.virtualenvs\\\\code-qsdUdabf (for module-05 lesson)\\nThe environment name is the name of the last folder in the folder directory where we used the pipenv install command (or any other pipenv command). E.g. If you run any pipenv command in folder path ~/home/user/Churn-Flask-app, it will create an environment named Churn-Flask-app-some_random_characters, and it's path will be like this: /home/user/.local/share/virtualenvs/churn-flask-app-i_mzGMjX.\\nAll libraries of this environment will be installed inside this folder. To activate this environment, I will need to cd into the project folder again, and type pipenv shell. In short, the location of the project folder acts as an identifier for an environment, in place of any name.\\n(Memoona Tahira)\", 'section': '5. Deploying Machine Learning Models', 'question': 'Where does pipenv create environments and how does it name them?', 'course': 'machine-learning-zoomcamp', 'doc_id': 'c4059020b8c86385c133680dc95f071c'}, {'text': 'Launch the container image in interactive mode and overriding the entrypoint, so that it starts a bash command.\\ndocker run -it --entrypoint bash <image>\\nIf the container is already running, execute a command in the specific container:\\ndocker ps (find the container-id)\\ndocker exec -it <container-id> bash\\n(Marcos MJD)', 'section': '5. Deploying Machine Learning Models', 'question': 'How do I debug a docker container?', 'course': 'machine-learning-zoomcamp', 'doc_id': '61d50f53fe4c63a2cd0ec015a612e7af'}, {'text': \"$ docker exec -it 1e5a1b663052 bash\\nthe input device is not a TTY.  If you are using mintty, try prefixing the command with 'winpty'\\nFix:\\nwinpty docker exec -it 1e5a1b663052 bash\\nA TTY is a terminal interface that supports escape sequences, moving the cursor around, etc.\\nWinpty is a Windows software package providing an interface similar to a Unix pty-master for communicating with Windows console programs.\\nMore info on terminal, shell, console applications hi and so on:\\nhttps://conemu.github.io/en/TerminalVsShell.html\\n(Marcos MJD)\", 'section': '5. Deploying Machine Learning Models', 'question': 'The input device is not a TTY when running docker in interactive mode (Running Docker on Windows in GitBash)', 'course': 'machine-learning-zoomcamp', 'doc_id': '6f45fff70e012e3d61fb337dca42f479'}, {'text': 'Initially, I did not assume there was a model2. I copied the original model1.bin and dv.bin. Then when I tried to load using\\nCOPY [\"model2.bin\", \"dv.bin\", \"./\"]\\nthen I got the error above in MINGW64 (git bash) on Windows.\\nThe temporary solution I found was to use\\nCOPY [\"*\", \"./\"]\\nwhich I assume combines all the files from the original docker image and the files in your working directory.\\nAdded by Muhammed Tan', 'section': '5. Deploying Machine Learning Models', 'question': 'Error: failed to compute cache key: \"/model2.bin\" not found: not found', 'course': 'machine-learning-zoomcamp', 'doc_id': 'ee65666b0720f3844f1fd364a1e02aec'}, {'text': 'Create a virtual environment using the Cmd command (command) and use pip freeze command to write the requirements in the text file\\nKrishna Anand', 'section': '5. Deploying Machine Learning Models', 'question': 'Failed to write the dependencies to pipfile and piplock file', 'course': 'machine-learning-zoomcamp', 'doc_id': 'fb15e5735f6e8bf5d41909ab87b74e98'}, {'text': 'f-String not properly keyed in: does anyone knows why i am getting error after import pickle?\\nThe first error showed up because your f-string is using () instead of {} around C. So, should be: f’model_C={C}.bin’\\nThe second error as noticed by Sriniketh, your are missing one parenthesis it should be pickle.dump((dv, model), f_out)\\n(Humberto R.)', 'section': '5. Deploying Machine Learning Models', 'question': 'f-strings', 'course': 'machine-learning-zoomcamp', 'doc_id': '602ef70a7d39c55317ff4578f5301cc7'}, {'text': \"This error happens because pipenv is already installed but you can't access it from the path.\\nThis error comes out if you run.\\npipenv  --version\\npipenv shell\\nSolution for Windows\\nOpen this option\\nClick here\\nClick in Edit Button\\nMake sure the next two locations are on the PATH, otherwise, add it.\\nC:\\\\Users\\\\AppData\\\\....\\\\Python\\\\PythonXX\\\\\\nC:\\\\Users\\\\AppData\\\\....\\\\Python\\\\PythonXX\\\\Scripts\\\\\\nAdded by Alejandro Aponte\\nNote: this answer assumes you don’t use Anaconda. For Windows, using Anaconda would be a better choice and less prone to errors.\", 'section': '5. Deploying Machine Learning Models', 'question': \"'pipenv' is not recognized as an internal or external command, operable program or batch file.\", 'course': 'machine-learning-zoomcamp', 'doc_id': '59223c7494a81f02c374cfe9b07299a9'}, {'text': 'Following the instruction from video week-5.6, using pipenv to install python libraries throws below error\\nSolution to this error is to make sure that you are working with python==3.9 (as informed in the very first lesson of the zoomcamp) and not python==3.10.\\nAdded by Hareesh Tummala', 'section': '5. Deploying Machine Learning Models', 'question': 'AttributeError: module ‘collections’ has no attribute ‘MutableMapping’', 'course': 'machine-learning-zoomcamp', 'doc_id': '3dd44f9019eb41c373135c8b5e57b708'}, {'text': 'After entering `pipenv shell` don’t forget to use `exit` before `pipenv --rm`, as it may cause errors when trying to install packages, it is unclear whether you are “in the shell”(using Windows) at the moment as there are no clear markers for it.\\nIt can also mess up PATH, if that’s the case, here’s terminal commands for fixing that:\\n# for Windows\\nset VIRTUAL_ENV \"\"\\n# for Unix\\nexport VIRTUAL_ENV=\"\"\\nAlso manually re-creating removed folder at `C:\\\\Users\\\\username\\\\.virtualenvs\\\\removed-envname` can help, removed-envname can be seen at the error message.\\nAdded by Andrii Larkin', 'section': '5. Deploying Machine Learning Models', 'question': \"Q: ValueError: Path not found or generated: WindowsPath('C:/Users/username/.virtualenvs/envname/Scripts')\", 'course': 'machine-learning-zoomcamp', 'doc_id': '50cfac15bacd8a12f0f5df083ee9ced5'}, {'text': 'Set the host to ‘0.0.0.0’ on the flask app and dockerfile then RUN the url using localhost.\\n(Theresa S.)', 'section': '5. Deploying Machine Learning Models', 'question': \"ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\", 'course': 'machine-learning-zoomcamp', 'doc_id': '6570f58890d0000f3ad14dd0eb689c83'}, {'text': 'Solution:\\nThis error occurred because I used single quotes around the filenames. Stick to double quotes', 'section': '5. Deploying Machine Learning Models', 'question': 'docker  build ERROR [x/y] COPY …', 'course': 'machine-learning-zoomcamp', 'doc_id': 'd792e61efa896b0ba26aa604ae93e99c'}, {'text': 'I tried the first solution on Stackoverflow which recommended running `pipenv lock` to update the Pipfile.lock. However, this didn’t resolve it. But the following switch to the pipenv installation worked\\nRUN pipenv install --system --deploy --ignore-pipfile', 'section': '5. Deploying Machine Learning Models', 'question': 'Fix error during installation of Pipfile inside Docker container', 'course': 'machine-learning-zoomcamp', 'doc_id': '82129c3c40523f54e82e9c42d2f6eec1'}, {'text': 'Solution\\nThis error was because there was another instance of gunicorn running. So I thought of removing this along with the zoomcamp_test image. However, it didn’t let me remove the orphan container. So I did the following\\nRunning the following commands\\ndocker ps -a <to list all docker containers>\\ndocker images <to list images>\\ndocker stop <container ID>\\ndocker rm <container ID>\\ndocker rmi image\\nI rebuilt the Docker image, and ran it once again; this time it worked correctly and I was able to serve the test script to the endpoint.', 'section': '5. Deploying Machine Learning Models', 'question': 'How to fix error after running the Docker run command', 'course': 'machine-learning-zoomcamp', 'doc_id': '87b18641a7e97953d5db0ed42939755d'}, {'text': 'I was getting the below error when I rebuilt the docker image although the port was not allocated, and it was working fine.\\nError message:\\nError response from daemon: driver failed programming external connectivity on endpoint beautiful_tharp (875be95c7027cebb853a62fc4463d46e23df99e0175be73641269c3d180f7796): Bind for 0.0.0.0:9696 failed: port is already allocated.\\nSolution description\\nIssue has been resolved running the following command:\\ndocker kill $(docker ps -q)\\nhttps://github.com/docker/for-win/issues/2722\\nAsia Saeed', 'section': '5. Deploying Machine Learning Models', 'question': 'Bind for 0.0.0.0:9696 failed: port is already allocated', 'course': 'machine-learning-zoomcamp', 'doc_id': '366421de718a96cf8bb9f34a5e32312f'}, {'text': 'I was getting the error on client side with this\\nClient Side:\\nFile \"C:\\\\python\\\\lib\\\\site-packages\\\\urllib3\\\\connectionpool.py\", line 703, in urlopen …………………..\\nraise ConnectionError(err, request=request)\\nrequests.exceptions.ConnectionError: (\\'Connection aborted.\\', RemoteDisconnected(\\'Remote end closed connection without response\\'))\\nSevrer Side:\\nIt showed error for gunicorn\\nThe waitress  cmd was running smoothly from server side\\nSolution:\\nUse the ip-address as 0.0.0.0:8000 or 0.0.0.0:9696.They are the ones which do work max times\\nAamir Wani', 'section': '5. Deploying Machine Learning Models', 'question': 'Bind for 127.0.0.1:5000 showing error', 'course': 'machine-learning-zoomcamp', 'doc_id': '0711e4ba64a1d333ec786c78f3330be3'}, {'text': 'Install it by using command\\n% brew install md5sha1sum\\nThen run command to check hash for file to check if they the same with the provided\\n% md5sum model1.bin dv.bin\\nOlga Rudakova', 'section': '5. Deploying Machine Learning Models', 'question': 'Installing md5sum on Macos', 'course': 'machine-learning-zoomcamp', 'doc_id': '561cc6ba0ceb212861a37f4ed576b0e8'}, {'text': 'Problem description:\\nI started a web-server in terminal (command window, powershell, etc.). How can I run another python script, which makes a request to this server?\\nSolution description:\\nJust open another terminal (command window, powershell, etc.) and run a python script.\\nAlena Kniazeva', 'section': '5. Deploying Machine Learning Models', 'question': 'How to run a script while a web-server is working?', 'course': 'machine-learning-zoomcamp', 'doc_id': '1020bfc6e8d7f5d046ac334172d018a9'}, {'text': \"Problem description:\\nIn video 5.5 when I do pipenv shell and then pipenv run gunicorn --bind 0.0.0.0:9696 predict:app, I get the following warning:\\nUserWarning: Trying to unpickle estimator DictVectorizer from version 1.1.1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\\nSolution description:\\nWhen you create a virtual env, you should use the same version of Scikit-Learn that you used for training the model on this case it's 1.1.1. There is version conflicts so we need to make sure our model and dv files are created from the version we are using for the project.\\nBhaskar Sarma\", 'section': '5. Deploying Machine Learning Models', 'question': 'Version-conflict in pipenv', 'course': 'machine-learning-zoomcamp', 'doc_id': 'f80bf69df3d7fa738354986cd6992b31'}, {'text': \"If you install packages via pipenv install, and get an error that ends like this:\\npipenv.vendor.plette.models.base.ValidationError: {'python_version': '3.9', 'python_full_version': '3.9.13'}\\npython_full_version: 'python_version' must not be present with 'python_full_version'\\npython_version: 'python_full_version' must not be present with 'python_version'\\nDo this:\\nopen Pipfile in nano editor, and remove either the python_version or python_full_version line, press CTRL+X, type Y and click Enter to save changed\\nType pipenv lock to create the Pipfile.lock.\\nDone. Continue what you were doing\", 'section': '5. Deploying Machine Learning Models', 'question': 'Python_version and Python_full_version error after running pipenv install:', 'course': 'machine-learning-zoomcamp', 'doc_id': 'c6a2405883a6cc7012a9e4491c0bfeb4'}, {'text': 'If during running the  docker build command, you get an error like this:\\nYour Pipfile.lock (221d14) is out of date. Expected: (939fe0).\\nUsage: pipenv install [OPTIONS] [PACKAGES]...\\nERROR:: Aborting deploy\\nOption 1: Delete the pipfile.lock via rm Pipfile, and then rebuild the lock via  pipenv lock from the terminal before retrying the docker build command.\\nOption 2:  If it still doesn’t work, remove the pipenv environment, Pipfile and Pipfile.lock, and create a new one before building docker again. Commands to remove pipenv environment and removing pipfiles:\\npipenv  --rm\\nrm Pipfile*', 'section': '5. Deploying Machine Learning Models', 'question': 'Your Pipfile.lock (221d14) is out of date (during Docker build)', 'course': 'machine-learning-zoomcamp', 'doc_id': 'c0031a068ad0c1338e76cebcb4110cd5'}, {'text': 'Ans: Pip uninstall waitress mflow. Then reinstall just mlflow. By this time you should have successfully built your docker image so you dont need to reinstall waitress. All good. Happy learning.\\nAdded by 🅱🅻🅰🆀', 'section': '5. Deploying Machine Learning Models', 'question': 'You are using windows. Conda environment. You then use waitress instead of gunicorn. After a few runs, suddenly mlflow server fails to run.', 'course': 'machine-learning-zoomcamp', 'doc_id': 'e32e576dd75d0ae462c219c7bbfc892a'}, {'text': \"Ans: so you have created the env. You need to make sure you're in eu-west-1 (ireland) when you check the EB environments. Maybe you're in a different region in your console.\\nAdded by Edidiong Esu\", 'section': '5. Deploying Machine Learning Models', 'question': 'Completed creating the environment locally but could not find the environment on AWS.', 'course': 'machine-learning-zoomcamp', 'doc_id': '13ca66af0f968a442fb044297afeade5'}, {'text': 'Running \\'pip install waitress\\' as a command on GitBash was not downloading the executable file \\'waitress-serve.exe\\'. You need this file to be able to run commands with waitress in Git Bash. To solve this:\\nopen a Jupyter notebook and run the same command \\' pip install waitress\\'. This way the executable file will be downloaded. The notebook may give you this warning : \\'WARNING: The script waitress-serve.exe is installed in \\'c:\\\\Users\\\\....\\\\anaconda3\\\\Scripts\\' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\\'\\nAdd the path where \\'waitress-serve.exe\\' is installed into gitbash\\'s PATH as such:\\nenter the following command in gitbash: nano ~/.bashrc\\nadd the path to \\'waitress-serve.exe\\' to PATH using this command: export PATH=\"/path/to/waitress:$PATH\"\\nclose gitbash and open it again and you should be good to go\\nAdded by Bachar Kabalan', 'section': '5. Deploying Machine Learning Models', 'question': 'Installing waitress on Windows via GitBash: “waitress-serve” command not found', 'course': 'machine-learning-zoomcamp', 'doc_id': 'efde23609818da55f3d038d6a9353c3f'}, {'text': 'Q2.1: Use Pipenv to install Scikit-Learn version 1.3.1\\nThis is an error I got while executing the above step in the ml-zoomcamp conda environment. The error is not fatal and just warns you that explicit language specifications are not set out in our bash profile. A quick-fix is here:\\nhttps://stackoverflow.com/questions/49436922/getting-error-while-trying-to-run-this-command-pipenv-install-requests-in-ma\\nBut one can proceed without addressing it.\\nAdded by Abhirup Ghosh', 'section': '5. Deploying Machine Learning Models', 'question': 'Warning: the environment variable LANG is not set!', 'course': 'machine-learning-zoomcamp', 'doc_id': '432181a5aa3326a5b05d2cf943503821'}, {'text': 'The provided image FROM svizor/zoomcamp-model:3.10.12-slim has a model and dictvectorizer that should be used for question 6. \"model2.bin\", \"dv.bin\"\\nAdded by Quinn Avila', 'section': '5. Deploying Machine Learning Models', 'question': 'Module5 HW Question 6', 'course': 'machine-learning-zoomcamp', 'doc_id': '585556680fc532218c32484d6465d3f3'}, {'text': 'https://apps.microsoft.com/detail/windows-terminal/9N0DX20HK701?hl=es-419&gl=CO\\nAdded by Dawuta Smit', 'section': '5. Deploying Machine Learning Models', 'question': 'Terminal Used in Week 5 videos:', 'course': 'machine-learning-zoomcamp', 'doc_id': '53366c049a2d14ba8da33fa99da3b22b'}, {'text': \"Question:\\nWhen running\\npipenv run waitress-serve --listen=localhost:9696 q4-predict:app\\nI get the following:\\nThere was an exception (ValueError) importing your module.\\nIt had these arguments:\\n1. Malformed application 'q4-predict:app'\\nAnswer:\\nWaitress doesn’t accept a dash in the python file name.\\nThe solution is to rename the file replacing a dash with something else for instance with an underscore eg q4_predict.py\\nAdded by Alex Litvinov\", 'section': '5. Deploying Machine Learning Models', 'question': 'waitress-serve shows Malformed application', 'course': 'machine-learning-zoomcamp', 'doc_id': '497d3204cf6d98a0e0caf0e7c5de9034'}, {'text': 'I wanted to have a fast and simple way to check if the HTTP POST requests are working just running a request from command line. This can be done running ‘curl’. \\n(Used with WSL2 on Windows, should also work on Linux and MacOS)\\ncurl --json \\'<json data>\\' <url>\\n# piping the structure to the command\\ncat <json file path> | curl --json @- <url>\\necho \\'<json data>\\' | curl --json @- <url>\\n# example using piping\\necho \\'{\"job\": \"retired\", \"duration\": 445, \"poutcome\": \"success\"}\\'\\\\\\n| curl --json @- http://localhost:9696/predict\\nAdded by Sylvia Schmitt', 'section': '5. Deploying Machine Learning Models', 'question': 'Testing HTTP POST requests from command line using curl', 'course': 'machine-learning-zoomcamp', 'doc_id': '8464d54ee0edae1da0805894bb1d6e5d'}, {'text': 'Question:\\nWhen executing\\neb local run  --port 9696\\nI get the following error:\\nERROR: NotSupportedError - You can use \"eb local\" only with preconfigured, generic and multicontainer Docker platforms.\\nAnswer:\\nThere are two options to fix this:\\nRe-initialize by running eb init -i and choosing the options from a list (the first default option for docker platform should be fine).\\nEdit the ‘.elasticbeanstalk/config.yml’ directly changing the default_platform from Docker to default_platform: Docker running on 64bit Amazon Linux 2023\\nThe disadvantage of the second approach is that the option might not be available the following years\\nAdded by Alex Litvinov', 'section': '5. Deploying Machine Learning Models', 'question': 'NotSupportedError - You can use \"eb local\" only with preconfigured, generic and multicontainer Docker platforms.', 'course': 'machine-learning-zoomcamp', 'doc_id': 'a0e9cc981fdb8f5e9fece448728310ea'}, {'text': \"You need to include the protocol scheme: 'http://localhost:9696/predict'.\\nWithout the http:// part, requests has no idea how to connect to the remote server.\\nNote that the protocol scheme must be all lowercase; if your URL starts with HTTP:// for example, it won’t find the http:// connection adapter either.\\nAdded by George Chizhmak\", 'section': '5. Deploying Machine Learning Models', 'question': \"Requests Error: No connection adapters were found for 'localhost:9696/predict'.\", 'course': 'machine-learning-zoomcamp', 'doc_id': 'f91359a78eeaa2a789b3510ea9db6220'}, {'text': 'While running the docker image if you get the same result check which model you are using.\\nRemember you are using a model downloading model + python version so remember to change the model in your file when running your prediction test.\\nAdded by Ahmed Okka', 'section': '5. Deploying Machine Learning Models', 'question': 'Getting the same result', 'course': 'machine-learning-zoomcamp', 'doc_id': '0acab16f3be10af4e2feeb1cfe3e23f9'}, {'text': 'Ensure that you used pipenv to install the necessary modules including gunicorn. As pipfiles for virtual environments, you can use pipenv shell and then build+run your docker image. - Akshar Goyal', 'section': '5. Deploying Machine Learning Models', 'question': 'Trying to run a docker image I built but it says it’s unable to start the container process', 'course': 'machine-learning-zoomcamp', 'doc_id': 'd4e87080dc55c2be9024bc3b1251cfca'}, {'text': \"You can copy files from your local machine into a Docker container using the docker cp command. Here's how to do it:\\nTo copy a file or directory from your local machine into a running Docker container, you can use the `docker cp command`. The basic syntax is as follows:\\ndocker cp /path/to/local/file_or_directory container_id:/path/in/container\\nHrithik Kumar Advani\", 'section': '5. Deploying Machine Learning Models', 'question': 'How do I copy files from my local machine to docker container?', 'course': 'machine-learning-zoomcamp', 'doc_id': '2520a5a75ebaca5fa3569209d9913312'}, {'text': 'You can copy files from your local machine into a Docker container using the docker cp command. Here\\'s how to do it:\\nIn the Dockerfile, you can provide the folder containing the files that you want to copy over. The basic syntax is as follows:\\nCOPY [\"src/predict.py\", \"models/xgb_model.bin\", \"./\"]\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tGopakumar Gopinathan', 'section': '5. Deploying Machine Learning Models', 'question': 'How do I copy files from a different folder into docker container’s working directory?', 'course': 'machine-learning-zoomcamp', 'doc_id': 'e80b890fe2a2603a350689e3d5a42452'}, {'text': 'I struggled with the command :\\neb init -p docker tumor-diagnosis-serving -r eu-west-1\\nWhich resulted in an error when running : eb local run --port 9696\\nERROR: NotSupportedError - You can use \"eb local\" only with preconfigured, generic and multicontainer Docker platforms.\\nI replaced it with :\\neb init -p \"Docker running on 64bit Amazon Linux 2\" tumor-diagnosis-serving -r eu-west-1\\nThis allowed the recognition of the Dockerfile and the build/run of the docker container.\\nAdded by Mélanie Fouesnard', 'section': '5. Deploying Machine Learning Models', 'question': 'I can’t create the environment on AWS Elastic Beanstalk with the command proposed during the video', 'course': 'machine-learning-zoomcamp', 'doc_id': '41c9d54dca60a67eb6e9ac64c3ec45ce'}, {'text': \"I had this error when creating a AWS ElasticBean environment: eb create tumor-diagnosis-env\\nERROR   Instance deployment: Both 'Dockerfile' and 'Dockerrun.aws.json' are missing in your source bundle. Include at least one of them. The deployment failed.\\nI did not committed the files used to build the container, particularly the Dockerfile. After a git add and git commit of the modified files, the command works.\\nAdded by Mélanie Fouesnard\", 'section': '6. Decision Trees and Ensemble Learning', 'question': 'Dockerfile missing when creating the AWS ElasticBean environment', 'course': 'machine-learning-zoomcamp', 'doc_id': '15d5dbfd3db890303d72c10a9d20b31c'}, {'text': 'Week 6 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/06-trees/homework.md\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nHW 4 Solution: https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/cohorts/2022/04-evaluation/homework_4.ipynb\\nEvaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYouTube Link: 6.X --- https://www.youtube.com/watch?v=GJGmlfZoCoU&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=57\\nFAQs: https://docs.google.com/document/d/1LpPanc33QJJ6BSsyxVg-pWNMplal84TdZtq10naIhD8/edit#heading=h.lpz96zg7l47j\\n~~~Nukta Bhatia~~~', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'How to get started with Week 6?', 'course': 'machine-learning-zoomcamp', 'doc_id': '0ec2b0ab972965f701879e8707b1ed45'}, {'text': 'During the XGBoost lesson, we created a parser to extract the training and validation auc from the standard output. However, we can accomplish that in a more straightforward way.\\nWe can use the evals_result  parameters, which takes an empty dictionary and updates it for each tree. Additionally, you can store the data in a dataframe and plot it in an easier manner.\\nAdded by Daniel Coronel', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'How to get the training and validation metrics from XGBoost?', 'course': 'machine-learning-zoomcamp', 'doc_id': 'd9b24c009fa641e5bd61391366e51101'}, {'text': 'You should create sklearn.ensemble.RandomForestRegressor object. It’s rather similar to sklearn.ensemble.RandomForestClassificator for classification problems. Check https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html for more information.\\nAlena Kniazeva', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'How to solve regression problems with random forest in scikit-learn?', 'course': 'machine-learning-zoomcamp', 'doc_id': '9db04c3146607746b9f45d89057214fd'}, {'text': 'In question 6, I was getting ValueError: feature_names must be string, and may not contain [, ] or < when I was creating DMatrix for train and validation\\nSolution description\\nThe cause of this error is that some of the features names contain special characters like = and <, and I fixed the error by removing them as  follows:\\nfeatures= [i.replace(\"=<\", \"_\").replace(\"=\",\"_\") for i in features]\\nAsia Saeed\\nAlternative Solution:\\nIn my case the equal sign “=” was not a problem, so in my opinion the first part of Asias solution features= [i.replace(\"=<\", \"_\") should work as well.\\nFor me this works:\\nfeatures = []\\nfor f in dv.feature_names_:\\nstring = f.replace(“=<”, “-le”)\\nfeatures.append(string)\\nPeter Ernicke', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'ValueError: feature_names must be string, and may not contain [, ] or <', 'course': 'machine-learning-zoomcamp', 'doc_id': '27dec32eebbac5e1f15f9f11d2a72aaf'}, {'text': 'If you’re getting this error, It is likely because the feature names in dv.get_feature_names_out() are a np.ndarray instead of a list so you have to convert them into a list by using the to_list() method.\\nAli Osman', 'section': '6. Decision Trees and Ensemble Learning', 'question': \"`TypeError: Expecting a sequence of strings for feature names, got: <class 'numpy.ndarray'> ` when training xgboost model.\", 'course': 'machine-learning-zoomcamp', 'doc_id': 'c4df4ca30ff70e2119e8abccb6d80506'}, {'text': \"If you’re getting TypeError:\\n“TypeError: Expecting a sequence of strings for feature names, got: <class 'numpy.ndarray'>”,\\nprobably you’ve done this:\\nfeatures = dv.get_feature_names_out()\\nIt gets you np.ndarray instead of list. Converting to list list(features) will not fix this, read below.\\nIf you’re getting ValueError:\\n“ValueError: feature_names must be string, and may not contain [, ] or <”,\\nprobably you’ve either done:\\nfeatures = list(dv.get_feature_names_out())\\nor:\\nfeatures = dv.feature_names_\\nreason is what you get from DictVectorizer here looks like this:\\n['households',\\n'housing_median_age',\\n'latitude',\\n'longitude',\\n'median_income',\\n'ocean_proximity=<1H OCEAN',\\n'ocean_proximity=INLAND',\\n'population',\\n'total_bedrooms',\\n'total_rooms']\\nit has symbols XGBoost doesn’t like ([, ] or <).\\nWhat you can do, is either do not specify “feature_names=” while creating xgb.DMatrix or:\\nimport re\\nfeatures = dv.feature_names_\\npattern = r'[\\\\[\\\\]<>]'\\nfeatures = [re.sub(pattern, '  ', f) for f in features]\\nAdded by Andrii Larkin\", 'section': '6. Decision Trees and Ensemble Learning', 'question': 'Q6: ValueError or TypeError while setting xgb.DMatrix(feature_names=)', 'course': 'machine-learning-zoomcamp', 'doc_id': '32b2a3b60c0055145b85cefc29c7a293'}, {'text': 'To install Xgboost, use the code below directly in your jupyter notebook:\\n(Pip 21.3+ is required)\\npip install xgboost\\nYou can update your pip by using the code below:\\npip install --upgrade pip\\nFor more about xgbboost and installation, check here:\\nhttps://xgboost.readthedocs.io/en/stable/install.html\\nAminat Abolade', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'How to Install Xgboost', 'course': 'machine-learning-zoomcamp', 'doc_id': '8d39b7a4fa665044813dbc72826e2485'}, {'text': 'Sometimes someone might wonder what eta means in the tunable hyperparameters of XGBoost and how it helps the model.\\nETA is the learning rate of the model. XGBoost uses gradient descent to calculate and update the model. In gradient descent, we are looking for the minimum weights that help the model to learn the data very well. This minimum weights for the features is updated each time the model passes through the features and learns the features during training. Tuning the learning rate helps you tell the model what speed it would use in deriving the minimum for the weights.', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'What is eta in XGBoost', 'course': 'machine-learning-zoomcamp', 'doc_id': 'a8426da17557492761c080b5470d1eec'}, {'text': 'For ensemble algorithms, during the week 6, one bagging algorithm and one boosting algorithm were presented: Random Forest and XGBoost, respectively.\\nRandom Forest trains several models in parallel. The output can be, for example, the average value of all the outputs of each model. This is called bagging.\\nXGBoost trains several models sequentially: the previous model error is used to train the following model. Weights are used to ponderate the models such as the best models have higher weights and are therefore favored for the final output. This method is called boosting.\\nNote that boosting is not necessarily better than bagging.\\nMélanie Fouesnard\\nBagging stands for “Bootstrap Aggregation” - it involves taking multiple samples with replacement to derive multiple training datasets from the original training dataset (bootstrapping), training a classifier (e.g. decision trees or stumps for Random Forests) on each such training dataset, and then combining the the predictions (aggregation) to obtain the final prediction. For classification, predictions are combined via voting; for regression, via averaging. Bagging can be done in parallel, since the various classifiers are independent. Bagging decreases variance (but not bias) and is robust against overfitting.\\nBoosting, on the other hand, is sequential - each model learns from the mistakes of its predecessor. Observations are given different weights - observations/samples misclassified by the previous classifier are given a higher weight, and this process is continued until a stopping condition is reached (e.g. max. No. of models is reached, or error is acceptably small, etc.). Boosting reduces bias & is generally more accurate than bagging, but can be prone to overfitting.\\nRileen', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'What is the difference between bagging and boosting?', 'course': 'machine-learning-zoomcamp', 'doc_id': 'cb30043b27daf6cd6d9ac3b81ea0dca4'}, {'text': 'I wanted to directly capture the output from the xgboost training for multiple eta values to a dictionary without the need to run the same cell multiple times and manually editing the eta value in between or copy the code for a second eta value.\\nUsing the magic cell command “%%capture output” I was only able to capture the complete output for all iterations for the loop, but. I was able to solve this using the following approach. This is just a code sample to grasp the idea.\\n# This would be the content of the Jupyter Notebook cell\\nfrom IPython.utils.capture import capture_output\\nimport sys\\ndifferent_outputs = {}\\nfor i in range(3):\\nwith capture_output(sys.stdout) as output:\\nprint(i)\\nprint(\"testing capture\")\\ndifferent_outputs[i] = output.stdout\\n# different_outputs\\n# {0: \\'0\\\\ntesting capture\\\\n\\',\\n#  1: \\'1\\\\ntesting capture\\\\n\\',\\n#  2: \\'2\\\\ntesting capture\\\\n\\'}\\nAdded by Sylvia Schmitt', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'Capture stdout for each iterations of a loop separately', 'course': 'machine-learning-zoomcamp', 'doc_id': '6af8910a834fa11636fc06208de23d91'}, {'text': 'Calling roc_auc_score() to get auc is throwing the above error.\\nSolution to this issue is to make sure that you pass y_actuals as 1st argument and y_pred as 2nd argument.\\nroc_auc_score(y_train, y_pred)\\nHareesh Tummala', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'ValueError: continuous format is not supported', 'course': 'machine-learning-zoomcamp', 'doc_id': '4e31bc15b1e447cea1d3313959451276'}, {'text': 'When rmse stops improving means, when it stops to decrease or remains almost similar.\\nPastor Soto', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'Question 3 of homework 6 if i see that rmse goes up at a certain number of n_estimators but then goes back down lower than it was before, should the answer be the number of n_estimators after which rmse initially went up, or the number after which it was its overall lowest value?', 'course': 'machine-learning-zoomcamp', 'doc_id': '1d07dceb20e3a806e75233ea5bbed2ba'}, {'text': 'dot_data = tree.export_graphviz(regr, out_file=None,\\nfeature_names=boston.feature_names,\\nfilled=True)\\ngraphviz.Source(dot_data, format=\"png\")\\nKrishna Anand\\nfrom sklearn import tree\\ntree.plot_tree(dt,feature_names=dv.feature_names_)\\nAdded By Ryan Pramana', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'One of the method to visualize the decision trees', 'course': 'machine-learning-zoomcamp', 'doc_id': 'd1c6c2fbe088be06c9cc9381b9a25db9'}, {'text': 'Solution: This problem happens because you use DecisionTreeClassifier instead of DecisionTreeRegressor. You should check if you want to use a Decision tree for classification or regression.\\nAlejandro Aponte', 'section': '6. Decision Trees and Ensemble Learning', 'question': \"ValueError: Unknown label type: 'continuous'\", 'course': 'machine-learning-zoomcamp', 'doc_id': '39a837d5e05f091a6e43ccc158df3f2b'}, {'text': 'When I run dt = DecisionTreeClassifier() in jupyter in same laptop, each time I re-run it or do (restart kernel + run) I get different values of auc. Some of them are 0.674, 0.652, 0.642, 0.669 and so on.  Anyone knows why it could be? I am referring to 7:40-7:45 of video 6.3.\\nSolution: try setting the random seed e.g\\ndt = DecisionTreeClassifier(random_state=22)\\nBhaskar Sarma', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'Different values of auc, each time code is re-run', 'course': 'machine-learning-zoomcamp', 'doc_id': '96db1737e1def888edc68cbce43ed19c'}, {'text': \"They both do the same, it's just less typing from the script.\\nAsked by Andrew Katoch, Added by Edidiong Esu\", 'section': '6. Decision Trees and Ensemble Learning', 'question': 'Does it matter if we let the Python file create the server or if we run gunicorn directly?', 'course': 'machine-learning-zoomcamp', 'doc_id': '09c6f646dc6a6488242a712ca9305cbc'}, {'text': 'When I tried to run example from the video using function ping and can not import it. I use import ping and it was unsuccessful. To fix it I use the statement:\\n\\nfrom [file name] import ping\\nOlga Rudakova', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'No module named ‘ping’?', 'course': 'machine-learning-zoomcamp', 'doc_id': 'ea506af813490109f8566532ad670067'}, {'text': 'The DictVectorizer has a function to get the feature names get_feature_names_out(). This is helpful for example if you need to analyze feature importance but use the dict vectorizer for one hot encoding. Just keep in mind it does return a numpy array so you may need to convert this to a list depending on your usage for example dv.get_feature_names_out() will return a ndarray array of string objects. list(dv.get_feature_names_out()) will convert to a standard list of strings. Also keep in mind that you first need to fit the predictor and response arrays before you have access to the feature names.\\nQuinn Avila', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'DictVectorizer feature names', 'course': 'machine-learning-zoomcamp', 'doc_id': '4c7e4365fa4b8869cfd452b85b7be923'}, {'text': \"They both do the same, it's just less typing from the script.\", 'section': '6. Decision Trees and Ensemble Learning', 'question': 'Does it matter if we let the Python file create the server or if we run gunicorn directly?', 'course': 'machine-learning-zoomcamp', 'doc_id': 'b4d77e3c5cc5b68cca43d17bab4bf30b'}, {'text': 'This error occurs because the list of feature names contains some characters like \"<\" that are not supported. To fix this issue, you can replace those problematic characters with supported ones. If you want to create a consistent list of features with no special characters, you can achieve it like this:\\nYou can address this error by replacing problematic characters in the feature names with underscores, like so:\\nfeatures = [f.replace(\\'=<\\', \\'_\\').replace(\\'=\\', \\'_\\') for f in features]\\nThis code will go through the list of features and replace any instances of \"=<\" with \"\", as well as any \"=\" with \"\", ensuring that the feature names only consist of supported characters.', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'ValueError: feature_names must be string, and may not contain [, ] or <', 'course': 'machine-learning-zoomcamp', 'doc_id': '8c68ab5778d085d80d22fdd9281cbed1'}, {'text': \"To make it easier for us to determine which features are important, we can use a horizontal bar chart to illustrate feature importance sorted by value.\\n1. # extract the feature importances from the model\\nfeature_importances = list(zip(features_names, rdr_model.feature_importances_))\\nimportance_df = pd.DataFrame(feature_importances, columns=['feature_names', 'feature_importances'])\\n2. # sort descending the dataframe by using feature_importances value\\nimportance_df = importance_df.sort_values(by='feature_importances', ascending=False)\\n3. # create a horizontal bar chart\\nplt.figure(figsize=(8, 6))\\nsns.barplot(x='feature_importances', y='feature_names', data=importance_df, palette='Blues_r')\\nplt.xlabel('Feature Importance')\\nplt.ylabel('Feature Names')\\nplt.title('Feature Importance Chart')\\nRadikal Lukafiardi\", 'section': '6. Decision Trees and Ensemble Learning', 'question': 'Visualize Feature Importance by using horizontal bar chart', 'course': 'machine-learning-zoomcamp', 'doc_id': '1638ccc4263903c3017e64ab0395744b'}, {'text': 'Instead of using np.sqrt() as the second step. You can extract it using like this way :\\nmean_squared_error(y_val, y_predict_val,squared=False)\\nAhmed Okka', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'RMSE using metrics.root_meas_square()', 'course': 'machine-learning-zoomcamp', 'doc_id': '355cecc82ea80f633a225c12a811a67a'}, {'text': 'I like this visual implementation of features importance in scikit-learn library:\\nhttps://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\\nIt actually adds std.errors to features importance -> so that you can trace stability of features (important for a model’s explainability) over the different params of the model.\\nIvan Brigida', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'Features Importance graph', 'course': 'machine-learning-zoomcamp', 'doc_id': 'c14cca9134819afae33a076fe6afd363'}, {'text': 'Expanded error says: xgboost.core.XGBoostError: sklearn needs to be installed in order to use this module. So, sklearn in requirements solved the problem.\\nGeorge Chizhmak', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'xgboost.core.XGBoostError: This app has encountered an error. The original error message is redacted to prevent data leaks.', 'course': 'machine-learning-zoomcamp', 'doc_id': '7e27729ed3c52d4dd48475038d323607'}, {'text': 'Information gain  in Y due to X, or the mutual information of Y and X\\nWhere  is the entropy of Y. \\n\\nIf X is completely uninformative about Y:\\nIf X is completely informative about Y: )\\nHrithik Kumar Advani', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'Information Gain', 'course': 'machine-learning-zoomcamp', 'doc_id': '676f19982cad5de986a7b38ce2fdeecb'}, {'text': 'Filling in missing values using an entire dataset before splitting for training/testing/validation causes', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'Data Leakage', 'course': 'machine-learning-zoomcamp', 'doc_id': '01225a621c34e0e72cd8de074cdae79b'}, {'text': 'Save model by calling ‘booster.save_model’, see eg\\nLoad model:\\nDawuta Smit\\nThis section is moved to Projects', 'section': '8. Neural Networks and Deep Learning', 'question': 'Serialized Model Xgboost error', 'course': 'machine-learning-zoomcamp', 'doc_id': '61e2278f604f3966d4f3301ae8bfd5bb'}, {'text': 'TODO', 'section': '8. Neural Networks and Deep Learning', 'question': 'How to get started with Week 8?', 'course': 'machine-learning-zoomcamp', 'doc_id': 'c10ea3b6a9f954bd836fb69a8b9b5f52'}, {'text': 'Create or import your notebook into Kaggle.\\nClick on the Three dots at the top right hand side\\nClick on Accelerator\\nChoose T4 GPU\\nKhurram Majeed', 'section': '8. Neural Networks and Deep Learning', 'question': 'How to use Kaggle for Deep Learning?', 'course': 'machine-learning-zoomcamp', 'doc_id': 'd2e6a25f0321fd20215538d1b047d788'}, {'text': 'Create or import your notebook into Google Colab.\\nClick on the Drop Down at the top right hand side\\nClick on “Change runtime type”\\nChoose T4 GPU\\nKhurram Majeed', 'section': '8. Neural Networks and Deep Learning', 'question': 'How to use Google Colab for Deep Learning?', 'course': 'machine-learning-zoomcamp', 'doc_id': 'ddc2079bb6b77d174ca719ba8fea1832'}, {'text': 'Connecting your GPU on Saturn Cloud to Github repository is not compulsory, since you can just download the notebook and copy it to the Github folder. But if you like technology to do things for you, then follow the solution description below:\\nSolution description: Follow the instructions in these github docs to create an SSH private and public key:\\nhttps://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-ke\\ny-and-adding-it-to-the-ssh-agenthttps://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account?tool=webui\\nThen the second video on this module about saturn cloud would show you how to add the ssh keys to secrets and authenticate through a terminal.\\nOr alternatively, you could just use the public keys provided by Saturn Cloud by default. To do so, follow these steps:\\nClick on your username and on manage\\nDown below you will see the Git SSH keys section.\\nCopy the default public key provided by Saturn Cloud\\nPaste these key into the SSH keys section of your github repo\\nOpen a terminal on Saturn Cloud and run this command “ssh -T git@github.com”\\nYou will receive a successful authentication notice.\\nOdimegwu David', 'section': '8. Neural Networks and Deep Learning', 'question': 'How do I push from Saturn Cloud to Github?', 'course': 'machine-learning-zoomcamp', 'doc_id': '6e91ecf6b9aa8074396e735491493dcd'}, {'text': 'This template is referred to in the video 8.1b Setting up the Environment on Saturn Cloud\\nbut the location shown in the video is no longer correct.\\nThis template has been moved to “python deep learning tutorials’ which is shown on the Saturn Cloud home page.\\nSteven Christolis', 'section': '8. Neural Networks and Deep Learning', 'question': 'Where is the Python TensorFlow template on Saturn Cloud?', 'course': 'machine-learning-zoomcamp', 'doc_id': '8375cc3fae4fb7e4a7afe3ee627e81fd'}, {'text': 'The above error happens since module scipy is not installed in the saturn cloud tensorflow image. While creating the Jupyter server resource, in the “Extra Packages” section under pip in the textbox write scipy. Below the textbox, the pip install scipy command will be displayed. This will ensure when the resource spins up, the scipy package will be automatically installed. This approach can also be followed for additional python packages.\\nSumeet Lalla', 'section': '8. Neural Networks and Deep Learning', 'question': 'Getting error module scipy not found during model training in Saturn Cloud tensorflow image', 'course': 'machine-learning-zoomcamp', 'doc_id': 'db8cc281ccfc3f80e7389ea992bc5ed5'}, {'text': 'Problem description: Uploading the data to saturn cloud from kaggle can be time saving, specially if the dataset is large.\\nYou can just download to your local machine and then upload to a folder on saturn cloud, but there is a better solution that needs to be set once and you have access to all kaggle datasets in saturn cloud.\\nOn your notebook run:\\n!pip install -q kaggle\\nGo to Kaggle website (you need to have an account for this):\\nClick on your profile image -> Account\\nScroll down to the API box\\nClick on Create New API token\\nIt will download a json file with the name kaggle.json store on your local computer. We need to upload this file in the .kaggle folder\\nOn the notebook click on folder icon on the left upper corner\\nThis will take you to the root folder\\nClick on the .kaggle folder\\nOnce inside of the .kaggle folder upload the kaggle.json file that you downloaded\\nRun this command on your notebook:\\n!chmod 600 /home/jovyan/.kaggle/kaggle.json\\nDownload the data using this command:\\n!kaggle datasets download -d agrigorev/dino-or-dragon\\nCreate a folder to unzip your files:\\n!mkdir data\\nUnzip your files inside that folder\\n!unzip dino-or-dragon.zip -d data\\nPastor Soto', 'section': '8. Neural Networks and Deep Learning', 'question': 'How to upload kaggle data to Saturn Cloud?', 'course': 'machine-learning-zoomcamp', 'doc_id': '6bcb68cc13c1063d8999ba7134b9ccf6'}, {'text': 'In order to run tensorflow with gpu on your local machine you’ll need to setup cuda and cudnn.\\nThe process can be overwhelming. Here’s a simplified guide\\nOsman Ali', 'section': '8. Neural Networks and Deep Learning', 'question': 'How to install CUDA & cuDNN on Ubuntu 22.04', 'course': 'machine-learning-zoomcamp', 'doc_id': 'b24757fdba7f919208a5ce15c1c775e6'}, {'text': 'Problem description:\\nWhen loading saved model getting error: ValueError: Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights.\\nSolution description:\\nBefore loading model need to evaluate the model on input data: model.evaluate(train_ds)\\nAdded by Vladimir Yesipov', 'section': '8. Neural Networks and Deep Learning', 'question': 'Error: (ValueError: Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights.) when loading model.', 'course': 'machine-learning-zoomcamp', 'doc_id': '9bba8021c88751162684b437181da32d'}, {'text': 'Problem description:\\nWhen follow module 8.1b video to setup git in Saturn Cloud, run `ssh -T git@github.com` lead error `git@github.com: Permission denied (publickey).`\\nSolution description:\\nAlternative way, we can setup git in our Saturn Cloud env with generate SSH key in our Saturn Cloud and add it to our git account host. After it, we can access/manage our git through Saturn’s jupyter server. All steps detailed on this following tutorial: https://saturncloud.io/docs/using-saturn-cloud/gitrepo/\\nAdded by Ryan Pramana', 'section': '8. Neural Networks and Deep Learning', 'question': 'Getting error when connect git on Saturn Cloud: permission denied', 'course': 'machine-learning-zoomcamp', 'doc_id': '0d0f38725ec4ae51ac195e5cbdf46524'}, {'text': \"Problem description:\\nGetting an error using <git clone git@github.com:alexeygrigorev/clothing-dataset-small.git>\\nThe error:\\nCloning into 'clothing-dataset'...\\nHost key verification failed.\\nfatal: Could not read from remote repository.\\nPlease make sure you have the correct access rights\\nand the repository exists.\\nSolution description:\\nwhen cloning the repo, you can also chose https - then it should work. This happens when you don't have your ssh key configured.\\n<git clone https://github.com/alexeygrigorev/clothing-dataset-small.git>\\nAdded by Gregory Morris\", 'section': '8. Neural Networks and Deep Learning', 'question': 'Host key verification failed.', 'course': 'machine-learning-zoomcamp', 'doc_id': 'a9a4d23c32b7be586deb92f8883c4d82'}, {'text': \"Problem description\\nThe accuracy and the loss are both still the same or nearly the same while training.\\nSolution description\\nIn the homework, you should set class_mode='binary' while reading the data.\\nAlso, problem occurs when you choose the wrong optimizer, batch size, or learning rate\\nAdded by Ekaterina Kutovaia\", 'section': '8. Neural Networks and Deep Learning', 'question': 'The same accuracy on epochs', 'course': 'machine-learning-zoomcamp', 'doc_id': '3dc59418fe426b2af51e8c583685294e'}, {'text': 'Problem:\\nWhen resuming training after augmentation, the loss skyrockets (1000+ during first epoch) and accuracy settles around 0.5 – i.e. the model becomes as good as a random coin flip.\\nSolution:\\nCheck that the augmented ImageDataGenerator still includes the option “rescale” as specified in the preceding step.\\nAdded by Konrad Mühlberg', 'section': '8. Neural Networks and Deep Learning', 'question': 'Model breaking after augmentation – high loss + bad accuracy', 'course': 'machine-learning-zoomcamp', 'doc_id': '273da83bda937557297207bfafe7e9a5'}, {'text': \"While doing:\\nimport tensorflow as tf\\nfrom tensorflow import keras\\nmodel = tf.keras.models.load_model('model_saved.h5')\\nIf you get an error message like this:\\nValueError: The channel dimension of the inputs should be defined. The input_shape received is (None, None, None, None), where axis -1 (0-based) is the channel dimension, which found to be `None`.\\nSolution:\\nSaving a model (either yourself via model.save() or via checkpoint when save_weights_only = False) saves two things: The trained model weights (for example the best weights found during training) and the model architecture.  If the number of channels is not explicitly specified in the Input layer of the model, and is instead defined as a variable, the model architecture will not have the value in the variable stored. Therefore when the model is reloaded, it will complain about not knowing the number of channels. See the code below, in the first line, you need to specify number of channels explicitly:\\n# model architecture:\\ninputs = keras.Input(shape=(input_size, input_size, 3))\\nbase = base_model(inputs, training=False)\\nvectors = keras.layers.GlobalAveragePooling2D()(base)\\ninner = keras.layers.Dense(size_inner, activation='relu')(vectors)\\ndrop = keras.layers.Dropout(droprate)(inner)\\noutputs = keras.layers.Dense(10)(drop)\\nmodel = keras.Model(inputs, outputs)\\n(Memoona Tahira)\", 'section': '8. Neural Networks and Deep Learning', 'question': 'Missing channel value error while reloading model:', 'course': 'machine-learning-zoomcamp', 'doc_id': '11136fea8f7e66ae3aa7075ff6d161cc'}, {'text': \"Problem:\\nA dataset for homework is in a zipped folder. If you unzip it within a jupyter notebook by means of ! unzip command, you’ll see a huge amount of output messages about unzipping of each image. So you need to suppress this output\\nSolution:\\nExecute the next cell:\\n%%capture\\n! unzip zipped_folder_name.zip -d destination_folder_name\\nAdded by Alena Kniazeva\\nInside a Jupyter Notebook:\\nimport zipfile\\nlocal_zip = 'data.zip'\\nzip_ref = zipfile.ZipFile(local_zip, 'r')\\nzip_ref.extractall('data')\\nzip_ref.close()\", 'section': '8. Neural Networks and Deep Learning', 'question': 'How to unzip a folder with an image dataset and suppress output?', 'course': 'machine-learning-zoomcamp', 'doc_id': '55c1c7bce14a225cb153e59eacbd899d'}, {'text': 'Problem:\\nWhen we run train_gen.flow_from_directory() as in video 8.5, it finds images belonging to 10 classes. Does it understand the names of classes from the names of folders? Or, there is already something going on deep behind?\\nSolution:\\nThe name of class is the folder name\\nIf you just create some random folder with the name \"xyz\", it will also be considered as a class!! The name itself is saying flow_from_directory\\na clear explanation below:\\nhttps://vijayabhaskar96.medium.com/tutorial-image-classification-with-keras-flow-from-directory-and-generators-95f75ebe5720\\nAdded by Bhaskar Sarma', 'section': '8. Neural Networks and Deep Learning', 'question': 'How keras flow_from_directory know the names of classes in images?', 'course': 'machine-learning-zoomcamp', 'doc_id': '2f30087c83594875acab221ed6beb306'}, {'text': 'Problem:\\nI created a new environment in SaturnCloud and chose the image corresponding to Saturn with Tensorflow, but when I tried to fit the model it showed an error about the missing module: scipy\\nSolution:\\nInstall the module in a new cell: !pip install scipy\\nRestart the kernel and fit the model again\\nAdded by Erick Calderin', 'section': '8. Neural Networks and Deep Learning', 'question': 'Error with scipy missing module in SaturnCloud', 'course': 'machine-learning-zoomcamp', 'doc_id': '1eacb1a95c47393d6f8831307f3dca95'}, {'text': 'The command to read folders in the dataset in the tensorflow source code is:\\nfor subdir in sorted(os.listdir(directory)):\\n…\\nReference: https://github.com/keras-team/keras/blob/master/keras/preprocessing/image.py, line 563\\nThis means folders will be read in alphabetical order. For example, in the case of a folder named dino, and another named dragon, dino will read first and will have class label 0, whereas dragon will be read in next and will have class label 1.\\nWhen a Keras model predicts binary labels, it will only return one value, and this is the probability of class 1 in case of sigmoid activation function in the last dense layer with 2 neurons. The probability of class 0 can be found out by:\\nprob(class(0)) = 1- prob(class(1))\\nIn case of using from_logits to get results, you will get two values for each of the labels.\\nA prediction of 0.8 is saying the probability that the image has class label 1 (in this case dragon), is 0.8, and conversely we can infer the probability that the image has class label 0 is 0.2.\\n(Added by Memoona Tahira)', 'section': '8. Neural Networks and Deep Learning', 'question': 'How are numeric class labels determined in flow_from_directroy using binary class mode and what is meant by the single probability predicted by a binary Keras model:', 'course': 'machine-learning-zoomcamp', 'doc_id': 'd70f647f03bf7b998fa3687cca39b8ef'}, {'text': \"It's fine, some small changes are expected\\nAlexey Grigorev\", 'section': '8. Neural Networks and Deep Learning', 'question': 'Does the actual values matter after predicting with a neural network or it should be treated as like hood of falling in a class?', 'course': 'machine-learning-zoomcamp', 'doc_id': 'f7617cd0a8cf43612e4f91cca2bdfbae'}, {'text': 'Problem:\\nI found running the wasp/bee model on my mac laptop had higher reported accuracy and lower std deviation than the HW answers. This may be because of the SGD optimizer. Running this on my mac printed a message about a new and legacy version that could be used.\\nSolution:\\nTry running the same code on google collab or another way. The answers were closer for me on collab. Another tip is to change the runtime to use T4 and the model run’s faster than just CPU\\nAdded by Quinn Avila', 'section': '8. Neural Networks and Deep Learning', 'question': 'What if your accuracy and std training loss don’t match HW?', 'course': 'machine-learning-zoomcamp', 'doc_id': '1bab02ab87235b5f9f6fd62e4b5f3c9d'}, {'text': 'When running “model.fit(...)” an additional parameter “workers” can be specified for speeding up the data loading/generation. The default value is “1”. Try out which value between 1 and the cpu count on your system performs best.\\nhttps://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\\nAdded by Sylvia Schmitt', 'section': '8. Neural Networks and Deep Learning', 'question': 'Using multi-threading for data generation in “model.fit()”', 'course': 'machine-learning-zoomcamp', 'doc_id': 'cb62dfc4d46f311271fef8b6ca808750'}, {'text': 'Reproducibility for training runs can be achieved following these instructions: \\nhttps://www.tensorflow.org/versions/r2.8/api_docs/python/tf/config/experimental/enable_op_determinism\\nseed = 1234\\ntf.keras.utils.set_random_seed(seed)\\ntf.config.experimental.enable_op_determinism()\\nThis will work for a script, if this gets executed multiple times.\\nAdded by Sylvia Schmitt', 'section': '8. Neural Networks and Deep Learning', 'question': 'Reproducibility with TensorFlow using a seed point', 'course': 'machine-learning-zoomcamp', 'doc_id': '5c6ce80521bb8d7136e7b647da5d42b5'}, {'text': 'Pytorch is also a deep learning framework that allows to do equivalent tasks as keras. Here is a tutorial to create a CNN from scratch using pytorch :\\nhttps://blog.paperspace.com/writing-cnns-from-scratch-in-pytorch/\\nThe functions have similar goals. The syntax can be slightly different. For the lessons and the homework, we use keras, but one can feel free to make a pull request with the equivalent with pytorch for lessons and homework!\\nMélanie Fouesnard', 'section': '8. Neural Networks and Deep Learning', 'question': 'Can we use pytorch for this lesson/homework ?', 'course': 'machine-learning-zoomcamp', 'doc_id': '7f2a8ff92ccbdd41f846e4bc9c975455'}, {'text': \"While training a Keras model you get the error “Failed to find data adapter that can handle input: <class 'keras.src.preprocessing.image.ImageDataGenerator'>, <class 'NoneType'>” you may have unintentionally passed the image generator instead of the dataset to the model\\ntrain_gen = ImageDataGenerator(rescale=1./255)\\ntrain_ds = train_gen.flow_from_directory(…)\\nhistory_after_augmentation = model.fit(\\ntrain_gen, # this should be train_ds!!!\\nepochs=10,\\nvalidation_data=test_gen # this should be test_ds!!!\\n)\\nThe fix is simple and probably obvious once pointed out, use the training and validation dataset (train_ds and val_ds) returned from flow_from_directory\\nAdded by Tzvi Friedman\", 'section': '8. Neural Networks and Deep Learning', 'question': 'Keras model training fails with “Failed to find data adapter”', 'course': 'machine-learning-zoomcamp', 'doc_id': 'b3cf5754f3c323603c840cda731281b2'}, {'text': 'The command ‘nvidia-smi’ has a built-in function which will run it in subsequently updating it every N seconds without the need of using the command ‘watch’.\\nnvidia-smi -l <N seconds>\\nThe following command will run ‘nvidia-smi’ every 2 seconds until interrupted using CTRL+C.\\nnvidia-smi -l 2\\nAdded by Sylvia Schmitt', 'section': '8. Neural Networks and Deep Learning', 'question': 'Running ‘nvidia-smi’ in a loop without using ‘watch’', 'course': 'machine-learning-zoomcamp', 'doc_id': '591293975f887b506fbb3f9517711c78'}, {'text': 'The Python package ‘’ is an interactive GPU process viewer similar to ‘htop’ for CPU.\\nhttps://pypi.org/project//\\nImage source: https://pypi.org/project//\\nAdded by Sylvia Schmitt', 'section': '8. Neural Networks and Deep Learning', 'question': 'Checking GPU and CPU utilization using ‘nvitop’', 'course': 'machine-learning-zoomcamp', 'doc_id': 'df008438cfaced4c9ef5d1eeb9356c89'}, {'text': \"Let’s say we define our Conv2d layer like this:\\n>> tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3))\\nIt means our input image is RGB (3 channels, 150 by 150 pixels), kernel is 3x3 and number of filters (layer’s width) is 32.\\nIf we check model.summary() we will get this:\\n_________________________________________________________________\\nLayer (type)                Output Shape              Param #\\n=================================================================\\nconv2d (Conv2D)             (None, 148, 148, 32)      896\\nSo where does 896 params come from? It’s computed like this:\\n>>> (3*3*3 +1) * 32\\n896\\n# 3x3 kernel, 3 channels RGB, +1 for bias, 32 filters\\nWhat about the number of “features” we get after the Flatten layer?\\nFor our homework model.summary() for last MaxPooling2d and Flatten layers looked like this:\\n_________________________________________________________________\\nLayer (type)                Output Shape              Param #\\n=================================================================\\nmax_pooling2d_3       (None, 7, 7, 128)         0\\nflatten (Flatten)           (None, 6272)              0\\nSo where do 6272 vectors come from? It’s computed like this:\\n>>> 7*7*128\\n6272\\n# 7x7 “image shape” after several convolutions and poolings, 128 filters\\nAdded by Andrii Larkin\", 'section': '8. Neural Networks and Deep Learning', 'question': 'Q: Where does the number of Conv2d layer’s params come from? Where does the number of “features” we get after the Flatten layer come from?', 'course': 'machine-learning-zoomcamp', 'doc_id': '8e5f71565fad6b1219f22fb1640a2755'}, {'text': 'It’s quite useful to understand that all types of models in the course are a plain stack of layers where each layer has exactly one input tensor and one output tensor (Sequential model TF page, Sequential class).\\nYou can simply start from an “empty” model and add more and more layers in a sequential order.\\nThis mode is called “Sequential Model API”  (easier)\\nIn Alexey’s videos it is implemented as chained calls of different entities (“inputs”,“base”, “vectors”,  “outputs”) in a more advanced mode “Functional Model API”.\\nMaybe a more complicated way makes sense when you do Transfer Learning and want to separate “Base” model vs. rest, but in the HW you need to recreate the full model from scratch ⇒ I believe it is easier to work with a sequence of “similar” layers.\\nYou can read more about it in this TF2 tutorial.\\nA really useful Sequential model example is shared in the Kaggle’s “Bee or Wasp” dataset folder with code: notebook\\nAdded by Ivan Brigida\\nFresh Run on Neural Nets\\nWhile correcting an error on neural net architecture, it is advised to do fresh run by restarting kernel, else the model learns on top of previous runs.\\nAdded by Abhijit Chakraborty', 'section': '8. Neural Networks and Deep Learning', 'question': 'Sequential vs. Functional Model Modes in Keras (TF2)', 'course': 'machine-learning-zoomcamp', 'doc_id': 'c83dd270bf1b433dd80e50c4dc909e92'}, {'text': \"I found this code snippet fixed my OOM errors, as I have an Nvidia GPU. Can't speak to OOM errors on CPU, though.\\nhttps://www.tensorflow.org/api_docs/python/tf/config/experimental/set_memory_growth\\n```\\nphysical_devices = tf.configlist_physical_devices('GPU')\\ntry:\\ntf.config.experimental.set_memory_growth(physical_devices[0],True)\\nexcept:\\n# Invalid device or cannot modify virtual devices once initialized.\\npass\\n```\", 'section': '8. Neural Networks and Deep Learning', 'question': 'Out of memory errors when running tensorflow', 'course': 'machine-learning-zoomcamp', 'doc_id': '4df7998bfd58c2ffc7c0b71a9f16e746'}, {'text': 'When training the models, in the fit function, you can specify the number of workers/threads.\\nThe number of threads apparently also works for GPUs, and came very handy in google colab for the T4 GPU, since it was very very slow, and workers default value is 1.\\nI changed the workers variable to 2560, following this thread in stackoverflow. I am using the free T4 GPU.  (https://stackoverflow.com/questions/68208398/how-to-find-the-number-of-cores-in-google-colabs-gpu)\\nAdded by Ibai Irastorza', 'section': '8. Neural Networks and Deep Learning', 'question': 'Model training very slow in google colab with T4 GPU', 'course': 'machine-learning-zoomcamp', 'doc_id': 'ced6e47975e87a13d6f1f29d50073851'}, {'text': 'From the keras documentation:\\nDeprecated: tf.keras.preprocessing.image.ImageDataGenerator is not recommended for new code. Prefer loading images with tf.keras.utils.image_dataset_from_directory and transforming the output tf.data.Dataset with preprocessing layers. For more information, see the tutorials for loading images and augmenting images, as well as the preprocessing layer guide.\\nHrithik Kumar Advani', 'section': '9. Serverless Deep Learning', 'question': 'Using image_dataset_from_directory instead of ImageDataGeneratorn for loading images', 'course': 'machine-learning-zoomcamp', 'doc_id': '8f728a4f4d6501724df41ffb55630ba7'}, {'text': 'TODO', 'section': '9. Serverless Deep Learning', 'question': 'How to get started with Week 9?', 'course': 'machine-learning-zoomcamp', 'doc_id': '60f7f42de59b3faa600193c5dd566ba9'}, {'text': 'The week 9 uses a link to github to fetch the models.\\nThe original link was moved to here:\\nhttps://github.com/DataTalksClub/machine-learning-zoomcamp/releases', 'section': '9. Serverless Deep Learning', 'question': 'Where is the model for week 9?', 'course': 'machine-learning-zoomcamp', 'doc_id': '621faa30f52767ad72c1f8fb4b9e0b07'}, {'text': 'Solution description\\nIn the unit 9.6, Alexey ran the command echo ${REMOTE_URI} which turned the URI address in the terminal. There workaround is to set a local variable (REMOTE_URI) and assign your URI address in the terminal and use it to login the registry, for instance, REMOTE_URI=2278222782.dkr.ecr.ap-south-1.amazonaws.com/clothing-tflite-images (fake address). One caveat is that you will lose this variable once the session is terminated.\\nI also had the same problem on Ubuntu terminal. I executed the following two commands:\\n$ export REMOTE_URI=1111111111.dkr.ecr.us-west-1.amazonaws.com/clothing-tflite-images:clothing-model-xception-v4-001\\n$ echo $REMOTE_URI\\n111111111.dkr.ecr.us-west-1.amazonaws.com/clothing-tflite-images:clothing-model-xception-v4-001\\nNote: 1. no curly brackets (e.g. echo ${REMOTE_URI}) needed unlike in video 9.6,\\n2. Replace REMOTE_URI with your URI\\n(Bhaskar Sarma)', 'section': '9. Serverless Deep Learning', 'question': 'Executing the command echo ${REMOTE_URI} returns nothing.', 'course': 'machine-learning-zoomcamp', 'doc_id': '7e321de8c3fd80a24b6e8174cb72bf22'}, {'text': 'The command aws ecr get-login --no-include-email returns an invalid choice error:\\nThe solution is to use the following command instead:  aws ecr get-login-password\\nCould simplify the login process with, just replace the <ACCOUNT_NUMBER> and <REGION> with your values:\\nexport PASSWORD=`aws ecr get-login-password`\\ndocker login -u AWS -p $PASSWORD <ACCOUNT_NUMBER>.dkr.ecr.<REGION>.amazonaws.com/clothing-tflite-images\\nAdded by Martin Uribe', 'section': '9. Serverless Deep Learning', 'question': 'Getting a syntax error while trying to get the password from aws-cli', 'course': 'machine-learning-zoomcamp', 'doc_id': '2977f27e7bde5689fbddbfb8ac53a640'}, {'text': 'We can use the keras.models.Sequential() function to pass many parameters of the cnn at once.\\nKrishna Anand', 'section': '9. Serverless Deep Learning', 'question': 'Pass many parameters in the model at once', 'course': 'machine-learning-zoomcamp', 'doc_id': '9603a34f367ea284b18184925065dab4'}, {'text': 'This error is produced sometimes when building your docker image from the Amazon python base image.\\nSolution description: The following could solve the problem.\\nUpdate your docker desktop if you haven’t done so.\\nOr restart docker desktop and terminal and then build the image all over again.\\nOr if all else fails, first run the following command: DOCKER_BUILDKIT=0  docker build .  then build your image.\\n(optional) Added by Odimegwu David', 'section': '9. Serverless Deep Learning', 'question': 'Getting  ERROR [internal] load metadata for public.ecr.aws/lambda/python:3.8', 'course': 'machine-learning-zoomcamp', 'doc_id': 'af2b9a536bb7f627cb54ef7934e63ad8'}, {'text': \"When trying to run the command  !ls -lh in windows jupyter notebook  , I was getting an error message that says “'ls' is not recognized as an internal or external command,operable program or batch file.\\nSolution description :\\nInstead of !ls -lh , you can use this command !dir , and you will get similar output\\nAsia Saeed\", 'section': '9. Serverless Deep Learning', 'question': \"Problem: 'ls' is not recognized as an internal or external command, operable program or batch file.\", 'course': 'machine-learning-zoomcamp', 'doc_id': '8e2857ede622bffe9684e97aa9f0133e'}, {'text': 'When I run   import tflite_runtime.interpreter as tflite , I get an error message says “ImportError: generic_type: type \"InterpreterWrapper\" is already registered!”\\nSolution description\\nThis error occurs when you import both tensorflow  and tflite_runtime.interpreter  “import tensorflow as tf” and “import tflite_runtime.interpreter as tflite” in the same notebook.  To fix the issue, restart the kernel and import only tflite_runtime.interpreter \" import tflite_runtime.interpreter as tflite\".\\nAsia Saeed', 'section': '9. Serverless Deep Learning', 'question': 'ImportError: generic_type: type \"InterpreterWrapper\" is already registered!', 'course': 'machine-learning-zoomcamp', 'doc_id': '141590484a728217929538da71278186'}, {'text': 'Problem description:\\nIn command line try to do $ docker build -t dino_dragon\\ngot this Using default tag: latest\\n[2022-11-24T06:48:47.360149000Z][docker-credential-desktop][W] Windows version might not be up-to-date: The system cannot find the file specified.\\nerror during connect: This error may indicate that the docker daemon is not running.: Post\\n.\\nSolution description:\\nYou need to make sure that Docker is not stopped by a third-party program.\\nAndrei Ilin', 'section': '9. Serverless Deep Learning', 'question': 'Windows version might not be up-to-date', 'course': 'machine-learning-zoomcamp', 'doc_id': '9303ef7093060d61a61f1541a19f6be8'}, {'text': 'When running docker build -t dino-dragon-model it returns the above error\\nThe most common source of this error in this week is because Alex video shows a version of the wheel with python 8, we need to find a wheel with the version that we are working on. In this case python 9. Another common error is to copy the link, this will also produce the same error, we need to download the raw format:\\nhttps://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.7.0-cp39-cp39-linux_x86_64.whl\\nPastor Soto', 'section': '9. Serverless Deep Learning', 'question': 'WARNING: You are using pip version 22.0.4; however, version 22.3.1 is available', 'course': 'machine-learning-zoomcamp', 'doc_id': '7758843297650c72ac994e45cacd7bb9'}, {'text': 'Problem description:\\nIn video 9.6, after installing aswcli, we should configure it with aws configure . There it asks for Access Key ID, Secret Access Key, Default Region Name and also Default output format. What we should put for Default output format? Leaving it as  None is okay?\\nSolution description:\\nYes, in my I case I left everything as the provided defaults (except, obviously, the Access key and the secret access key)\\nAdded by Bhaskar Sarma', 'section': '9. Serverless Deep Learning', 'question': 'How to do AWS configure after installing awscli', 'course': 'machine-learning-zoomcamp', 'doc_id': '00f558adaab1229863167ed29b1ec776'}, {'text': 'Problem:\\nWhile passing local testing of the lambda function without issues, trying to test the same input with a running docker instance results in an error message like\\n{‘errorMessage’: ‘Unable to marshal response: Object of type float32 is not JSON serializable’, ‘errorType’: ‘Runtime.MarshalError’, ‘requestId’: ‘f155492c-9af2-4d04-b5a4-639548b7c7ac’, ‘stackTrace’: []}\\nThis happens when a model (in this case the dino vs dragon model) returns individual estimation values as numpy float32 values (arrays). They need to be converted individually to base-Python floats in order to become “serializable”.\\nSolution:\\nIn my particular case, I set up the dino vs dragon model in such a way as to return a label + predicted probability for each class as follows (below is a two-line extract of function predict() in the lambda_function.py):\\npreds = [interpreter.get_tensor(output_index)[0][0], \\\\\\n1-interpreter.get_tensor(output_index)[0][0]]\\nIn which case the above described solution will look like this:\\npreds = [float(interpreter.get_tensor(output_index)[0][0]), \\\\\\nfloat(1-interpreter.get_tensor(output_index)[0][0])]\\nThe rest can be made work by following the chapter 9 (and/or chapter 5!) lecture videos step by step.\\nAdded by Konrad Muehlberg', 'section': '9. Serverless Deep Learning', 'question': 'Object of type float32 is not JSON serializable', 'course': 'machine-learning-zoomcamp', 'doc_id': '43454dc810a6f315779efc9726479902'}, {'text': 'I had this error when running the command line : interpreter.set_tensor(input_index, x) that can be seen in the video 9.3 around 12 minutes.\\nValueError: Cannot set tensor: Got value of type UINT8 but expected type FLOAT32 for input 0, name: serving_default_conv2d_input:0\\nThis is because the X is an int but a float is expected.\\nSolution:\\nI found this solution from this question here https://stackoverflow.com/questions/76102508/valueerror-cannot-set-tensor-got-value-of-type-float64-but-expected-type-float :\\n# Need to convert to float32 before set_tensor\\nX = np.float32(X)\\nThen, it works. I work with tensorflow 2.15.0, maybe the fact that this version is more recent involves this change ?\\nAdded by Mélanie Fouesnard', 'section': '9. Serverless Deep Learning', 'question': 'Error with the line “interpreter.set_tensor(input_index, X”)', 'course': 'machine-learning-zoomcamp', 'doc_id': '594a994630887c94cfd61e9f4f052142'}, {'text': 'To check your file size using the powershell terminal, you can do the following command lines:\\n$File = Get-Item -Path path_to_file\\n$FileSize = (Get-Item -Path $FilePath).Length\\nNow you can check the size of your file, for example in MB:\\nWrite-host \"MB\":($FileSize/1MB)\\nSource: https://www.sharepointdiary.com/2020/10/powershell-get-file-size.html#:~:text=To%20get%20the%20size%20of,the%20file%2C%20including%20its%20size.\\nAdded by Mélanie Fouesnard', 'section': '9. Serverless Deep Learning', 'question': 'How to easily get file size in powershell terminal ?', 'course': 'machine-learning-zoomcamp', 'doc_id': '0f4100ec6e99ee180b2d45c98ebab2d5'}, {'text': 'I wanted to understand how lambda container images work in depth and how lambda functions are initialized, for this reason, I found the following documentation\\nhttps://docs.aws.amazon.com/lambda/latest/dg/images-create.html\\nhttps://docs.aws.amazon.com/lambda/latest/dg/runtimes-api.html\\nAdded by Alejandro aponte', 'section': '9. Serverless Deep Learning', 'question': 'How do Lambda container images work?', 'course': 'machine-learning-zoomcamp', 'doc_id': 'd03538833252857f5032dbebd61179c4'}, {'text': 'The docker image for aws lambda can be created and pushed to aws ecr and the same can be exposed as a REST API through APIGatewayService in a single go using AWS Serverless Framework. Refer the below article for a detailed walkthrough.\\nhttps://medium.com/hoonio/deploy-containerized-serverless-flask-to-aws-lambda-c0eb87c1404d\\nAdded by Sumeet Lalla', 'section': '9. Serverless Deep Learning', 'question': 'How to use AWS Serverless Framework to deploy on AWS Lambda and expose it as REST API through APIGatewayService?', 'course': 'machine-learning-zoomcamp', 'doc_id': '9d28ad6f3b4bcfcbc983665fc020a993'}, {'text': 'Problem:\\nWhile trying to build docker image in Section 9.5 with the command:\\ndocker build -t clothing-model .\\nIt throws a pip install error for the tflite runtime whl\\nERROR: failed to solve: process \"/bin/sh -c pip install https://github.com/alexeygrigorev/tflite-aws-lambda/blob/main/tflite/tflite_runtime-2.14.0-cp310-cp310-linux_x86_64.whl\" did not complete successfully: exit code: 1\\nTry to use this link: https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.14.0-cp310-cp310-linux_x86_64.whl\\nIf the link above does not work:\\nThe problem is because of the arm architecture of the M1. You will need to run the code on a PC or Ubuntu OS.\\nOr try the code bellow.\\nAdded by Dashel Ruiz Perez\\nSolution:\\nTo build the Docker image, use the command:\\ndocker build --platform linux/amd64 -t clothing-model .\\nTo run the built image, use the command:\\ndocker run -it --rm -p 8080:8080 --platform linux/amd64 clothing-model:latest\\nAdded by Daniel Egbo', 'section': '9. Serverless Deep Learning', 'question': 'Error building docker image on M1 Mac', 'course': 'machine-learning-zoomcamp', 'doc_id': '06fcbe1939e788f8c5e431f5c6374f6c'}, {'text': \"Problem: Trying to test API gateway in 9.7 - API Gateway: Exposing the Lambda Function, running: $ python test.py\\nWith error message:\\n{'message': 'Missing Authentication Token'}\\nSolution:\\nNeed to get the deployed API URL for the specific path you are invoking. Example:\\nhttps://<random string>.execute-api.us-east-2.amazonaws.com/test/predict\\nAdded by Andrew Katoch\", 'section': '9. Serverless Deep Learning', 'question': 'Error invoking API Gateway deploy API locally', 'course': 'machine-learning-zoomcamp', 'doc_id': '5319cdb77465358c03ac9e3860943ca5'}, {'text': 'Problem: When trying to install tflite_runtime with\\n!pip install --extra-index-url https://google-coral.github.io/py-repo/ tflite_runtime\\none gets an error message above.\\nSolution:\\nfflite_runtime is only available for the os-python version combinations that can be found here: https://google-coral.github.io/py-repo/tflite-runtime/\\nyour combination must be missing here\\nyou can see if any of these work for you https://github.com/alexeygrigorev/tflite-aws-lambda/tree/main/tflite\\nand install the needed one using pip\\neg\\npip install https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.7.0-cp38-cp38-linux_x86_64.whl\\nas it is done in the lectures code:\\nhttps://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/09-serverless/code/Dockerfile#L4\\nAlternatively, use a virtual machine (with VM VirtualBox, for example) with a Linux system. The other way is to run a code at a virtual machine within cloud service, for example you can use Vertex AI Workbench at GCP (notebooks and terminals are provided there, so all tasks may be performed).\\nAdded by Alena Kniazeva, modified by Alex Litvinov', 'section': '9. Serverless Deep Learning', 'question': 'Error: Could not find a version that satisfies the requirement tflite_runtime (from versions:none)', 'course': 'machine-learning-zoomcamp', 'doc_id': '0a61e05189bf7dd0380871e4c209d3e6'}, {'text': 'docker: Error response from daemon: mkdir /var/lib/docker/overlay2/37be849565da96ac3fce34ee9eb2215bd6cd7899a63ebc0ace481fd735c4cb0e-init: read-only file system.\\nYou need to restart the docker services to get rid of the above error\\nKrishna Anand', 'section': '9. Serverless Deep Learning', 'question': 'Docker run error', 'course': 'machine-learning-zoomcamp', 'doc_id': '5049e2902559dd3669c5940427cbc9a7'}, {'text': 'The docker image can be saved/exported to tar format in local machine using the below command:\\ndocker image save <image-name> -o <name-of-tar-file.tar>\\nThe individual layers of the docker image for the filesystem content can be viewed by extracting the layer.tar present in the <name-of-tar-file.tar> created from above.\\nSumeet Lalla', 'section': '9. Serverless Deep Learning', 'question': 'Save Docker Image to local machine and view contents', 'course': 'machine-learning-zoomcamp', 'doc_id': 'a1a1bb6b2249cb798ad0b72e657c8302'}, {'text': 'On vscode running jupyter notebook. After I ‘pip install pillow’, my notebook did not recognize using the import for example from PIL import image. After restarting the jupyter notebook the imports worked.\\nQuinn Avila', 'section': '9. Serverless Deep Learning', 'question': 'Jupyter notebook not seeing package', 'course': 'machine-learning-zoomcamp', 'doc_id': 'fd7a9ca97288539a604b80e4034da149'}, {'text': 'Due to experimenting back and forth so much without care for storage, I just ran out of it on my 30-GB AWS instance. It turns out that deleting docker images does not actually free up any space as you might expect. After removing images, you also need to run docker system prune', 'section': '9. Serverless Deep Learning', 'question': 'Running out of space for AWS instance.', 'course': 'machine-learning-zoomcamp', 'doc_id': 'a2476a1a4e01cbc9d99eba05d5373743'}, {'text': 'Using the 2.14 version with python 3.11 works fine.\\nIn case it doesn’t work, I tried with tensorflow 2.4.4 whl, however, make sure to run it on top of supported python versions like 3.8, else there will be issues installing tf==2.4.4\\nAdded by Abhijit Chakraborty', 'section': '9. Serverless Deep Learning', 'question': 'Using Tensorflow 2.15 for AWS deployment', 'course': 'machine-learning-zoomcamp', 'doc_id': 'c948222d538f461124a1277a453c163e'}, {'text': 'see here', 'section': '9. Serverless Deep Learning', 'question': 'Command aws ecr get-login --no-include-email returns “aws: error: argument operation: Invalid choice…”', 'course': 'machine-learning-zoomcamp', 'doc_id': '61d91abc64e54524a56fa4d1e8cf27f8'}, {'text': 'Sign in to the AWS Console: Log in to the AWS Console.\\nNavigate to IAM: Go to the IAM service by clicking on \"Services\" in the top left corner and selecting \"IAM\" under the \"Security, Identity, & Compliance\" section.\\nCreate a new policy: In the left navigation pane, select \"Policies\" and click on \"Create policy.\"\\nSelect the service and actions:\\nClick on \"JSON\" and copy and paste the JSON policy you provided earlier for the specific ECR actions.\\nReview and create the policy:\\nClick on \"Review policy.\"\\nProvide a name and description for the policy.\\nClick on \"Create policy.\"\\nJSON policy:\\n{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"VisualEditor0\",\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"ecr:CreateRepository\",\\n\"ecr:GetAuthorizationToken\",\\n\"ecr:BatchCheckLayerAvailability\",\\n\"ecr:BatchGetImage\",\\n\"ecr:InitiateLayerUpload\",\\n\"ecr:UploadLayerPart\",\\n\"ecr:CompleteLayerUpload\",\\n\"ecr:PutImage\"\\n],\\n\"Resource\": \"*\"\\n}\\n]\\n}\\nAdded by: Daniel Muñoz-Viveros\\nERROR: failed to solve: public.ecr.aws/lambda/python:3.10: error getting credentials - err: exec: \"docker-credential-desktop.exe\": executable file not found in $PATH, out: ``\\n(WSL2 system)\\nSolved: Delete the file ~/.docker/config.json\\nYishan Zhan', 'section': '9. Serverless Deep Learning', 'question': 'What IAM permission policy is needed to complete Week 9: Serverless?', 'course': 'machine-learning-zoomcamp', 'doc_id': '338f372c8aa3fd7c3f7867bb464a4f53'}, {'text': 'Add the next lines to vim /etc/docker/daemon.json\\n{\\n\"dns\": [\"8.8.8.8\", \"8.8.4.4\"]\\n}\\nThen, restart docker:  sudo service docker restart\\nIbai Irastorza', 'section': '9. Serverless Deep Learning', 'question': 'Docker Temporary failure in name resolution', 'course': 'machine-learning-zoomcamp', 'doc_id': '096d6a2f35d8211c7d4f19013d2203b0'}, {'text': \"Solution: add compile = False to the load_model function\\nkeras.models.load_model('model_name.h5', compile=False)\\nNadia Paz\", 'section': '9. Serverless Deep Learning', 'question': 'Keras model *.h5 doesn’t load. Error: weight_decay is not a valid argument, kwargs should be empty  for `optimizer_experimental.Optimizer`', 'course': 'machine-learning-zoomcamp', 'doc_id': '578115d16ba2842512511913118092da'}, {'text': 'This deployment setup can be tested locally using AWS RIE (runtime interface emulator).\\nBasically, if your Docker image was built upon base AWS Lambda image (FROM public.ecr.aws/lambda/python:3.10) - just use certain ports for “docker run” and a certain “localhost link” for testing:\\ndocker run -it --rm -p 9000:8080 name\\nThis command runs the image as a container and starts up an endpoint locally at:\\nlocalhost:9000/2015-03-31/functions/function/invocations\\nPost an event to the following endpoint using a curl command:\\ncurl -XPOST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d \\'{}\\'\\nExamples of curl testing:\\n* windows testing:\\ncurl -XPOST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d \"{\\\\\"url\\\\\": \\\\\"https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg\\\\\"}\"\\n* unix testing:\\ncurl -XPOST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d \\'{\"url\": \"https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg\"}\\'\\nIf during testing you encounter an error like this:\\n# {\"errorMessage\": \"Unable to marshal response: Object of type float32 is not JSON serializable\", \"errorType\": \"Runtime.MarshalError\", \"requestId\": \"7ea5d17a-e0a2-48d5-b747-a16fc530ed10\", \"stackTrace\": []}\\njust turn your response at lambda_handler() to string - str(result).\\nAdded by Andrii Larkin', 'section': '9. Serverless Deep Learning', 'question': 'How to test AWS Lambda + Docker locally?', 'course': 'machine-learning-zoomcamp', 'doc_id': 'ded28c389c4b0c24d220468caacc29e3'}, {'text': 'Make sure all codes in test.py dont have any dependencies with tensorflow library. One of most common reason that lead the this error is tflite still imported from tensorflow. Change import tensorflow.lite as tflite to import tflite_runtime.interpreter as tflite\\nAdded by Ryan Pramana', 'section': '9. Serverless Deep Learning', 'question': '\"Unable to import module \\'lambda_function\\': No module named \\'tensorflow\\'\" when run python test.py', 'course': 'machine-learning-zoomcamp', 'doc_id': '59628b07aea809626e8f2d1027ae8b3f'}, {'text': 'I’ve tried to do everything in Google Colab. Here is a way to work with Docker in Google Colab:\\nhttps://gist.github.com/mwufi/6718b30761cd109f9aff04c5144eb885\\n\\uec03%%shell\\npip install udocker\\nudocker --allow-root install\\n\\uec02!udocker --allow-root run hello-world\\nAdded by Ivan Brigida\\nLambda API Gateway errors:\\n`Authorization header requires \\'Credential\\' parameter. Authorization header requires \\'Signature\\' parameter. Authorization header requires \\'SignedHeaders\\' parameter. Authorization header requires existence of either a \\'X-Amz-Date\\' or a \\'Date\\' header.`\\n`Missing Authentication Token`\\nimport boto3\\nclient = boto3.client(\\'apigateway\\')\\nresponse = client.test_invoke_method(\\nrestApiId=\\'your_rest_api_id\\',\\nresourceId=\\'your_resource_id\\',\\nhttpMethod=\\'POST\\',\\npathWithQueryString=\\'/test/predict\\', #depend how you set up the api\\nbody=\\'{\"url\": \"https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg\"}\\'\\n)\\nprint(response[\\'body\\'])\\nYishan Zhan\\nUnable to run pip install tflite_runtime from github wheel links?\\nTo overcome this issue, you can download the whl file to your local project folder and in the Docker file add the following lines:\\nCOPY <file-name> .\\nRUN pip install <file-name>\\nAbhijit Chakraborty', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Install Docker (udocker) in Google Colab', 'course': 'machine-learning-zoomcamp', 'doc_id': '37d72ff6b561af17a4f078cfe12f72e4'}, {'text': 'TODO', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'How to get started with Week 10?', 'course': 'machine-learning-zoomcamp', 'doc_id': '0a2852d73cf0ef0ed751377e6d1e56a9'}, {'text': 'Running a CNN on your CPU can take a long time and once you’ve run out of free time on some cloud providers, it’s time to pay up. Both can be tackled by installing tensorflow with CUDA support on your local machine if you have the right hardware.\\nI was able to get it working by using the following resources:\\nCUDA on WSL :: CUDA Toolkit Documentation (nvidia.com)\\nInstall TensorFlow with pip\\nStart Locally | PyTorch\\nI included the link to PyTorch so that you can get that one installed and working too while everything is fresh on your mind. Just select your options, and for Computer Platform, I chose CUDA 11.7 and it worked for me.\\nAdded by Martin Uribe', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'How to install Tensorflow in Ubuntu WSL2', 'course': 'machine-learning-zoomcamp', 'doc_id': 'e98ad19f6a1e5e8195d5adfb94768b65'}, {'text': 'If you are running tensorflow on your own machine and you start getting the following errors:\\nAllocator (GPU_0_bfc) ran out of memory trying to allocate 6.88GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\\nTry adding this code in a cell at the beginning of your notebook:\\nconfig = tf.compat.v1.ConfigProto()\\nconfig.gpu_options.allow_growth = True\\nsession = tf.compat.v1.Session(config=config)\\nAfter doing this most of my issues went away. I say most because there was one instance when I still got the error once more, but only during one epoch. I ran the code again, right after it finished, and I never saw the error again.\\nAdded by Martin Uribe', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Getting: Allocator ran out of memory errors?', 'course': 'machine-learning-zoomcamp', 'doc_id': 'ceb7f4feaeca95bcd1c48af44dd815ae'}, {'text': 'In session 10.3, when creating the virtual environment with pipenv and trying to run the script gateway.py, you might get this error:\\nTypeError: Descriptors cannot not be created directly.\\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\\n1. Downgrade the protobuf package to 3.20.x or lower.\\n2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates\\nThis will happen if your version of protobuf is one of the newer ones. As a workaround, you can fix the protobuf version to an older one. In my case I got around the issue by creating the environment with:\\npipenv install --python 3.9.13 requests grpcio==1.42.0 flask gunicorn \\\\\\nkeras-image-helper tensorflow-protobuf==2.7.0 protobuf==3.19.6\\nAdded by Ángel de Vicente', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Problem with recent version of protobuf', 'course': 'machine-learning-zoomcamp', 'doc_id': '0440ffba02182dd352129ddf89cbdd79'}, {'text': 'Due to the uncertainties associated with machines, sometimes you can get the error message like this when you try to run a docker command:\\n”Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?”\\nSolution: The solution is simple. The Docker Desktop might no longer be connecting to the WSL Linux distro. What you need to do is go to your Docker Desktop setting and then click on resources. Under resources, click on WSL Integration. You will get a tab like the image below:\\nJust enable additional distros. That’s all. Even if the additional distro is the same as the default WSL distro.\\nOdimegwu David', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'WSL Cannot Connect To Docker Daemon', 'course': 'machine-learning-zoomcamp', 'doc_id': '915257c65c5e7ed9e61d164570dc3881'}, {'text': 'In case the HPA instance does not run correctly even after installing the latest version of Metrics Server from the components.yaml manifest with:\\n>>kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\\nAnd the targets still appear as <unknown>\\nRun >>kubectl edit deploy -n kube-system metrics-server\\nAnd search for this line:\\nargs:\\n- --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\\nAdd this line in the middle:  - --kubelet-insecure-tls\\nSo that it stays like this:\\nargs:\\n- --kubelet-insecure-tls\\n- --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\\nSave and run again >>kubectl get hpa\\nAdded by Marilina Orihuela', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'HPA instance doesn’t run properly', 'course': 'machine-learning-zoomcamp', 'doc_id': 'ebe79c3e41db5d90262e26ddeb58a6d0'}, {'text': 'In case the HPA instance does not run correctly even after installing the latest version of Metrics Server from the components.yaml manifest with:\\n>>kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\\nAnd the targets still appear as <unknown>\\nRun the following command:\\nkubectl apply -f https://raw.githubusercontent.com/Peco602/ml-zoomcamp/main/10-kubernetes/kube-config/metrics-server-deployment.yaml\\nWhich uses a metrics server deployment file already embedding the - --kubelet-insecure-tls option.\\nAdded by Giovanni Pecoraro', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'HPA instance doesn’t run properly (easier solution)', 'course': 'machine-learning-zoomcamp', 'doc_id': 'a4cbffa230ddc2cff653ba8ecd714a3f'}, {'text': \"When I run pip install grpcio==1.42.0 tensorflow-serving-api==2.7.0 to install the libraries in windows machine,  I was getting the below error :\\nERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\\\\\Users\\\\\\\\Asia\\\\\\\\anaconda3\\\\\\\\Lib\\\\\\\\site-packages\\\\\\\\google\\\\\\\\protobuf\\\\\\\\internal\\\\\\\\_api_implementation.cp39-win_amd64.pyd'\\nConsider using the `--user` option or check the permissions.\\nSolution description :\\nI was able to install the libraries using below command:\\npip --user install grpcio==1.42.0 tensorflow-serving-api==2.7.0\\nAsia Saeed\", 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Could not install packages due to an OSError: [WinError 5] Access is denied', 'course': 'machine-learning-zoomcamp', 'doc_id': '9d5a8b136403df77c867b27a18c99fe4'}, {'text': 'Problem description\\nI was getting the below error message when I run gateway.py after modifying the code & creating virtual environment in  video 10.3 :\\nFile \"C:\\\\Users\\\\Asia\\\\Data_Science_Code\\\\Zoompcamp\\\\Kubernetes\\\\gat.py\", line 9, in <module>\\nfrom tensorflow_serving.apis import predict_pb2\\nFile \"C:\\\\Users\\\\Asia\\\\.virtualenvs\\\\Kubernetes-Ge6Ts1D5\\\\lib\\\\site-packages\\\\tensorflow_serving\\\\apis\\\\predict_pb2.py\", line 14, in <module>\\nfrom tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\\nFile \"C:\\\\Users\\\\Asia\\\\.virtualenvs\\\\Kubernetes-Ge6Ts1D5\\\\lib\\\\site-packages\\\\tensorflow\\\\core\\\\framework\\\\tensor_pb2.py\", line 14, in <module>\\nfrom tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2\\nFile \"C:\\\\Users\\\\Asia\\\\.virtualenvs\\\\Kubernetes-Ge6Ts1D5\\\\lib\\\\site-packages\\\\tensorflow\\\\core\\\\framework\\\\resource_handle_pb2.py\", line 14, in <module>\\nfrom tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\\nFile \"C:\\\\Users\\\\Asia\\\\.virtualenvs\\\\Kubernetes-Ge6Ts1D5\\\\lib\\\\site-packages\\\\tensorflow\\\\core\\\\framework\\\\tensor_shape_pb2.py\", line 36, in <module>\\n_descriptor.FieldDescriptor(\\nFile \"C:\\\\Users\\\\Asia\\\\.virtualenvs\\\\Kubernetes-Ge6Ts1D5\\\\lib\\\\site-packages\\\\google\\\\protobuf\\\\descriptor.py\", line 560, in __new__\\n_message.Message._CheckCalledFromGeneratedFile()\\nTypeError: Descriptors cannot not be created directly.\\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\\n1. Downgrade the protobuf package to 3.20.x or lower.\\n2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\\nSolution description:\\nIssue has been resolved by downgrading protobuf to version 3.20.1.\\npipenv install protobuf==3.20.1\\nAsia Saeed', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'TypeError: Descriptors cannot not be created directly.', 'course': 'machine-learning-zoomcamp', 'doc_id': '294c230af467061a021c9913e274508e'}, {'text': 'To install kubectl on windows using the terminal in vscode (powershell), I followed this tutorial: https://medium.com/@ggauravsigra/install-kubectl-on-windows-af77da2e6fff\\nI first downloaded kubectl with curl, with these command lines: https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/#install-kubectl-binary-with-curl-on-windows\\nAt step 3, I followed the tutorial with the copy of the exe file in a specific folder on C drive.\\nThen I added this folder path to PATH in my environment variables.\\nKind can be installed the same way with the curl command on windows, by specifying a folder that will be added to the path environment variable.\\nAdded by Mélanie Fouesnard', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'How to install easily kubectl on windows ?', 'course': 'machine-learning-zoomcamp', 'doc_id': 'abeb2197c569223d7a2d2f243c311cbd'}, {'text': \"First you need to launch a powershell terminal with administrator privilege.\\nFor this we need to install choco library first through the following syntax in powershell:\\nSet-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1'))\\nKrishna Anand\", 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Install kind through choco library', 'course': 'machine-learning-zoomcamp', 'doc_id': 'ca51435199986f273f24448c99711933'}, {'text': 'If you are having challenges installing Kind through the Windows Powershell as provided on the website and Choco Library as I did, you can simply install Kind through Go.\\n> Download and Install Go (https://go.dev/doc/install)\\n> Confirm installation by typing the following in Command Prompt -  go version\\n> Proceed by installing Kind by following this command - go install sigs.k8s.io/kind@v0.20.0\\n>Confirm Installation kind --version\\nIt works perfectly.', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Install Kind via Go package', 'course': 'machine-learning-zoomcamp', 'doc_id': '0b057a8fdf9abd478c1e29fab7e6c6c4'}, {'text': \"I ran into an issue where kubectl wasn't working.\\nI kept getting the following error:\\nkubectl get service\\nThe connection to the server localhost:8080 was refused - did you specify the right host or port?\\nI searched online for a resolution, but everyone kept talking about creating an environment variable and creating some admin.config file in my home directory.\\nAll hogwash.\\nThe solution to my problem was to just start over.\\nkind delete cluster\\nrm -rf ~/.kube\\nkind create cluster\\nNow when I try the same command again:\\nkubectl get service\\nNAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\\nkubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   53s\\nAdded by Martin Uribe\", 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'The connection to the server localhost:8080 was refused - did you specify the right host or port?', 'course': 'machine-learning-zoomcamp', 'doc_id': 'ac3c3ce21e1b4ec7872c1e7108d02bac'}, {'text': 'Problem description\\nDue to experimenting back and forth so much without care for storage, I just ran out of it on my 30-GB AWS instance.\\nMy first reflex was to remove some zoomcamp directories, but of course those are mostly code so it didn’t help much.\\nSolution description\\n> docker images\\nrevealed that I had over 20 GBs worth of superseded / duplicate models lying around, so I proceeded to > docker rmi\\na bunch of those — but to no avail!\\nIt turns out that deleting docker images does not actually free up any space as you might expect. After removing images, you also need to run\\n> docker system prune\\nSee also: https://stackoverflow.com/questions/36799718/why-removing-docker-containers-and-images-does-not-free-up-storage-space-on-wind\\nAdded by Konrad Mühlberg', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Running out of storage after building many docker images', 'course': 'machine-learning-zoomcamp', 'doc_id': 'f50665e446a9f2ac310d638d91669521'}, {'text': 'Yes, the question does require for you to specify values for CPU and memory in the yaml file, however the question that it is use in the form only refers to the port which do have a define correct value for this specific homework.\\nPastor Soto', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'In HW10 Q6 what does it mean “correct value for CPU and memory”? Aren’t they arbitrary?', 'course': 'machine-learning-zoomcamp', 'doc_id': 'e4e1c54f2f795ab38bcaf3ec23eef563'}, {'text': 'In Kubernetes resource specifications, such as CPU requests and limits, the \"m\" stands for milliCPU, which is a unit of computing power. It represents one thousandth of a CPU core.\\ncpu: \"100m\" means the container is requesting 100 milliCPUs, which is equivalent to 0.1 CPU core.\\ncpu: \"500m\" means the container has a CPU limit of 500 milliCPUs, which is equivalent to 0.5 CPU core.\\nThese values are specified in milliCPUs to allow fine-grained control over CPU resources. It allows you to express CPU requirements and limits in a more granular way, especially in scenarios where your application might not need a full CPU core.\\nAdded by Andrii Larkin', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Why cpu vals for Kubernetes deployment.yaml look like “100m” and “500m”? What does \"m\" mean?', 'course': 'machine-learning-zoomcamp', 'doc_id': '27cea99988c1266e03a3ae0f9e7e75a0'}, {'text': 'Problem: Failing to load docker-image to cluster (when you’ved named a cluster)\\nkind load docker-image zoomcamp-10-model:xception-v4-001\\nERROR: no nodes found for cluster \"kind\"\\nSolution: Specify cluster name with -n\\nkind -n clothing-model load docker-image zoomcamp-10-model:xception-v4-001\\nAndrew Katoch', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Kind cannot load docker image', 'course': 'machine-learning-zoomcamp', 'doc_id': '0112e5b9e588d63f17f279ed5a3fb9e6'}, {'text': \"Problem: I download kind from the next command:\\ncurl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.17.0/kind-windows-amd64\\nWhen I try\\nkind --version\\nI get: 'kind' is not recognized as an internal or external command, operable program or batch file\\nSolution: The default name of executable is kind-windows-amd64.exe, so that you have to rename this file to  kind.exe. Put this file in specific folder, and add it to PATH\\nAlejandro Aponte\", 'section': '10. Kubernetes and TensorFlow Serving', 'question': \"'kind' is not recognized as an internal or external command, operable program or batch file. (In Windows)\", 'course': 'machine-learning-zoomcamp', 'doc_id': '075bc676aadf8c0ee4c69d72b04ef783'}, {'text': 'Using kind with Rootless Docker or Rootless Podman requires some changes on the system (Linux), see kind – Rootless (k8s.io).\\nSylvia Schmitt', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Running kind on Linux with Rootless Docker or Rootless Podman', 'course': 'machine-learning-zoomcamp', 'doc_id': '75d2e5daff67a186a23360f80ace3039'}, {'text': 'Deploy and Access the Kubernetes Dashboard\\nLuke', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Kubernetes-dashboard', 'course': 'machine-learning-zoomcamp', 'doc_id': 'f581e9ab3fae36f172c3a5c83210c38a'}, {'text': 'Make sure you are on AWS CLI v2 (check with aws --version)\\nhttps://docs.aws.amazon.com/cli/latest/userguide/cliv2-migration-instructions.html', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Correct AWS CLI version for eksctl', 'course': 'machine-learning-zoomcamp', 'doc_id': '0ba10287f39b3d3891b4f20d9a930fe8'}, {'text': 'Problem Description:\\nIn video 10.3, when I was testing a flask service, I got the above error. I ran docker run .. in one terminal. When in second terminal I run python gateway.py, I get the above error.\\nSolution: This error has something to do with versions of Flask and Werkzeug. I got the same error, if I just import flask with from flask import Flask.\\nBy running pip freeze > requirements.txt,I found that their versions are Flask==2.2.2 and Werkzeug==2.2.2. This error appears while using an old version of werkzeug (2.2.2) with new version of flask (2.2.2). I solved it by pinning version of Flask into an older version with pipenv install Flask==2.1.3.\\nAdded by Bhaskar Sarma', 'section': '10. Kubernetes and TensorFlow Serving', 'question': \"TypeError: __init__() got an unexpected keyword argument 'unbound_message' while importing Flask\", 'course': 'machine-learning-zoomcamp', 'doc_id': 'ab74a5e4a79eaaa12fe484085939d60c'}, {'text': 'As per AWS documentation:\\nhttps://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html\\nYou need to do: (change the fields in red)\\naws ecr get-login-password --region region | docker login --username AWS --password-stdin aws_account_id.dkr.ecr.region.amazonaws.com\\nAlternatively you can run the following command without changing anything given you have a default region configured\\naws ecr get-login-password --region $(aws configure get region) | docker login --username AWS --password-stdin \"$(aws sts get-caller-identity --query \"Account\" --output text).dkr.ecr.$(aws configure get region).amazonaws.com\"\\nAdded by Humberto Rodriguez', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Command aws ecr get-login --no-include-email returns “aws: error: argument operation: Invalid choice…”', 'course': 'machine-learning-zoomcamp', 'doc_id': '4e1366f3af8cc429a3400e12d7a9d13c'}, {'text': 'While trying to run the docker code on M1:\\ndocker run --platform linux/amd64 -it --rm \\\\\\n-p 8500:8500 \\\\\\n-v $(pwd)/clothing-model:/models/clothing-model/1 \\\\\\n-e MODEL_NAME=\"clothing-model\" \\\\\\ntensorflow/serving:2.7.0\\nIt outputs the error:\\nError:\\nStatus: Downloaded newer image for tensorflow/serving:2.7.0\\n[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/generated_message_reflection.cc:2345] CHECK failed: file != nullptr:\\nterminate called after throwing an instance of \\'google::protobuf::FatalException\\'\\nwhat():  CHECK failed: file != nullptr:\\nqemu: uncaught target signal 6 (Aborted) - core dumped\\n/usr/bin/tf_serving_entrypoint.sh: line 3:     8 Aborted                 tensorflow_model_server --port=8500 --rest_api_port=8501 --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \"$@\"\\nSolution\\ndocker pull emacski/tensorflow-serving:latest\\ndocker run -it --rm \\\\\\n-p 8500:8500 \\\\\\n-v $(pwd)/clothing-model:/models/clothing-model/1 \\\\\\n-e MODEL_NAME=\"clothing-model\" \\\\\\nemacski/tensorflow-serving:latest-linux_arm64\\nSee more here: https://github.com/emacski/tensorflow-serving-arm\\nAdded by Daniel Egbo', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Error downloading  tensorflow/serving:2.7.0 on Apple M1 Mac', 'course': 'machine-learning-zoomcamp', 'doc_id': 'cc50497d6aee99da4ff4262bc737fc05'}, {'text': 'Similar to the one above but with a different solution the main reason is that emacski doesn’t seem to maintain the repo any more, the latest image is from 2 years ago at the time of writing (December 2023)\\nProblem:\\nWhile trying to run the docker code on Mac M2 apple silicon:\\ndocker run --platform linux/amd64 -it --rm \\\\\\n-p 8500:8500 \\\\\\n-v $(pwd)/clothing-model:/models/clothing-model/1 \\\\\\n-e MODEL_NAME=\"clothing-model\" \\\\\\ntensorflow/serving\\nYou get an error:\\n/usr/bin/tf_serving_entrypoint.sh: line 3:     7 Illegal instruction     tensorflow_model_server --port=8500 --rest_api_port=8501 --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \"$@\"\\nSolution:\\nUse bitnami/tensorflow-serving base image\\nLaunch it either using docker run\\ndocker run -d \\\\\\n--name tf_serving \\\\\\n-p 8500:8500 \\\\\\n-p 8501:8501 \\\\\\n-v $(pwd)/clothing-model:/bitnami/model-data/1 \\\\\\n-e TENSORFLOW_SERVING_MODEL_NAME=clothing-model \\\\\\nbitnami/tensorflow-serving:2\\nOr the following docker-compose.yaml\\nversion: \\'3\\'\\nservices:\\ntf_serving:\\nimage: bitnami/tensorflow-serving:2\\nvolumes:\\n- ${PWD}/clothing-model:/bitnami/model-data/1\\nports:\\n- 8500:8500\\n- 8501:8501\\nenvironment:\\n- TENSORFLOW_SERVING_MODEL_NAME=clothing-model\\nAnd run it with\\ndocker compose up\\nAdded by Alex Litvinov', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Illegal instruction error when running tensorflow/serving image on Mac M2 Apple Silicon (potentially on M1 as well)', 'course': 'machine-learning-zoomcamp', 'doc_id': '2ce105dde7ed320837722951c2211f00'}, {'text': 'Problem: CPU metrics Shows Unknown\\nNAME         REFERENCE           TARGETS         MINPODS   MAXPODS   REPLICAS   AGE\\ncredit-hpa   Deployment/credit   <unknown>/20%   1         3         1          18s\\nFailedGetResourceMetric       2m15s (x169 over 44m)  horizontal-pod-autoscaler  failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API:\\nSolution:\\n-> Delete HPA (kubectl delete hpa credit-hpa)\\n-> kubectl apply -f https://raw.githubusercontent.com/pythianarora/total-practice/master/sample-kubernetes-code/metrics-server.yaml\\n-> Create HPA\\nThis should solve the cpu metrics report issue.\\nAdded by Priya V', 'section': '11. KServe', 'question': 'HPA doesn’t show CPU metrics', 'course': 'machine-learning-zoomcamp', 'doc_id': 'b06639a62405b002972be022c08411a3'}, {'text': 'Problem description:\\nRunning this:\\ncurl -s \"https://raw.githubusercontent.com/kserve/kserve/release-0.9/hack/quick_install.sh\" | bash\\nFails with errors because of istio failing to update resources, and you are on kubectl > 1.25.0.\\nCheck kubectl version with kubectl version\\nSolution description\\nEdit the file “quick_install.bash” by downloading it with curl without running bash. Edit the versions of Istio and Knative as per the matrix on the KServe website.\\nRun the bash script now.\\nAdded by Andrew Katoch', 'section': '11. KServe', 'question': 'Errors with istio during installation', 'course': 'machine-learning-zoomcamp', 'doc_id': 'e61c6e5eca9d79791178b763984ad543'}, {'text': 'Problem description\\nSolution description\\n(optional) Added by Name', 'section': 'Projects (Midterm and Capstone)', 'question': 'Problem title', 'course': 'machine-learning-zoomcamp', 'doc_id': '1467e5ede07206cf5952dd4e3f97719e'}, {'text': 'Answer: You can see them here (it’s taken from the 2022 cohort page). Go to the cohort folder for your own cohort’s deadline.', 'section': 'Projects (Midterm and Capstone)', 'question': 'What are the project deadlines?', 'course': 'machine-learning-zoomcamp', 'doc_id': 'ace5e52c7be8c00aa5e25dbd88c14861'}, {'text': 'Answer: All midterms and capstones are meant to be solo projects. [source @Alexey]', 'section': 'Projects (Midterm and Capstone)', 'question': 'Are projects solo or collaborative/group work?', 'course': 'machine-learning-zoomcamp', 'doc_id': '2353e349b07f608580ce6edc9f8c0080'}, {'text': 'Answer: Ideally midterms up to module-06, capstones include all modules in that cohort’s syllabus. But you can include anything extra that you want to feature. Just be sure to document anything not covered in class.\\nAlso watch office hours from previous cohorts. Go to DTC youtube channel and click on Playlists and search for {course yyyy}. ML Zoomcamp was first launched in 2021.\\nMore discussions:\\n[source1] [source2] [source3]', 'section': 'Projects (Midterm and Capstone)', 'question': 'What modules, topics, problem-sets should a midterm/capstone project cover? Can I do xyz?', 'course': 'machine-learning-zoomcamp', 'doc_id': '5f3ab56d8fa9eb9235c76866f0e44203'}, {'text': \"These links apply to all projects, actually. Again, for some cohorts, the modules/syllabus might be different, so always check in your cohort’s folder as well for additional or different instructions, if any.\\nMidterm Project Sample: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp/cohorts/2021/07-midterm-project\\nMidTerm Project Deliverables: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp/projects\\nSubmit MidTerm Project: https://docs.google.com/forms/d/e/1FAIpQLSfgmOk0QrmHu5t0H6Ri1Wy_FDVS8I_nr5lY3sufkgk18I6S5A/viewform\\nDatasets:\\nhttps://www.kaggle.com/datasets and https://www.kaggle.com/competitions\\nhttps://archive.ics.uci.edu/ml/index.php\\nhttps://data.europa.eu/en\\nhttps://www.openml.org/search?type=data\\nhttps://newzealand.ai/public-data-sets\\nhttps://datasetsearch.research.google.com\\nWhat to do and Deliverables\\nThink of a problem that's interesting for you and find a dataset for that\\nDescribe this problem and explain how a model could be used\\nPrepare the data and doing EDA, analyze important features\\nTrain multiple models, tune their performance and select the best model\\nExport the notebook into a script\\nPut your model into a web service and deploy it locally with Docker\\nBonus points for deploying the service to the cloud\", 'section': 'Projects (Midterm and Capstone)', 'question': 'Crucial Links', 'course': 'machine-learning-zoomcamp', 'doc_id': '6871008d8570757c7fd465d42e0594e3'}, {'text': 'Answer: Previous cohorts projects page has instructions (youtube).\\nhttps://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2022/projects.md#midterm-project\\nAlexey and his team will compile a g-sheet with links to submitted projects with our hashed emails (just like when we check leaderboard for homework) that are ours to review within the evaluation deadline.\\n~~~ Added by Nukta Bhatia ~~~', 'section': 'Projects (Midterm and Capstone)', 'question': 'How to conduct peer reviews for projects?', 'course': 'machine-learning-zoomcamp', 'doc_id': 'fc8bcc0c2c53cd932edab7239d3103a4'}, {'text': 'See the answer here.', 'section': 'Projects (Midterm and Capstone)', 'question': 'Computing the hash for project review', 'course': 'machine-learning-zoomcamp', 'doc_id': 'ab88940ae0bc1f047182302d0a3047a2'}, {'text': 'For the learning in public for this midterm project it seems that has a total value of 14!, Does this mean that we need make 14 posts?, Or the regular seven posts for each module and each one with a value of 2?, Or just one with a total value of 14?\\n14 posts, one for each day', 'section': 'Projects (Midterm and Capstone)', 'question': 'Learning in public links for the projects', 'course': 'machine-learning-zoomcamp', 'doc_id': 'c189205da73c79259cf1099a8c7ff7b2'}, {'text': 'You can use git-lfs (https://git-lfs.com/) for upload large file to github repository.\\nRyan Pramana', 'section': 'Projects (Midterm and Capstone)', 'question': \"My dataset is too large and I can't loaded in GitHub , do anyone knows about a solution?\", 'course': 'machine-learning-zoomcamp', 'doc_id': '94c75e7781e43c7c9d2b049669f38bd1'}, {'text': 'If you have submitted two projects (and peer-reviewed at least 3 course-mates’ projects for each submission), you will get the certificate for the course. According to the course coordinator, Alexey Grigorev, only two projects are needed to get the course certificate.\\n(optional) David Odimegwu', 'section': 'Projects (Midterm and Capstone)', 'question': 'What If I submitted only two projects and failed to submit the third?', 'course': 'machine-learning-zoomcamp', 'doc_id': 'ca63f072ddca8fb2ed87bf2fca34f69e'}, {'text': 'Yes. You only need to review peers when you submit your project.\\nConfirmed on Slack by Alexey Grigorev (added by Rileen Sinha)', 'section': 'Projects (Midterm and Capstone)', 'question': \"I did the first two projects and skipped the last one so I wouldn't have two peer review in second capstone right?\", 'course': 'machine-learning-zoomcamp', 'doc_id': '5056561ffc1494759b96754946154e7b'}, {'text': 'Regarding Point 4 in the midterm deliverables, which states, \"Train multiple models, tune their performance, and select the best model,\" you might wonder, how many models should you train? The answer is simple: train as many as you can. The term \"multiple\" implies having more than one model, so as long as you have more than one, you\\'re on the right track.', 'section': 'Projects (Midterm and Capstone)', 'question': 'How many models should I train?', 'course': 'machine-learning-zoomcamp', 'doc_id': '2d6c03861f5e54f688123ea688ae572b'}, {'text': 'I am not sure how the project evaluate assignment works? Where do I find this? I have access to all the capstone 2 project, perhaps, I can randomly pick any to review.\\nAnswer:\\nThe link provided for example (2023/Capstone link ): https://docs.google.com/forms/d/e/1FAIpQLSdgoepohpgbM4MWTAHWuXa6r3NXKnxKcg4NDOm0bElAdXdnnA/viewform contains a list of all submitted projects to be evaluated. More specific, you are to review 3 assigned peer projects. In the spreadsheet are 3 hash values of your assigned peer projects. However, you need to derive the your hash value of your email address and find the value on the spreadsheet under the (reviewer_hash) heading.\\nTo calculate your hash value run the python code below:\\nfrom hashlib import sha1\\ndef compute_hash(email):\\nreturn sha1(email.lower().encode(\\'utf-8\\')).hexdigest()\\n# Example usage **** enter your email below (Example1@gmail.com)****\\nemail = \"Example1@gmail.com\"\\nhashed_email = compute_hash(email)\\nprint(\"Original Email:\", email)\\nprint(\"Hashed Email (SHA-1):\", hashed_email)\\nEdit the above code to replace Example1@gmail.com as your email address\\nStore and run the above python code from your terminal. See below as the Hashed Email (SHA-1) value\\nYou then go to the link: https://docs.google.com/spreadsheets/d/e/2PACX-1vR-7RRtq7AMx5OzI-tDbkzsbxNLm-NvFOP5OfJmhCek9oYcDx5jzxtZW2ZqWvBqc395UZpHBv1of9R1/pubhtml?gid=876309294&single=true\\nLastly, copy the “Hashed Email (SHA-1): bd9770be022dede87419068aa1acd7a2ab441675” value and search for 3 identical entries. There you should see your peer project to be reviewed.\\nBy Emmanuel Ayeni', 'section': 'Projects (Midterm and Capstone)', 'question': 'How does the project evaluation work for you as a peer reviewer?', 'course': 'machine-learning-zoomcamp', 'doc_id': 'ac3315adba73af2c744392977b855d93'}, {'text': 'Alexey Grigorev: “It’s based on all the scores to make sure most of you pass.”                                                   By Annaliese Bronz\\nOther course-related questions that don’t fall into any of the categories above or can apply to more than one category/module', 'section': 'Miscellaneous', 'question': 'Do you pass a project based on the average of everyone else’s scores or based on the total score you earn?', 'course': 'machine-learning-zoomcamp', 'doc_id': '5fe708a5fba5b58eded57b557f10c188'}, {'text': 'Answer: The train.py file will be used by your peers to review your midterm project. It is for them to cross-check that your training process works on someone else’s system. It should also be included in the environment in conda or with pipenv.\\nOdimegwu David', 'section': 'Miscellaneous', 'question': 'Why do I need to provide a train.py file when I already have the notebook.ipynb file?', 'course': 'machine-learning-zoomcamp', 'doc_id': 'cbe3eab9746a2abed3f8bcc80368a41d'}, {'text': \"Pip install pillow - install pillow library\\nfrom PIL import Image\\nimg = Image.open('aeroplane.png')\\nFrom numpy import asarray\\nnumdata=asarray(img)\\nKrishna Anand\", 'section': 'Miscellaneous', 'question': 'Loading the Image with PILLOW library and converting to numpy array', 'course': 'machine-learning-zoomcamp', 'doc_id': '151ad0962b2cb9bb352255df00611290'}, {'text': \"Ans: train.py has to be a python file. This is because running a python script for training a model is much simpler than running a notebook and that's how training jobs usually look like in real life.\", 'section': 'Miscellaneous', 'question': 'Is a train.py file necessary when you have a train.ipynb file in your midterm project directory?', 'course': 'machine-learning-zoomcamp', 'doc_id': '6d53c71ee5b7ddb2286f5cc5963cfd19'}, {'text': 'Yes, you can create a mobile app or interface that manages these forms and validations. But you should also perform validations on backend.\\nYou can also check Streamlit: https://github.com/DataTalksClub/project-of-the-week/blob/main/2022-08-14-frontend.md\\nAlejandro Aponte', 'section': 'Miscellaneous', 'question': 'Is there a way to serve up a form for users to enter data for the model to crunch on?', 'course': 'machine-learning-zoomcamp', 'doc_id': '4df9c654ac3b0b58d392e91684a1051e'}, {'text': \"Using model.feature_importances_ can gives you an error:\\nAttributeError: 'Booster' object has no attribute 'feature_importances_'\\nAnswer: if you train the model like this: model = xgb.train you should use get_score() instead\\nEkaterina Kutovaia\", 'section': 'Miscellaneous', 'question': 'How to get feature importance for XGboost model', 'course': 'machine-learning-zoomcamp', 'doc_id': '4f66960804eb377e09f8731539827458'}, {'text': 'In the Elastic Container Service task log, error “[Errno 12] Cannot allocate memory” showed up.\\nJust increase the RAM and CPU in your task definition.\\nHumberto Rodriguez', 'section': 'Miscellaneous', 'question': '[Errno 12] Cannot allocate memory in AWS Elastic Container Service', 'course': 'machine-learning-zoomcamp', 'doc_id': 'c20b18871c86fca4d29b26ea37369f66'}, {'text': \"When running a docker container with waitress serving the app.py for making predictions, pickle will throw an error that can't get attribute <name_of_class> on module __main__.\\nThis does not happen when Flask is used directly, i.e. not through waitress.\\nThe problem is that the model uses a custom column transformer class, and when the model was saved, it was saved from the __main__ module (e.g. python train.py). Pickle will reference the class in the global namespace (top-level code): __main__.<custom_class>.\\nWhen using waitress, waitress will load the predict_app module and this will call pickle.load, that will try to find __main__.<custom_class> that does not exist.\\nSolution:\\nPut the class into a separate module and import it in both the script that saves the model (e.g. train.py) and the script that loads the model (e.g. predict.py)\\nNote: If Flask is used (no waitress) in predict.py, and predict.py has the definition of the class, When  it is run: python predict.py, it will work because the class is in the same namespace as the one used when the model was saved (__main__).\\nDetailed info: https://stackoverflow.com/questions/27732354/unable-to-load-files-using-pickle-and-multiple-modules\\nMarcos MJD\", 'section': 'Miscellaneous', 'question': 'Pickle error: can’t get attribute XXX on module __main__', 'course': 'machine-learning-zoomcamp', 'doc_id': 'abc1a129f3f84f8670ca97b7c8226f8a'}, {'text': 'There are different techniques, but the most common used are the next:\\nDataset transformation (for example, log transformation)\\nClipping high values\\nDropping these observations\\nAlena Kniazeva', 'section': 'Miscellaneous', 'question': 'How to handle outliers in a dataset?', 'course': 'machine-learning-zoomcamp', 'doc_id': '85dd4a4e49b90feb5320b368eae7b3c0'}, {'text': 'I was getting the below error message when I was trying to create docker image using bentoml\\n[bentoml-cli] `serve` failed: Failed loading Bento from directory /home/bentoml/bento: Failed to import module \"service\": No module named \\'sklearn\\'\\nSolution description\\nThe cause was because , in bentofile.yaml, I wrote sklearn instead of scikit-learn. Issue was fixed after I modified the packages list as below.\\npackages: # Additional pip packages required by the service\\n- xgboost\\n- scikit-learn\\n- pydantic\\nAsia Saeed', 'section': 'Miscellaneous', 'question': 'Failed loading Bento from directory /home/bentoml/bento: Failed to import module \"service\": No module named \\'sklearn\\'', 'course': 'machine-learning-zoomcamp', 'doc_id': '3b652cf0bc53a2cd353195841f32f8cf'}, {'text': \"You might see a long error message with something about sparse matrices, and in the swagger UI, you get a code 500 error with “” (empty string) as output.\\nPotential reason: Setting DictVectorizer or OHE to sparse while training, and then storing this in a pipeline or custom object in the benotml model saving stage in train.py. This means that when the custom object is called in service.py, it will convert each input to a different sized sparse matrix, and this can't be batched due to inconsistent length. In this case, bentoml model signatures should have batchable set to False for production during saving the bentoml mode in train.py.\\n(Memoona Tahira)\", 'section': 'Miscellaneous', 'question': 'BentoML not working with –production flag at any stage: e.g. with bentoml serve and while running the bentoml container', 'course': 'machine-learning-zoomcamp', 'doc_id': '28f6ff86960aed563603acd5424957c0'}, {'text': 'Problem description:\\nDo we have to run everything?\\nYou are encouraged, if you can, to run them. As this provides another opportunity to learn from others.\\nNot everyone will be able to run all the files, in particular the neural networks.\\nSolution description:\\nAlternatively, can you see that everything you need to reproduce is there: the dataset is there, the instructions are there, are there any obvious errors and so on.\\nRelated slack conversation here.\\n(Gregory Morris)', 'section': 'Miscellaneous', 'question': 'Reproducibility', 'course': 'machine-learning-zoomcamp', 'doc_id': 'd2b2794b865902bbc7a43170740bad8f'}, {'text': \"If your model is too big for github one option is to try and compress the model using joblib. For example joblib.dump(model, model_filename, compress=('zlib', 6) will use zlib to compress the model. Just note this could take a few moments as the model is being compressed.\\nQuinn Avila\", 'section': 'Miscellaneous', 'question': 'Model too big', 'course': 'machine-learning-zoomcamp', 'doc_id': 'd9b44fa2c323742f5aa77a872f561df1'}, {'text': \"When you try to push the docker image to Google Container Registry and get this message “unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials.”, type this below on console, but first install https://cloud.google.com/sdk/docs/install, this is to be able to use gcloud in console:\\ngcloud auth configure-docker\\n(Jesus Acuña)\", 'section': 'Miscellaneous', 'question': 'Permissions to push docker to Google Container Registry', 'course': 'machine-learning-zoomcamp', 'doc_id': 'e19b6fad0d25ecd80ba0f19a06c75d1f'}, {'text': 'I am getting this error message when I tried to install tflite in a pipenv environment\\nError:  An error occurred while installing tflite_runtime!\\nError text:\\nERROR: Could not find a version that satisfies the requirement tflite_runtime (from versions: none)\\nERROR: No matching distribution found for tflite_runtime\\nThis version of tflite do not run on python 3.10, the way we can make it work is by install python 3.9, after that it would install the tflite_runtime without problem.\\nPastor Soto\\nCheck all available versions here:\\nhttps://google-coral.github.io/py-repo/tflite-runtime/\\nIf you don’t find a combination matching your setup, try out the options at\\nhttps://github.com/alexeygrigorev/tflite-aws-lambda/tree/main/tflite\\nwhich you can install as shown in the lecture, e.g.\\npip install https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.7.0-cp38-cp38-linux_x86_64.whl\\nFinally, if nothing works, use the TFLite included in TensorFlow for local development, and use Docker for testing Lambda.\\nRileen Sinha (based on discussions on Slack)', 'section': 'Miscellaneous', 'question': 'Tflite_runtime unable to install', 'course': 'machine-learning-zoomcamp', 'doc_id': '4cdb7e28200277e80e4fb376801dd66a'}, {'text': \"Error: ImageDataGenerator name 'scipy' is not defined.\\nCheck that scipy is installed in your environment.\\nRestart jupyter kernel and try again.\\nMarcos MJD\", 'section': 'Miscellaneous', 'question': 'Error when running ImageDataGenerator.flow_from_dataframe', 'course': 'machine-learning-zoomcamp', 'doc_id': 'f68ad91be6c09ac117e7f5f7ce2137c1'}, {'text': 'Tim from BentoML has prepared a dedicated video tutorial wrt this use case here:\\nhttps://www.youtube.com/watch?v=7gI1UH31xb4&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=97\\nKonrad Muehlberg', 'section': 'Miscellaneous', 'question': 'How to pass BentoML content / docker container to Amazon Lambda', 'course': 'machine-learning-zoomcamp', 'doc_id': '4b0823f5c7fc68d962111c3cbe35b33b'}, {'text': \"In deploying model part, I wanted to test my model locally on a test-image data and I had this silly error after the following command:\\nurl = 'https://github.com/bhasarma/kitchenware-classification-project/blob/main/test-image.jpg'\\nX = preprocessor.from_url(url)\\nI got the error:\\nUnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x7f797010a590>\\nSolution:\\nAdd ?raw=true after .jpg in url. E.g. as below\\nurl = ‘https://github.com/bhasarma/kitchenware-classification-project/blob/main/test-image.jpg?raw=true’\\nBhaskar Sarma\", 'section': 'Miscellaneous', 'question': 'Error UnidentifiedImageError: cannot identify image file', 'course': 'machine-learning-zoomcamp', 'doc_id': '7231e71b11e0556966ad0963f15af2af'}, {'text': 'Problem: If you run pipenv install and get this message. Maybe manually change Pipfile and Pipfile.lock.\\nSolution: Run: ` pipenv lock` for fix this problem and dependency files\\nAlejandro Aponte', 'section': 'Miscellaneous', 'question': '[pipenv.exceptions.ResolutionFailure]: Warning: Your dependencies could not be resolved. You likely have a mismatch in your sub-dependencies', 'course': 'machine-learning-zoomcamp', 'doc_id': 'e9a81ff9b453184cebfa12b4df363785'}, {'text': 'Problem: In the course this function worked to get the features from the dictVectorizer instance: dv.get_feature_names(). But in my computer did not work. I think it has to do with library versions and but apparently that function will be deprecated soon:\\nOld: https://scikit-learn.org/0.22/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.get_feature_names\\nNew: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.get_feature_names\\nSolution: change the line dv.get_feature_names() to list(dv.get_feature_names_out))\\nIbai Irastorza', 'section': 'Miscellaneous', 'question': 'Get_feature_names() not found', 'course': 'machine-learning-zoomcamp', 'doc_id': 'fcf2af4e2a6f37ad1cc51776a42e8c90'}, {'text': 'Problem happens when contacting the server waiting to send your predict-test and your data here in the correct shape.\\nThe problem was the format input to the model wasn’t in the right shape. Server receives the data in json format (dict) which is not suitable for the model. U should convert it to like numpy arrays.\\nAhmed Okka', 'section': 'Miscellaneous', 'question': 'Error decoding JSON response: Expecting value: line 1 column 1 (char 0)', 'course': 'machine-learning-zoomcamp', 'doc_id': '3bf31b54530e664bf31290e56aff2e02'}, {'text': \"Q: Hii folks, I tried deploying my docker image on Render, but it won't I get SIGTERM everytime.\\nI think .5GB RAM is not enough, is there any other free alternative available ?\\nA: aws (amazon), gcp (google), saturn.\\nBoth aws and gcp give microinstance for free for a VERY long time, and a bunch more free stuff.\\nSaturn even provides free GPU instances. Recent promo link from mlzoomcamp for Saturn:\\n“You can sign up here: https://bit.ly/saturn-mlzoomcamp\\nWhen you sign up, write in the chat box that you're an ML Zoomcamp student and you should get extra GPU hours (something like 150)”\\nAdded by Andrii Larkin\", 'section': 'Miscellaneous', 'question': 'Free cloud alternatives', 'course': 'machine-learning-zoomcamp', 'doc_id': 'da4de71db0570bbbcc2c5bab9059e476'}, {'text': \"Problem description: I have one column day_of_the_month . It has values 1, 2, 20, 25 etc. and int . I have a second column month_of_the_year. It has values jan, feb, ..dec. and are string. I want to convert these two columns into one column day_of_the_year and I want them to be int. 2 and jan should give me 2, i.e. 2nd day of the year, 1 and feb should give me 32, i.e. 32 nd day of the year. What is the simplest pandas-way to do it?\\nSolution description:\\nconvert dtype in day_of_the_month column from int to str with df['day_of_the_month'] = df['day_of_the_month'].map(str)\\nconvert month_of_the_year column in jan, feb ...,dec into 1,2, ..,12 string using map()\\nconvert day and month into a datetime object with:\\ndf['date_formatted'] = pd.to_datetime(\\ndict(\\nyear='2055',\\nmonth=df['month'],\\nday=df['day']\\n)\\n)\\nget day of year with: df['day_of_year']=df['date_formatted'].dt.dayofyear\\n(Bhaskar Sarma)\", 'section': 'Miscellaneous', 'question': 'Getting day of the year from day and month column', 'course': 'machine-learning-zoomcamp', 'doc_id': '19590d745940b3ac629c9d3a5fc0ffff'}, {'text': 'How to visualize the predictions per classes after training a neural net\\nSolution description\\nclasses, predictions = zip(*dict(zip(classes, predictions)).items())\\nplt.figure(figsize=(12, 3))\\nplt.bar(classes, predictions)\\nLuke', 'section': 'Miscellaneous', 'question': 'Chart for classes and predictions', 'course': 'machine-learning-zoomcamp', 'doc_id': 'f442f5f66810a0a9962be0a6ac4d87e8'}, {'text': 'You can convert the prediction output values to a datafarme using \\ndf = pd.DataFrame.from_dict(dict, orient=\\'index\\' , columns=[\"Prediction\"])\\nEdidiong Esu', 'section': 'Miscellaneous', 'question': 'Convert dictionary values to Dataframe table', 'course': 'machine-learning-zoomcamp', 'doc_id': '38a09ec7bb690b7e82b2b986533662b2'}, {'text': 'The image dataset for the competition was in a different layout from what we used in the dino vs dragon lesson. Since that’s what was covered, some folks were more comfortable with that setup, so I wrote a script that would generate it for them\\nIt can be found here: kitchenware-dataset-generator | Kaggle\\nMartin Uribe', 'section': 'Miscellaneous', 'question': 'Kitchenware Classification Competition Dataset Generator', 'course': 'machine-learning-zoomcamp', 'doc_id': 'e159bc922cbe7b2b8836ce4e5447bf14'}, {'text': 'Install Nvidia drivers: https://www.nvidia.com/download/index.aspx.\\nWindows:\\nInstall Anaconda prompt https://www.anaconda.com/\\nTwo options:\\nInstall package ‘tensorflow-gpu’ in Anaconda\\nInstall the Tensorflow way https://www.tensorflow.org/install/pip#windows-native\\nWSL/Linux:\\nWSL: Use the Windows Nvida drivers, do not touch that.\\nTwo options:\\nInstall the Tensorflow way https://www.tensorflow.org/install/pip#linux_1\\nMake sure to follow step 4 to install CUDA by environment\\nAlso run:\\necho ‘export XLA_FLAGS=--xla_gpu_cuda_data_dir=$CONDA_PREFIX/lib/> $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh\\nInstall CUDA toolkit 11.x.x https://developer.nvidia.com/cuda-toolkit-archive\\nInstall https://developer.nvidia.com/rdp/cudnn-download\\nNow you should be able to do training/inference with GPU in Tensorflow\\n(Learning in public links Links to social media posts where you share your progress with others (LinkedIn, Twitter, etc). Use #mlzoomcamp tag. The scores for this part will be capped at 7 points. Please make sure the posts are valid URLs starting with \"https://\" Does it mean that I should provide my linkedin link? or it means that I should write a post that I have completed my first assignement? (\\nANS (by ezehcp7482@gmail.com): Yes, provide the linkedIN link to where you posted.\\nezehcp7482@gmail.com:\\nPROBLEM: Since I had to put up a link to a public repository, I had to use Kaggle and uploading the dataset therein was a bit difficult; but I had to ‘google’ my way out.\\nANS: See this link for a guide (https://www.kaggle.com/code/dansbecker/finding-your-files-in-kaggle-kernels/notebook)', 'section': 'Miscellaneous', 'question': 'CUDA toolkit and cuDNN Install for Tensorflow', 'course': 'machine-learning-zoomcamp', 'doc_id': '6df8f72f8265cd9b94b8515f5b054473'}, {'text': 'When multiplying matrices, the order of multiplication is important.\\nFor example:\\nA (m x n) * B (n x p) = C (m x p)\\nB (n x p) * A (m x n) = D (n x n)\\nC and D are matrices of different sizes and usually have different values. Therefore the order is important in matrix multiplication and changing the order changes the result.\\nBaran Akın', 'section': 'Miscellaneous', 'question': 'About getting the wrong result when multiplying matrices', 'course': 'machine-learning-zoomcamp', 'doc_id': 'a3ec382a0376921c029a4718936b4c5a'}, {'text': 'Refer to https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/01-intro/06-environment.md\\n(added by Rileen Sinha)', 'section': 'Miscellaneous', 'question': 'None of the videos have how to install the environment in Mac, does someone have instructions for Mac with M1 chip?', 'course': 'machine-learning-zoomcamp', 'doc_id': 'b54b0caeec9e361c6bc8dd1e5d0740b8'}, {'text': \"Depends on whether the form will still be open. If you're lucky and it's open, you can submit your homework and it will be evaluated. if closed - it's too late.\\n(Added by Rileen Sinha, based on answer by Alexey on Slack)\", 'section': 'Miscellaneous', 'question': 'I may end up submitting the assignment late. Would it be evaluated?', 'course': 'machine-learning-zoomcamp', 'doc_id': '3c6fa58fac642deaf805cbf7360a47c2'}, {'text': 'Yes. Whoever corrects the homework will only be able to access the link if the repository is public.\\n(added by Tano Bugelli)\\nHow to install Conda environment in my local machine?\\nWhich ide is recommended for machine learning?', 'section': 'Miscellaneous', 'question': 'Does the github repository need to be public?', 'course': 'machine-learning-zoomcamp', 'doc_id': '85615f5486c458fa1fd40b5e6b3ca204'}, {'text': 'Install w get:\\n!which wget\\nDownload data:\\n!wget -P /content/drive/My\\\\ Drive/Downloads/ URL\\n(added by Paulina Hernandez)', 'section': 'Miscellaneous', 'question': 'How to use wget with Google Colab?', 'course': 'machine-learning-zoomcamp', 'doc_id': 'df45abafe1046415f746199c0ec2adc3'}, {'text': \"Features (X) must always be formatted as a 2-D array to be accepted by scikit-learn.\\nUse reshape to reshape a 1D array to a 2D.\\n\\t\\t\\t\\t\\t\\t\\t(-Aileah) :>\\n(added by Tano\\nfiltered_df = df[df['ocean_proximity'].isin(['<1H OCEAN', 'INLAND'])]\\n# Select only the desired columns\\nselected_columns = [\\n'latitude',\\n'longitude',\\n'housing_median_age',\\n'total_rooms',\\n'total_bedrooms',\\n'population',\\n'households',\\n'median_income',\\n'median_house_value'\\n]\\nfiltered_df = filtered_df[selected_columns]\\n# Display the first few rows of the filtered DataFrame\\nprint(filtered_df.head())\", 'section': 'Miscellaneous', 'question': 'Features in scikit-learn?', 'course': 'machine-learning-zoomcamp', 'doc_id': '16f99fe1e7e90ef8756210ba20130a94'}, {'text': 'FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead', 'section': 'Miscellaneous', 'question': 'When I plotted using Matplot lib to check if median has a tail, I got the error below how can one bypass?', 'course': 'machine-learning-zoomcamp', 'doc_id': '03001f0b00ed4abe5cc1d53a7d5bf5a2'}, {'text': 'When trying to rerun the docker file in Windows, as opposed to developing in WSL/Linux, I got the error of:\\n```\\nWarning: Python 3.11 was not found on your system…\\nNeither ‘pipenv’ nor ‘asdf’ could be found to install Python.\\nYou can specify specific versions of Python with:\\n$ pipenv –python path\\\\to\\\\python\\n```\\nThe solution was to add Python311 installation folder to the PATH and restart the system and run the docker file again. That solved the error.\\n(Added by Abhijit Chakraborty)', 'section': 'Miscellaneous', 'question': 'Reproducibility in different OS', 'course': 'machine-learning-zoomcamp', 'doc_id': 'b5e135ae63fe68074963969844d36afa'}, {'text': 'You may quickly deploy your project to DigitalOcean App Cloud. The process is relatively straightforward. The deployment costs about 5 USD/month. The container needs to be up until the end of the project evaluation.\\nSteps:\\nRegister in DigitalOcean\\nGo to Apps -> Create App.\\nYou will need to choose GitHub as a service provider.\\nEdit Source Directory (if your project is not in the repo root)\\nIMPORTANT: Go to settings -> App Spec and edit the Dockerfile path so it looks like ./project/Dockerfile path relative to your repo root\\nRemember to add model files if they are not built automatically during the container build process.\\nBy Dmytro Durach', 'section': 'Miscellaneous', 'question': 'Deploying to Digital Ocean', 'course': 'machine-learning-zoomcamp', 'doc_id': '926b920089f94c8f3bb09217cc2b70e5'}, {'text': \"I’m just looking back at the lessons in week 3 (churn prediction project), and lesson 3.6 talks about Feature Importance for categorical values. At 8.12, the mutual info scores show that the some features are more important than others, but then in lesson 3.10 the Logistic Regression model is trained on all of the categorical variables (see 1:35). Once we have done feature importance, is it best to train your model only on the most important features?\\nNot necessarily - rather, any feature that can offer additional predictive value should be included (so, e.g. predict with & without including that feature; if excluding it drops performance, keep it, else drop it). A few individually important features might in fact be highly correlated with others, & dropping some might be fine. There are many feature selection algorithms, it might be interesting to read up on them (among the methods we've learned so far in this course, L1 regularization (Lasso) implicitly does feature selection by shrinking some weights all the way to zero).\\nBy Rileen Sinha\", 'section': 'Miscellaneous', 'question': 'Is it best to train your model only on the most important features?', 'course': 'machine-learning-zoomcamp', 'doc_id': '8691f27730fb77d7f618b1ea6b907b88'}, {'text': 'You can consider several different approaches:\\nSampling: In the exploratory phase, you can use random samples of the data.\\nChunking: When you do need all the data, you can read and process it in chunks that do fit in the memory.\\nOptimizing data types: Pandas’ automatic data type inference (when reading data in) might result in e.g. float64 precision being used to represent integers, which wastes space. You might achieve substantial memory reduction by optimizing the data types.\\nUsing Dask, an open-source python project which parallelizes Numpy and Pandas.\\n(see, e.g. https://www.vantage-ai.com/en/blog/4-strategies-how-to-deal-with-large-datasets-in-pandas)\\nBy Rileen Sinha', 'section': 'Miscellaneous', 'question': 'How can I work with very large datasets, e.g. the New York Yellow Taxi dataset, with over a million rows?', 'course': 'machine-learning-zoomcamp', 'doc_id': '6d3f457298134de9c80764a2ed50458d'}, {'text': 'Technically, yes. Advisable? Not really. Reasons:\\nSome homework(s) asks for specific python library versions.\\nAnswers may not match in MCQ options if using different languages other than Python 3.10 (the recommended version for 2023 cohort)\\nAnd as for midterms/capstones, your peer-reviewers may not know these other languages. Do you want to be penalized for others not knowing these other languages?\\nYou can create a separate repo using course’s lessons but written in other languages for your own learnings, but not advisable for submissions.\\ntx[source]', 'section': 'Miscellaneous', 'question': 'Can I do the course in other languages, like R or Scala?', 'course': 'machine-learning-zoomcamp', 'doc_id': '09b79fee79cb13c29b69bb607492fac0'}, {'text': 'Yes, it’s allowed (as per Alexey).\\nAdded By Rileen Sinha', 'section': 'Miscellaneous', 'question': 'Is use of libraries like fast.ai or huggingface allowed in the capstone and competition, or are they considered to be \"too much help\"?', 'course': 'machine-learning-zoomcamp', 'doc_id': '260001ea4e3531efaad000d9a02a3aae'}, {'text': 'The TF and TF Serving versions have to match (as per solution from the slack channel)\\nAdded by Chiedu Elue', 'section': 'Miscellaneous', 'question': 'Flask image was built and tested successfully, but tensorflow serving image was built and unable to test successfully. What could be the problem?', 'course': 'machine-learning-zoomcamp', 'doc_id': 'c4c8b27c0fe69d03955c85cbe6f01a90'}, {'text': 'I’ve seen LinkedIn users list DataTalksClub as Experience with titles as:\\nMachine Learning Fellow\\nMachine Learning Student\\nMachine Learning Participant\\nMachine Learning Trainee\\nPlease note it is best advised that you do not list the experience as an official “job” or “internship” experience since DataTalksClub did not hire you, nor financially compensate you.\\nOther ways you can incorporate the experience in the following sections:\\nOrganizations\\nProjects\\nSkills\\nFeatured\\nOriginal posts\\nCertifications\\nCourses\\nBy Annaliese Bronz\\nInteresting question, I put the link of my project into my CV as showcase and make posts to show my progress.\\nBy Ani Mkrtumyan', 'section': 'Miscellaneous', 'question': 'Any advice for adding the Machine Learning Zoomcamp experience to your LinkedIn profile?', 'course': 'machine-learning-zoomcamp', 'doc_id': '488fac1e084f401cff20a96e222c668f'}, {'text': 'MLOps Zoomcamp FAQ\\nThe purpose of this document is to capture frequently asked technical questions.\\nWe did this for our data engineering course, and it worked quite well. Check this document for inspiration on how to structure your questions and answers:\\nData Engineering Zoomcamp FAQ\\n[Problem description]\\n[Solution description]\\n(optional) Added by Name', 'section': '+-General course questions', 'question': 'Format for questions: [Problem title]', 'course': 'mlops-zoomcamp', 'doc_id': 'de5571f2b29aba5627b6ddd455741919'}, {'text': 'Approximately 3 months. For each module, about 1 week with possible deadline extensions (in total 6~9 weeks), 2 weeks for working on the capstone project and 1 week for peer review.', 'section': '+-General course questions', 'question': 'What is the expected duration of this course or that for each module?', 'course': 'mlops-zoomcamp', 'doc_id': 'd020b236701aa37c5dc2d3d6f31bae7e'}, {'text': 'The difference is the Orchestration and Monitoring modules. Those videos will be re-recorded. The rest should mostly be the same.\\nAlso all of the homeworks will be changed for the 2023 cohort.', 'section': '+-General course questions', 'question': 'What’s the difference between the 2023 and 2022 course?', 'course': 'mlops-zoomcamp', 'doc_id': 'f37ba2636bea20f7a957a4587cfad0d9'}, {'text': 'Yes, it will start in May 2024', 'section': '+-General course questions', 'question': 'Will there be a 2024 Cohort? When will the 2024 cohort start?', 'course': 'mlops-zoomcamp', 'doc_id': 'ac9e1fb42e479db5281670651ebc96c8'}, {'text': 'Please choose the closest one to your answer. Also do not post your answer in the course slack channel.', 'section': '+-General course questions', 'question': 'What if my answer is not exactly the same as the choices presented?', 'course': 'mlops-zoomcamp', 'doc_id': 'a5efe489cdfcbad1ec90da6b8637d6a9'}, {'text': 'Please pick up a problem you want to solve yourself. Potential datasets can be found on either Kaggle, Hugging Face, Google, AWS, or the UCI Machine Learning Datasets Repository.', 'section': '+-General course questions', 'question': 'Are we free to choose our own topics for the final project?', 'course': 'mlops-zoomcamp', 'doc_id': 'f83dfca33a0ff6cab84de25836b1b414'}, {'text': 'In order to obtain the certificate, completion of the final capstone project is mandatory. The completion of weekly homework assignments is optional, but they can contribute to your overall progress and ranking on the top 100 leaderboard.', 'section': '+-General course questions', 'question': 'Can I still graduate when I didn’t complete homework for week x?', 'course': 'mlops-zoomcamp', 'doc_id': 'ea4a7f64506743f953f09eefbf42278a'}, {'text': 'You can get a few cloud points by using kubernetes even if you deploy it only locally. Or you can use local stack too to mimic AWS\\nAdded by Ming Jun, Asked by Ben Pacheco, Answered by Alexey Grigorev', 'section': 'Module 1: Introduction', 'question': 'For the final project, is it required to be put on the cloud?', 'course': 'mlops-zoomcamp', 'doc_id': '5104db77514719ad3938b9c1013f213d'}, {'text': 'For those who are not using VSCode (or other similar IDE), you can automate port-forwarding for Jupyter Notebook by adding the following line of code to your\\n~/.ssh/config file (under the mlops-zoomcamp host):\\nLocalForward 127.0.0.1:8899 127.0.0.1:8899\\nThen you can launch Jupyter Notebook using the following command: jupyter notebook --port=8899 --no-browser and copy paste the notebook URL into your browser.\\nAdded by Vishal', 'section': 'Module 1: Introduction', 'question': 'Port-forwarding without Visual Studio', 'course': 'mlops-zoomcamp', 'doc_id': '16104e9253a9a6e6775c35b9eb9df0b7'}, {'text': 'You can install the Jupyter extension to open notebooks in VSCode.\\nAdded by Khubaib', 'section': 'Module 1: Introduction', 'question': 'Opening Jupyter in VSCode', 'course': 'mlops-zoomcamp', 'doc_id': 'e33dab806bfef18c4638888031782458'}, {'text': 'In case one would like to set a github repository (e.g. for Homeworks), one can follow 2 great tutorials that helped a lot\\nSetting up github on AWS instance - this\\nSetting up keys on AWS instance - this\\nThen, one should be able to push to its repo\\nAdded by Daniel Hen (daniel8hen@gmail.com)', 'section': 'Module 1: Introduction', 'question': 'Configuring Github to work from the remote VM', 'course': 'mlops-zoomcamp', 'doc_id': 'a2cfc7ff1df75af95e3a531f82fd7f32'}, {'text': \"Faced issue while setting up JUPYTER NOTEBOOK on AWS. I was unable to access it from my desktop. (I am not using visual studio and hence faced problem)\\nRun\\njupyter notebook --generate-config\\nEdit file /home/ubuntu/.jupyter/jupyter_notebook_config.py to add following line:\\nNotebookApp.ip = '*'\\nAdded by Atul Gupta (samatul@gmail.com)\", 'section': 'Module 1: Introduction', 'question': 'Opening Jupyter in AWS', 'course': 'mlops-zoomcamp', 'doc_id': '094e4e81d7f4f9c4160ffdadc19fd8c4'}, {'text': 'If you wish to use WSL on your windows machine, here are the setup instructions:\\nCommand: Sudo apt install wget\\nGet Anaconda download address here. wget <download address>\\nTurn on Docker Desktop WFree Download | AnacondaSL2\\nCommand: git clone <github repository address>\\nVSCODE on WSL\\nJupyter: pip3 install jupyter\\nAdded by Gregory Morris (gwm1980@gmail.com)\\nAll in all softwares at one shop:\\nYou can use anaconda which has all built in services like pycharm, jupyter\\nAdded by Khaja Zaffer (khajazaffer@aln.iseg.ulisboa.pt)\\nFor windows “wsl --install” in Powershell\\nAdded by Vadim Surin (vdmsurin@gmai.com)', 'section': 'Module 1: Introduction', 'question': 'WSL instructions', 'course': 'mlops-zoomcamp', 'doc_id': '4869e57c522676ac7043af1a05abc14e'}, {'text': 'If you create a folder data and download datasets or raw files in your local repository. Then to push all your code to remote repository without this files or folder please use gitignore file. The simple way to create it do the following steps\\n1. Create empty .txt file (using text editor or command line)\\n2. Safe as .gitignore (. must use the dot symbol)\\n3. Add rules\\n *.parquet - to ignore all parquet files\\ndata/ - to ignore all files in folder data\\n\\nFor more pattern read GIT documentation\\nhttps://git-scm.com/docs/gitignore\\nAdded by Olga Rudakova (olgakurgan@gmail.com)', 'section': 'Module 1: Introduction', 'question': '.gitignore how-to', 'course': 'mlops-zoomcamp', 'doc_id': '54a0bc5e96c8a8f0c4f6dcb22fe13342'}, {'text': \"Make sure when you stop an EC2 instance that it actually stops (there's a meme about it somewhere). There are green circles (running), orange (stopping), and red (stopped). Always refresh the page to make sure you see the red circle and status of stopped.\\nEven when an EC2 instance is stopped, there WILL be other charges that are incurred (e.g. if you uploaded data to the EC2 instance, this data has to be stored somewhere, usually an EBS volume and this storage incurs a cost).\\nYou can set up billing alerts. (I've never done this, so no advice on how to do this).\\n(Question by: Akshit Miglani (akshit.miglani09@gmail.com) and Answer by Anna Vasylytsya)\", 'section': 'Module 1: Introduction', 'question': 'AWS suggestions', 'course': 'mlops-zoomcamp', 'doc_id': '4a4574e5ef12fcfccfd7075a7c75a4eb'}, {'text': 'You can get invitation code by coursera and use it in account to verify it it has different characteristics.\\nI really love it\\nhttps://www.youtube.com/watch?v=h_GdX6KtXjo', 'section': 'Module 1: Introduction', 'question': 'IBM Cloud an alternative for AWS', 'course': 'mlops-zoomcamp', 'doc_id': '3a11ec901cb051b7faf0bd2f4eff8344'}, {'text': \"I am worried about the cost of keeping an AWS instance running during the course.\\nWith the instance specified during working environment setup, if you remember to Stop Instance once you finished your work for the day.  Using that strategy, in a day with about 5 hours of work you will pay around $0.40 USD which will account for $12 USD per month, which seems to be an affordable amount.\\nYou must remember that you would have a different IP public address every time you Restart your instance, and you would need to edit your ssh Config file.  It's worth the time though.\\nAdditionally, AWS enables you to set up an automatic email alert if a predefined budget is exceeded.\\nHere is a tutorial to set this up.\\nAlso, you can estimate the cost yourself, using AWS pricing calculator (to use it you don’t even need to be logged in).\\nAt the time of writing (20.05.2023) t3a.xlarge instance with 2 hr/day usage (which translates to 10 hr/week that should be enough to complete the course) and 30GB EBS monthly cost is 10.14 USD\\nHere’s a link to the estimate\\nAdded by Alex Litvinov (aaalex.lit@gmail.com)\", 'section': 'Module 1: Introduction', 'question': 'AWS costs', 'course': 'mlops-zoomcamp', 'doc_id': '9b211b68d0c9a20e97d056c37533762a'}, {'text': 'For many parts - yes. Some things like kinesis are not in AWS free tier, but you can do it locally with localstack.', 'section': 'Module 1: Introduction', 'question': 'Is the AWS free tier enough for doing this course?', 'course': 'mlops-zoomcamp', 'doc_id': '97f3149c35490eb1be805c4e6ce66e44'}, {'text': 'When I click an open IP-address in an AWS EC2 instance I get an error: “This site can’t be reached”. What should I do?\\nThis ip-address is not required to be open in a browser. It is needed to connect to the running EC2 instance via terminal from your local machine or via terminal from a remote server with such command, for example if:\\nip-address is 11.111.11.111\\ndownloaded key name is razer.pem (the key should be moved to a hidden folder .ssh)\\nyour user name is user_name\\nssh -i /Users/user_name/.ssh/razer.pem ubuntu@11.111.11.111', 'section': 'Module 1: Introduction', 'question': 'AWS EC2: this site can’t be reached', 'course': 'mlops-zoomcamp', 'doc_id': 'a64dffd54ced277b03e5bceb88262bdc'}, {'text': 'After this command `ssh -i ~/.ssh/razer.pem ubuntu@XX.XX.XX.XX` I got this error: \"unprotected private key file\". This page (https://99robots.com/how-to-fix-permission-error-ssh-amazon-ec2-instance/) explains how to fix this error. Basically you need to change the file permissions of the key file with this command: chmod 400 ~/.ssh/razer.pem', 'section': 'Module 1: Introduction', 'question': 'Unprotected private key file!', 'course': 'mlops-zoomcamp', 'doc_id': '0dcc74c08245ec55d3102debc32de14a'}, {'text': 'My SSH connection to AWS cannot last more than a few minutes, whether via terminal or VS code.\\nMy config:\\n# Copy Configuration in local nano editor, then Save it!\\nHost mlops-zoomcamp                                         # ssh connection calling name\\nUser ubuntu                                             # username AWS EC2\\nHostName <instance-public-IPv4-addr>                    # Public IP, it changes when Source EC2 is turned off.\\nIdentityFile ~/.ssh/name-of-your-private-key-file.pem   # Private SSH key file path\\nLocalForward 8888 localhost:8888                        # Connecting to a service on an internal network from the outside, static forward or set port user forward via on vscode\\nStrictHostKeyChecking no\\nAdded by Muhammed Çelik\\nThe disconnection will occur whether I SSH via WSL2 or via VS Code, and usually occurs after I run some code, i.e. “import mlflow”, so not particularly intense computation.\\nI cannot reconnect to the instance without stopping and restarting with a new IPv4 address.\\nI’ve gone through steps listed on this page: https://aws.amazon.com/premiumsupport/knowledge-center/ec2-linux-resolve-ssh-connection-errors/\\nInbound rule should allow all incoming IPs for SSH.\\nWhat I expect to happen:\\nSSH connection should remain while I’m actively using the instance, and if it does disconnect, I should be able to reconnect back.\\nSolution: sometimes the hang ups are caused by the instance running out of memory. In one instance, using EC2 feature to view screenshot of the instance as a means to troubleshoot, it was the OS out-of-memory feature which killed off some critical processes. In this case, if we can’t use a higher compute VM with more RAM, try adding a swap file, which uses the disk as RAM substitute and prevents the OOM error. Follow Ubuntu’s documentation here: https://help.ubuntu.com/community/SwapFaq.\\nAlternatively follow AWS’s own doc, which mirrors Ubuntu’s: https://aws.amazon.com/premiumsupport/knowledge-center/ec2-memory-swap-file/', 'section': 'Module 1: Introduction', 'question': 'AWS EC2 instance constantly drops SSH connection', 'course': 'mlops-zoomcamp', 'doc_id': 'cb0315f608ed4c3c6539d08ac6cf7524'}, {'text': 'Everytime I restart my EC2 instance I keep getting different IP and need to update the config file manually.\\n\\nSolution: You can create a script like this to automatically update the IP address of your EC2 instance.https://github.com/dimzachar/mlops-zoomcamp/blob/master/notes/Week_1/update_ssh_config.md', 'section': 'Module 1: Introduction', 'question': 'AWS EC2 IP Update', 'course': 'mlops-zoomcamp', 'doc_id': 'ef418500228056c9f1931686175edb85'}, {'text': 'Make sure to use an instance with enough compute capabilities such as a t2.xlarge. You can check the monitoring tab in the EC2 dashboard to monitor your instance.', 'section': 'Module 1: Introduction', 'question': 'VS Code crashes when connecting to Jupyter', 'course': 'mlops-zoomcamp', 'doc_id': 'cb37aa1c58b620f36a4b682b1bf0b86b'}, {'text': 'Error “ValueError: X has 526 features, but LinearRegression is expecting 525 features as input.” when running your Linear Regression Model on the validation data set:\\nSolution: The DictVectorizer creates an initial mapping for the features (columns). When calling the DictVecorizer again for the validation dataset transform should be used as it will ignore features that it did not see when fit_transform was last called. E.g.\\nX_train = dv.fit_transform(train_dict)\\nX_test = dv.transform(test_dict)', 'section': 'Module 1: Introduction', 'question': 'X has 526 features, but expecting 525 features', 'course': 'mlops-zoomcamp', 'doc_id': '0f608fdaff538d8adccaa417145713ed'}, {'text': 'If some dependencies are missing\\nInstall following packages\\npandas\\nmatplotlib\\nscikit-learn\\nfastparquet\\npyarrow\\nseaborn\\npip install -r requirements.txt\\nI have seen this error when using pandas.read_parquet(), the solution is to install pyarrow or fastparquet by doing !pip install pyarrow in the notebook\\nNOTE: if you’re using Conda instead of pip, install fastparquet rather than pyarrow, as it is much easier to install and it’s functionally identical to pyarrow for our needs.', 'section': 'Module 1: Introduction', 'question': 'Missing dependencies', 'course': 'mlops-zoomcamp', 'doc_id': 'fdbdaed1bb6cfa7ce6bed60ee183e9ee'}, {'text': 'The evaluation RMSE I get doesn’t figure within the options!\\nIf you’re evaluating the model on the entire February data, try to filter outliers using the same technique you used on the train data (0≤duration≤60) and you’ll get a RMSE which is (approximately) in the options. Also don’t forget to convert the columns data types to str before using the DictVectorizer.\\nAnother option: Along with filtering outliers, additionally filter on null values by replacing them with -1.  You will get a RMSE which is (almost same as) in the options. Use ‘.round(2)’ method to round it to 2 decimal points.\\nWarning deprecation\\nThe python interpreter warning of modules that have been deprecated  and will be removed in future releases as well as making suggestion how to go about your code.\\nFor example\\nC:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\seaborn\\\\distributions.py:2619:\\nFutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\\nwarnings.warn(msg, FutureWarning)\\nTo suppress the warnings, you can include this code at the beginning of your notebook\\nimport warnings\\nwarnings.filterwarnings(\"ignore\")', 'section': 'Module 1: Introduction', 'question': 'No RMSE value in the options', 'course': 'mlops-zoomcamp', 'doc_id': '0e14f0199be6021f2f4d52314305f13a'}, {'text': 'sns.distplot(df_train[\"duration\"])\\nCan be replaced with\\nsns.histplot(\\ndf_train[\"duration\"] , kde=True,\\nstat=\"density\", kde_kws=dict(cut=3), bins=50,\\nalpha=.4, edgecolor=(1, 1, 1, 0.4),\\n)\\nTo get almost identical result', 'section': 'Module 1: Introduction', 'question': 'How to replace distplot with histplot', 'course': 'mlops-zoomcamp', 'doc_id': '8dcf139e3f9238821cdf6f4e412eb0de'}, {'text': 'You need to replace the capital letter “L” with a small one “l”', 'section': 'Module 1: Introduction', 'question': \"KeyError: 'PULocationID'  or  'DOLocationID'\", 'course': 'mlops-zoomcamp', 'doc_id': '5cd639ad001fe27b79fd0cb3939f62f7'}, {'text': 'I have faced a problem while reading the large parquet file. I tried some workarounds but they were NOT successful with Jupyter.\\nThe error message is:\\nIndexError: index 311297 is out of bounds for axis 0 with size 131743\\nI solved it by performing the homework directly as a python script.\\nAdded by Ibraheem Taha (ibraheemtaha91@gmail.com)\\nYou can try using the Pyspark library\\nAnswered by kamaldeen (kamaldeen32@gmail.com)', 'section': 'Module 1: Introduction', 'question': 'Reading large parquet files', 'course': 'mlops-zoomcamp', 'doc_id': '206ddc41c00d74c24164480ca7ee7a10'}, {'text': 'First remove the outliers (trips with unusual duration) before plotting\\nAdded by Ibraheem Taha (ibraheemtaha91@gmail.com)', 'section': 'Module 1: Introduction', 'question': 'Distplot takes too long', 'course': 'mlops-zoomcamp', 'doc_id': '90b885d78f8c3c3e929594dc0d9ebe16'}, {'text': 'Problem: RMSE on test set was too high when hot encoding the validation set with a previously fitted OneHotEncoder(handle_unknown=’ignore’) on the training set, while DictVectorizer would yield the correct RMSE.\\nIn principle both transformers should behave identically when treating categorical features (at least in this week’s homework where we don’t have sequences of strings in each row):\\nFeatures are put into binary columns encoding their presence (1) or absence (0)\\nUnknown categories are imputed as zeroes in the hot-encoded matrix', 'section': 'Module 1: Introduction', 'question': 'RMSE on test set too high', 'course': 'mlops-zoomcamp', 'doc_id': '786af40ca7da8563e4fb939e029ffe95'}, {'text': 'A: Alexey’s answer https://www.youtube.com/watch?v=8uJ36ZZr_Is&t=13s\\nIn summary,\\npd.get_dummies or OHE can come up with result in different orders and handle missing data differently, so train and val set would have different columns during train and validation\\nDictVectorizer would ignore missing (in train) and new (in val) datasets\\nOther sources:\\nhttps://datascience.stackexchange.com/questions/9443/when-to-use-one-hot-encoding-vs-labelencoder-vs-dictvectorizor\\nhttps://scikit-learn.org/stable/modules/feature_extraction.html\\nhttps://innovation.alteryx.com/encode-smarter/\\n~ ellacharmed', 'section': 'Module 1: Introduction', 'question': 'Q: Using of OneHotEncoder instead of DictVectorizer', 'course': 'mlops-zoomcamp', 'doc_id': 'a2f77d04d827c58779fa5f8ba276180e'}, {'text': \"Why didn't get_dummies in pandas library or OneHotEncoder in scikit-learn library be used for one-hot encoding? I know OneHotEncoder is the most common and useful. One-hot coding can also be done using the eye or identity components of the NumPy library.\\nM.Sari\\nOneHotEncoder has the option to output a row column tuple matrix. DictVectorizer is a one step method to encode and support row column tuple matrix output.\\nHarinder(sudwalh@gmail.com)\", 'section': 'Module 1: Introduction', 'question': 'Q: Why did we not use OneHotEncoder(sklearn) instead of DictVectorizer ?', 'course': 'mlops-zoomcamp', 'doc_id': '02138bceab351188ea1d056b1ad99378'}, {'text': 'How to check that we removed the outliers?\\nUse the pandas function describe() which can provide a report of the data distribution along with the statistics to describe the data. For example, after clipping the outliers using boolean expression, the min and max can be verified using\\ndf[‘duration’].describe()', 'section': 'Module 1: Introduction', 'question': 'Clipping outliers', 'course': 'mlops-zoomcamp', 'doc_id': 'bfe8a2fd46f6ffcfe4dc4700330a3ca1'}, {'text': 'pd.get_dummies and DictVectorizer both create a one-hot encoding on string values. Therefore you need to convert the values in PUlocationID and DOlocationID to string.\\nIf you convert the values in PUlocationID and DOlocationID from numeric to string, the NaN values get converted to the string \"nan\".  With DictVectorizer the RMSE is the same whether you use \"nan\" or \"-1\" as string representation for the NaN values. Therefore the representation doesn\\'t have to be \"-1\" specifically, it could also be some other string.', 'section': 'Module 1: Introduction', 'question': 'Replacing NaNs for pickup location and drop off location with -1 for One-Hot Encoding', 'course': 'mlops-zoomcamp', 'doc_id': 'c26077350764aff451ace6f4f937a562'}, {'text': 'Problem: My LinearRegression RSME is very close to the answer but not exactly the same. Is this normal?\\nAnswer: No, LinearRegression is an deterministic model, it should always output the same results when given the same inputs.\\nAnswer:\\nCheck if you have treated the outlier properly for both train and validation sets\\nCheck if the one hot encoding has been done properly by looking at the shape of one hot encoded feature matrix. If it shows 2 features, there is wrong with one hot encoding. Hint: the drop off and pick up codes need to be converted to proper data format and then DictVectorizer is fitted.\\nHarshit Lamba (hlamba19@gmail.com)', 'section': 'Module 1: Introduction', 'question': 'Slightly different RSME', 'course': 'mlops-zoomcamp', 'doc_id': '7d35018068805f3b826caf902377acd7'}, {'text': 'Problem: I’m facing an extremely low RMSE score (eg: 4.3451e-6) - what shall I do?\\nAnswer: Recheck your code to see if your model is learning the target prior to making the prediction. If the target variable is passed in as a parameter while fitting the model, chances are the model would score extremely low. However, that’s not what you would want and would much like to have your model predict that. A good way to check that is to make sure your X_train doesn’t contain any part of your y_train. The same stands for validation too.\\nSnehangsu De (desnehangsu@gmail.com)', 'section': 'Module 1: Introduction', 'question': 'Extremely low RSME', 'course': 'mlops-zoomcamp', 'doc_id': 'eaafb995cb99444531a5ef33b71f780b'}, {'text': 'Problem: how to enable auto completion in jupyter notebook? Tab doesn’t work for me\\nSolution: !pip install --upgrade jedi==0.17.2\\nChristopher R.J.(romanjaimesc@gmail.com)', 'section': 'Module 1: Introduction', 'question': 'Enabling Auto-completion in jupyter notebook', 'course': 'mlops-zoomcamp', 'doc_id': 'e2730accf21f2599e48b43ec688d3d42'}, {'text': \"Problem: While following the steps in the videos you may have problems trying to download with wget the files. Usually it is a 403 error type (Forbidden access).\\nSolution: The links point to files on cloudfront.net, something like this:\\nhttps://d37ci6vzurychx.cloudfront.net/tOSError: Could not open parquet input source '<Buffer>': Invalid: Parquet OSError: Could not open parquet input source '<Buffer>': Invalid: Parquet rip+data/green_tripdata_2021-01.parquet\\nI’m not download the dataset directly, i use dataset URL and run this in the file.\\nUpdate(27-May-2023): Vikram\\nI am able to download the data from the below link. This is from the official  NYC trip record page (https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page). Copy link from page directly as the below url might get changed if the NYC decides to move away from this. Go to the page , right click and use copy link.\\nwget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2021-01.parquet\\n(Asif)\\nCopy the link address and replace the cloudfront.net part with s3.amazonaws.com/nyc-tlc/, so it looks like this:\\nhttps://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2021-01.parquet\\nMario Tormo (mario@tormo-romero.eu)\\nOSError: Could not open parquet input source '<Buffer>': Invalid: Parquet\", 'section': 'Module 1: Introduction', 'question': 'Downloading the data from the NY Taxis datasets gives error : 403 Forbidden', 'course': 'mlops-zoomcamp', 'doc_id': '8f0f92a59fa0c721f284b19e68c27195'}, {'text': 'Problem: PyCharm (remote) doesn’t see conda execution path. So, I cannot use conda env (which is located on a remote server).\\nSolution: In remote server in command line write “conda activate envname”, after write “which python” - it gives you python execution path. After you can use this path when you will add new interpreter in PyCharm: add local interpreter -> system interpreter -> and put the path with python.\\nSalimov Ilnaz (salimovilnaz777@gmail.com)', 'section': 'Module 1: Introduction', 'question': 'Using PyCharm & Conda env in remote development', 'course': 'mlops-zoomcamp', 'doc_id': 'ec75504b4eb9aa6fa03201a6a4b2c357'}, {'text': 'Problem: The output of DictVectorizer was taking up too much memory. So much so, that I couldn’t even fit the linear regression model before running out of memory on my 16 GB machine.\\nSolution: In the example for DictVectorizer in the scikit-learn website, they set the parameter “sparse” as False. Although this helps with viewing the results, this results in a lot of memory usage. The solution is to either use “sparse=True” instead, or leave it at the default which is also True.\\nAhmed Fahim (afahim03@yahoo.com)', 'section': 'Module 1: Introduction', 'question': 'Running out of memory', 'course': 'mlops-zoomcamp', 'doc_id': 'a0fa3d1b33c448fef8124e3af7565e7e'}, {'text': 'Problem: For me, Installing anaconda didn’t modify the .bashrc profile. That means Anaconda env was not activated even after exiting and relaunching the unix shell.\\nSolution:\\nFor bash : Initiate conda again, which will add entries for anaconda in .bashrc file.\\n$ cd YOUR_PATH_ANACONDA/bin $ ./conda init bash\\nThat will automatically edit your .bashrc.\\nReload:\\n$ source ~/.bashrc\\nAhamed Irshad (daisyfuentesahamed@gmail.com)', 'section': 'Module 1: Introduction', 'question': 'Activating Anaconda env in .bashrc', 'course': 'mlops-zoomcamp', 'doc_id': '4bf86ad66627592964535a008f098fbf'}, {'text': 'While working through the HW1, you will realize that the training and the validation data set feature sizes are different. I was trying to figure out why and went down the entire rabbit hole only to see that I wasn’t doing ```transform``` on the premade dictionary vectorizer instead of ```fit_transform```. You already have the dictionary vectorizer made so no need to execute the fit pipeline on the model.\\nSam Lim(changhyeonlim@gmail.com)', 'section': 'Module 1: Introduction', 'question': 'The feature size is different for training set and validation set', 'course': 'mlops-zoomcamp', 'doc_id': '013b0f2cc2675be9065927ea161fb5d7'}, {'text': 'I found a good guide how to get acces to your machine again when you removed your public key.\\nUsing the following link you can go to Session Manager and log in to your instance and create public key again. https://repost.aws/knowledge-center/ec2-linux-fix-permission-denied-errors\\nThe main problem for me here was to get my old public key, so for doing this you should run the following command: ssh-keygen -y -f /path_to_key_pair/my-key-pair.pem\\nFor more information: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/describe-keys.html#retrieving-the-public-key\\nHanna Zhukavets (a.zhukovec1901@gmail.com)', 'section': 'Module 1: Introduction', 'question': 'Permission denied (publickey) Error (when you remove your public key on the AWS machine)', 'course': 'mlops-zoomcamp', 'doc_id': '0e0321eeab46d2da8f108961d1192211'}, {'text': 'Problem: The February dataset has been used as a validation/test dataset and been stripped of the outliers in a similar manner to the train dataset (taking only the rows for the duration between 1 and 60, inclusive). The RMSE obtained afterward is in the thousands.\\nAnswer: The sparsematrix result from DictVectorizer shouldn’t be turned into an ndarray. After removing that part of the code, I ended up receiving a correct result .\\nTahina Mahatoky (tahinadanny@gmail.com)', 'section': 'Module 1: Introduction', 'question': 'Overfitting: Absurdly high RMSE on the validation dataset', 'course': 'mlops-zoomcamp', 'doc_id': 'e17359f221b43593ad1470632dcca645'}, {'text': 'more specific error line:\\nfrom sklearn.feature_extraction import DictVectorizer\\nI had this issue and to solve it I did\\n!pip install scikit-learn\\nJoel Auccapuclla (auccapuclla 2013@gmail.com)', 'section': 'Module 2: Experiment tracking', 'question': 'Can’t import sklearn', 'course': 'mlops-zoomcamp', 'doc_id': '5971bf10e1fbd24c9068eafc1e3ceb8e'}, {'text': 'Problem: Localhost:5000 Unavailable // Access to Localhost Denied // You don’t have authorization to view this page (127.0.0.1:5000)\\n\\nSolution: If you are on an chrome browser you need to head to `chrome://net-internals/#sockets` and press “Flush Socket Pools”', 'section': 'Module 2: Experiment tracking', 'question': 'Access Denied at Localhost:5000 - Authorization Issue', 'course': 'mlops-zoomcamp', 'doc_id': '249a160aa0bcbcbd116fef138f714142'}, {'text': \"You have something running on the 5000 port. You need to stop it.\\nAnswer: On terminal in mac .\\nRun ps -A | grep gunicorn\\nLook for the number process id which is the 1st number after running the command\\nkill 13580\\nwhere 13580  represents the process number.\\nSource\\nwarrie.warrieus@gmail.com\\nOr by executing the following command it will kill all the processes using port 5000:\\n>> sudo fuser -k 5000/tcp\\nAnswered by Vaibhav Khandelwal\\nJust execute in the command below in he command line to kill the running port\\n->> kill -9 $(ps -A | grep python | awk '{print $1}')\\nAnswered by kamaldeen (kamaldeen32@gmail.com)\\nChange to different port (5001 in this case)\\n>> mlflow ui --backend-store-uri sqlite:///mlflow.db --port 5001\\nAnswered by krishna (nellaikrishna@gmail.com)\", 'section': 'Module 2: Experiment tracking', 'question': \"Connection in use: ('127.0.0.1', 5000)\", 'course': 'mlops-zoomcamp', 'doc_id': '307c3f7cfa8ac3b19acbd554afe88bb0'}, {'text': 'Running python register_model.py results in the following error:\\nValueError: could not convert string to float: \\'0 int\\\\n1   float\\\\n2     hyperopt_param\\\\n3       Literal{n_estimators}\\\\n4       quniform\\\\n5         Literal{10}\\\\n6         Literal{50}\\\\n7         Literal{1}\\'\\nFull Traceback:\\nTraceback (most recent call last):\\nFile \"/Users/name/Desktop/Programming/DataTalksClub/MLOps-Zoomcamp/2. Experiment tracking and model management/homework/scripts/register_model.py\", line 101, in <module>\\nrun(args.data_path, args.top_n)\\nFile \"/Users/name/Desktop/Programming/DataTalksClub/MLOps-Zoomcamp/2. Experiment tracking and model management/homework/scripts/register_model.py\", line 67, in run\\ntrain_and_log_model(data_path=data_path, params=run.data.params)\\nFile \"/Users/name/Desktop/Programming/DataTalksClub/MLOps-Zoomcamp/2. Experiment tracking and model management/homework/scripts/register_model.py\", line 41, in train_and_log_model\\nparams = space_eval(SPACE, params)\\nFile \"/Users/name/miniconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/hyperopt/fmin.py\", line 618, in space_eval\\nrval = pyll.rec_eval(space, memo=memo)\\nFile \"/Users/name/miniconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/hyperopt/pyll/base.py\", line 902, in rec_eval\\nrval = scope._impls[node.name](*args, **kwargs)\\nValueError: could not convert string to float: \\'0 int\\\\n1   float\\\\n2     hyperopt_param\\\\n3       Literal{n_estimators}\\\\n4       quniform\\\\n5         Literal{10}\\\\n6         Literal{50}\\\\n7         Literal{1}\\'\\nSolution: There are two plausible errors to this. Both are in the hpo.py file where the hyper-parameter tuning is run. The objective function should look like this.\\n\\n   def objective(params):\\n# It\\'s important to set the \"with\" statement and the \"log_params\" function here\\n# in order to properly log all the runs and parameters.\\nwith mlflow.start_run():\\n# Log the parameters\\nmlflow.log_params(params)\\nrf = RandomForestRegressor(**params)\\nrf.fit(X_train, y_train)\\ny_pred = rf.predict(X_valid)\\n# Calculate and log rmse\\nrmse = mean_squared_error(y_valid, y_pred, squared=False)\\nmlflow.log_metric(\\'rmse\\', rmse)\\nIf you add the with statement before this function, and just after the following line\\nX_valid, y_valid = load_pickle(os.path.join(data_path, \"valid.pkl\"))\\nand you log the parameters just after the search_space dictionary is defined, like this\\nsearch_space = {....}\\n# Log the parameters\\nmlflow.log_params(search_space)\\nThen there is a risk that the parameters will be logged in group. As a result, the\\nparams = space_eval(SPACE, params)\\nregister_model.py file will receive the parameters in group, while in fact it expects to receive them one by one. Thus, make sure that the objective function looks as above.\\nAdded by Jakob Salomonsson', 'section': 'Module 2: Experiment tracking', 'question': 'Could not convert string to float - ValueError', 'course': 'mlops-zoomcamp', 'doc_id': '1e0239c8d7cba02d0ec32e4c413edfff'}, {'text': 'Make sure you launch the mlflow UI from the same directory as thec that is running the experiments (same directory that has the mlflow directory and the database that stores the experiments).\\nOr navigate to the correct directory when specifying the tracking_uri.\\nFor example:\\nIf the mlflow.db is in a subdirectory called database, the tracking uri would be ‘sqllite:///database/mlflow.db’\\nIf the mlflow.db is a directory above your current directory: the tracking uri would be ‘sqlite:///../mlflow.db’\\nAnswered by Anna Vasylytsya\\nAnother alternative is to use an absolute path to mlflow.db rather than relative path\\nAnd yet another alternative is to launch the UI from the same notebook by executing the following code cell\\nimport subprocess\\nMLFLOW_TRACKING_URI = \"sqlite:///data/mlflow.db\"\\nsubprocess.Popen([\"mlflow\", \"ui\", \"--backend-store-uri\", MLFLOW_TRACKING_URI])\\nAnd then using the same MLFLOW_TRACKING_URI when initializing mlflow or the client\\nclient = MlflowClient(tracking_uri=MLFLOW_TRACKING_URI)\\nmlflow.set_tracking_uri(MLFLOW_TRACKING_URI)', 'section': 'Module 2: Experiment tracking', 'question': 'Experiment not visible in MLflow UI', 'course': 'mlops-zoomcamp', 'doc_id': 'ae11ab52124371633aea0b764dfcf1d3'}, {'text': \"Problem:\\nGetting\\nERROR: THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE\\nduring MLFlow's installation process, particularly while installing the Numpy package using pip\\nWhen I installed mlflow using ‘pip install mlflow’ on 27th May 2022, I got the following error while numpy was getting installed through mlflow:\\n\\nCollecting numpy\\nDownloading numpy-1.22.4-cp310-cp310-win_amd64.whl (14.7 MB)\\n|██████████████              \\t| 6.3 MB 107 kB/s eta 0:01:19\\nERROR: THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE.\\nIf you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.\\nnumpy from https://files.pythonhosted.org/packages/b5/50/d7978137464251c393df28fe0592fbb968110f752d66f60c7a53f7158076/numpy-1.22.4-cp310-cp310-win_amd64.whl#sha256=3e1ffa4748168e1cc8d3cde93f006fe92b5421396221a02f2274aab6ac83b077 (from mlflow):\\nExpected sha256 3e1ffa4748168e1cc8d3cde93f006fe92b5421396221a02f2274aab6ac83b077\\nGot    \\t15e691797dba353af05cf51233aefc4c654ea7ff194b3e7435e6eec321807e90\\nSolution:\\nThen when I install numpy separately (and not as part of mlflow), numpy gets installed (same version), and then when I do 'pip install mlflow', it also goes through.\\nPlease note that the above may not be consistently simulatable, but please be aware of this issue that could occur during pip install of mlflow.\\nAdded by Venkat Ramakrishnan\", 'section': 'Module 2: Experiment tracking', 'question': 'Hash Mismatch Error with Package Installation', 'course': 'mlops-zoomcamp', 'doc_id': '1ad3eb7a8560790ddce7d2fb813b7ce6'}, {'text': 'After deleting an experiment from UI, the deleted experiment still persists in the database.\\nSolution: To delete this experiment permanently, follow these steps.\\nAssuming you are using sqlite database;\\nInstall ipython sql using the following command: pip install ipython-sql\\nIn your jupyter notebook, load the SQL magic scripts with this: %load_ext sql\\nLoad the database with this: %sql sqlite:///nameofdatabase.db\\nRun the following SQL script to delete the experiment permanently: check link', 'section': 'Module 2: Experiment tracking', 'question': 'How to Delete an Experiment Permanently from MLFlow UI', 'course': 'mlops-zoomcamp', 'doc_id': 'ee548078f9a9a1a6b4b667d149a04ca4'}, {'text': 'Problem: I cloned the public repo, made edits, committed and pushed them to my own repo. Now I want to get the recent commits from the public repo without overwriting my own changes to my own repo. Which command(s) should I use?\\nThis is what my config looks like (in case this might be useful):\\n[core]\\nrepositoryformatversion = 0\\nfilemode = true\\nbare = false\\nlogallrefupdates = true\\nignorecase = true\\nprecomposeunicode = true\\n[remote \"origin\"]\\nurl = git@github.com:my_username/mlops-zoomcamp.git\\nfetch = +refs/heads/*:refs/remotes/origin/*\\n[branch \"main\"]\\nremote = origin\\nmerge = refs/heads/main\\nSolution: You should fork DataClubsTak’s repo instead of cloning it. On GitHub, click “Fetch and Merge” under the menu “Fetch upstream” at the main page of your own', 'section': 'Module 2: Experiment tracking', 'question': 'How to Update Git Public Repo Without Overwriting Changes', 'course': 'mlops-zoomcamp', 'doc_id': '9f78c8a4e83c42ce56befe7315f756fb'}, {'text': 'This is caused by ```mlflow.xgboost.autolog()``` when version 1.6.1 of xgboost\\nDowngrade to 1.6.0\\n```pip install xgboost==1.6.0``` or update requirements file with xgboost==1.6.0 instead of xgboost\\nAdded by Nakul Bajaj', 'section': 'Module 2: Experiment tracking', 'question': 'Image size of 460x93139 pixels is too large. It must be less than 2^16 in each direction.', 'course': 'mlops-zoomcamp', 'doc_id': '6daf29d95200a36b6f717ff94a6d1e80'}, {'text': 'Since the version 1.29 the list_experiments method was deprecated and then removed in the later version\\nYou should use search_experiments instead\\nAdded by Alex Litvinov', 'section': 'Module 2: Experiment tracking', 'question': \"MlflowClient object has no attribute 'list_experiments'\", 'course': 'mlops-zoomcamp', 'doc_id': '479916e2ff58426c63126cb993a3a1a0'}, {'text': 'Make sure `mlflow.autolog()` ( or framework-specific autolog ) written BEFORE `with mlflow.start_run()` not after.\\nAlso make sure that all dependencies for the autologger are installed, including matplotlib. A warning about uninstalled dependencies will be raised.\\nMohammed Ayoub Chettouh', 'section': 'Module 2: Experiment tracking', 'question': 'MLflow Autolog not working', 'course': 'mlops-zoomcamp', 'doc_id': '431ae75021ca64b81e1f710f8cfe29f0'}, {'text': 'If you’re running MLflow on a remote VM, you need to forward the port too like we did in Module 1 for Jupyter notebook port 8888. Simply connect your server to VS Code, as we did, and add 5000 to the PORT like in the screenshot:\\nAdded by Sharon Ibejih\\nIf you are running MLflow locally and 127.0.0.1:5000 shows a blank page navigate to localhost:5000 instead.', 'section': 'Module 2: Experiment tracking', 'question': 'MLflow URL (http://127.0.0.1:5000), doesn’t open.', 'course': 'mlops-zoomcamp', 'doc_id': '283369fc1142b58f9fd844ae7281c54b'}, {'text': 'Got the same warning message as Warrie Warrie when using “mlflow.xgboost.autolog()”\\nIt turned out that this was just a warning message and upon checking MLflow UI (making sure that no “tag” filters were included), the model was actually automatically tracked in the MLflow.\\nAdded by Bengsoon Chuah, Asked by Warrie Warrie, Answered by Anna Vasylytsya & Ivan Starovit', 'section': 'Module 2: Experiment tracking', 'question': 'MLflow.xgboost Autolog Model Signature Failure', 'course': 'mlops-zoomcamp', 'doc_id': 'e6e47eb50689a92d5a1fd92fd3b5e697'}, {'text': \"mlflow.exceptions.MlflowException: Cannot set a deleted experiment 'cross-sell' as the active experiment. You can restore the experiment, or permanently delete the  experiment to create a new one.\\nThere are many options to solve in this link: https://stackoverflow.com/questions/60088889/how-do-you-permanently-delete-an-experiment-in-mlflow\", 'section': 'Module 2: Experiment tracking', 'question': 'MlflowException: Unable to Set a Deleted Experiment', 'course': 'mlops-zoomcamp', 'doc_id': '762755fb10edbe607e3291303186c4a3'}, {'text': 'You do not have enough disk space to install the requirements. You can either increase the base EBS volume by following this link or add an external disk to your instance and configure conda installation to happen on the external disk.\\nAbinaya Mahendiran\\nOn GCP: I added another disk to my vm and followed this guide to mount the disk. Confirm the mount by running df -H (disk free) command in bash shell. I also deleted Anaconda and instead used miniconda. I downloaded miniconda in the additional disk that I mounted and when installing miniconda, enter the path to the extra disk instead of the default disk, this way conda is installed on the extra disk.\\nYang Cao', 'section': 'Module 2: Experiment tracking', 'question': 'No Space Left on Device - OSError[Errno 28]', 'course': 'mlops-zoomcamp', 'doc_id': '29c2f41d45ec35d6d52de9a1e1c55e9e'}, {'text': 'I was using an old version of sklearn due to which I got the wrong number of parameters because in the latest version min_impurity_split for randomforrestRegressor was deprecated. Had to upgrade to the latest version to get the correct number of params.', 'section': 'Module 2: Experiment tracking', 'question': 'Parameters Mismatch in Homework Q3', 'course': 'mlops-zoomcamp', 'doc_id': '9b2dfec8068d863f5d6d4842c19d3c75'}, {'text': \"Error: I installed all the libraries from the requirements.txt document in a new environment as follows:\\npip install -r requirementes.txt\\nThen when I run mlflow from my terminal like this:\\nmlflow\\nI get this error:\\nSOLUTION: You need to downgrade the version of 'protobuf' module to 3.20.x or lower. Initially, it was version=4.21, I installed protobuf==3.20\\npip install protobuf==3.20\\nAfter which I was able to run mlflow from my terminal.\\n-Submitted by Aashnna Soni\", 'section': 'Module 2: Experiment tracking', 'question': 'Protobuf error when installing MLflow', 'course': 'mlops-zoomcamp', 'doc_id': '4e8a6d84f1c3d511e5c5b431811dbbe0'}, {'text': 'Please check your current directory while running the mlflow ui command. You need to run mlflow ui or mlflow server command in the right directory.', 'section': 'Module 2: Experiment tracking', 'question': 'Setting up Artifacts folders', 'course': 'mlops-zoomcamp', 'doc_id': '8d5bfaabbfd4fc5dfb8da9a88f706cb3'}, {'text': 'If you have problem with setting up MLflow for experiment tracking on GCP, you can check these two links:\\nhttps://kargarisaac.github.io/blog/mlops/data%20engineering/2022/06/15/MLFlow-on-GCP.html\\nhttps://kargarisaac.github.io/blog/mlops/2022/08/26/machine-learning-workflow-orchestration-zenml.html', 'section': 'Module 2: Experiment tracking', 'question': 'Setting up MLflow experiment tracker on GCP', 'course': 'mlops-zoomcamp', 'doc_id': '6f698aec31e0538b6b36115de1adfc50'}, {'text': 'Solution: Downgrade setuptools (I downgraded 62.3.2 -> 49.1.0)', 'section': 'Module 2: Experiment tracking', 'question': 'Setuptools Replacing Distutils - MLflow Autolog Warning', 'course': 'mlops-zoomcamp', 'doc_id': '51c0dbdb9b731bdcd66a567141043e19'}, {'text': 'I can’t sort runs in MLFlow\\nMake sure you are in table view (not list view) in the MLflow UI.\\nAdded and Answered by Anna Vasylytsya', 'section': 'Module 2: Experiment tracking', 'question': 'Sorting runs in MLflow UI', 'course': 'mlops-zoomcamp', 'doc_id': '7ff6720ecd6f27ad253c8aa800fcd1f9'}, {'text': 'Problem: When I ran `$ mlflow ui` on a remote server and try to open it in my local browser I got an exception  and the page with mlflow ui wasn’t loaded.\\nSolution: You should `pip uninstall flask` on your remote server on conda env and after it install Flask `pip install Flask`. It is because the base conda env has ~flask<1.2, and when you clone it to your new work env, you are stuck with this old version.\\nAdded by Salimov Ilnaz', 'section': 'Module 2: Experiment tracking', 'question': \"TypeError: send_file() unexpected keyword 'max_age' during MLflow UI Launch\", 'course': 'mlops-zoomcamp', 'doc_id': 'd08a40b84a7e04fbad3530edcb466391'}, {'text': 'Problem: After successfully installing mlflow using pip install mlflow on my Windows system, I am trying to run the mlflow ui command but it throws the following error:\\nFileNotFoundError: [WinError 2] The system cannot find the file specified\\nSolution: Add C:\\\\Users\\\\{User_Name}\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\Scripts to the PATH\\nAdded by Alex Litvinov', 'section': 'Module 2: Experiment tracking', 'question': 'mlflow ui on Windows FileNotFoundError: [WinError 2] The system cannot find the file specified', 'course': 'mlops-zoomcamp', 'doc_id': '3e36a02a6d4a26e8dd4dadb22a308e3d'}, {'text': 'Running “python hpo.py --data_path=./your-path --max_evals=50” for the homework leads to the following error: TypeError: unsupported operand type(s) for -: \\'str\\' and \\'int\\'\\nFull Traceback:\\nFile \"~/repos/mlops/02-experiment-tracking/homework/hpo.py\", line 73, in <module>\\nrun(args.data_path, args.max_evals)\\nFile \"~/repos/mlops/02-experiment-tracking/homework/hpo.py\", line 47, in run\\nfmin(\\nFile \"~/Library/Caches/pypoetry/virtualenvs/mlflow-intro-SyTqwt0D-py3.9/lib/python3.9/site-packages/hyperopt/fmin.py\", line 540, in fmin\\nreturn trials.fmin(\\nFile \"~/Library/Caches/pypoetry/virtualenvs/mlflow-intro-SyTqwt0D-py3.9/lib/python3.9/site-packages/hyperopt/base.py\", line 671, in fmin\\nreturn fmin(\\nFile \"~/Library/Caches/pypoetry/virtualenvs/mlflow-intro-SyTqwt0D-py3.9/lib/python3.9/site-packages/hyperopt/fmin.py\", line 586, in fmin\\nrval.exhaust()\\nFile \"~/Library/Caches/pypoetry/virtualenvs/mlflow-intro-SyTqwt0D-py3.9/lib/python3.9/site-packages/hyperopt/fmin.py\", line 364, in exhaust\\nself.run(self.max_evals - n_done, block_until_done=self.asynchronous)\\nTypeError: unsupported operand type(s) for -: \\'str\\' and \\'int\\'\\nSolution:\\nThe --max_evals argument in hpo.py has no defined datatype and will therefore implicitly be treated as string. It should be an integer, so that the script can work correctly. Add type=int to the argument definition:\\nparser.add_argument(\\n\"--max_evals\",\\ntype=int,\\ndefault=50,\\nhelp=\"the number of parameter evaluations for the optimizer to explore.\"\\n)', 'section': 'Module 2: Experiment tracking', 'question': 'Unsupported Operand Type Error in hpo.py', 'course': 'mlops-zoomcamp', 'doc_id': '0eae1161956abb2f99e39188ada82bf9'}, {'text': 'Getting the following warning when running mlflow.sklearn:\\n\\n2022/05/28 04:36:36 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of sklearn. If you encounter errors during autologging, try upgrading / downgrading sklearn to a supported version, or try upgrading MLflow. […]\\nSolution: use 0.22.1 <= scikit-learn <= 1.1.0\\nReference: https://www.mlflow.org/docs/latest/python_api/mlflow.sklearn.html', 'section': 'Module 2: Experiment tracking', 'question': 'Unsupported Scikit-Learn version', 'course': 'mlops-zoomcamp', 'doc_id': '9789072446754f52e953c4674ce946ab'}, {'text': 'Problem: CLI commands (mlflow experiments list) do not return experiments\\nSolution description: need to set environment variable for the Tracking URI:\\n$ export MLFLOW_TRACKING_URI=http://127.0.0.1:5000\\nAdded and Answered by Dino Vitale', 'section': 'Module 2: Experiment tracking', 'question': 'Mlflow CLI does not return experiments', 'course': 'mlops-zoomcamp', 'doc_id': 'ce2b5d584316c9da8198195a6981f90f'}, {'text': 'Problem: After starting the tracking server, when we try to use the mlflow cli commands as listed here, most of them can’t seem to find the experiments that have been run with the tracking server\\nSolution: We need to set the environment variable MLFLOW_TRACKING_URI to the URI of the sqlite database. This is something like “export MLFLOW_TRACKING_URI=sqlite:///{path to sqlite database}” . After this, we can view the experiments from the command line using commands like “mlflow experiments search”\\nEven after this commands like “mlflow gc” doesn’t seem to get the tracking uri, and they have to be passed explicitly as an argument every time the command is run.\\nAhmed Fahim (afahim03@yahoo.com)', 'section': 'Module 2: Experiment tracking', 'question': 'Viewing MLflow Experiments using MLflow CLI', 'course': 'mlops-zoomcamp', 'doc_id': 'd0b457a1d02e8424132e6df3943f2174'}, {'text': 'All the experiment and other tracking information in mlflow are stored in sqllite database provided while initiating the mlflow ui command. This database can be inspected using Pycharm’s Database tab by using the SQLLite database type. Once the connection is created as below, the tables can be queried and inspected using regular SQL. The same applies for any SQL backed database such as postgres as well.\\nThis is very useful to understand the entity structure of the data being stored within mlflow and useful for any kind of systematic archiving of model tracking for longer periods.\\nAdded by Senthilkumar Gopal', 'section': 'Module 2: Experiment tracking', 'question': 'Viewing SQLlite Data Raw & Deleting Experiments Manually', 'course': 'mlops-zoomcamp', 'doc_id': '9afff3ca35f1c7fe02d3a9143139de0f'}, {'text': 'Solution : It is another way to start it for remote hosting a mlflow server. For example, if you are multiple colleagues working together on something you most likely would not run mlflow on one laptop but rather everyone would connect to the same server running mlflow\\nAnswer by Christoffer Added by Akshit Miglani (akshit.miglani09@gmail.com)', 'section': 'Module 2: Experiment tracking', 'question': 'What does launching the tracking server locally mean?', 'course': 'mlops-zoomcamp', 'doc_id': 'f91d6e75bd0462c5fd44009500c2c9be'}, {'text': 'Problem: parameter was not recognized during the model registry\\nSolution: parameters should be added in previous to the model registry. The parameters can be added by mlflow.log_params(params) so that the dictionary can be directly appended to the data.run.params.\\nAdded and Answered by Sam Lim', 'section': 'Module 2: Experiment tracking', 'question': 'Parameter adding in case of max_depth not recognized', 'course': 'mlops-zoomcamp', 'doc_id': 'd87db980a9bc4ec3a18498ed37de2edc'}, {'text': 'Problem: Max_depth is not recognize even when I add the mlflow.log_params\\nSolution: the mlflow.log_params(params) should be added to the hpo.py script, but if you run it it will append the new model to the previous run that doesn’t contain the parameters, you should either remove the previous experiment or change it\\nPastor Soto', 'section': 'Module 2: Experiment tracking', 'question': 'Max_depth is not recognize even when I add the mlflow.log_params', 'course': 'mlops-zoomcamp', 'doc_id': '766cebf96cab78fe44e961424b42efcc'}, {'text': \"Problem: About week_2 homework: The register_model.py  script, when I copy it into a jupyter notebook fails and spits out the following error. AttributeError: 'tuple' object has no attribute 'tb_frame'\\nSolution: remove click decorators\", 'section': 'Module 2: Experiment tracking', 'question': \"AttributeError: 'tuple' object has no attribute 'tb_frame'\", 'course': 'mlops-zoomcamp', 'doc_id': 'b9566b5aa78114e289749aa11b237a9d'}, {'text': 'Problem: when running the preprocess_data.py file you get the following error:\\n\\nwandb: ERROR api_key not configured (no-tty). call wandb.login(key=[your_api_key])\\nSolution: Go to your WandB profile (top RHS) → user settings → scroll down to “Danger Zone” and copy your API key. \\n\\nThen before running preprocess_data.py, add and run the following cell in your notebook:\\n\\n%%bash\\n\\nWandb login <YOUR_API_KEY_HERE>.\\nAdded and Answered by James Gammerman (jgammerman@gmail.com)', 'section': 'Module 2: Experiment tracking', 'question': 'WandB API error', 'course': 'mlops-zoomcamp', 'doc_id': '4f4269db5664281f83a850ff1680d3fb'}, {'text': 'Please make sure you following the order below nd enabling the autologging before constructing the dataset. If you still have this issue check that your data is in format compatible with XGBoost.\\n# Enable MLflow autologging for XGBoost\\nmlflow.xgboost.autolog()\\n# Construct your dataset\\nX_train, y_train = ...\\n# Train your XGBoost model\\nmodel = xgb.XGBRegressor(...)\\nmodel.fit(X_train, y_train)\\nAdded by Olga Rudakova', 'section': 'Module 2: Experiment tracking', 'question': 'WARNING mlflow.xgboost: Failed to infer model signature: could not sample data to infer model signature: please ensure that autologging is enabled before constructing the dataset.', 'course': 'mlops-zoomcamp', 'doc_id': '896c9be2f0e7ce716731f07db7398a58'}, {'text': 'Problem\\nUsing wget command to download either data or python scripts on Windows, I am using the notebook provided by Visual Studio and despite having a python virtual env, it did not recognize the pip command.\\nSolution: Use python -m pip, this same for any other command. Ie. python -m wget\\nAdded by Erick Calderin', 'section': 'Module 2: Experiment tracking', 'question': 'wget not working', 'course': 'mlops-zoomcamp', 'doc_id': '4b6d5425deccab2c2c9cf1629045ed5a'}, {'text': \"Problem: Open/run github notebook(.ipynb) directly in Google Colab\\nSolution: Change the domain from 'github.com' to 'githubtocolab.com'. The notebook will open in Google Colab.\\nOnly works with Public repo.\\nAdded by Ming Jun\\nNavigating in Wandb UI became difficult to me, I had to intuit some options until I found the correct one.\\nSolution: Go to the official doc.\\nAdded by Erick Calderin\", 'section': 'Module 2: Experiment tracking', 'question': 'Open/run github notebook(.ipynb) directly in Google Colab', 'course': 'mlops-zoomcamp', 'doc_id': '505f06b8351840a50d99b07f4f10d299'}, {'text': 'Problem: Someone asked why we are using this type of split approach instead of just a random split.\\nSolution: For example, I have some models at work that train on Jan 1 2020 — Aug 1 2021 time period, and then test on Aug 1 - Dec 31 2021, and finally validate on Jan - March or something\\nWe do these “out of time”  validations to do a few things:\\nCheck for seasonality of our data\\nWe know if the RMSE for Test is 5 say, and then RMSE for validation is 20, then there’s serious seasonality to the data we are looking at, and now we might change to Time Series approaches\\nIf I’m predicting on Mar 30 2023 the outcomes for the next 3 months, the “random sample” in our train/test would have caused data leakage, overfitting, and poor model performance in production. We mustn’t take information about the future and apply it to the present when we are predicting in a model context.\\nThese are two of, I think, the biggest points for why we are doing jan/feb/march. I wouldn’t do it any other way.\\nTrain: Jan\\nTest: Feb\\nValidate: March\\nThe point of validation is to report out model metrics to leadership, regulators, auditors, and record the models performance to then later analyze target drift\\nAdded by Sam LaFell\\nProblem: If you get an error while trying to run the mlflow server on AWS CLI with S3 bucket and POSTGRES database:\\nReproducible Command:\\nmlflow server -h 0.0.0.0 -p 5000 --backend-store-uri postgresql://<DB_USERNAME>:<DB_PASSWORD>@<DB_ENDPOINT>:<DB_PORT>/<DB_NAME> --default-artifact-root s3://<BUCKET_NAME>\\nError:\\n\"urllib3 v2.0 only supports OpenSSL 1.1.1+, currently \"\\nImportError: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the \\'ssl\\' module is compiled with \\'OpenSSL 1.0.2k-fips  26 Jan 2017\\'. See: https://github.com/urllib3/urllib3/issues/2168\\nSolution: Upgrade mlflow using\\nCode: pip3 install --upgrade mlflow\\nResolution: It downgrades urllib3 2.0.3 to 1.26.16 which is compatible with mlflow and ssl 1.0.2\\nInstalling collected packages: urllib3\\nAttempting uninstall: urllib3\\nFound existing installation: urllib3 2.0.3\\nUninstalling urllib3-2.0.3:\\nSuccessfully uninstalled urllib3-2.0.3\\nSuccessfully installed urllib3-1.26.16\\nAdded by Sarvesh Thakur', 'section': 'Module 3: Orchestration', 'question': 'Why do we use Jan/Feb/March for Train/Test/Validation Purposes?', 'course': 'mlops-zoomcamp', 'doc_id': 'f97ea3ccf8ec39f702b9948a7a70bf00'}, {'text': 'Problem description\\nSolution description\\n(optional) Added by Name', 'section': 'Module 3: Orchestration', 'question': 'Problem title', 'course': 'mlops-zoomcamp', 'doc_id': 'a5e6f1bae9c35eb1985b52fd2b089014'}, {'text': 'Here', 'section': 'Module 4: Deployment', 'question': 'Where is the FAQ for Prefect questions?', 'course': 'mlops-zoomcamp', 'doc_id': '51bb6b2926d6e5350c2b70d15d9e7c2c'}, {'text': 'Windows with AWS CLI already installed\\nAWS CLI version:\\naws-cli/2.4.24 Python/3.8.8 Windows/10 exe/AMD64 prompt/off\\nExecuting\\n$(aws ecr get-login --no-include-email)\\nshows error\\naws.exe: error: argument operation: Invalid choice, valid choices are…\\nUse this command instead. More info here:\\nhttps://docs.aws.amazon.com/cli/latest/reference/ecr/get-login-password.html\\naws ecr get-login-password \\\\\\n--region <region> \\\\\\n| docker login \\\\\\n--username AWS \\\\\\n--password-stdin <aws_account_id>.dkr.ecr.<region>.amazonaws.com\\nAdded by MarcosMJD', 'section': 'Module 4: Deployment', 'question': 'aws.exe: error: argument operation: Invalid choice — Docker can not login to ECR.', 'course': 'mlops-zoomcamp', 'doc_id': '33eaf37bce297bd2feab85ed3931507b'}, {'text': 'Use ` at the end of each line except the last. Note that multiline string does not need `.\\nEscape “ to “\\\\ .\\nUse $env: to create env vars (non-persistent). E.g.:\\n$env:KINESIS_STREAM_INPUT=\"ride_events\"\\naws kinesis put-record --cli-binary-format raw-in-base64-out `\\n--stream-name $env:KINESIS_STREAM_INPUT `\\n--partition-key 1 `\\n--data \\'{\\n\\\\\"ride\\\\\": {\\n\\\\\"PULocationID\\\\\": 130,\\n\\\\\"DOLocationID\\\\\": 205,\\n\\\\\"trip_distance\\\\\": 3.66\\n},\\n\\\\\"ride_id\\\\\": 156\\n}\\'\\nAdded by MarcosMJD', 'section': 'Module 4: Deployment', 'question': 'Multiline commands in Windows Powershell', 'course': 'mlops-zoomcamp', 'doc_id': '895c6f82871a13a8d45a48a139ec3a45'}, {'text': \"If one gets pipenv failures for pipenv install command -\\nAttributeError: module 'collections' has no attribute 'MutableMapping'\\nIt happens because you use the system Python (3.10) for pipenv.\\nIf you previously installed pipenv with apt-get, remove it - sudo-apt remove pipenv\\nMake sure you have a non-system Python installed in your environment. The easiest way to do it is to install anaconda or miniconda\\nNext, install pipenv to your non-system Python. If you use the setup from the lectures, it’s just this: pip install pipenv\\nNow re-run pipenv install XXXX (relevant dependencies) - should work\\nTested and worked on AWS instance, similar to the config Alexey presented in class.\\nAdded by Daniel HenSSL\", 'section': 'Module 4: Deployment', 'question': \"Pipenv installation not working (AttributeError: module 'collections' has no attribute 'MutableMapping')\", 'course': 'mlops-zoomcamp', 'doc_id': '42de97a12a5254118539615c24dcacbd'}, {'text': 'First check if SSL module configured with following command:\\nPython -m ssl\\n\\nIf the output of this is empty there is no problem with SSL configuration.\\n\\nThen you should upgrade your pipenv package in your current environment to resolve the problem.\\nAdded by Kenan Arslanbay', 'section': 'Module 4: Deployment', 'question': \"module is not available (Can't connect to HTTPS URL)\", 'course': 'mlops-zoomcamp', 'doc_id': 'de18f769b10ca2825125b598041ead71'}, {'text': \"During scikit-learn installation via the command:\\npipenv install scikit-learn==1.0.2\\nThe following error is raised:\\nModuleNotFoundError: No module named 'pip._vendor.six'\\nThen, one should:\\nsudo apt install python-six\\npipenv --rm\\npipenv install scikit-learn==1.0.2\\nAdded by Giovanni Pecoraro\", 'section': 'Module 4: Deployment', 'question': \"No module named 'pip._vendor.six'\", 'course': 'mlops-zoomcamp', 'doc_id': '25e72e5dd439891eb2398bc1e4f4135f'}, {'text': 'Problem description. How can we use Jupyter notebooks with the Pipenv environment?\\nSolution: Refer to this stackoverflow question. Basically install jupyter and ipykernel using pipenv. And then register the kernel with `python -m ipykernel install --user --name=my-virtualenv-name` inside the Pipenv shell. If you are using Jupyter notebooks in VS Code, doing this will also add the virtual environment in the list of kernels.\\nAdded by Ron Medina', 'section': 'Module 4: Deployment', 'question': 'Pipenv with Jupyter', 'course': 'mlops-zoomcamp', 'doc_id': 'c694fe6ca1a6d260738173c045765f7b'}, {'text': \"Problem: I tried to run starter notebook on pipenv environment but had issues with no output on prints. \\nI used scikit-learn==1.2.2 and python==3.10\\nTornado version was 6.3.2\\n\\nSolution: The error you're encountering seems to be a bug related to Tornado, which is a Python web server and networking library. It's used by Jupyter under the hood to handle networking tasks.\\nDowngrading to tornado==6.1 fixed the issue\\nhttps://stackoverflow.com/questions/54971836/no-output-jupyter-notebook\", 'section': 'Module 4: Deployment', 'question': 'Pipenv with Jupyter no output', 'course': 'mlops-zoomcamp', 'doc_id': '2a9f0d9567341b678f1d5ec0f0abe3b2'}, {'text': 'Problem description:  You might get an error ‘Invalid base64’ after running the ‘aws kinesis put-record’ command on your local machine. This might be the case if you are using the AWS CLI version 2 (note that in the video 4.4, around 57:42, you can see a warning since the instructor is using v1 of the CLI.\\nSolution description: To get around this, pass the argument ‘--cli-binary-format raw-in-base64-out’. This will encode your data string into base64 before passing it to kinesis\\nAdded by M', 'section': 'Module 4: Deployment', 'question': '‘Invalid base64’ error after running `aws kinesis put-record`', 'course': 'mlops-zoomcamp', 'doc_id': '75a7db3c1f7f8d3ec7adde510142686d'}, {'text': 'Problem description:   Running starter.ipynb in homework’s Q1 will show up this error.\\nSolution description: Update pandas (actually pandas version was the latest, but several dependencies are updated).\\nAdded by Marcos Jimenez', 'section': 'Module 4: Deployment', 'question': 'Error index 311297 is out of bounds for axis 0 with size 131483 when loading parquet file.', 'course': 'mlops-zoomcamp', 'doc_id': 'aa9c7b9c891da9d8ed6974cb45a3fea2'}, {'text': 'Use command $pipenv lock to force the creation of Pipfile.lock\\nAdded by Bijay P.', 'section': 'Module 4: Deployment', 'question': 'Pipfile.lock was not created along with Pipfile', 'course': 'mlops-zoomcamp', 'doc_id': '195a5f78be1035d1fded1bdcbebaf1d0'}, {'text': 'This issue is usually due to the pythonfinder module in pipenv.\\nThe solution to this involves manually changing the scripts as describe here python_finder_fix\\nAdded by Ridwan Amure', 'section': 'Module 4: Deployment', 'question': 'Permission Denied using Pipenv', 'course': 'mlops-zoomcamp', 'doc_id': 'a5091feacf9f542c208fe3545c323b59'}, {'text': 'When passing arguments to a script via command line and converting it to a 4 digit number using f’{year:04d}’, this error showed up.\\nThis happens because all inputs from the command line are read as string by the script. They need to be converted to numeric/integer before transformation in fstring.\\nyear = int(sys.argv[1])\\nf’{year:04d}’\\nIf you use click library just edit a decorator\\n@click.command()\\n@click.option( \"--year\",  help=\"Year for evaluation\",   type=int)\\ndef  your_function(year):\\n<<Your code>>\\nAdded by Taras Sh', 'section': 'Module 4: Deployment', 'question': \"Error while parsing arguments via CLI  [ValueError: Unknown format code 'd' for object of type 'str']\", 'course': 'mlops-zoomcamp', 'doc_id': '71fd63e1f928ff37f5084016b79c9868'}, {'text': 'Ensure the correct image is being used to derive from.\\nCopy the data from local to the docker image using the COPY command to a relative path. Using absolute paths within the image might be troublesome.\\nUse paths starting from /app and don’t forget to do WORKDIR /app before actually performing the code execution.\\nMost common commands\\nBuild container using docker build -t mlops-learn .\\nExecute the script using docker run -it --rm mlops-learn\\n<mlops-learn> is just a name used for the image and does not have any significance.', 'section': 'Module 4: Deployment', 'question': 'Dockerizing tips', 'course': 'mlops-zoomcamp', 'doc_id': '5fee2d116c3aa0be8478320edb0623ff'}, {'text': 'If you are trying to run Flask gunicorn & MLFlow server from the same container, defining both in Dockerfile with CMD will only run MLFlow & not Flask.\\nSolution: Create separate shell script with server run commands, for eg:\\n> \\tscript1.sh\\n#!/bin/bash\\ngunicorn --bind=0.0.0.0:9696 predict:app\\nAnother script with e.g. MLFlow server:\\n>\\tscript2.sh\\n#!/bin/bash\\nmlflow server -h 0.0.0.0 -p 5000 --backend-store-uri=sqlite:///mlflow.db --default-artifact-root=g3://zc-bucket/mlruns/\\nCreate a wrapper script to run above 2 scripts:\\n>\\twrapper_script.sh\\n#!/bin/bash\\n# Start the first process\\n./script1.sh &\\n# Start the second process\\n./script2.sh &\\n# Wait for any process to exit\\nwait -n\\n# Exit with status of process that exited first\\nexit $?\\nGive executable permissions to all scripts:\\nchmod +x *.sh\\nNow we can define last line of Dockerfile as:\\n> \\tDockerfile\\nCMD ./wrapper_script.sh\\nDont forget to expose all ports defined by services!', 'section': 'Module 4: Deployment', 'question': 'Running multiple services in a Docker container', 'course': 'mlops-zoomcamp', 'doc_id': '51bdd592d6db17f6c87ffcaf1b60291f'}, {'text': 'Problem description cannot generate pipfile.lock raise InstallationError( pip9.exceptions.InstallationError: Command \"python setup.py egg_info\" failed with error code 1\\nSolution: you need to force and upgrade wheel and pipenv\\nJust run the command line :\\npip install --user --upgrade --upgrade-strategy eager pipenv wheel', 'section': 'Module 4: Deployment', 'question': 'Cannot generate pipfile.lock raise InstallationError( pip9.exceptions.InstallationError)', 'course': 'mlops-zoomcamp', 'doc_id': '9e22e0a08dd5b0de17533058b54fc33a'}, {'text': \"Problem description. How can we connect s3 bucket to MLFLOW?\\nSolution: Use boto3 and AWS CLI to store access keys. The access keys are what will be used by boto3 (AWS' Python API tool) to connect with the AWS servers. If there are no Access Keys how can they make sure that they have the right to access this Bucket? Maybe you're a malicious actor (Hacker for ex). The keys must be present for boto3 to talk to the AWS servers and they will provide access to the Bucket if you possess the right permissions. You can always set the Bucket as public so anyone can access it, now you don't need access keys because AWS won't care.\\nRead more here: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html\\nAdded by Akshit Miglani\", 'section': 'Module 4: Deployment', 'question': 'Connecting s3 bucket to MLFLOW', 'course': 'mlops-zoomcamp', 'doc_id': '94171bc778d9dee91a4a35430d2748a4'}, {'text': 'Even though the upload works using aws cli and boto3 in Jupyter notebook.\\nSolution set the AWS_PROFILE environment variable (the default profile is called default)', 'section': 'Module 4: Deployment', 'question': 'Uploading to s3 fails with An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The AWS Access Key Id you provided does not exist in our records.\"', 'course': 'mlops-zoomcamp', 'doc_id': '2d9fda811c15f543064ac26c70aee65d'}, {'text': 'Problem description: lib_lightgbm.so Reason: image not found\\nSolution description: Add “RUN apt-get install libgomp1” to your docker. (change installer command based on OS)\\nAdded by Kazeem Hakeem', 'section': 'Module 4: Deployment', 'question': 'Dockerizing lightgbm', 'course': 'mlops-zoomcamp', 'doc_id': 'ef9c425a124bc6a06b0dfcff86f17888'}, {'text': 'When the request is processed in lambda function, mlflow library raises:\\n2022/09/19 21:18:47 WARNING mlflow.pyfunc: Encountered an unexpected error (AttributeError(\"module \\'dataclasses\\' has no attribute \\'__version__\\'\")) while detecting model dependency mismatches. Set logging level to DEBUG to see the full traceback.\\nSolution: Increase the memory of the lambda function.\\nAdded by MarcosMJD', 'section': 'Module 4: Deployment', 'question': 'Error raised when executing mlflow’s pyfunc.load_model in lambda function.', 'course': 'mlops-zoomcamp', 'doc_id': 'd037e0c1ae983488650946df9cdfdd4f'}, {'text': 'Just a note if you are following the video but also using the repo’s notebook The notebook is the end state of the video which eventually uses mlflow pipelines.\\nJust watch the video and be patient. Everything will work :)\\nAdded by Quinn Avila', 'section': 'Module 4: Deployment', 'question': '4.3 FYI Notebook is end state of Video -', 'course': 'mlops-zoomcamp', 'doc_id': '8d88dd227ae11ab76c8eb89382e15e6b'}, {'text': 'Problem description: I was having issues because my python script was not reading AWS credentials from env vars, after building the image I was running it like this:\\ndocker run -it homework-04 -e AWS_ACCESS_KEY_ID=xxxxxxxx -e AWS_SECRET_ACCESS_KEY=xxxxxx\\nSolution 1:\\n\\nEnvironment Variables: \\nYou can set the AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and AWS_SESSION_TOKEN (if you are using AWS STS) environment variables. You can set these in your shell, or you can include them in your Docker run command like this:\\nI found out by myself that those variables must be passed before specifying the name of the image, as follow:\\ndocker run -e AWS_ACCESS_KEY_ID=xxxxxxxx -e AWS_SECRET_ACCESS_KEY=xxxxxx -it homework-04\\nAdded by Erick Cal\\nSolution 2 (if AWS credentials were not found):\\nAWS Configuration Files: \\nThe AWS SDKs and CLI will check the ~/.aws/credentials and ~/.aws/config files for credentials if they exist. You can map these files into your Docker container using volumes:\\n\\ndocker run -it --rm -v ~/.aws:/root/.aws homework:v1', 'section': 'Module 4: Deployment', 'question': 'Passing envs to my docker image', 'course': 'mlops-zoomcamp', 'doc_id': '9787f610a2d74c3430ff0956a60ac67f'}, {'text': 'If anyone is troubleshooting or just interested in seeing the model listed on the image svizor/zoomcamp-model:mlops-3.10.0-slim.\\nCreate a dockerfile. (yep thats all) and build “docker build -t zoomcamp_test .”\\nFROM svizor/zoomcamp-model:mlops-3.10.0-slim\\nRun “docker run -it zoomcamp_test ls /app” output -> model.bin\\nThis will list the contents of the app directory and “model.bin” should output. With this you could just copy your files, for example “copy myfile .” maybe a requirements file and this can be run for example “docker run -it myimage myscript arg1 arg2 ”. Of course keep in mind a build is needed everytime you change the Dockerfile.\\nAnother variation is to have it run when you run the docker file.\\n“””\\nFROM svizor/zoomcamp-model:mlops-3.10.0-slim\\nWORKDIR /app\\nCMD ls\\n“””\\nJust keep in mind CMD is needed because the RUN commands are used for building the image and the CMD is used at container runtime. And in your example you probably want to run a script or should we say CMD a script.\\nQuinn Avila', 'section': 'Module 4: Deployment', 'question': 'How to see the model in the docker container in app/?', 'course': 'mlops-zoomcamp', 'doc_id': 'd4a7032feae1759992c78ffe90345e1a'}, {'text': 'To resolve this make sure to build the docker image with the platform tag, like this:\\n“docker build -t homework:v1 --platform=linux/arm64 .”', 'section': 'Module 4: Deployment', 'question': \"WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\", 'course': 'mlops-zoomcamp', 'doc_id': '26ddd8b9fd8752486cc1069cd834f359'}, {'text': \"Solution: instead of input_file = f'https://s3.amazonaws.com/nyc-tlc/trip+data/{taxi_type}_tripdata_{year:04d}-{month:02d}.parquet'  use input_file = f'https://d37ci6vzurychx.cloudfront.net/trip-data/{taxi_type}_tripdata_{year:04d}-{month:02d}.parquet'\\nIlnaz Salimov\\nsalimovilnaz777@gmail.com\", 'section': 'Module 4: Deployment', 'question': 'HTTPError: HTTP Error 403: Forbidden when call apply_model() in score.ipynb', 'course': 'mlops-zoomcamp', 'doc_id': '44c7dff8226a72063959e1a424c8046b'}, {'text': 'i\\'m getting this error ModuleNotFoundError: No module named \\'pipenv.patched.pip._vendor.urllib3.response\\'\\nand Resolved from this command pip install pipenv --force-reinstall\\ngetting this errror site-packages\\\\pipenv\\\\patched\\\\pip\\\\_vendor\\\\urllib3\\\\connectionpool.py\"\\nResolved from this command pip install -U pip and pip install requests\\nAsif', 'section': 'Module 5: Monitoring', 'question': \"ModuleNotFoundError: No module named 'pipenv.patched.pip._vendor.urllib3.response'\", 'course': 'mlops-zoomcamp', 'doc_id': '97001e4f7fcdc2566fe08c637afcef3d'}, {'text': 'Problem description: When running docker-compose up as shown in the video 5.2 if you go to http://localhost:3000/ you get asked for a username and a password.\\nSolution: for both of them the default is “admin”. Then you can enter your new password. \\nSee also here\\nAdded by JaimeRV', 'section': 'Module 5: Monitoring', 'question': 'Login window in Grafana', 'course': 'mlops-zoomcamp', 'doc_id': 'e98a42fa0d06c0599547359bc091f70c'}, {'text': 'Problem Description : In Linux, when starting services using docker compose up --build  as shown in video 5.2, the services won’t start and instead we get message unknown flag: --build in command prompt.\\nSolution : Since we install docker-compose separately in Linux, we have to run docker-compose up --build instead of docker compose up --build\\nAdded by Ashish Lalchandani', 'section': 'Module 5: Monitoring', 'question': 'Error in starting monitoring services in Linux', 'course': 'mlops-zoomcamp', 'doc_id': '0b51fbbb7fcb14d12e9dfbcffa9544b2'}, {'text': 'Problem: When running prepare.py getting KeyError: ‘content-length’\\nSolution: From Emeli Dral:\\nIt seems to me that the link we used in prepare.py to download taxi data does not work anymore. I substituted the instruction:\\nurl = f\"https://nyc-tlc.s3.amazonaws.com/trip+data/{file}\\nby the\\nurl = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/{file}\"\\nin the prepare.py and it worked for me. Hopefully, if you do the same you will be able to get those data.', 'section': 'Module 5: Monitoring', 'question': 'KeyError ‘content-length’ when running prepare.py', 'course': 'mlops-zoomcamp', 'doc_id': '7a57570937bcf253b7052ee11aab74ea'}, {'text': 'Problem description\\nWhen I run the command “docker-compose up –build” and send the data to the real-time prediction service. The service will return “Max retries exceeded with url: /api”.\\nIn my case it because of my evidently service exit with code 2 due to the “app.py” in evidently service cannot import “from pyarrow import parquet as pq”.\\nSolution description\\nThe first solution is just install the pyarrow module “pip install pyarrow”\\nThe second solution is restart your machine.\\nThe third solution is if the first and second one didn’t work with your machine. I found that “app.py” of evidently service didn’t use that module. So comment the pyarrow module out and the problem was solved for me.\\nAdded by Surawut Jirasaktavee', 'section': 'Module 5: Monitoring', 'question': 'Evidently service exit with code 2', 'course': 'mlops-zoomcamp', 'doc_id': 'aa10a4a8148ac297c92d90b837d5a12b'}, {'text': 'When using evidently if you get this error.\\nYou probably forgot to and parentheses () just and opening and closing and you are good to go.\\nQuinn Avila', 'section': 'Module 5: Monitoring', 'question': 'ValueError: Incorrect item instead of a metric or metric preset was passed to Report', 'course': 'mlops-zoomcamp', 'doc_id': '5a7afd62f24c4ba7c75c01fa96023a90'}, {'text': 'You will get an error if you didn’t add a target=’duration_min’\\nIf you want to use RegressionQualityMetric() you need a target=’duration_min and you need this added to you current_data[‘duration_min’]\\nQuinn Avila', 'section': 'Module 5: Monitoring', 'question': 'For the report RegressionQualityMetric()', 'course': 'mlops-zoomcamp', 'doc_id': '28d09f34ee06b1570e75b8a6a5ac0909'}, {'text': 'Problem description\\nValueError: Found array with 0 sample(s) (shape=(0, 6)) while a minimum of 1 is required by LinearRegression.\\nSolution description\\nThis happens because the generated data is based on an early date therefore the training dataset would be empty.\\nAdjust the following\\nbegin = datetime.datetime(202X, X, X, 0, 0)\\nAdded by Luke', 'section': 'Module 5: Monitoring', 'question': 'Found array with 0 sample(s)', 'course': 'mlops-zoomcamp', 'doc_id': '8e4dcb5aaee41544dd4fd1e11f368f66'}, {'text': 'Problem description\\nGetting “target columns” “prediction columns” not present errors after adding a metric\\nSolution description\\nMake sure to read through the documentation on what is required or optional when adding the metric. I added DatasetCorrelationsMetric which doesn’t require any parameters because the metric evaluates for correlations among the features.\\nSam Lim', 'section': 'Module 5: Monitoring', 'question': 'Adding additional metric', 'course': 'mlops-zoomcamp', 'doc_id': 'ba95528064d06b6aac5d7c695d7cde9b'}, {'text': 'When you try to login in Grafana with standard requisites (admin/admin) it throw up an error.\\nAfter run grafana-cli admin reset-admin-password admin in Grafana container the problem will be fixed\\nAdded by Artem Glazkov', 'section': 'Module 5: Monitoring', 'question': 'Standard login in Grafana does not work', 'course': 'mlops-zoomcamp', 'doc_id': 'bf7ce471bd833f01add470cc1acce4db'}, {'text': 'Problem description. While my metric generation script was still running, I noticed that the charts in Grafana don’t get updated.\\nSolution description. There are two things to pay attention to:\\nRefresh interval: set it to a small value: 5-10-30 seconds\\nUse your local timezone in a call to `pytz.timezone` – I couldn’t get updates before changing this from the original value “Europe/London” to my own zone', 'section': 'Module 5: Monitoring', 'question': 'The chart in Grafana doesn’t get updates', 'course': 'mlops-zoomcamp', 'doc_id': '1cf8a1bdccb9e16ce04d30a681a92f78'}, {'text': 'Problem description. Prefect server was not running locally, I ran `prefect server start` command but it stopped immediately..\\nSolution description. I used Prefect cloud to run the script, however I created an issue on the Prefect github.\\nBy Erick Calderin', 'section': 'Module 5: Monitoring', 'question': 'Prefect server was not running locally', 'course': 'mlops-zoomcamp', 'doc_id': 'a325661019183981ead4f7e7f9ab8500'}, {'text': 'Solution. Using docker CLI run docker system prune to remove unused things (build cache, containers, images etc)\\nAlso, to see what’s taking space before pruning you can run docker system df\\nBy Alex Litvinov', 'section': 'Module 5: Monitoring', 'question': 'no disk space left error when doing docker compose up', 'course': 'mlops-zoomcamp', 'doc_id': '80c9291b69fc37d5f5213e222b2a25d2'}, {'text': 'Problem: when run docker-compose up –build, you may see this error. To solve, add `command: php -S 0.0.0.0:8080 -t /var/www/html` in adminer block in yml file like:\\nadminer:\\ncommand: php -S 0.0.0.0:8080 -t /var/www/html\\nimage: adminer\\n…\\nIlnaz Salimov\\nsalimovilnaz777@gmail.com', 'section': 'Module 5: Monitoring', 'question': 'Failed to listen on :::8080 (reason: php_network_getaddresses: getaddrinfo failed: Address family for hostname not supported)', 'course': 'mlops-zoomcamp', 'doc_id': 'e1ba3b69ee91dbb28273f62ae9d2f1f0'}, {'text': 'Problem: Can we generate charts like Evidently inside Grafana?\\nSolution: In Grafana that would be a stat panel (just a number) and scatter plot panel (I believe it requires a plug-in). However, there is no native way to quickly recreate this exact Evidently dashboard. You\\'d need to make sure you have all the relevant information logged to your Grafana data source, and then design your own plots in Grafana.\\nIf you want to recreate the Evidently visualizations externally, you can export the Evidently output in JSON with include_render=True\\n(more details here https://docs.evidentlyai.com/user-guide/customization/json-dict-output) and then parse information from it for your external visualization layer. To include everything you need for non-aggregated visuals, you should also add \"raw_data\": True  option (more details here https://docs.evidentlyai.com/user-guide/customization/report-data-aggregation).\\nOverall, this specific plot with under- and over-performance segments is more useful during debugging, so might be easier to access it ad hoc using Evidently.\\nAdded by Ming Jun, Asked by Luke, Answered by Elena Samuylova', 'section': 'Module 6: Best practices', 'question': 'Generate Evidently Chart in Grafana', 'course': 'mlops-zoomcamp', 'doc_id': 'ce26dbe4e119384a1451ab82a704d829'}, {'text': \"You may get an error ‘{'errorMessage': 'Unable to locate credentials', …’ from the print statement in test_docker.py after running localstack with kinesis.\\nTo fix this, in the docker-compose.yaml file, in addition to the environment variables like AWS_DEFAULT_REGION, add two other variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY. Their value is not important; anything like abc will suffice\\nAdded by M\\nOther possibility is just to run\\naws --endpoint-url http://localhost:4566 configure\\nAnd providing random values for AWS Access Key ID , AWS Secret Access Key, Default region name, and Default output format.\\nAdded by M.A. Monjas\", 'section': 'Module 6: Best practices', 'question': 'Get an error ‘Unable to locate credentials’ after running localstack with kinesis', 'course': 'mlops-zoomcamp', 'doc_id': '0ca8062600a96090813095df210757d1'}, {'text': \"You may get an error while creating a bucket with localstack and the boto3 client:\\nbotocore.exceptions.ClientError: An error occurred (IllegalLocationConstraintException) when calling the CreateBucket operation: The unspecified location constraint is incompatible for the region specific endpoint this request was sent to.\\nTo fix this, instead of creating a bucket via\\ns3_client.create_bucket(Bucket='nyc-duration')\\nCreate it with\\ns3_client.create_bucket(Bucket='nyc-duration', CreateBucketConfiguration={\\n'LocationConstraint': AWS_DEFAULT_REGION})\\nyam\\nAdded by M\", 'section': 'Module 6: Best practices', 'question': 'Get an error ‘ unspecified location constraint is incompatible ’', 'course': 'mlops-zoomcamp', 'doc_id': '055fae9ff5aac649bf41ebbde8c3cb75'}, {'text': 'When executing an AWS CLI command (e.g., aws s3 ls), you can get the error <botocore.awsrequest.AWSRequest object at 0x7fbaf2666280>.\\nTo fix it, simply set the AWS CLI environment variables:\\nexport AWS_DEFAULT_REGION=eu-west-1\\nexport AWS_ACCESS_KEY_ID=foobar\\nexport AWS_SECRET_ACCESS_KEY=foobar\\nTheir value is not important; anything would be ok.\\nAdded by Giovanni Pecoraro', 'section': 'Module 6: Best practices', 'question': 'Get an error “<botocore.awsrequest.AWSRequest object at 0x7fbaf2666280>” after running an AWS CLI command', 'course': 'mlops-zoomcamp', 'doc_id': 'b16440f055a72ba074270b6824df3b31'}, {'text': 'At every commit the above error is thrown and no pre-commit hooks are ran.\\nMake sure the indentation in .pre-commit-config.yaml is correct. Especially the 4 spaces ahead of every `repo` statement\\nAdded by M. Ayoub C.', 'section': 'Module 6: Best practices', 'question': 'Pre-commit triggers an error at every commit: “mapping values are not allowed in this context”', 'course': 'mlops-zoomcamp', 'doc_id': 'ea817a38edd157119f4c0a43ae2da931'}, {'text': 'No option to remove pytest test\\nRemove .vscode folder located on the folder you previously used for testing, e.g. folder code (from week6-best-practices) was chosen to test, so you may remove .vscode inside the folder.\\nAdded by Rizdi Aprilian', 'section': 'Module 6: Best practices', 'question': 'Could not reconfigure pytest from zero after getting done with previous folder', 'course': 'mlops-zoomcamp', 'doc_id': 'bc51c77ec280383e48a0b6a1a0d8378a'}, {'text': 'Problem description\\nFollowing video 6.3, at minute 11:23, get records command returns empty Records.\\nSolution description\\nAdd --no-sign-request to Kinesis get records call:\\n aws --endpoint-url=http://localhost:4566 kinesis get-records --shard-iterator […] --no-sign-request', 'section': 'Module 6: Best practices', 'question': 'Empty Records in Kinesis Get Records with LocalStack', 'course': 'mlops-zoomcamp', 'doc_id': '7a0b4e451d811e4a5bb6b91b52032399'}, {'text': \"Problem description\\ngit commit -m 'Updated xxxxxx'\\nAn error has occurred: InvalidConfigError:\\n==> File .pre-commit-config.yaml\\n=====> 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\\nSolution description\\nSet uft-8 encoding when creating the pre-commit yaml file:\\npre-commit sample-config | out-file .pre-commit-config.yaml -encoding utf8\\nAdded by MarcosMJD\", 'section': 'Module 6: Best practices', 'question': 'In Powershell, Git commit raises utf-8 encoding error after creating pre-commit yaml file', 'course': 'mlops-zoomcamp', 'doc_id': 'ac70ec8a940c84538b5785c939a006c3'}, {'text': \"Problem description\\ngit commit -m 'Updated xxxxxx'\\n[INFO] Initializing environment for https://github.com/pre-commit/pre-commit-hooks.\\n[INFO] Installing environment for https://github.com/pre-commit/pre-commit-hooks.\\n[INFO] Once installed this environment will be reused.\\nAn unexpected error has occurred: CalledProcessError: command:\\n…\\nreturn code: 1\\nexpected return code: 0\\nstdout:\\nAttributeError: 'PythonInfo' object has no attribute 'version_nodot'\\nSolution description\\nClear app-data of the virtualenv\\npython -m virtualenv api -vvv --reset-app-data\\nAdded by MarcosMJD\", 'section': 'Module 6: Best practices', 'question': \"Git commit with pre-commit hook raises error ‘'PythonInfo' object has no attribute 'version_nodot'\", 'course': 'mlops-zoomcamp', 'doc_id': 'bae24d3ecb9d08d8f0221637d2cba5ca'}, {'text': 'Problem description\\nProject structure:\\n/sources/production/model_service.py\\n/sources/tests/unit_tests/test_model_service.py (“from production.model_service import ModelService)\\nWhen running python test_model_service.py from the sources directory, it works.\\nWhen running pytest ./test/unit_tests fails. ‘No module named ‘production’’\\nSolution description\\nUse python -m pytest ./test/unit_tests\\nExplanation: pytest does not add to the sys.path the path where pytest is run.\\nYou can run python -m pytest, or alternatively export PYTHONPATH=. Before executing pytest\\nAdded by MarcosMJD', 'section': 'Module 6: Best practices', 'question': 'Pytest error ‘module not found’ when if using custom packages in the source code', 'course': 'mlops-zoomcamp', 'doc_id': '26571e186fa55eb06f58b0b3656296cd'}, {'text': 'Problem description\\nProject structure:\\n/sources/production/model_service.py\\n/sources/tests/unit_tests/test_model_service.py (“from production.model_service import ModelService)\\ngit commit -t ‘test’ raises ‘No module named ‘production’’ when calling pytest hook\\n- repo: local\\nhooks:\\n- id: pytest-check\\nname: pytest-check\\nentry: pytest\\nlanguage: system\\npass_filenames: false\\nalways_run: true\\nargs: [\\n\"tests/\"\\n]\\nSolution description\\nUse this hook instead:\\n- repo: local\\nhooks:\\n- id: pytest-check\\nname: pytest-check\\nentry: \"./sources/tests/unit_tests/run.sh\"\\nlanguage: system\\ntypes: [python]\\npass_filenames: false\\nalways_run: true\\nAnd make sure that run.sh sets the right directory and run pytest:\\ncd \"$(dirname \"$0\")\"\\ncd ../..\\nexport PYTHONPATH=.\\npipenv run pytest ./tests/unit_tests\\nAdded by MarcosMJD', 'section': 'Module 6: Best practices', 'question': 'Pytest error ‘module not found’ when using pre-commit hooks if using custom packages in the source code', 'course': 'mlops-zoomcamp', 'doc_id': '256ce11fbb773de610aa6d94e2a4859a'}, {'text': 'Problem description\\nThis is the step in the ci yml file definition:\\n- name: Run Unit Tests\\nworking-directory: \"sources\"\\nrun: ./tests/unit_tests/run.sh\\nWhen executing github ci action, error raises:\\n…/tests/unit_test/run.sh Permission error\\nError: Process completed with error code 126\\nSolution description\\nAdd execution  permission to the script and commit+push:\\ngit update-index --chmod=+x .\\\\sources\\\\tests\\\\unit_tests\\\\run.sh\\nAdded by MarcosMJD', 'section': 'Module 6: Best practices', 'question': 'Github actions: Permission denied error when executing script file', 'course': 'mlops-zoomcamp', 'doc_id': 'a8ed7eaf24e97b506f589801fb0c95e9'}, {'text': 'Problem description\\nWhen a docker-compose file contains a lot of containers, running the containers may take too much resource. There is a need to easily select only a group of containers while ignoring irrelevant containers during testing.\\nSolution description\\nAdd profiles: [“profile_name”] in the service definition.\\nWhen starting up the service, add `--profile profile_name` in the command.\\nAdded by Ammar Chalifah', 'section': 'Module 6: Best practices', 'question': 'Managing Multiple Docker Containers with docker-compose profile', 'course': 'mlops-zoomcamp', 'doc_id': '0e4752977464f817fda294828b1ae9cf'}, {'text': 'Problem description\\nIf you are having problems with the integration tests and kinesis double check that your aws regions match on the docker-compose and local config. Otherwise you will be creating a stream in the wrong region\\nSolution description\\nFor example set ~/.aws/config region = us-east-1 and the docker-compose.yaml - AWS_DEFAULT_REGION=us-east-1\\nAdded by Quinn Avila', 'section': 'Module 6: Best practices', 'question': 'AWS regions need to match docker-compose', 'course': 'mlops-zoomcamp', 'doc_id': '2d79c5c6ffb2916afff8734235c3587a'}, {'text': 'Problem description\\nPre-commit command was failing with isort repo.\\nSolution description\\nSet version to 5.12.0\\nAdded by Erick Calderin', 'section': 'Module 6: Best practices', 'question': 'Isort Pre-commit', 'course': 'mlops-zoomcamp', 'doc_id': '1ee71c5700f4cbe479fa13272ccb838e'}, {'text': 'Problem description\\nInfrastructure created in AWS with CD-Deploy Action needs to be destroyed\\nSolution description\\nFrom local:\\nterraform init -backend-config=\"key=mlops-zoomcamp-prod.tfstate\" --reconfigure\\nterraform destroy --var-file vars/prod.tfvars\\nAdded by Erick Calderin', 'section': 'Module 6: Best practices', 'question': 'How to destroy infrastructure created via GitHub Actions', 'course': 'mlops-zoomcamp', 'doc_id': '229c6b390dce6d14c9344e792ca1ec08'}]\n"
     ]
    }
   ],
   "source": [
    "# import hashlib\n",
    "\n",
    "# def generate_document_id(doc):\n",
    "#     # combined = f\"{doc['course']}-{doc['question']}\"\n",
    "#     combined = f\"{doc['course']}-{doc['question']}-{doc['text'][:10]}\"\n",
    "#     hash_object = hashlib.md5(combined.encode())\n",
    "#     hash_hex = hash_object.hexdigest()\n",
    "#     document_id = hash_hex[:8]\n",
    "#     return document_id\n",
    "\n",
    "# for doc in documents:\n",
    "#     doc['id'] = generate_document_id(doc)\n",
    "\n",
    "\n",
    "from rag.data.loader import DocumentLoader\n",
    "\n",
    "loader = DocumentLoader()\n",
    "\n",
    "for doc in documents:\n",
    "    # Generate and add unique ID\n",
    "    doc_id = loader.generate_id(doc)\n",
    "    doc[\"doc_id\"] = doc_id\n",
    "\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4672632-acc2-4c1c-96b4-d30f24598aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"You don't need it. You're accepted. You can also just start learning and submitting homework without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.\",\n",
       " 'section': 'General course-related questions',\n",
       " 'question': 'Course - I have registered for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?',\n",
       " 'course': 'data-engineering-zoomcamp',\n",
       " 'doc_id': '386dcf67c83b203e5b424f2ba7489370'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "470b703c-29d6-4ca8-a68e-4c461b3e7a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "410f08db-2302-4c50-926c-511037b46c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashes = defaultdict(list)\n",
    "\n",
    "for doc in documents:\n",
    "    doc_id = doc['doc_id']\n",
    "    hashes[doc_id].append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47c69c01-e952-4818-a307-94ea224ca423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(948, 948)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hashes), len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3e495f9-fdac-436f-88e9-44ae68844ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, values in hashes.items():\n",
    "    if len(values) > 1:\n",
    "        print(k, len(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d25e980-8a04-4186-94ef-c357bdc1b255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "686a488a-67af-4f21-8538-2180dc085fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('documents-with-ids.json', 'wt') as f_out:\n",
    "    json.dump(documents, f_out, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c567515e-a923-487f-9b4d-9f4ce370e2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"text\": \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  \\u201cOffice Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon\\u2019t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
      "    \"section\": \"General course-related questions\",\n",
      "    \"question\": \"Course - When will the course start?\",\n",
      "    \"course\": \"data-engineering-zoomcamp\",\n",
      "    \"doc_id\": \"bae7a31e6abaddb52b4061dcf238fc61\"\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"GitHub - DataTalksClub data-engineering-zoomcamp#prerequisites\",\n"
     ]
    }
   ],
   "source": [
    "!head documents-with-ids.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c062325-5608-4da6-80bf-b9ac371bc17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You emulate a student who's taking our course.\n",
    "Formulate 5 questions this student might ask based on a FAQ record. The record\n",
    "should contain the answer to the questions, and the questions should be complete and not too short.\n",
    "If possible, use as fewer words as possible from the record. \n",
    "\n",
    "The record:\n",
    "\n",
    "section: {section}\n",
    "question: {question}\n",
    "answer: {text}\n",
    "\n",
    "Provide the output in parsable JSON without using code blocks:\n",
    "\n",
    "[\"question1\", \"question2\", ..., \"question5\"]\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51b8f259-eed8-4a50-b50c-2186fb154853",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from rag.llm.openai_client import OpenAIClient\n",
    "\n",
    "custom_client = OpenAIClient(load_env=True)\n",
    "key = custom_client.api_key\n",
    "\n",
    "client = OpenAI()\n",
    "client = OpenAI(api_key=key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "534ac1ae-b5e0-43a7-b8ad-103fd56ced54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(doc):\n",
    "    prompt = prompt_template.format(**doc)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    json_response = response.choices[0].message.content\n",
    "    return json_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb2353f8-411b-4ab9-a4c2-0d158495491e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25a48077-18c0-4d31-82ab-00a44d426279",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "742f4b7c-7632-4475-b6df-b3c02d343287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cd8ea39e2284d60846e98ff530cd5aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/948 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for doc in tqdm(documents): \n",
    "    doc_id = doc['doc_id']\n",
    "    if doc_id in results:\n",
    "        continue\n",
    "\n",
    "    questions = generate_questions(doc)\n",
    "    results[doc_id] = questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "031ecc50-a9a3-4508-8149-cf716e045d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d372539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to binary file\n",
    "with open('results.pkl', 'wb') as f_out:\n",
    "    pickle.dump(results, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de64d355-a4c8-4633-b179-952bb38923c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results.pkl', 'rb') as f_in:\n",
    "    results = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d5cd8f8-ddfd-4802-908b-504722511a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\\n  \"What alteration is necessary if Docker throws an invalid mode error related to the path?\",\\n  \"How can I resolve a Docker daemon error involving \\\\Program Files\\\\Git\\\\var\\\\lib\\\\postgresql\\\\data?\",\\n  \"What\\'s an appropriate mounting path to use instead of \\\\Program Files\\\\Git\\\\var\\\\lib\\\\postgresql\\\\data in Docker?\",\\n  \"Which mounting path adjustments can fix a Docker invalid mode error regarding PostgreSQL data?\",\\n  \"What changes should I make to the mounting path when encountering an invalid mode error with Docker?\"\\n]'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['09505439c01f8a62ddf2d893d6ffc433']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08b07dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bae7a31e6abaddb52b4061dcf238fc61': '[\\n  \"When is the exact start date and time for the course?\",\\n  \"Can you tell me how to subscribe to the course Google Calendar?\",\\n  \"What steps should I take before the course begins?\",\\n  \"How can I join the Telegram channel for course announcements?\",\\n  \"Where should I register to join the course discussion channel?\"\\n]', '3e5d4959603c68a1e154fa2a6bd9d1e8': '[\\n  \"Where can I find the prerequisites for the course?\",\\n  \"What should I review before starting this course?\",\\n  \"Is there a location where I can see the course prerequisites?\",\\n  \"What prior knowledge is required for this course?\",\\n  \"Where are the details on the course prerequisites listed?\"\\n]', '60a31bbef930b3d6b127405fcd0b618e': '[\\n  \"Can I participate without registering?\",\\n  \"Is it possible to join after the course begins?\",\\n  \"Are there consequences for late registration?\",\\n  \"Will I be able to submit assignments without prior registration?\",\\n  \"Is there a deadline for project submissions?\"\\n]', '386dcf67c83b203e5b424f2ba7489370': '[\\n  \"When should I expect to get a confirmation email after enrolling in the Data Engineering Bootcamp?\",\\n  \"Do I need to wait for a confirmation email to start the Data Engineering Bootcamp?\",\\n  \"Is the registration required to begin coursework for the Data Engineering Bootcamp?\",\\n  \"What should I do if I have not received a confirmation email for the Data Engineering Bootcamp?\",\\n  \"Is there any reason to register for the Data Engineering Bootcamp if it is not mandatory for participation?\"\\n]', '6e3550ba00f652ce2fa74706751c4983': '[\\n  \"What steps should I take to prepare before the course begins?\",\\n  \"Which tools and software do I need to install for this course?\",\\n  \"How can I ensure I am ready for the course?\",\\n  \"What are the prerequisite technologies for this course?\",\\n  \"Where can I find the syllabus and prerequisites to review?\"\\n]', 'f8323339d264dc9d40d9dad5a34c06b5': '[\\n  \"How many Zoom Camps are conducted annually?\",\\n  \"When is the Data-Engineering Zoomcamp typically held?\",\\n  \"Can I join the Zoomcamps at any time if I don\\'t want a certificate?\",\\n  \"Are the schedules for each Zoomcamp cohort consistent every year?\",\\n  \"Is there more than one live cohort for each Zoomcamp yearly?\"\\n]', 'd10eed624489c36f17500750ff21c868': '[\\n  \"Will the current course cohort use Mage AI?\",\\n  \"Are there any changes to the terraform videos in the new cohort?\",\\n  \"Did the previous cohort use Prefect for 2023?\",\\n  \"Is Mage AI being introduced in the 2024 cohort?\",\\n  \"Was Airflow used in 2023 for this course?\"\\n]', 'cb86516adcdcafa29f0758ae6ca28a0b': '[\\n    \"Can I still access the course materials when the course ends?\",\\n    \"Is it possible to view homework assignments after the course has finished?\",\\n    \"Can I work on the capstone project post-course completion?\",\\n    \"Will I be able to prepare for the next cohort after the course concludes?\",\\n    \"Are the materials available for self-paced learning after the course ends?\"\\n]', 'fa5c1523945f27f6bb5d9a04f2146a7a': '[\\n  \"Will I still have access to support resources if I choose the self-paced option for the course?\",\\n  \"Where should I look first if I have questions while taking the course?\",\\n  \"Can I use the slack channel to ask questions during the self-paced course mode?\",\\n  \"Is there a way to search for answers to my questions about the course content?\",\\n  \"How reliable is the @ZoomcampQABot for answering course-related queries?\"\\n]', 'a5737de4f33219a4fcfe02f2c746d5a3': '[\\n  \"Where can I find the main course videos on YouTube?\",\\n  \"How do I access the GitHub repository with video thumbnails?\",\\n  \"Is there a specific playlist for year-specific videos and office hours?\",\\n  \"Where can I find the link to the main playlist on Slack?\",\\n  \"What is the URL for the Main \\'DATA ENGINEERING\\' playlist?\"\\n]', 'c3b5714cc4d5a6db4fd5912404f30c31': '[\\n  \"How much time should I dedicate weekly to the course?\",\\n  \"What is the weekly time commitment for this course?\",\\n  \"How many weekly hours do I need to plan for the course?\",\\n  \"What is the expected time investment per week for this course?\",\\n  \"How long should I spend on the course each week?\"\\n]', 'c47f302e889f61f7fd3cee025ef939b6': '[\\n  \"Can I earn a certificate if I complete the course at my own pace?\",\\n  \"Is it possible to receive certification for the course through self-paced learning?\",\\n  \"Do I need to join a live cohort to qualify for a certificate?\",\\n  \"Why are certificates not available for those who choose the self-paced course mode?\",\\n  \"Is peer-reviewing a requirement for obtaining a course certificate?\"\\n]', '6a439fc15426567b38e84acc6142c461': '[\\n    \"How can students participate in Office Hour sessions, given they don\\'t have the Zoom link?\",\\n    \"Where can students submit their questions during the Office Hour or workshop sessions?\",\\n    \"How will students be notified of the video stream URL before the Office Hour begins?\",\\n    \"Is it permissible for students to post questions in the chat during the Office Hour session?\",\\n    \"Where can students watch the live stream of the Office Hour sessions on YouTube?\"\\n]', '093b06705cc2cc1141f667e241e14a06': '[\\n    \"Will the Office Hours sessions be recorded if I can\\'t make it?\",\\n    \"Can I watch the Office Hours recording at any time?\",\\n    \"Are the workshop sessions available to view after they end?\",\\n    \"What should I do if I miss the live Office Hours?\",\\n    \"Is it possible to rewatch the Office Hours after the session?\"\\n]', '1e1c6528694d3abbfc7488bb3abea5e2': '[\\n  \"Where can I find the most recent deadlines for homework and projects?\",\\n  \"How can I stay informed about any deadline changes or extensions?\",\\n  \"Is there a specific place where updated deadlines might be shown if an instructor changes them?\",\\n  \"Who should I pay attention to for announcements about deadline changes or news?\",\\n  \"What should I do to ensure I don\\'t miss any important deadline updates?\"\\n]', '3a90237d0692b97f3f9b322a855cc4fd': '[\\n    \"Can I submit homework after the deadline if the form remains open?\",\\n    \"Is it possible to turn in homework late if the submission portal hasn\\'t closed?\",\\n    \"Are there exceptions for late homework if I can still access the submission form?\",\\n    \"How strict are the submission deadlines if the form is accessible after the due date?\",\\n    \"What happens if I submit homework late but the form is still open?\"\\n]', '9a0107663b3bab95ee3787a55673cde6': '[\\n  \"What is the URL where I should submit my homework?\",\\n  \"Where should I host my course code work?\",\\n  \"Where can I find the appropriate location for my homework files?\",\\n  \"Can you clarify where to upload weekly exercise solutions?\",\\n  \"Which platforms are acceptable for hosting my code repository?\"\\n]', 'b666ae0b62d6a6dfc38396ba510cdc6b': '[\\n    \"How are points awarded for submitting homework on the course management platform?\",\\n    \"Where can I see the total points I have earned from homework submissions?\",\\n    \"How is the leaderboard score calculated for students participating in the course?\",\\n    \"What point is given for contributing to the FAQ section in the course?\",\\n    \"How many points are awarded for each Learning in Public submission?\"\\n]', 'd418a703e60b8555c41dc80863f589dc': '[\\n    \"How can I find which name I am listed under on the leaderboard?\",\\n    \"Where do I edit my Display name for the course leaderboard?\",\\n    \"How do I change my assigned name on the leaderboard?\",\\n    \"What steps should I follow to view my Display name in the course?\",\\n    \"Is it possible to modify the random name assigned to me on the leaderboard?\"\\n]', '2a0887600c276fd6f9d5a5ead914df0b': '[\\n    \"What version of Python is recommended for our course in 2024?\",\\n    \"Should we use Python 3.9 during the course for troubleshooting purposes?\",\\n    \"Is it okay to use Python 3.10 instead of 3.9 in the class?\",\\n    \"Will Python 3.11 be compatible with our course materials?\",\\n    \"Is Python 3.9 chosen primarily for stability reasons?\"    \\n]', '821ffc7eef9760e690cf6f6f3d01f191': '[\\n  \"What are the options for setting up my working environment for the course?\",\\n  \"Can I use my local machine for the environment setup if I work on Windows?\",\\n  \"What should I start with if I prefer working on my local machine?\",\\n  \"How can I set up my environment if I want to use a virtual machine?\",\\n  \"Why might someone choose to use a virtual machine for this course?\"\\n]', 'c4138ffefdd3d00b85496250ace2c836': '[\"Can GitHub Codespaces be used instead of CLI or Git Bash for data ingestion and Docker file creation?\", \"Does GitHub Codespaces provide Linux resources with pre-installed tools?\", \"What pre-installed tools are available in GitHub Codespaces?\", \"Can any GitHub repository be opened in a GitHub Codespace?\", \"Is Python included in the tools pre-installed in GitHub Codespaces?\"]', '13cc1cfada6c696dab0c83e781774b93': '[\\n    \"Is it necessary to work on GitHub codespaces for this course?\",\\n    \"Am I required to use GitHub codespaces if I have my own setup?\",\\n    \"Do I have options other than GitHub codespaces for the course environment?\",\\n    \"Can I complete the course on my personal laptop without using GitHub codespaces?\",\\n    \"Is using a GCP VM mandatory for the course tasks?\"\\n]', '3cc081b0deeb66dac1f6e6f80b859bc3': '[\\n  \"Is it necessary to use both GitHub Codespaces and GCP for the course project?\",\\n  \"Can I rely on just GitHub Codespaces or GCP, and which would you suggest?\",\\n  \"Should I focus on GCP to benefit from BigQuery?\",\\n  \"Is setting up a local environment sufficient for the course?\",\\n  \"Would learning BigQuery be more beneficial than using GitHub Codespaces?\"\\n]', '5a76ede2a7f68e863f6da7276d0ce106': '[\\n  \"What are the steps to open the Run command window on Windows?\",\\n  \"How can I access the Registry Editor to change registry values?\",\\n  \"Where can I find the \\'Autorun\\' registry value to modify it?\",\\n  \"What is an alternative solution to changing the registry values for connecting to a GCP VM?\",\\n  \"Where is the known_hosts file located on a Windows machine?\"\\n]', '905a9a5de263c002c6b8b311a20f4f03': '[\\n  \"Why is GCP preferred over Azure or AWS for this course?\",\\n  \"Can I choose a cloud provider other than GCP for my course projects?\",\\n  \"What are the benefits of using GCP for new users in this course?\",\\n  \"Why do we use BigQuery as part of the coursework?\",\\n  \"Is a credit card mandatory to access the GCP free trial for this course?\"\\n]', '6cb80ff3a8a0c09a2273b073933b7311': '[\\n    \"Can I avoid paying when using GCP?\",\\n    \"Is there a cost if I utilize the GCP free trial?\",\\n    \"Are cloud service fees mandatory with GCP?\",\\n    \"Will I incur fees on GCP if I use the free trial?\",\\n    \"Does GCP free trial require payment?\"\\n]', 'e5a35da13d8561565696b031a6a2f7be': '[\\n  \"Is it possible to complete most of your course without using a cloud provider?\",\\n  \"Can I run the course materials locally if GCP isn\\'t available?\",\\n  \"Are there local alternatives available for all course components excluding BigQuery?\",\\n  \"Does the course provide guidelines for setting up a home lab environment?\",\\n  \"Is it possible to complete the entire course using local resources instead of GCP?\"\\n]', '6777d05ae24b183acd0ce819e589662c': '[\\n  \"Is it acceptable to use AWS for building the data pipeline and visualization tasks in the course?\",\\n  \"Can I use AWS for the coursework and are there any specific adaptations I should be aware of?\",\\n  \"Will there be any challenges in getting assistance if I choose to use AWS instead of GCP for the course?\",\\n  \"What should I consider when deciding to use AWS over GCP for the course tasks?\",\\n  \"Is peer support available if I run into issues while using AWS for this course?\"\\n]', 'cd540c158b2253368bde0deecc09cfea': '[\\n  \"Will there be live calls scheduled during the Capstone period?\",\\n  \"Are additional live Zoom sessions planned apart from Office Hours?\",\\n  \"How will I know if calls are scheduled during the Capstone period?\",\\n  \"Is there a chance of having live Zoom calls outside of Office Hours?\",\\n  \"When will we be informed about extra Zoom calls during Capstone?\"\\n]', '66ed5eeacae24a08854edfbdd2cef8c6': '[\"Are we still using the NYC Trip data for the same project as last year?\", \"Will the NYC Trip project data for January stay the same as last year?\", \"Is the NYC Trip data from January 2021 being utilized again for the course project?\", \"Are the NYC Trip datasets for this year\\'s project identical to last year\\'s?\", \"Will the course project continue using the January 2021 NYC Trip data or switch to 2022 data?\"]', '43f190716d165eb1f69e48c7354d84d8': '[\\n    \"Where can I find the materials from the 2022 repository?\",\\n    \"Has the content from 2022 been relocated?\",\\n    \"Is it possible to access the materials from 2022?\",\\n    \"Have the 2022 resources been transferred to a new location?\",\\n    \"Can I still view the 2022 repository\\'s content?\"\\n]', 'd9faa54bdb2c801f4fc21fd5d8632468': '[\\n    \"Is it permissible to utilize different tools for the final project?\",\\n    \"Am I allowed to choose any software for my project?\",\\n    \"Can I select my own preferred tool for the final project?\",\\n    \"May I use a tool that\\'s not specified for my project?\",\\n    \"Is it acceptable to implement a different tool of choice for the project?\"\\n]', '0d049ccb21a1875fe936664a7738f885': '[\"Is it possible to use an alternative tool instead of the ones mentioned in the course?\", \"Can I opt for Airflow or Prefect rather than Mage during the course?\", \"Is it allowed to use AWS or Snowflake instead of GCP products?\", \"Am I permitted to use Tableau instead of Metabase or Google Data Studio?\", \"What are the requirements if I decide to use a different tool stack for my project?\"]', '70bab3635e1e14c864d64ff31a7f2086': '[\\n    \"What actions can we take to support this course?\",\\n    \"Is there a way to promote the course to others?\",\\n    \"How can we propose improvements to course materials?\",\\n    \"Can we improve the structure of the course repository?\",\\n    \"What is a simple way to show appreciation for the course?\"\\n]', 'e38d8a50c8c70bc8ab52ee1454385d5f': '[\\n    \"Is Linux the ideal operating system for this course?\",\\n    \"Can I take this course using a Windows computer?\",\\n    \"Will I be able to follow the course on a Mac?\",\\n    \"Does the course support multiple operating systems?\",\\n    \"Have previous students used different OSes successfully in this course?\"\\n]', 'db45cb7fc7b4b9952fac580fe960b547': '[\\n    \"What can Windows users do when they encounter issues with shell scripts in certain modules?\",\\n    \"Why are Windows users advised to set up a WSL environment from the beginning of the course?\",\\n    \"Is using git bash or MINGW64 a viable solution for running shell scripts on Windows?\",\\n    \"What specific modules use shell scripts that might be problematic for Windows users?\",\\n    \"How did previous cohorts of Windows users manage shell script roadblocks given the lack of clear guidance?\"\\n]', 'be3ebb503529b4378a61baf869e1283c': '[\"Are there books or resources you suggest for this course?\", \"Can you direct me to any recommended reading materials?\", \"Do you have a list of useful resources I should check out?\", \"Is there a document with additional resources you endorse?\", \"Where can I find books or materials you recommend for this course?\"]', '7d4799d5ce9bf3ba9fbc9fb9dfb8b6b0': '[\\n  \"What happens if I don\\'t submit my project on time?\",\\n  \"How many chances do I have to pass the project?\",\\n  \"What is the difference between the first and second project attempt?\",\\n  \"Can I still submit if I miss the initial project deadline?\",\\n  \"What should I do if I fail the first project submission?\"\\n]', '27967023a6e120e0295d6ccbeea93ab3': '[\\n  \"What are the first steps I should take when I encounter an issue during the course?\",\\n  \"How can I effectively search for solutions to problems I face?\",\\n  \"What information should I include when asking a question on Slack?\",\\n  \"Why is it not recommended to use screenshots when asking for help?\",\\n  \"How should I handle a situation where the problem persists even after trying suggested solutions?\"\\n]', 'a4a893511970b4f7c6042d6dd3245aab': '[\"What information should I include to ask questions about course issues?\", \"When is it necessary to ask someone else for help with my code?\", \"What details are essential when explaining the coding environment for troubleshooting?\", \"Why should I mention what I\\'ve already tried when asking for help?\", \"How can I avoid repetitive instructions when seeking help from others?\"]', 'b94eb7c49602d342b87e6fa16e0933e6': '[\\n  \"What is the first step to using GitHub for this course?\",\\n  \"How do I access the instructors’ code locally from GitHub?\",\\n  \"Where can I find guidance on creating my own repositories?\",\\n  \"What types of files should I avoid storing in a git repository?\",\\n  \"What precautions should I take regarding sensitive information in a git repository?\"\\n]', '410e2883d6fde933ca239f0561ffcdf3': '[\\n  \"What should I do if I encounter an error about a missing separator in a Makefile?\",\\n  \"How can I resolve errors related to tabs in VS Code when working with Makefiles?\",\\n  \"What is the solution if VS Code uses spaces instead of tabs and causes a Makefile error?\",\\n  \"Why is there a \\'missing separator\\' error in my Makefile and how do I fix it?\",\\n  \"How do I convert spaces to tabs in VS Code to prevent Makefile errors?\"\\n]', '4eacb446aca37f3c80e188822e941b64': '[\"How can I open an HTML file from Linux on WSL with a Windows browser?\", \"What do I need to install to open an HTML file on WSL with a Windows browser?\", \"Which command allows you to open a webpage on WSL?\", \"How can I specify a different browser when opening HTML files on WSL?\", \"What is the purpose of setting the BROWSER environment variable in WSL?\"]', '1c0bf6109b0a22dcc724356f80bdc64b': '[\\n  \"How can I set up Chrome Remote Desktop on a Debian Linux virtual machine instance on Compute Engine?\",\\n  \"What is the solution if I encounter an \\'ERROR 403: Forbidden\\' while downloading 2021 data from the TLC website?\",\\n  \"Where can I find a backup for the 2021 Yellow Taxi Trip Records data?\",\\n  \"What command should I use to unzip a \\'gz\\' file since the \\'unzip\\' command doesn\\'t work for this format?\",\\n  \"What service allows remote access to applications with a graphical user interface on Compute Engine?\"\\n]', '986c62fecab40efdcdb546d45bc9eaaa': '[\\n    \"How should we handle taxi data files now that they are available as *.csv.gz?\",\\n    \"What is the alternative method to store the data file correctly instead of using output.csv?\",\\n    \"How can we determine the correct csv_name from the taxi data file URL?\",\\n    \"What part of the URL should be used to correctly name the taxi data file?\",\\n    \"Can the pandas read_csv function handle files with a csv.gz extension?\"\\n]', '45fafc4085fb6c0006de2cd36daa932a': '[\\n  \"Where can I find the data dictionary for yellow taxi trips in New York?\",\\n  \"What is the URL for the data dictionary of green taxi trips?\",\\n  \"How do I access the yellow trip records data dictionary for NY taxis?\",\\n  \"Could you provide the link to the green taxi data dictionary?\",\\n  \"Where is the data dictionary for New York yellow trip records available online?\"\\n]', 'f8d07d24cff4ccd512186da29f1a0f29': '[\\n  \"How do I unzip a downloaded parquet file from the taxi data in the command line?\",\\n  \"What is the recommended way to import an unzipped CSV file into pandas as shown in the videos?\",\\n  \"How can I modify the Python script ingest_data.py to work directly with parquet files?\",\\n  \"What line should be added to the def main(params) in ingest_data.py for handling parquet files?\",\\n  \"How can I convert a downloaded parquet file to CSV and ensure relevance to the existing code?\"\\n]', '7e938b178c613e38d4ad0b60cbbd5ec5': '[\"How can I install wget on an Ubuntu system?\", \"What is the simplest method to install wget on a MacOS computer?\", \"What are the steps to install wget on a Windows machine using Chocolatey?\", \"How can I use wget with Python for file downloading?\", \"Where can I download the wget binary for Windows computers?\"]', 'ee6ddd0a3149f6fb83a51da174ca36b0': '[\"How can I resolve a certificate verification error in wget on MacOS when using Jupyter Notebook?\", \"What should I add before wget when facing a certificate error in a Jupyter environment?\", \"How can I bypass certificate checks when using wget command line?\", \"Which Python library can be used with wget to handle certificate errors?\", \"What option can be added to wget to ignore certificate verification errors?\"]', 'af897dc72fc98964eb311728956ac0cf': '[\\n  \"How can I use the backslash as an escape character in Git Bash for Windows?\",\\n  \"What command should I use to set the backslash as an escape character in Git Bash?\",\\n  \"Is it necessary to include the bash escape character command in the .bashrc file?\",\\n  \"What specific terminal command allows the use of backslash as an escape character in Git Bash?\",\\n  \"How does Alexey use the backslash as an escape character in Git Bash on Windows?\"\\n]', 'da30e93aa55a403a1b97ea56540179a8': '[\\n  \"How can secrets be stored for use in GitHub Codespaces?\",\\n  \"What instructions are available for managing account-specific secrets in GitHub Codespaces?\",\\n  \"Where can I find guidelines on storing secrets in GitHub Codespaces?\",\\n  \"How do I manage my account-specific secrets for GitHub Codespaces?\",\\n  \"Can you direct me to the documentation for storing secrets in GitHub Codespaces?\"\\n]', '0fe88ef23e6b302f64c0306a9a4db629': '[\\n    \"How can I resolve the error that says it cannot connect to the Docker daemon?\",\\n    \"What should I do if Docker isn\\'t running properly on my system?\",\\n    \"How do I check if the Docker daemon is running after receiving a connection error?\",\\n    \"What steps should I follow to troubleshoot a Docker connection issue?\",\\n    \"How can I update WSL in PowerShell to address a Docker daemon error?\"\\n]', '2c141bf23500c4a0b3310bb6041cac2f': '[\\n  \"What are the backends Docker engine can use on Windows according to its official documentation?\",\\n  \"What must Windows 10 Pro or 11 Pro users enable to use the Hyper-V back-end for Docker?\",\\n  \"Why can\\'t Windows 10 Home or 11 Home users use Hyper-V for Docker?\",\\n  \"Where can I find instructions on installing WSL on Windows 11 for Docker?\",\\n  \"What should I do if I encounter the error \\'WslRegisterDistribution failed with error: 0x800701bc\\' when installing WSL2?\"\\n]', 'b1da87c134123eec20fbdba58a0b0aaa': '[\\n    \"Why might I receive a \\'requested access to the resource is denied\\' message when pulling a Docker image?\",\\n    \"What should I check if I get a \\'repository does not exist or may require docker login\\' error?\",\\n    \"Is a docker login required to fetch images during the Data Engineering Zoomcamp?\",\\n    \"What steps should be taken if I encounter a \\'permission denied\\' error on macOS M1 when mounting a volume for a PostgreSQL Docker container?\",\\n    \"How can I resolve compatibility issues when creating a PostgreSQL container on macOS M1 with Rancher Desktop?\"\\n]', '30313b1b331a278d08a0a4a12497cdb5': '[\\n  \"How can I delete a folder mounted to a Docker volume on my local machine?\",\\n  \"Why can\\'t I delete a folder created by Docker even when I try to drag it to the trash?\",\\n  \"What command should I use to remove a directory that Docker has protected?\",\\n  \"What permissions issue might prevent Obsidian from starting when a folder is mounted to Docker?\",\\n  \"How do you forcefully remove a directory created by a PostgreSQL Docker container?\"\\n]', 'd529aa12653e124c6ca7aacf8ff02a3f': '[\\n    \"What should I do if Docker won\\'t start or is stuck in settings on Windows 10 or 11?\",\\n    \"How can I ensure I am using the latest version of Docker for Windows?\",\\n    \"What steps should I take if Docker is stuck on starting?\",\\n    \"How can I switch Docker containers between Windows and Linux on Windows 10 or 11?\",\\n    \"What are the requirements for running Docker with Hyper-V on Windows Pro Edition?\"\\n]', '5bcdc9b262f8cacfc146e5850803809b': '[\"Which file system should I use to run docker commands on Windows?\", \"Is it possible to run Docker on Windows 10 or 11 Home Edition?\", \"What should I do if Docker gets stuck after ensuring WSL2 or Hyper-V is correctly set up?\", \"Where can I find a tutorial for setting up Docker on WSL2?\", \"How can Docker be reset if it\\'s not functioning properly?\"]', 'e91c73638688cb35359988653675589c': '[\"Where should Windows 10 Home users store code for optimal Docker performance?\", \"Why is WSL2 recommended for Docker on Windows 10 Home?\", \"What backend does Docker use by default on Windows 10 Home?\", \"Where can I find best practices for Docker on Windows?\", \"Why is the default Linux distro used for code storage in Docker?\"]', '46d432046a6c65e5a83f8e2c6d975e7c': '[\\n    \"What should I do if I encounter the error \\'the input device is not a TTY\\' on Docker for Windows?\",\\n    \"How can I solve the problem with Docker on Windows when using mintty?\",\\n    \"What command prefix is recommended to resolve the TTY error in Docker on Windows?\",\\n    \"Can I create an alias to automatically use winpty with Docker on Windows?\",\\n    \"Where should I add the alias for winpty when using Docker on Windows?\"\\n]', '330244beecd57dd10ec034a87be3b486': '[\"What error might occur when trying to pip install on a Docker container in Windows?\", \"How can one fix a connection error when pip installing in Docker on Windows?\", \"What could cause a temporary failure in name resolution during pip installation in Docker?\", \"What DNS solution could resolve a broken connection during pip installation in Windows Docker?\", \"Which Docker command could help establish a new connection for pip install on Windows?\"]', 'ef728ab7bd73e5885bb969c5ed448735': '[\\n    \"How can I resolve the issue of the ny_taxi_postgres_data folder being empty after running a Docker script on Windows?\",\\n    \"What are the necessary environment variables to set when using Docker for the ny_taxi database?\",\\n    \"What is the significance of using quotes around the absolute path in the -v parameter while running Docker on Windows?\",\\n    \"Which PostgreSQL version is recommended for setting up the ny_taxi database with Docker as mentioned in Module 1?\",\\n    \"How do I ensure that the files are visible in the VS Code ny_taxi folder after running the Docker script?\"\\n]', '01c76f428c5063a556ab487016fc8d92': '[\\n  \"What is the recommended method for setting up Docker on macOS according to the record?\",\\n  \"Is there a specific article mentioned for setting up Docker on a Mac in the record?\",\\n  \"Why might the method mentioned in the article for setting up Docker on macOS be considered outdated?\",\\n  \"What is the alternative method suggested for downloading Docker on macOS in the record?\",\\n  \"Has the alternative method for installing Docker on macOS caused any issues according to the record?\"\\n]', 'baf320ea02c708552e1587f4074271cb': '[\\n  \"What should I do if I encounter \\'could not change permissions of directory \\\\\"/var/lib/postgresql/data\\\\\": Operation not permitted\\' when running Docker?\",\\n  \"How can I create a local Docker volume to solve permission issues with PostgreSQL data directory?\",\\n  \"How can I ensure my Docker volume is correctly created and mapped for PostgreSQL data?\",\\n  \"What should I do if I see the error \\'directory \\\\\"/var/lib/postgresql/data\\\\\" exists but is not empty\\' when initializing a PostgreSQL database?\",\\n  \"How can I verify that the Docker volume dtc_postgres_volume_local is listed correctly in Docker Desktop app?\"\\n]', 'f8af77b84b0f9ede78ea7ae3f1d88e23': '{\\n  \"questions\": [\\n    \"What should I do if Docker\\'s mapping volumes on Windows doesn\\'t work as shown in the course video?\",\\n    \"How can I resolve the Docker error \\'invalid reference format: repository name must be lowercase\\' when mounting volumes on Windows?\",\\n    \"What are the suggested command adjustments for running Docker on Windows when dealing with data directories?\",\\n    \"Why does using \\'/c\\' not work for volume mapping on Windows and what alternative should I use?\",\\n    \"What are the options for using volume names instead of paths in Docker and how does it differ for Windows and Mac?\"\\n  ]\\n}', '09505439c01f8a62ddf2d893d6ffc433': '[\\n  \"What alteration is necessary if Docker throws an invalid mode error related to the path?\",\\n  \"How can I resolve a Docker daemon error involving \\\\Program Files\\\\Git\\\\var\\\\lib\\\\postgresql\\\\data?\",\\n  \"What\\'s an appropriate mounting path to use instead of \\\\Program Files\\\\Git\\\\var\\\\lib\\\\postgresql\\\\data in Docker?\",\\n  \"Which mounting path adjustments can fix a Docker invalid mode error regarding PostgreSQL data?\",\\n  \"What changes should I make to the mounting path when encountering an invalid mode error with Docker?\"\\n]', 'c4773a912a8a34adf75bf48cca50dcd4': '[\\n  \"What causes the error message when running Docker with volume mounting for the second time?\",\\n  \"How should I adjust my Docker command to prevent an error related to buildmount source path on repeated runs?\",\\n  \"Why should I avoid mounting a volume the second time I run a Docker container with specific PostgreSQL settings?\",\\n  \"Can you explain how to modify the Docker run command to fix the error about creating buildmount source path?\",\\n  \"What is the command to run a PostgreSQL Docker container without volume mounting to avoid errors?\"\\n]', '11534759f9dbcaaefe703778ce36a4e5': '[\"What is the cause of the Docker build error when it states \\'can\\'t stat\\' regarding a directory?\", \"How can I resolve a permission issue with the \\'ny_taxi_postgres_data\\' directory during a Docker build?\", \"What files should be included to successfully run the Docker build command in this context?\", \"Where can I find more detailed information about Docker build context errors?\", \"How can I change the directory permissions on Ubuntu to fix a Docker build issue?\"]', 'deda982166aa674f4387114bb2964474': '[\\n  \"What should I do if I see the error \\'ERRO[0000] error waiting for container: context canceled\\'?\",\\n  \"How can I verify if Docker is installed via snap?\",\\n  \"What does the message \\'error: unknown command \\\\\"status\\\\\", see \\'snap help\\'.\\' indicate when checking Docker installation?\",\\n  \"If \\'Bind for 0.0.0.0:5432 failed: port is a\\' occurs, what might be the issue?\",\\n  \"What steps are needed to deinstall Docker installed via snap and reinstall it from the official website?\"\\n]', '3a2aef76af2ccede4862e0e153659735': '[\"Why am I getting a build error related to \\'can’t stat\\' on my project when using Docker?\", \"What might cause a folder to appear empty on PopOS Linux despite having files when using Docker?\", \"How can I solve the error caused by lack of authorization rights to a host folder in Docker?\", \"What command can I use to give permission to a folder for everyone causing Docker issues?\", \"How should I change folder permissions to fix build errors in Docker on PopOS Linux?\"]', 'db1a5ca1aaf92db8951bdcb973be73b5': '[\"What should I do if I encounter a \\'permission denied\\' error when rebuilding a Docker container on Ubuntu/Linux systems?\", \"How can I resolve a Docker build error related to \\'failed to read dockerfile: error from sender\\'?\", \"What command do I use to grant permissions to a folder when Docker build fails due to permission issues?\", \"If I still see permission problems after using chmod 755, what should I do next?\", \"Why does a permission error occur when executing the Docker build command multiple times on Linux?\"]', 'cc748fbd54f00462f5d629d496ce6ad8': '[\\n  \"How can I find the name of a network using Docker?\",\\n  \"What is the command to list Docker networks?\",\\n  \"How do I determine the name of my Docker network?\",\\n  \"What\\'s the procedure to obtain a Docker network\\'s name?\",\\n  \"What command helps me see the network names in Docker?\"\\n]', 'b37556b8c705e8232cf2d850e9752ebb': '[\\n    \"What should I do if I get a conflict error because the container name \\'pg-database\\' is already in use?\",\\n    \"How can I resolve the Docker conflict error related to container names when trying to restart an image?\",\\n    \"What command should I use to remove an existing Docker container with a conflicting name?\",\\n    \"How can I restart a Docker image configured with a network name without removing it?\",\\n    \"What is the solution for the error response from Docker daemon concerning a conflict with a container name?\"\\n]', 'c7280035dea7d06185338a83effb576f': '[\\n  \"What error might occur when using docker-compose with ingestion scripts?\",\\n  \"Which error message indicates a failure to translate the host name in docker-compose?\",\\n  \"How should the network name be changed when using docker-compose for ingestion?\",\\n  \"What should be checked in docker-compose up -d if there\\'s a translation failure with the host name?\",\\n  \"How can I identify the correct database name to use in my docker-compose setup?\"\\n]', '22a55ea2c032175dac184c3df8b5addd': '[\\n  \"What command should I run before starting my VM to solve Docker installation issues on a MacOS or Windows 11 VM on Linux?\",\\n  \"How can I enable nested virtualization for a VM running Docker on a Linux host?\",\\n  \"What steps should I take to resolve nested virtualization problems on an Intel CPU while installing Docker?\",\\n  \"How do I configure nested virtualization on an AMD CPU for Docker installation on MacOS/Windows 11 VMs?\",\\n  \"What is the process to handle nested virtualization issues when using Docker on a VM environment?\"\\n]', 'fb4bf292ef5fcbd94ca9c5338e4705e0': '[\\n   \"How can I manage Docker containers and images using VS Code?\",\\n   \"What is required to connect VS Code to Docker running on WSL2?\",\\n   \"Where in VS Code can I launch the Docker extension?\",\\n   \"How do you stop a running Docker container?\",\\n   \"Can VS Code interface with a Docker instance on Linux?\"\\n]', 'df8ca5ad410a99111bb9b4353e020576': '[\\n    \"Why does my PostgreSQL container refuse connections and show a shutdown message in the logs?\",\\n    \"What should I do if my PostgreSQL container\\'s logs state that the database directory contains a database and the system is shut down?\",\\n    \"How can I resolve connection failures to my PostgreSQL container caused by unexpected server shutdowns?\",\\n    \"What is the solution if my PostgreSQL container terminates abnormally before processing requests?\",\\n    \"How do I fix my PostgreSQL container that isn\\'t accepting requests and shows the server closed connection unexpectedly?\"\\n]', '8cb2378ff9ca8c4020541c368f49a033': '[\\n    \"How can Docker be installed on some Ubuntu versions?\",\\n    \"Is there an alternative method to install Docker on Ubuntu?\",\\n    \"What command is used to install Docker via snap on Ubuntu?\",\\n    \"Which tool can be used if Docker cannot be installed directly on Ubuntu?\",\\n    \"What is the sudo command to install Docker with snap on Ubuntu?\"\\n]', '59f9a9d64404a78ca0b6793117f47b8e': '[\"What should I do if I encounter a permission error when mounting a directory with Docker-Compose?\", \"How can I define a named volume in a Docker-Compose file?\", \"Why are all services in a Docker-Compose file part of the same network?\", \"How do I locate the mount point of a Docker volume?\", \"What steps should I follow if Docker-Compose creates a directory with an incorrect name?\"]', '6500cf47f217d6d0894d6c7e11bc3b73': '[\\n    \"What should I ensure is running if I encounter an error translating host name to address with Docker-Compose?\",\\n    \"Which command do I use to start Docker containers in detached mode?\",\\n    \"How can I view the running Docker containers after starting them?\",\\n    \"What command should I use to view logs for a specific Docker container?\",\\n    \"What should I do if \\'docker ps\\' doesn\\'t show the pgdatabase container as running?\"\\n]', '8e23179495b674317d1772d9963aec43': '[\\n    \"What should I do if I encounter the error \\'could not translate host name pg-database to address\\' when using Docker-Compose?\",\\n    \"How can I resolve issues with data loss after running docker-compose up when developing with Docker and Terraform?\",\\n    \"What steps can I take if I receive an OperationalError related to pg-database in Docker Compose?\",\\n    \"How do I find the new network name created by Docker Compose after executing docker compose up?\",\\n    \"What alternative tools can I use if I face persistent issues with pgcli in the context of Docker and Terraform?\"\\n]', 'cd5f39582fb837c66f23571914f8715e': '[\\n  \"What can I do if I see an error response from the daemon indicating a network not found?\",\\n  \"How can I resolve the issue when the server is unable to connect because the hostname does not resolve?\",\\n  \"What should I check in the docker-compose.yml file to ensure connectivity between containers?\",\\n  \"Which command should I use to view all stopped and running containers for troubleshooting?\",\\n  \"How should I alter the hostname in docker-compose if it cannot be translated to an address?\"\\n]', '0103de3a45e2a6037776195e87a4c8e4': '[\"How can I ensure that PGAdmin data is persisted when using docker-compose on GCP?\", \"What is a common issue with postgres data persistence when using docker-compose on GCP?\", \"Why might the volume mapping \\'./pgadmin:/var/lib/pgadmin:wr\\' fail to persist data on GCP?\", \"What is the recommended solution for persisting PGAdmin data with docker-compose on GCP?\", \"What change should be made to the docker-compose file to ensure data persistence for PGAdmin?\"]', '6bfc4d459578c2b5915e9657999d4fad': '[\\n  \"What should I do if my Docker engine continuously crashes and won\\'t work after a restart?\",\\n  \"How do I resolve an issue where Docker fails to fetch extensions and keeps stopping?\",\\n  \"Why does my Docker show errors and crash continuously after updating?\",\\n  \"What can I do if updating Docker doesn\\'t fix the crashing and extension fetching issues?\",\\n  \"Is there a way to resolve Docker crashing issues without losing my images?\"\\n]', 'a35444e7d68896d5b61ce6df2d9a47a8': '[\\n    \"How can I persist pgAdmin configuration using Docker-Compose?\",\\n    \"What is the purpose of the \\'volumes\\' section in the Docker-Compose configuration for pgAdmin?\",\\n    \"How do I configure user access to the \\'pgAdmin_data\\' folder for pgAdmin in Docker-Compose?\",\\n    \"What is the significance of the pgAdmin container\\'s username and group in relation to the mounted volume?\",\\n    \"Which command is used to give write access to the pgAdmin container for the \\'pgAdmin_data\\' folder?\"\\n]', 'e01623d2ae1685ee89e1960a99b997cd': '[\\n  \"What should I do if I encounter a permission denied error while using Docker-Compose?\",\\n  \"How can I maintain the pgAdmin state so it remembers my previous database connection?\",\\n  \"What steps are necessary if I want to run Docker without using sudo?\",\\n  \"How do I set up a volume for pgAdmin in my docker-compose.yaml file?\",\\n  \"What changes should I make to my docker-compose.yaml file to persist pgAdmin data?\"\\n]', 'd52183c3c3682727977fbfb8d5c7991c': '[\\n    \"Why is the docker-compose command unavailable after modifying .bashrc?\",\\n    \"What should I do if docker-compose is not recognized in Google Cloud VM?\",\\n    \"How can I resolve the issue of docker-compose not being found after installation?\",\\n    \"What changes are needed if the docker-compose file downloaded from GitHub doesn\\'t work?\",\\n    \"Why does docker-compose-linux-x86_64 need to be renamed to docker-compose?\"\\n]', '46cf484a8e6061e243b8a6c302fe5773': '[\\n    \"What could solve the credential error in Docker-Compose related to docker-compose up -d?\",\\n    \"How can the credential issue with Docker-Compose be addressed?\",\\n    \"What command might fix an error with credentials after using docker-compose up?\",\\n    \"How do you address the credential error in Docker-Compose after doing docker-compose up -d?\",\\n    \"Where can I find more information about the Docker-Compose credential issue fix?\"\\n]', 'b5f20c6db58fcb09be95a2ca60a8780a': '[\\n    \"What steps should I follow if I\\'m experiencing issues with Docker Compose related to the docker-compose.yml and pgadmin setup?\",\\n    \"How can I resolve data import issues in PostgreSQL using Docker Compose for this course?\",\\n    \"What changes are necessary in the docker-compose.yml file to correctly set up pgadmin and postgres?\",\\n    \"What is the correct order of executing commands when working with docker-compose and data ingestion in this module?\",\\n    \"How should I configure pgadmin to match the settings in my docker-compose.yml after setting up Docker Compose?\"\\n]', '0abf41c5114a6dfbbae5c5670b748607': '[\\n  \"How do I resolve a Docker Compose error about missing docker-credential-desktop?\",\\n  \"What should I do if I encounter \\'docker-credential-desktop\\' not found in PATH?\",\\n  \"Where can I find the config.json file for Docker to fix credential issues?\",\\n  \"How do I edit the Docker config.json to handle a credsStore error?\",\\n  \"What changes are needed in config.json to fix a credential store error in Docker Compose?\"\\n]', '3806ea67375141d89d62dc0d0d1d1940': '[\\n    \"How do I determine the correct docker-compose binary for my WSL system?\",\\n    \"What commands can I use to check my system\\'s compatibility for docker-compose on WSL?\",\\n    \"Where can I download the appropriate docker-compose binary for WSL?\",\\n    \"What does the command \\'uname -s\\' typically return when checking for docker-compose on WSL?\",\\n    \"How do I use curl to download docker-compose for my specific system architecture?\"\\n]', '7e8ff37a5bb11dba5470c2c81f3755a3': '[\\n    \"What should I do if I receive an \\'undefined volume\\' error in Docker-Compose while using Windows/WSL?\",\\n    \"How can I fix the issue where the service \\'pgdatabase\\' refers to an undefined volume in my docker-compose file?\",\\n    \"What additional lines do I need to add to my docker-compose.yaml file to address the \\'invalid compose project\\' error?\",\\n    \"In which section of the docker-compose file should I include the definition for dtc_postgres_volume_local?\",\\n    \"What is the correct level for placing the volumes section in a docker-compose file when encountering errors?\"\\n]', '738a0fec53294dcda0d394ab868f24d5': '[\\n  \"Why do WSL and Windows cause a permissions issue when using the Windows file system instead of the WSL file system?\",\\n  \"What is the solution to resolving directory permissions errors in WSL when using Docker?\",\\n  \"Why is it unnecessary to specify the \\'user:\\' field if using Docker volumes?\",\\n  \"What are the benefits of using Docker volumes to resolve permissions issues in WSL?\",\\n  \"When configuring docker-compose.yaml, what should the \\'services: postgres:\\' section include to address permission errors?\"\\n]', '5118357924412cfb8a903a2a9ac416e8': '[\\n    \"What should I do if pgadmin isn\\'t working for querying in Postgres on Windows?\",\\n    \"Why does pgadmin have issues with querying on Windows git bash or VM?\",\\n    \"What libraries might I need if pgadmin fails on Windows?\",\\n    \"How can I query Postgres if pgadmin is not functioning correctly?\",\\n    \"What tool can I use as an alternative to pgadmin for queries?\"\\n]', 'b356773f0fdfa7e80e8379ec97de86b0': '[\\n  \"How can insufficient system resources issues in WSL be resolved?\",\\n  \"What might cause \\'Insufficient system resources\\' errors in WSL?\",\\n  \"What steps should I take to update Windows Terminal?\",\\n  \"How do I update Windows security updates to resolve WSL issues?\",\\n  \"Where do I find updates for Windows Terminal in Windows?\"\\n]', 'a05dd29d16052d5e6944af79b86471e2': '[\\n  \"What should I do if my WSL integration with Ubuntu stops unexpectedly with exit code 1?\",\\n  \"How can I fix the DNS issue related to WSL integration with Ubuntu?\",\\n  \"What is the first step to resolve the WSL integration issue by fixing DNS?\",\\n  \"What should be done after enabling Dnscache for fixing WSL integration?\",\\n  \"How can I switch to Linux containers to address the WSL integration issue?\"\\n]', 'cfa2686924e94e02180b430cb5556481': '[\"Why might I face a \\'permissions too open\\' issue when running a GPC VM through SSH in WSL2?\", \"What command can I try if I encounter an error when using ssh in WSL2?\", \"How can I adjust permissions for my SSH private key in WSL2?\", \"How can I set up the .ssh folder directly in the home directory of WSL2 to resolve permission issues?\", \"What should be done if adjusting permissions alone doesn’t resolve the SSH permissions issue in WSL2?\"]', '766825e6d472908d617f6287be837d22': '[\\n    \"What should I do if WSL2 cannot resolve a host name in Docker and Terraform?\",\\n    \"How do I set the correct ssh/config path in WSL2 to resolve hostname issues?\",\\n    \"Where should I create a config file to solve host name resolution issues in WSL2?\",\\n    \"What should be included in the .ssh/config file for WSL2 to function properly?\",\\n    \"Why might WSL2 fail to reference the correct .ssh/config path, and how can I fix it?\"\\n]', '8461cae11d9a724ddd21d69fe0da75e8': '[\\n  \"What should I do if I encounter a connection refused error on PGCLI related to port 5432?\",\\n  \"How can I solve a PGCLI error that says connection failed?\",\\n  \"What are the steps to address a PGCLI connection issue due to SSL negotiation packet problems?\",\\n  \"How do I change the connection to a socket when facing errors in PGCLI?\",\\n  \"What command should I use in PGCLI to connect to a local database on port 5432?\"\\n]', '2b529f6b4ece1e2fa1817993427da1dc': '[\\n  \"What should I do if I encounter an error when running PGCLI --help?\",\\n  \"How can I troubleshoot a PGCLI installation error?\",\\n  \"What might cause an error with PGCLI\\'s --help command?\",\\n  \"Is an installation issue likely if PGCLI --help doesn\\'t work?\",\\n  \"What steps should I take if PGCLI\\'s --help command fails?\"\\n]', '8657e19f6e95966969fe364abe7d3ce2': '[\\n  \"Is it necessary to use pgcli within a separate Docker container in this module?\",\\n  \"Can pgcli be accessed from my computer for the postgres database?\",\\n  \"Do I need to execute pgcli inside a different container for PostgreSQL access?\",\\n  \"Is running pgcli on the local system sufficient for this course section?\",\\n  \"Should I map pgcli to another container for accessing the database?\"\\n]', '0d0995ec00a0eb51e3f38fab144c133a': '[\"What should I do if password authentication fails for user \\'root\\' while using PGCLI with Postgres?\", \"How can I connect to my Postgres Docker container using PGCLI if I have a local Postgres installation?\", \"Which command helps identify which application is using a specific port on MacOS?\", \"How can I unload the running PostgreSQL service on my local machine using MacOS?\", \"Why is changing the port to 5431 helpful in avoiding authentication errors with Postgres?\"]', '9aeef98f6e5d4bb793b02167b1ad9b9c': '[\\n    \"How can I resolve a PermissionError when using pgcli related to config directory access?\",\\n    \"What is the recommended way to install pgcli without using sudo to avoid permission errors?\",\\n    \"What should I do if conda installation gets stuck at \\'Solving environment\\' for pgcli?\",\\n    \"Why might I get a PermissionError when pgcli tries to create a config directory?\",\\n    \"Which method should I use to install pgcli to prevent affecting the system Python?\"\\n]', '9c3d1319cf17c5448fbe281beb82273c': '[\"What should I do if I encounter the \\'ImportError: no pq wrapper available\\' while using PGCLI?\", \"How can I resolve issues related to the \\'psycopg_c\\' and \\'psycopg_binary\\' implementations?\", \"What Python version is required to avoid psycopg2-binary installation failures?\", \"How do I check my current Python version to ensure compatibility with psycopg2-binary?\", \"What are the steps to install the PostgreSQL library and PGCLI correctly?\"]', '4d8e1ce0bfa05b09f7506758f11fd97a': '[\\n  \"What should I do if my Bash prompt is stuck on the password command for Postgres while using PGCLI?\",\\n  \"How can I resolve a connection failure with PGCLI due to password authentication for the user \\'root\\'?\",\\n  \"What should I consider if I keep facing a password authentication error using PGCLI despite entering the correct password?\",\\n  \"Is there a new solution dated 27/01/2024 for resolving a connection failure with PGCLI due to a password authentication issue?\",\\n  \"What is the likely cause and solution if I encounter a PGCLI error after closing the connection to the Postgres image?\"\\n]', 'da8a8b98320a89ff9299c5492705d52e': '[\\n    \"What should I do if pgcli is installed but my terminal doesn\\'t recognize it?\",\\n    \"How do I resolve the error \\'pgcli: command not found\\' on Git Bash?\",\\n    \"What path should I add to the Windows PATH if pgcli isn\\'t recognized?\",\\n    \"Where might Python be installed if pgcli isn\\'t working and it\\'s not in the usual location?\",\\n    \"How do I find the correct Python path to add for pgcli to work?\"\\n]', '0bf9851e18246f955c3993427b0180c4': '[\"How can I avoid installing pgcli locally while following the course?\", \"What is the command to run pgcli in a Docker container based on the course setup?\", \"Which network name should I use when running pgcli in Docker?\", \"What hostname and port should I specify for pgcli in the container as shown in the course?\", \"Can you provide a sample command to connect to the ny_taxi database using pgcli in Docker?\"]', '474e5cb0387de8a3fa98663235ab6d0e': '[\\n    \"Why is PULocationID not recognized in PGCLI?\",\\n    \"What should I do if a column name in PGCLI has capital letters?\",\\n    \"How should I format a capitalized column name to be recognized in PGCLI?\",\\n    \"Why are unquoted local identifiers not working as expected?\",\\n    \"What documentation explains the case sensitivity of PGCLI column names?\"\\n]', '47c218676f2ca86c5714d1bd3f9120fb': '[\\n  \"What should I do if there\\'s an error about column c.relhasoids not existing when using a command in Module 1?\",\\n  \"How can I resolve the issue of column c.relhasoids not existing in this course\\'s module?\",\\n  \"What steps are needed when encountering the error column c.relhasoids does not exist as described in the FAQ?\",\\n  \"If I receive an error about column c.relhasoids in our course, what is the resolution?\",\\n  \"How do I fix the column c.relhasoids does not exist error according to the course\\'s FAQ?\"\\n]', 'd665e7a40bc0b83e407ca3c1e1c64fb0': '[\\n  \"What should I do if I encounter a password authentication error while connecting to Postgres in Jupyter Notebook?\",\\n  \"Why am I unable to connect to Postgres on port 5432 when using a Jupyter Notebook connection?\",\\n  \"How can I troubleshoot a persistent OperationalError related to Postgres password authentication failure?\",\\n  \"What could be the reason behind a port conflict error when trying to connect to Postgres on localhost?\",\\n  \"How can stopping a service in Windows help resolve a Postgres connection error when using Docker?\"\\n]', '1976886c64619cf18931254f88831eb8': '[\\n  \"Why am I getting a FATAL: role \\'root\\' does not exist error when connecting to a local Postgres server using pgcli?\",\\n  \"What could be the reason for a psycopg2 OperationalError when uploading data via a connection in a Jupyter notebook?\",\\n  \"How can changing the port number help resolve connection issues with Postgres on my local machine?\",\\n  \"What steps should I take to verify if there is a root user in my Docker container?\",\\n  \"How can modifying docker compose settings resolve the role \\'root\\' does not exist error?\"\\n]', '593d1694e3f338425e3e749f9f819ae4': '[\\n    \"What could cause a psycopg2.OperationalError regarding a missing database called ny_taxi?\",\\n    \"How can I verify if my local Postgres server is operational?\",\\n    \"What should I do if I encounter a FATAL error when connecting to a Postgres database on port 5432?\",\\n    \"Why might my connection to the Postgres server on localhost at port 5432 fail?\",\\n    \"If Postgres is already installed on my computer, how can I resolve port conflicts when running Docker?\"\\n]', 'b3903d19ed9c8d403dab875132c243ad': '[\\n  \"How can I resolve the error ModuleNotFoundError: No module named \\'psycopg2\\' in Postgres?\",\\n  \"What steps should I follow if pip install psycopg2-binary does not fix the error on my setup?\",\\n  \"What command should I use to update conda to fix psycopg2 module issues?\",\\n  \"What should I do if psycopg2 installation fails with pg_config not found on a Mac?\",\\n  \"How can I ensure psycopg2 is properly installed or upgraded using pip?\"\\n]', '55c8c14fd799533f29123b58bdc6aef1': '[\\n    \"What error occurs when column names are mentioned directly in join queries on a MacBook Pro M2?\",\\n    \"How can I resolve the \\'Column does not exist\\' error in Postgres using Psycopg2 on a MacBook Pro M2?\",\\n    \"What should be used to enclose column names in join queries to avoid errors in Postgres?\",\\n    \"What is the correct way to enclose column names to bypass the \\'Column does not exist\\' error?\",\\n    \"What happens if column names are enclosed in single quotes in join queries with Postgres on MacBook Pro M2?\"\\n]', '7cb12c69e4689e432f52fe3353d80838': '[\\n    \"Why is the create server dialog missing in pgAdmin?\",\\n    \"What should I do if the server dialog doesn\\'t show up in pgAdmin?\",\\n    \"Has pgAdmin changed the way to create a server?\",\\n    \"Where do I find the option to register a server in pgAdmin now?\",\\n    \"Is there a new method to access the create server function in pgAdmin?\"\\n]', 'd434b352c745f11ba24099850947f8e9': '[\"What might cause a blank screen after logging into pgAdmin when using GitHub Codespaces in a browser?\", \"How can the CSRFError 400 Bad Request issue be resolved when using pgAdmin in a Docker container?\", \"What environment variable can be set to fix the CSRFError in pgAdmin Docker setup?\", \"What is one method to run the pgAdmin container to avoid the blank screen issue?\", \"How does using locally installed VSCode with GitHub Codespaces help with the pgAdmin login issue?\"]', '4eadd1fb6a425a8d92ef1b50ca897eff': '[\\n  \"How can I access the pgAdmin address if I am unable to do so via my browser on a Mac Pro?\",\\n  \"What modifications are needed in the docker run command to access pgAdmin?\",\\n  \"How should I modify the docker-compose.yaml configuration to access pgAdmin?\",\\n  \"What error arises due to the absence of sqlite3.dll in the Anaconda Dll path?\",\\n  \"How can I resolve the ModuleNotFoundError for \\'pysqlite2\\' when using Anaconda?\"\\n]', '838d870c752e8d210b37db869c71ac61': '[\"Why are 100000 records missing after running the ingestion script twice in the Jupyter notebook?\", \"What should be removed in the Jupyter notebook to avoid missing the first chunk of records?\", \"Where should the call to df_iter be placed to ensure all data is ingested correctly?\", \"Is the Jupyter notebook intended to be run from top to bottom for data ingestion?\", \"How is the data ingestion logic refined after the Jupyter notebook testing phase?\"]', 'dae5f85b563f21eac8ff6d80e4979f22': '[\\n  \"How can I unzip a gzip file on an Ubuntu machine if I want to keep the CSV uncompressed?\",\\n  \"What software might I need to install to handle gzip files on Ubuntu?\",\\n  \"What command should not be used for unzipping gzip files?\",\\n  \"Why might it be beneficial to keep a CSV file uncompressed?\",\\n  \"What Python library can I use to read a CSV file?\"\\n]', 'cac3fadd8ba053b07a34dd33e23d47d7': '[\\n    \"How can Pandas interpret string column values as datetime when using read_csv?\",\\n    \"What parameter should be used in pd.read_csv to parse date columns directly?\",\\n    \"Can you provide an example showing the use of parse_dates in pd.read_csv?\",\\n    \"What will the data type of a date column be after using the parse_dates parameter in Pandas?\",\\n    \"How many columns and entries are there in the sample data frame from the example provided?\"\\n]', 'd4d3bcb0e4486b4423f29a40da25d631': '[\\n    \"How can I use curl in Python to download data from a GitHub link?\",\\n    \"What Python command allows data ingestion from a URL using curl?\",\\n    \"Which method in Python helps solve issues with downloading files from GitHub using curl?\",\\n    \"Can you suggest a Python solution for fetching data from a GitHub link with curl?\",\\n    \"What is the proper Python syntax to execute a curl command for downloading a file?\"\\n]', 'd3fdf1cf741b217b53e694b3737e1839': '[\\n  \"How can I read a Gzip compressed CSV file using Pandas?\",\\n  \"What file extension is used for a Gzip compressed CSV file?\",\\n  \"What function in Pandas is used to read a compressed CSV file?\",\\n  \"What parameters does the read_csv function accept when reading a Gzip file?\",\\n  \"Which argument should be passed to read_csv to read a \\'.csv.gz\\' file?\"\\n]', '7bba89ed698a8f4f950ad2506e085967': '[\"How can we iterate through a parquet file using PyArrow?\", \"What library is recommended for iterating parquet files?\", \"How do we clear a table before ingesting parquet data to SQL?\", \"Is there a batch size limit when iterating through parquet files?\", \"What function in PyArrow helps in reading parquet files?\"]', '74e89ec4c6a4118ac167c50e5e7aca03': '[\\n  \"How do I resolve the ImportError related to \\'TypeAliasType\\' from \\'typing_extensions\\'?\",\\n  \"What version of \\'typing_extensions\\' is needed to fix the SQLAlchemy ImportError?\",\\n  \"Which package manager can update the \\'typing_extensions\\' module to the required version?\",\\n  \"What execution error occurs with \\'create_engine\\' in Python?\",\\n  \"Which Python module\\'s version must be at least 4.6.0 to avoid ImportError in this context?\"\\n]', 'f27d2763e7ca34ea23375754d4ff8942': '[\\n    \"What is the correct format for the connection string to solve the TypeError in SQLAlchemy?\",\\n    \"How do I correctly use create_engine to connect to a PostgreSQL database in SQLAlchemy?\",\\n    \"What should be included in the connection string to avoid a \\'module\\' object is not callable error?\",\\n    \"Can you provide a solution to fix a TypeError when connecting to a local PostgreSQL database with SQLAlchemy?\",\\n    \"What modifications are necessary in the connection string when facing a callable error using SQLAlchemy with PostgreSQL?\"\\n]', '0a696602479b26b83bc0b80851ff21ca': '[\\n    \"What error occurs when executing a Jupyter notebook cell with SQLAlchemy?\",\\n    \"How can you solve the ModuleNotFoundError for psycopg2?\",\\n    \"Which Python module is missing if you encounter a ModuleNotFoundError with SQLAlchemy?\",\\n    \"What command should be executed to install psycopg2 using Conda or pip?\",\\n    \"What does the SQLAlchemy connection string to PostgreSQL look like when causing the error?\"\\n]', '1f1d169182750a5cd92d6705ffe705db': '[\\n    \"What steps should I take if the Google Cloud SDK installer cannot update the system PATH on Windows?\",\\n    \"How can I add Gitbash to the Windows PATH to resolve SDK PATH issues?\",\\n    \"What should I ensure during the installation of Anaconda for using conda with Google Cloud SDK?\",\\n    \"How do I configure Gitbash as the default terminal in Windows after its installation?\",\\n    \"What actions are necessary if I want to use Unix tools with Git on Windows?\"\\n]', '4ff9c46e52a92b5ce1eebf4fd4dffd33': '[\\n  \"What should I do if project creation fails with an HttpError related to an existing entity?\",\\n  \"Why might I not be able to create a new GCP project with the name I chose?\",\\n  \"How can I resolve an ALREADY_EXISTS status when creating a project in GCP?\",\\n  \"What does it mean when my project ID in GCP shows as already in use?\",\\n  \"Is there a way to find out if a GCP project ID is already taken before attempting to create it?\"\\n]', '7eec07b94c454090c53fc445b5766c26': '[\\n  \"What should I do if I encounter an error indicating my GCP project is associated with an absent billing account?\",\\n  \"Why might I receive a 403 error in Module 1 related to billing in GCP?\",\\n  \"Where can I find the unique project ID required for my GCP setup?\",\\n  \"Who is the instructor mentioned in Module 1 related to Docker and Terraform issues?\",\\n  \"What alternative reason could cause a billing account error in my GCP project?\"\\n]', '68f546f4e94d64ebe4a55a50db686ffc': '[\"What can I do if my GCP free trial account is suspended?\", \"Is there an alternative solution if Google refuses my credit card for GCP?\", \"Why might Kaspi be problematic for GCP transactions?\", \"Which credit card companies might work with a GCP account?\", \"Does a Pyypl web-card work with Google Cloud accounts?\"]', 'ab1f26c64bbe505158385472c5c6dcf2': '[\\n    \"How do I locate the \\'ny-rides.json\\' file for my Docker and Terraform course in GCP?\",\\n    \"What steps are needed to find and create the \\'ny-rides.json\\' file in Google Cloud Platform?\",\\n    \"Could you explain the process to access the \\'ny-rides.json\\' key in GCP?\",\\n    \"Where do I navigate in Google Cloud Platform to generate the \\'ny-rides.json\\'?\",\\n    \"What is the procedure for adding a JSON key in the Keys tab to obtain \\'ny-rides.json\\'?\"\\n]', '3dc3b920c9043a99c2824813f71dc7be': '[\\n    \"Do I need to delete my Google Cloud instance after the lecture?\",\\n    \"Is it necessary to remove my instance from Google Cloud to follow the course?\",\\n    \"Should I erase my Google Cloud platform instance as mentioned by Alexey?\",\\n    \"Is deleting the instance on Google Cloud part of the week 1 tasks?\",\\n    \"Will I need to delete the Google Cloud instance more than once if I do it now?\"\\n]', 'd16b86b87ef4c3e81b5b1b5593de6e4e': '[\\n  \"What command can be used to display real-time system resource usage, including CPU and memory?\",\\n  \"How can I check the memory usage and availability on my system using a command?\",\\n  \"Which command should I use to list all running processes and get detailed information about them?\",\\n  \"What commands provide detailed information about network interface configurations and active network connections?\",\\n  \"How can I find out which packages are installed on an Ubuntu or Debian-based system?\"\\n]', 'f9d95ffad70b2a47c66591a5aa94773d': '[\\n  \"What should I do if I receive a billing not enabled error after setting up my billing account?\",\\n  \"How can I resolve the Error 403 related to billing for a project in Google Cloud?\",\\n  \"What\\'s a possible fix for the error message stating billing has not been enabled for a project?\",\\n  \"If my billing account is set but I\\'m still getting a billing error, what steps should I take?\",\\n  \"How can I enable billing for my project if I\\'ve already set up the billing account and still face issues?\"\\n]', 'fe2179cb72e3706187b171791c188dd5': '[\"What should I do if I have trouble installing the Google Cloud SDK on Windows?\", \"How do I fix the \\'quota exceeded\\' error when using Google Cloud SDK?\", \"What steps should I follow to set up a quota project with Google Cloud SDK?\", \"How do I reinstall the SDK using the unzip file \\'install.bat\\'?\", \"What can I do if my GCP virtual machine won\\'t start due to a lack of resources?\"]', '1b3faa0c54f2a7788eb06c1b871a7ca4': '[\\n  \"Why is there a video about the GCP VM included in the course?\",\\n  \"Can I configure my own environment instead of using a GCP VM?\",\\n  \"What issues might arise when setting up my own environment?\",\\n  \"What is a significant benefit of using my own environment over a GCP VM?\",\\n  \"Why can\\'t I commit changes directly when using a GCP VM?\"\\n]', 'cc1e34dcdf6e567b4c7ddbffd9434e62': '[\\n    \"Why am I getting a \\'Permission denied\\' error when trying to create a directory on GCP VM?\",\\n    \"Where should I create the \\'.ssh\\' directory to avoid permission issues?\",\\n    \"Why is creating a directory in the root folder causing problems?\",\\n    \"Can you explain where the correct location to run the \\'mkdir .ssh\\' command is?\",\\n    \"What steps should I follow to resolve the error when using \\'mkdir\\' on a GCP VM?\"\\n]', '9be68d4032955da7b706d3f0d102647f': '[\\n  \"What is the recommended command to change file ownership when experiencing a file-saving error in VS Code on a GCP VM?\",\\n  \"How do I resolve a \\'permission denied\\' error when trying to save a file on a GCP VM using VS Code?\",\\n  \"What steps should I follow if I encounter an EACCES error on my GCP VM when using VS Code to edit files?\",\\n  \"Which command should I use to ensure I can edit files in the Airflow directory via VS Code on a GCP VM?\",\\n  \"How can I fix the inability to write files due to permissions issues while using SSH remote in VS Code on my GCP VM?\"\\n]', '837fe6c1dda3544931ebf521d053eb5c': '[\"What should I do if I can\\'t connect to my GCP VM due to a timeout?\", \"How can I resolve a persistent SSH connection timeout with my VM?\", \"What steps can be taken if my VM connection attempt is timing out on SSH?\", \"How do I fix a VM SSH issue that worked last week but not now?\", \"What actions are needed when my VM\\'s External IP seems to cause a connection timeout?\"]', 'b0d79af2c8720962d8a44bb18d287b64': '[\"How can I fix a connection issue to a GCP VM on port 22?\", \"What steps should I take to allow SSH access to my Google Cloud VM?\", \"What startup script should I add to enable port 22 access on my GCP VM?\", \"How do I modify my VM settings to resolve a no route to host error on port 22?\", \"Which script should be added to the VM automation section to allow SSH?\"]', '362708e7257a7e78abe1e958c98730d7': '[\\n    \"How do I forward ports for pgAdmin and postgres from a GCP VM without using VS Code?\",\\n    \"What command should I run on the VM to start Docker and Jupyter Notebook?\",\\n    \"How can I forward port 5432 from my local machine using ssh?\",\\n    \"What should I do if I lose the Jupyter Notebook access token?\",\\n    \"How can I access pgAdmin on my local machine through a web browser?\"\\n]', '8c200425dd2261cd9dc3807ce732323b': '[\\n    \"What should I do if gcloud auth hangs in VS Code using WSL2?\",\\n    \"How can I fix the error message that appears when gcloud auth opens a browser?\",\\n    \"What action is needed to make the gcloud login page pop up without issues?\",\\n    \"How do I handle the message that appears when using gcloud auth on WSL2?\",\\n    \"What steps should I follow if gcloud does not successfully login via browser?\"\\n]', 'b968f1e7beb645edfc97fcf4580eb7dd': '[\\n    \"What might be causing Terraform to fail querying available provider packages for hashicorp/google?\",\\n    \"How should I resolve a Terraform error when it says it cannot retrieve the list of available versions?\",\\n    \"What should I check if Terraform cannot query the provider registry for hashicorp/google?\",\\n    \"How can I fix an internet connectivity error with Terraform failing to access the online registry?\",\\n    \"What steps can I take when Terraform suggests a network issue due to a failed provider registry request?\"\\n]', 'f872f9661869c9fbabed56388c86b53a': '[\\n    \"What can I do if I get a timeout error related to oauth2 when using Terraform and Google services?\",\\n    \"Why might Terraform have trouble fetching a token from Google services in certain countries?\",\\n    \"How can a VPN app help resolve network issues with Terraform and Google\\'s API?\",\\n    \"What should be configured if the terminal program does not follow the system proxy settings during Terraform operations?\",\\n    \"Who should I contact if I need assistance with network issues when using Terraform with a VPN?\"\\n]', 'd56632943fea4ae66a38c17e2df1323b': '[\\n  \"Where can I find instructions to install Terraform on Windows 10 using WSL?\",\\n  \"How do I configure Terraform for the Linux Subsystem on Windows 10?\",\\n  \"Is there a guide available for setting up Terraform in WSL on a Windows machine?\",\\n  \"Can you direct me to a resource for configuring Terraform with WSL on Windows 10?\",\\n  \"What is the link for installing Terraform under the Windows Subsystem for Linux on Windows 10?\"\\n]', '0886519ec09b02d126814c2da22fd892': '[\\n  \"What can I do if I encounter an error acquiring the state lock in Terraform?\",\\n  \"Where can I find more information about the state lock error in Terraform?\",\\n  \"Is there a specific GitHub issue related to Terraform\\'s state lock error?\",\\n  \"How should I address the error message related to acquiring the state lock in Terraform?\",\\n  \"Could you direct me to a resource for troubleshooting Terraform\\'s state lock issues?\"\\n]', '621a9647244b93237d63e108a1f43051': '[\\n  \"What should I do if I encounter an invalid JWT token error while running terraform apply on WSL?\",\\n  \"How can I resolve a 400 Bad Request error due to an invalid JWT token on WSL2?\",\\n  \"What causes the invalid JWT token error with Terraform in WSL, and how can it be fixed?\",\\n  \"If I get an error about an invalid JWT token on WSL, what command can sync my system time?\",\\n  \"Why might there be a time desync on my machine leading to a JWT token error with Terraform?\"\\n]', '057d96a4e708905c66f01563cc260c86': '[\\n    \"What does Terraform Error 403 indicate?\",\\n    \"How can I address the Issue of Terraform Error 403 related to access?\",\\n    \"What file should the $GOOGLE_APPLICATION_CREDENTIALS point to resolve Error 403?\",\\n    \"Which command should be executed to activate the service account for resolving Terraform Error 403?\",\\n    \"What might cause a googleapi Error 403 indicating access denied in Terraform?\"\\n]', 'aa94f799d68c5cd307b7e037b4e7691d': '[\\n  \"Is a separate service account necessary for Terraform in this course?\",\\n  \"Can I use one service account for all resources in this course?\",\\n  \"Do I need multiple service accounts for different services?\",\\n  \"What should I do after obtaining the credential file for Terraform?\",\\n  \"Is it sufficient to set the environment variable after getting the JSON file for Terraform?\"\\n]', 'd425496ebe596e7f60ae2f4256510cdd': '[\\n  \"Where is the download link for Terraform 1.1.3 for Linux AMD 64?\",\\n  \"How can I access Terraform 1.1.3 for Linux AMD 64?\",\\n  \"Could you tell me where to find Terraform 1.1.3 for Linux on AMD 64 architecture?\",\\n  \"What is the URL for downloading Terraform 1.1.3 Linux AMD 64?\",\\n  \"Where can I download Terraform 1.1.3 Linux AMD 64 version?\"\\n]', '97d4b899113daaddc4210407655e7be9': '[\\n  \"Why do I get an error saying Terraform initialized in an empty directory?\",\\n  \"What should I do if I run terraform init outside the working directory?\",\\n  \"How can I resolve the issue of having no Terraform configuration files in my directory?\",\\n  \"What is the first step before running terraform init?\",\\n  \"Where should I navigate before executing the terraform init command?\"\\n]', '8050de712036dd9d17c6c578add83733': '[\"What should I check if I encounter a Google API error 403 due to insufficient authentication scopes?\", \"How can I resolve the \\'Error creating Dataset\\' with Google API error 403?\", \"What steps should I follow to set GOOGLE_APPLICATION_CREDENTIALS correctly?\", \"Where can I find instructions for correctly setting the GOOGLE_APPLICATION_CREDENTIALS environment variable?\", \"What is the solution for fixing access denied errors related to Google API authentication scopes?\"]', 'db61f1180ab41ee6b823624698c289ee': '[\\n  \"What is the solution if I encounter a permission denied error for creating a bucket in Terraform?\",\\n  \"How can I resolve a 403 error indicating that access is forbidden when creating a bucket on Google Cloud using Terraform?\",\\n  \"What should I do if I see an error stating that I lack storage.buckets.create access to a Google Cloud project?\",\\n  \"How can I fix the issue where Terraform gives a 403 error related to storage.buckets.create?\",\\n  \"What change is needed in Terraform configuration if I get an error about storage.buckets.create permission?\"\\n]', '561430539007cc4c0b2e36880549f2ea': '[\\n  \"How can I protect my credentials file when using Terraform with Google Cloud?\",\\n  \"What is the recommended way to input a credentials file for Terraform\\'s Google provider?\",\\n  \"What variable is used to define the project in the Google provider configuration?\",\\n  \"Is there a way to specify the region in the Google provider configuration using a variable?\",\\n  \"What variable is utilized to set the zone in the Google provider block?\"\\n]', '45e79d2efeb36a996bc050f30f4cc4c0': '[\\n  \"What should I do if I get an error saying that the column Zone doesn\\'t exist while using a SQL SELECT query?\",\\n  \"How can I resolve issues with SQL columns that start with uppercase letters?\",\\n  \"Is there an \\'Astoria Zone\\' in the dataset I received for my homework?\",\\n  \"How can I modify my SQL query if the initial column name I used doesn\\'t exist in the dataset?\",\\n  \"What is the correct way to query the zones table to find entries for the Astoria Zone?\"\\n]', 'b0aa2fa601aac05f213da374cc329f30': '[\\n  \"How can I fix the error Column Zone doesn\\'t exist when using SQL SELECT Zone FROM taxi_zones?\",\\n  \"What should I do if I encounter column case sensitivity issues in my database queries?\",\\n  \"How can I ensure consistent column naming in my dataset to avoid SQL errors?\",\\n  \"What is a recommended practice for handling column names to prevent SQL errors due to case sensitivity?\",\\n  \"When reading a CSV into Pandas, how can I modify column names to avoid SQL errors?\"\\n]', 'ff00ff8663add6a1b39fef34b7477ea2': '[\\n    \"What is the solution for resolving a host error in curl on macOS?\",\\n    \"How can mac users solve the host resolution issue with curl when fetching output.csv?\",\\n    \"What command should macOS users execute to fix \\'Could not resolve host\\' in curl?\",\\n    \"Which command can solve a curl error for output.csv on mac systems?\",\\n    \"What is the recommended fix for curl host issues on macOS using os.system?\"\\n]', 'e82d352bc0ff03e047f304319f9f8fc8': '[\\n    \"Where do I check if the SSH config file is stored correctly to resolve a hostname error?\",\\n    \"What should I verify to fix a \\'Name or service not known\\' error during SSH in Module 1?\",\\n    \"Which directory should the SSH config file be in to avoid hostname resolution issues?\",\\n    \"How do I resolve the error related to unresolved hostname for SSH in the Docker and Terraform module?\",\\n    \"What is the correct path for the SSH configuration file to avoid connection errors?\"\\n]', '3984133995b8a76f1ae618199651489c': '[\"How can I resolve the \\'pip\\' command not recognized issue if I am using Anaconda?\", \"What steps should I follow to add Anaconda to the PATH on a Linux or MacOS system?\", \"What is the process to add Anaconda to the PATH using Git Bash on Windows?\", \"How do I permanently add Anaconda to the PATH on Windows without using Git Bash?\", \"What should I do after adding Anaconda to the PATH to ensure pip works correctly from the command line?\"]', '59d0236f6f5b44f8447b5c06b0761a21': '[\\n  \"What should I do if I encounter \\'error starting userland proxy: listen tcp4 0.0.0.0:8080: bind: address already in use\\' while using Docker?\",\\n  \"How can I resolve the \\'error response from daemon: cannot stop container\\' issue when using Docker?\",\\n  \"If I am unable to import the module psycopg2, what steps should I take?\",\\n  \"Why might I encounter \\'docker build Error checking context: can\\'t stat\\' when building a Docker image, and how can I fix it?\",\\n  \"What are the steps to take if Docker cannot access a file due to insufficient permissions?\"\\n]', '9f6c7b8f81e33ec230b86a42abd27b77': '[\"How can I create a pip-compatible requirements.txt from Anaconda?\", \"Why should I avoid using \\'conda list -d\\' to generate requirements.txt?\", \"What might happen if I use \\'pip freeze\\' for requirements.txt in Anaconda?\", \"What is the first step to generate a pip-friendly requirements.txt in Anaconda?\", \"Is \\'pip freeze\\' reliable for creating requirements.txt with Anaconda?\"]', '0e1bab7b5559bcc859dffb68eccd0797': '[\"Where can I find the FAQ questions from past cohorts for Prefect?\", \"What is the link to the previous cohort FAQs for Airflow?\", \"How can I access the past FAQ document for Prefect in this module?\", \"Is there a document with FAQs from previous students for Airflow?\", \"Where is the Prefect FAQ from earlier cohorts located online?\"]', '871e643f39ae68f0d0963b1c8dd81552': '[\\n  \"Why do Docker containers exit with code 132 during docker compose up in Module 2?\",\\n  \"What does Mage documentation suggest as a cause for Docker containers exiting with code 132?\",\\n  \"Is a hardware issue the likely reason for Docker containers exiting with code 132?\",\\n  \"What is the suggested solution for Docker containers exiting with code 132 if replacing hardware is not possible?\",\\n  \"What limitations might affect the conclusions about the issue experienced by a student using a VirtualBox VM?\"\\n]', 'a973039a99b7e76c247c95796eb24403': '[\\n  \"What are unexpected kernel restarts in Module 2\\'s WSL context?\",\\n  \"How can I resolve WSL kernel running out of memory in Module 2?\",\\n  \"Why is Docker not functioning well on WSL 2 in the second module?\",\\n  \"What should I edit in the .wslconfig file for Module 2\\'s Docker setup?\",\\n  \"Which command shuts down WSL as per Module 2 instructions?\"\\n]', 'abb04c2a86adaed198fe2471a56cd84b': '[\"How can I find the solution to the issue discussed in Module 2: Workflow Orchestration?\", \"Where can I access the link to resolve the PostgreSQL configuration problem mentioned in the course?\", \"What resource is recommended for fixing the PostgreSQL setup issue in the workflow module?\", \"In which online platform can I find the discussion link about configuring Postgres from the course?\", \"Where is the suggested solution for configuring Postgres located as per the Workflow Orchestration module?\"]', 'e8e815bf20be8c36b32b978ff62acbbb': '[\\n\"Why do I get a connection refused error on port 5431 when using MAGE?\",\\n\"Which port should the POSTGRES_PORT variable in io_config.yml be set to for MAGE?\",\\n\"Is the POSTGRES_PORT variable related to the mage container or the host machine?\",\\n\"Do I need to change POSTGRES_PORT to 5431 if there\\'s a conflicting postgres on my host?\",\\n\"What is the default postgres port that should be used in io_config.yml?\"\\n]', '5080b067441e5b77172a8087e63e9407': '[\\n  \"What could be the reason for encountering a KeyError when running SELECT 1 in MAGE?\",\\n  \"How can I avoid a KeyError in MAGE when executing a SQL statement?\",\\n  \"Why do I get a KeyError on MAGE with the command SELECT 1?\",\\n  \"When running SELECT 1 in MAGE, what should I check if I get a KeyError?\",\\n  \"How do I resolve a KeyError when executing SELECT 1 in MAGE 2.2.4?\"\\n]', '84b7376fde34ce26b13ff0f045482124': '[\\n    \"What should I do if I encounter a ConnectionError related to a write operation timeout in MAGE 2.2.4?\",\\n    \"How can I fix a 404 error indicating a dataset was not found in MAGE 2.2.4?\",\\n    \"What changes should be made to the io_config.yaml file to avoid timeout issues in MAGE?\",\\n    \"Is there a specific setting to check when experiencing a BigQuery dataset not found error in MAGE?\",\\n    \"How do I ensure my changes to the timeout value in the io_config.yaml file are applied?\"\\n]', 'ad22a1c032a1ac6758cb57a483798a4c': '[\\n  \"What should I do if I encounter a RefreshError related to an invalid grant during Module 2?\",\\n  \"How can I resolve an issue with an invalid JWT caused by token lifetime constraints?\",\\n  \"Where can I find a solution online for a problem with a JWT token not being short-lived enough?\",\\n  \"What is the recommended way to fix a token timeframe issue mentioned in the FAQ for Module 2?\",\\n  \"Which online resource is suggested for troubleshooting an invalid JWT error due to exp and iat values?\"\\n]', '615dac37a3de92b207e05ae5198bb71e': '[\\n  \"What might cause an IndexError: list index out of range error in Mage version 0.9.61?\",\\n  \"How can updating the docker container help with the IndexError in Mage?\",\\n  \"Is there an updated version of Mage that might resolve the list index out of range issue?\",\\n  \"What specific command should be used to pull the updated docker image to fix the Mage error?\",\\n  \"Where can I find the original discussion that led to the solution for the Mage error?\"\\n]', 'c102b9840636e6bcea9b9d07498671a0': '[\\n    \"What should I do when I encounter an OSError while trying to save a file due to a non-existent directory?\",\\n    \"How can I solve the issue of saving a file when the directory path does not exist in Python?\",\\n    \"What is the solution for handling directory creation when facing an OSError in file saving?\",\\n    \"How can directories be automatically created before saving a file if they do not exist?\",\\n    \"Where can I find more information on preventing OSError related to non-existent directories?\"\\n]', '70e8d91bf57879d15e1be588cf3d522d': '[\"Why was the deployment of Mage using Terraform to GCP not covered in the video DE Zoomcamp 2.2.7?\", \"What is the default value required for project_id in variables.tf during Mage deployment to GCP?\", \"Which API needs to be enabled in the Google Cloud Console for deploying Mage to GCP?\", \"What command should be run to initialize Terraform before deploying Mage to GCP?\", \"What step follows after Terraform prompts for the PostgreSQL password during the apply process?\"]', 'd51ba2b3458a57d6413eec259a2644f3': '[\\n    \"How can you run multiple Mage instances using Docker from different directories?\",\\n    \"What changes are necessary in the docker-compose.yml file to run multiple Docker containers from different directories?\",\\n    \"How can you troubleshoot the error when deploying Mage to Google Cloud using terraform?\",\\n    \"What steps are needed to resolve the error caused by insufficient authentication scopes in GCP?\",\\n    \"How can you modify the permissions of an existing VM in GCP to allow full access to Cloud APIs?\"\\n]', 'fe6091cbd5e71b99a13fe981a027f62a': '[\\n    \"What steps should I take if I encounter a Load Balancer Problem related to Security Policies quota in Module 2?\",\\n    \"Why might a free trial GCP account face issues with deploying infrastructures via terraform?\",\\n    \"What is the proposed solution to handle the Load Balancer Problem when using a free trial GCP account?\",\\n    \"Which specific lines in the main.tf file should be altered to address the Load Balancer Problem?\",\\n    \"How can I clean up failed infrastructure deployment attempts in GCP?\"\\n]', '25dfa9a94fe4323b2c2883f8f88ba145': '[\"How can I resolve the error when I run terraform apply in GCP?\", \"What should I do before running terraform apply again if I interrupted the deployment process?\", \"Why are my GCP free credits depleting quickly when using MAGE Terraform files?\", \"How long does it take to deploy MAGE Terraform files on GCP using a Virtual Machine?\", \"How can I ensure all partially created resources are deleted before rerunning terraform apply?\"]', '7bfd7e24a07483f406d28072dd0804b6': '{\\n    \"questions\": [\\n        \"What error occurs when attempting to create a VPC access connector with insufficient permissions?\",\\n        \"What is the specific permission needed to create a VPC access connector?\",\\n        \"Where in the Google Cloud infrastructure does the permission \\'vpcaccess.connectors.create\\' apply?\",\\n        \"What IAM role should be added to resolve the permission error?\",\\n        \"What line in the configuration script specifies the resource causing the error?\"\\n    ]\\n}', '55965256a3d83eb17799cad6a9f24404': '[\\n  \"Why am I unable to save a file into a non-existent directory with the path \\'data/green\\'?\",\\n  \"What should I do if Git doesn’t push an empty folder to GitHub?\",\\n  \"How can I create a missing directory in my code before saving a file?\",\\n  \"Why does the relative path for writing locally fail when using GitHub storage?\",\\n  \"What is a workaround for using paths when writing locally and uploading to a GCS bucket?\"\\n]', '35f4ad343b536b28a4f6974b20d6c0b6': '[\\n  \"What are the pickup datetime column names for the green and yellow datasets?\",\\n  \"How should I adjust my scripts to accommodate different pickup datetime columns in the datasets?\",\\n  \"Why does the green dataset use lpep_pickup_datetime instead of tpep_pickup_datetime?\",\\n  \"Can you explain the difference in pickup datetime column names in green and yellow datasets?\",\\n  \"What specific changes are necessary in scripts for different dataset pickup datetime columns?\"\\n]', 'f22ae696e3dbe6750d7d3c97f2276e1d': '[\\n    \"How can I read data in chunks when my process is stopped immediately using Pandas?\",\\n    \"What is the recommended solution if my workflow is terminated after starting a large CSV download?\",\\n    \"Which Pandas function should I use to read a CSV file in parts?\",\\n    \"How can I append data to a parquet file in a workflow orchestration using Pandas?\",\\n    \"What engine is suggested for appending data to a parquet file to resolve a workflow orchestration issue?\"\\n]', '02de1aac9f211c660302696213204326': '[\"Why am I receiving an \\'access denied\\' error when pushing a Docker image?\", \"What might cause a denial error when attempting to push a Docker image?\", \"What should I check if my push to a Docker image fails with an access denial?\", \"How can incorrect login details affect my ability to push Docker images?\", \"What errors may occur if I use a wrong username while pushing a Docker image?\"]', '3807894b51e8b031b157127e91aeefa2': '[\\n  \"What should I do if my flow script fails with a \\'killed\\' message?\",\\n  \"How can I fix a memory issue if my VM run is killed unexpectedly?\",\\n  \"What does a \\'killed\\' message indicate in a workflow orchestration?\",\\n  \"How can increasing RAM help with the \\'killed\\' message in the flow run?\",\\n  \"What steps should be taken if my flow run is terminated with the \\'killed\\' message?\"  \\n]', '64d0c71192fdbd893ed186e46972c5b9': '[\"What steps do I take if my GCP VM runs out of disk space while using Prefect?\", \"How can I identify which directory is consuming the most space on my VM?\", \"Where can I delete older cached flows when my GCP VM is full?\", \"What should I do if I encounter an SSL certificate verification error on a Mac when running flows?\", \"How do I install certificates for Python on a Mac to resolve SSL issues?\"]', 'a4a592a521b05eee7232aad390d93198': '[\\n  \"What does it mean when a Docker container crashes with status code 137?\",\\n  \"How can I prevent a container from consuming all available RAM?\",\\n  \"What are the recommended actions if a Docker container keeps crashing due to memory issues?\",\\n  \"What should I do if my computer lacks the resources to run a memory-intensive Docker container?\",\\n  \"Can you suggest an online service for running Docker containers if my local resources are insufficient?\"\\n]', 'a8ee017005c03aeb6747dbda613e54b6': '[\"What causes the timeout issue when uploading data to GCS?\", \"How is yellow data processed before uploading to GCS?\", \"What problems can arise from a slow internet connection when running the ETL script?\", \"What is a proposed solution to prevent timeout during upload to GCS?\", \"What file format and argument settings are suggested to avoid network type errors with large files?\"]', '6bfbbd7224075e2fcf16558a25f97da4': '[\\n  \"What should I do if I encounter an error saying columns do not exist when exporting transformed green_taxi data to PostgreSQL?\",\\n  \"How can I resolve the UndefinedColumn error for the relation green_taxi during data export?\",\\n  \"What steps are needed to fix the issue of missing columns during the export of green_taxi data to PostgreSQL?\",\\n  \"Why am I getting an error about non-existent columns when re-running the export block for green_taxi data to PostgreSQL?\",\\n  \"What is the solution if I face a column does not exist error while exporting green_taxi data?\"\\n]', '1ec21efeaae547cdfc85b1ca6e44d872': '[\\n    \"What should I do to avoid SettingWithCopyWarning in my DataFrame operations?\",\\n    \"How can I ensure that I\\'m assigning to the original DataFrame rather than a copy when adding a new column?\",\\n    \"Why does SettingWithCopyWarning occur when modifying a DataFrame slice?\",\\n    \"What syntax should I use instead of df[] = value to prevent SettingWithCopyWarning?\",\\n    \"How can I differentiate between modifying a copy of a DataFrame and the original?\"\\n]', '7b3a838be3f9073df33993e32c59a63a': '[\\n    \"What is the alternative to using Pandas/Python kernel when working with large CSV files in Mage?\",\\n    \"Where can I find information on using the PySpark kernel in Mage?\",\\n    \"Why should I consider using the PySpark kernel instead of the Pandas/Python kernel for large CSV files?\",\\n    \"How can I improve performance on a slow laptop when processing big CSV files in Mage?\",\\n    \"Is there documentation available for using the PySpark kernel in Mage for handling large data?\"\\n]', '8e1f1e15ee5f1e0d9ee50357837fa1eb': '[\\n    \"How do I address an error encountered when deleting a block in the pipeline?\",\\n    \"What is the initial step to take when an error occurs in deleting a block from the pipeline?\",\\n    \"What should be done to remove a connection from a pipeline block?\",\\n    \"Can you explain the process for eliminating connections between blocks in a pipeline?\",\\n    \"What is the correct procedure for removing a block from a pipeline after encountering an error?\"\\n]', 'f784a9be56c5971a4aae8e7c5f07cc67': '[\\n  \"What should I do if I encounter a permission denied error when editing the Pipeline name in Mage UI?\",\\n  \"How can I bypass a permission error prohibiting me from editing the Pipeline name in Mage?\",\\n  \"What steps can I take if Mage UI prevents me from changing the Pipeline name due to permission issues?\",\\n  \"Is there a workaround for editing the Pipeline name in Mage UI without permission?\",\\n  \"After getting a permission denied error in Mage UI, how can I eventually edit the Pipeline name?\"\\n]', 'e905ac509dbed2c56dd84feed73b5518': '[\\n    \"What are the steps to download all partitioned files we created using PyArrow libraries?\",\\n    \"How can I download only specified dates from partitioned files into BigQuery using Mage?\",\\n    \"What should I do if I encounter the \\'UndefinedColumn: column vendor_id\\' error when dealing with green_taxi?\",\\n    \"How can the `ParquetDataset` class be used to read multiple partitioned files?\",\\n    \"What are the two possible ways to resolve the issue with the \\'green_taxi\\' table\\'s missing column in SQL?\"\\n]', 'dd5b1118497e328fc57af3670f614f81': '[\"What files must be submitted for Homework 2 in Module 2?\", \"How can I access the necessary MAGE files locally for Homework 2?\", \"Where can I find the .py or .sql files for the blocks I created?\", \"What steps should I take to download the pipeline with all necessary files?\", \"How should I move the downloaded files to my GitHub repository?\"]', 'c46d5dcf3e3edfe9632aedbb563a108b': '[\"How can I integrate files from the Mage repository into my version of the Data Engineering Zoomcamp repository?\", \"What steps should I take to add Mage repo files to my DE Zoomcamp repo?\", \"Why does GitHub not automatically include Mage repo files in my DE Zoomcamp repository?\", \"What should I do with the .gitignore file when including Mage files in my DE Zoomcamp repo?\", \"Which terminal commands do I need to execute to prepare the Mage folder for inclusion in my DE Zoomcamp repository?\"]', '9040804b0502dcbd85d1c1f271c73e29': '[\\n    \"What should I do when getting a ValueError about the truth value of a Series being ambiguous when testing assertions?\",\\n    \"How can I resolve the ambiguity error related to truth value of a Series in my code?\",\\n    \"What was the initial code causing a ValueError in Module 2: Workflow Orchestration when testing output?\",\\n    \"What solution did the student find on Stackoverflow for the ValueError regarding truth value of a Series?\",\\n    \"How did the student modify the code to fix the ValueError encountered during validation assertions?\"\\n]', '4c1f7e5273e94cdca09ca47cd72aa690': '[\\n    \"What should I do if my files disappear after launching the Mage web interface?\",\\n    \"What steps should be taken if Mage files are missing after I boot up my PC?\",\\n    \"How can I resolve the issue of disappearing Mage AI files?\",\\n    \"What is a potential solution if the Mage files are no longer visible after running docker compose up?\",\\n    \"Where can I verify the correct repository before starting docker compose up for Mage?\"\\n]', 'dd6b76d0d707b5ac094893276ff4e363': '[\\n    \"What type of errors occur in the io.config.yaml file in Module 2?\",\\n    \"How can I fix trailing side errors in io.config.yaml file?\",\\n    \"What should I use instead of the current quotes in io.config.yaml to solve errors?\",\\n    \"Could you explain how to correct issues in the io.config.yaml file as discussed in Module 2?\",\\n    \"Krishna Anand suggests changing what to resolve io.config.yaml file errors?\"\\n]', '65596cceb05c47082f0851bf26edb60b': '[\\n    \"What issue arises when exporting data from Mage to a GCS bucket using pyarrow?\",\\n    \"How can I resolve the ArrowException related to opening credentials file in Mage?\",\\n    \"What is the suggested file path for Google Cloud credentials in Mage?\",\\n    \"How should I configure the Mage app to access GCP credentials?\",\\n    \"Why is an OAuth2 access token necessary for exporting data from Mage?\"\\n]', '7572252a86d49641ef23e5b270d77581': '[\\n  \"What can cause a Google Cloud authentication error related to OAuth2 access tokens in a workflow orchestration module?\",\\n  \"How can I resolve an error caused by exhaustion of the retry policy when using Google Cloud services?\",\\n  \"Why might I receive a message that the request could not be sent due to a failure to create an OAuth2 access token?\",\\n  \"What could be the reason for an error stating \\'couldn\\'t resolve host name\\' when performing work with Google Cloud?\",\\n  \"Where can I learn more about resolving authentication errors for Google Cloud requests?\"\\n]', 'e81c4a41477de7b40f6de2f644b17107': '[\\n    \"How can a permission error occur when exporting data from Mage to a Google Cloud Storage bucket?\",\\n    \"What does the error message indicate if a service account lacks storage.buckets.get access in GCS?\",\\n    \"What is the solution if the service account does not have permission to access a Google Cloud Storage bucket?\",\\n    \"What role should be added to the service account to resolve a permission error in GCS?\",\\n    \"Where in the Google Cloud Console can you modify the roles for a service account facing access issues with a GCS bucket?\"\\n]', '95a162ef411ada22f55a6aefbd54cab1': '[\\n    \"How do I prepare and send a pyspark script to a Dataproc cluster from Mage?\",\\n    \"What are the necessary steps to create a Dataproc Cluster for triggering from Mage?\",\\n    \"Which role should be added to the service account editing for Mage to trigger Dataproc?\",\\n    \"Where should I place the python script to ensure Dataproc execution through Mage?\",\\n    \"How can I ensure Mage has the necessary access to trigger Dataproc and execute scripts?\"\\n]', 'b1271b4c8fb328b047bae36331c600e4': '[\\n  \"What can I do if docker-compose is taking too long to install zip packages on Linux?\",\\n  \"How can I ensure apt-get installs additional packages without delay?\",\\n  \"Is there a Python alternative for unpacking datasets if zip installation is slow?\",\\n  \"What flag should I add to apt-get to avoid manual confirmation for additional installations?\",\\n  \"Which Python package can be used as an alternative to zip, that is pre-included in distributions?\"\\n]', '2ce2553e01dc146b8ee68ae0747fbe8b': '[\\n    \"What should I check if I encounter an error when transferring data from the web to a GCS bucket?\",\\n    \"How can I resolve an issue when writing data to a GCS bucket?\",\\n    \"Which data types should I use when transferring web data to GCS to avoid errors?\",\\n    \"What could be a potential solution for errors when saving data to GCS?\",\\n    \"How do Nullable dataTypes help when writing data into GCS from the web?\"\\n]', 'ce2bb8eab25b105cd61ecb3cd8ba3dab': '[\\n    \"What must be ensured about the schema when ingesting data into a BigQuery table from a directory?\",\\n    \"How does the variance in column data types between different files affect importing parquet files to BigQuery?\",\\n    \"What problem can arise if subsequent files have different schemas than the first file when importing into BigQuery?\",\\n    \"What steps should be taken to avoid schema mismatch errors when uploading data to BigQuery?\",\\n    \"Why is it important to enforce column data types on a DataFrame before serializing to BigQuery?\"\\n]', '56e80b1d7fe0c6ac13fcf48bb94b65a1': '[\\n  \"What should I do if I get a gzip.BadGzipFile error when importing FHV data?\",\\n  \"How can I resolve the issue of receiving a \\'Not a gzipped file\\' error with FHV data?\",\\n  \"What is the correct URL format for accessing the FHV dataset for GCS import?\",\\n  \"Why am I receiving an error indicating a file is not gzipped when working with the FHV dataset?\",\\n  \"What part of the FHV dataset URL should I pay special attention to when importing data?\"\\n]', '4652be8ac3b1a65a4220321a96c2b76b': '[\\n  \"Who can assist with loading data from a URL list into a GCS bucket?\",\\n  \"Which expert can I contact for help with GCS buckets in Module 3?\",\\n  \"Who is the go-to person for data loading into a GCP Bucket?\",\\n  \"In Module 3, who manages queries about GCS bucket data loading?\",\\n  \"Who might I reach out to for guidance on GCP bucket operations in our course?\"\\n]', 'e18466e72ccb88874a9c7e696e33e7ce': '[\\n  \"What should I check if I encounter a Bad character error when querying my GCS dataset?\",\\n  \"How can I resolve a formatting issue after querying my dataset?\",\\n  \"What is the recommended way to upload CSV.GZ files to avoid errors?\",\\n  \"Where can I find additional tips for resolving Bad character errors?\",\\n  \"Is using pandas necessary when uploading datasets to a GCS bucket?\"\\n]', '7461ee1437f1acc68752628f4eec9c51': '[\\n    \"What should I do if I see the error \\'bq: command not found\\'?\",\\n    \"How do I verify if the BigQuery Command Line Tool is installed?\",\\n    \"Is there an alternative command I can use instead of \\'bq\\'?\",\\n    \"What command lets me check the installation of the BigQuery Command Line Tool?\",\\n    \"How can I resolve the issue of \\'bq: command not found\\'?\"\\n]', 'b6ed61ad03a19d2c0d58d3ccd5942e05': '[\\n    \"What precautions should I take when using BigQuery to avoid unexpected charges?\",\\n    \"How did creating a BigQuery dataset result in an $80 bill?\",\\n    \"Why is it important to use free credits with BigQuery?\",\\n    \"What should I do with BigQuery datasets after they are created?\",\\n    \"Why is it important to check my billing regularly, especially when using a VM?\"\\n]', 'ea86ccb241434434ae5c33a185f469c8': '[\\n    \"What should I do if my BigQuery dataset and GCS bucket are in different regions?\",\\n    \"Why can\\'t I load data from GCS to BigQuery if they are located in different regions?\",\\n    \"How can I fix the issue of incompatible regions between my GCS bucket and BigQuery dataset?\",\\n    \"What steps are necessary to ensure data loading from GCS to BigQuery occurs smoothly?\",\\n    \"Is it possible to read and write data between different regions in GCP using BigQuery and GCS?\"\\n]', '028c7ec99028bed4f21ef64aca40a762': '[\\n  \"Why am I encountering an error about reading and writing in different locations on GCP BigQuery?\",\\n  \"How can I resolve the issue of location mismatch between BigQuery dataset and GCS Bucket?\",\\n  \"What should I do if my GCS Bucket is in \\'us-central1\\' and I need to create a BigQuery dataset?\",\\n  \"Can I use regions in BigQuery that differ from my GCS Bucket location?\",\\n  \"What is the requirement for matching BigQuery dataset location with GCS Bucket?\"\\n]', '97d9cbb84690c2717488f2e5209a03f5': '[\\n  \"What should I do to avoid losing progress in BigQuery if my Chrome tab freezes?\",\\n  \"How can I ensure my SQL script is saved in BigQuery?\",\\n  \"Is there a way to conveniently format my queries outside of BigQuery?\",\\n  \"What happens if I don\\'t save my queries regularly in BigQuery SQL Editor?\",\\n  \"Where can I find my saved files in BigQuery after saving?\"\\n]', '22a9ae0bbe950219e56bc581b1fec5db': '[\\n    \"Is BigQuery suitable for real-time analytics in this project?\",\\n    \"Does BigQuery support real-time data processing?\",\\n    \"Can real-time analytics be achieved with BigQuery in this module?\",\\n    \"Are there real-time streaming capabilities in BigQuery for this application?\",\\n    \"How can BigQuery\\'s real-time features be integrated into our project?\"\\n]', 'e258d7d874a417e373e721df5f292757': '[\\n    \"What is the cause of the error when loading data from external tables into a materialized table in BigQuery?\",\\n    \"How can I identify the problem if there is an invalid timestamp error in my data?\",\\n    \"What can be done to allow queries to work when there\\'s an invalid timestamp in BigQuery?\",\\n    \"How can I handle invalid rows when importing data into a materialized table from an external source?\",\\n    \"What data type should be used in the schema to address timestamp parsing issues during data import?\"\\n]', 'ad18b5172ca2ab722e148b476eb89107': '[\\n    \"Why do I get an error message in BigQuery regarding a valid Timestamp annotation and how can I resolve it?\",\\n    \"What is the recommended solution when encountering a BigQuery error related to parquet timestamp annotations?\",\\n    \"How can I adjust my `pq.write_to_dataset` function to avoid BigQuery timestamp errors?\",\\n    \"What parameter should I include in `pq.write_to_dataset` to ensure correct timestamp handling in BigQuery?\",\\n    \"Where can I find more information about issues with parquet files and timestamp formats in BigQuery?\"\\n]', '1e8b11f4057c788ea3597c05a5ae5038': '[\"How can datetime columns be correctly processed in Parquet files for BigQuery from Pandas?\", \"What method ensures Pandas datetime columns are converted to timestamps in BigQuery?\", \"How is PyArrow used to address integer columns appearing instead of datetimes in BigQuery?\", \"What steps should be followed in Mage when exporting data to ensure correct datetime handling?\", \"Why provide PyArrow with an explicit schema for datetime columns in Parquet files?\"]', '15a20156271f360b11536bb3d6a98fb6': '[\\n  \"How do I create an external table in Google Cloud BigQuery using Python?\",\\n  \"What is the source format required for creating an external table in BigQuery?\",\\n  \"Which object is used to set the external data source format in BigQuery?\",\\n  \"How can I specify where my data is stored in Google Cloud when creating an external table?\",\\n  \"What is the function used to create an external table in BigQuery with a specified configuration?\"\\n]', 'b947c4b9daf651d7856de5dddbf8ec78': '[\\n  \"How can I check if a table already exists in BigQuery using Python?\",\\n  \"What code snippet can I use to verify the existence of a BigQuery table before creating it?\",\\n  \"Is there a method to confirm a BigQuery table\\'s existence using tableID and client?\",\\n  \"Can you suggest an approach to check and delete a BigQuery table if it exists?\",\\n  \"What is a way to handle exceptions when verifying if a BigQuery table exists?\"\\n]', 'e0e5f221898f70204919ae5ac3f80077': '[\\n  \"What command can be used in BigQuery Cloud Shell to load data from Google Cloud Storage and avoid missing close double quote errors?\",\\n  \"How can I handle missing close double quote errors when uploading CSV files to BigQuery?\",\\n  \"Which options should be included in the bq load command to manage quoted newlines when loading data?\",\\n  \"How can you upload data from a Google Cloud Storage bucket to BigQuery while preventing close double quote errors?\",\\n  \"What is the correct bq load command syntax for uploading CSV files to avoid issues with quoted newlines?\"\\n]', 'e95f5ac7f88f301844a5dd045a125114': '[\"What is a solution if GCP BigQuery and Google Cloud Storage are in different regions?\", \"How can I resolve the issue of not being able to read and write in different Google Cloud regions?\", \"Where should I check the region for my Google Cloud bucket when facing location issues?\", \"In BigQuery, how do I ensure that the regions match with my Google Cloud bucket?\", \"What steps can I take to fix region mismatch between GCP storage and BigQuery?\"]', 'ac0606c0724c06bb318b337a55ca3b2c': '[\"What are the advantages of using Cloud Functions for automating tasks in Google Cloud?\", \"How can I use a Python script to load CSV files directly to BigQuery?\", \"What components are needed to define a schema for data in CSV.gz files when using BigQuery?\", \"How do I use the write disposition string to manage data loading in BigQuery?\", \"How does the provided Python Cloud Function script handle downloading and loading data from GitHub into BigQuery?\"]', 'a51285691d043314c28f7453912a8a33': '[\"Why does querying external and materialized tables with count(distinct(*)) yield identical results?\", \"What could cause identical results when using count(distinct(*)) on external and materialized tables?\", \"How can I resolve identical query results for count(distinct(*)) on two tables?\", \"What setting might I adjust if distinct counts on different tables return the same number?\", \"What should I uncheck in query settings to fix identical distinct count results?\"]', '993efca5e0227b9178433f34b6d531b7': '[\\n  \"What is the issue when there\\'s an error message about Parquet column DOlocationID not matching cpp_type DOUBLE in BigQuery?\",\\n  \"How can I fix the data type issue when using Pandas to inject data into Google Cloud Storage for BigQuery?\",\\n  \"Why does Pandas cast DOlocationID and PUlocationID columns to float data type when there are missing values?\",\\n  \"What should I do to ensure consistent data types between a Parquet file in GCS and a BigQuery schema?\",\\n  \"Where is it recommended to define the data type of columns in the ETL process for BigQuery?\"\\n]', 'f20fa7c9dd754957e78e4df64d35a7be': '[\\n  \"What should I do if I encounter an error while reading a Parquet table with a column type mismatch?\",\\n  \"How can I resolve an issue with an invalid project ID in GCP BigQuery?\",\\n  \"Why does a Parquet column type mismatch error occur with the \\'DOlocationID\\' field?\",\\n  \"What are the requirements for a valid GCP BigQuery project ID?\",\\n  \"How can I ensure proper syntax after the \\'from\\' clause in BigQuery SQL queries?\"\\n]', 'cf854d6d177758f5d3aa4b32c47ece9b': '[\"What happens when there\\'s a type mismatch in BigQuery?\", \"Can BigQuery handle more than one partition column?\", \"Why does the error mention \\'cpp_type DOUBLE\\' for a Parquet column?\", \"Is it possible to partition multiple columns in BigQuery?\", \"What is the error related to Parquet column \\'DOlocationID\\'?\"]', '404d1a954630b06f1522027ed80f3b08': '[\\n  \"What error occurs when reading the table \\'trips_data_all.external_fhv_tripdata\\'?\",\\n  \"What is required for a partition expression involving dates in BigQuery?\",\\n  \"What solution is suggested when encountering a partition error related to DATE in BigQuery?\",\\n  \"Which Parquet column type mismatch is mentioned in the error message?\",\\n  \"How can you convert a column to datetime format in the solution provided?\"\\n]', '23e7a284a81def51bd78b07b0fe680fa': '[\\n  \"What is the difference between native tables and external tables in BigQuery?\",\\n  \"Why is there an error with \\'DOlocationID\\' column type in the external table?\",\\n  \"Where is the data stored in native BigQuery tables?\",\\n  \"How does BigQuery handle data storage for external tables?\",\\n  \"Where can I find more resources about BigQuery and its table types?\"\\n]', 'd216ae3d1f7df0e131f9b65557c70849': '[\"What might cause an error reading the table \\'trips_data_all.external_fhv_tripdata\\' related to the type of \\'DOlocationID\\'?\", \"How do I resolve the error encountered while exporting an ML model from BigQuery to Google Cloud Storage?\", \"What specific error did I encounter when attempting the BigQuery extract command?\", \"Why might there be an issue with dataset location when running the export command from BigQuery to GCS?\", \"What is a common mistake people should avoid when exporting an ML model to Google Cloud Storage?\"]', '58a6ccecbef224117d090ea90fb9f221': '[\\n  \"Why am I getting an error with Parquet column \\'DOlocationID\\' type mismatch in trips_data_all.external_fhv_tripdata?\",\\n  \"How can I resolve the \\'Dim_zones.sql Dataset was not found in location US\\' error when running fact_trips.sql?\",\\n  \"What should I update to fix the issue when the fact_trips.sql is not running due to a missing dim_zones table?\",\\n  \"Is there a way to specify the location when creating the dim_zones table to prevent errors?\",\\n  \"What does it mean when the error message states that Parquet column type does not match the target cpp_type DOUBLE?\"\\n]', 'd8106c1b647455454cc129d60338e4d7': '[\"What is the solution for the error related to Parquet column \\'DOlocationID\\' type mismatch?\", \"How do I set up serving_dir on a MacBook with an Apple M1 chip?\", \"What docker image should I use instead of tensorflow/serving on an M1 MacBook?\", \"What are the required ports when running the docker command for model serving?\", \"Which environment variable should be set when running the docker command for model serving?\"]', 'ada54c16175f63fe0d0ab287c1873993': '[\\n  \"What should I do when my virtual machine runs out of storage space?\",\\n  \"How can I manage disk space issues on my VM?\",\\n  \"What steps can I take if I suspect Prefect files are taking up too much space on my VM?\",\\n  \"How do I clean up large files on my virtual machine?\",\\n  \"What actions should be taken if VM space is consumed by cached Prefect files?\"\\n]', '83aa9cfa3a68b5b70b705d49518e160e': '[\"What does the error \\'Parquet column DOlocationID has type INT64 which does not match the target cpp_type DOUBLE\\' mean?\", \"What should I do if I\\'m instructed to \\'stop with loading the files into a bucket\\'?\", \"Why is there an error when trying to read the table trips_data_all.external_fhv_tripdata?\", \"How should I handle data files if not converting them to parquet format?\", \"Why would they ask not to do more than load files into the bucket and create an external table?\"]', '7ac17b5474d555fc2542dbfb4ed97dcd': '[\\n  \"What error might I encounter when reading Parquet files directly from nyc.gov into pandas?\",\\n  \"Why does attempting to read Parquet files from nyc.gov result in an out of bounds error?\",\\n  \"How can I fix the out of bounds timestamp error when using pandas to read Parquet files?\",\\n  \"What workaround can be used to handle errant data records with future years in Parquet files?\",\\n  \"How can I filter out rows with invalid timestamps in Parquet files to avoid errors?\"\\n]', '4af3391cfed0c4153f92c937dd22d178': '[\\n  \"What should I do if I encounter an error message indicating a type mismatch when reading a table in BigQuery?\",\\n  \"Are all 12 parquet files for green taxi data from 2022 necessary for homework 3?\",\\n  \"How can I access the 2022 NYC taxi data parquet files for each month?\",\\n  \"What is the recommended method for referencing all 12 files when creating an external table in BigQuery?\",\\n  \"Can a wildcard be used to refer to multiple parquet files in BigQuery URIs?\" \\n]', '0aa7dbd4143a5324f7f6e85589eace61': '[\\n  \"What should I do if I encounter an error while reading table \\'trips_data_all.external_fhv_tripdata\\'?\",\\n  \"How can I upload files to Google Cloud Storage to avoid schema issues?\",\\n  \"Is it possible to upload multiple files at once when using the GCS GUI?\",\\n  \"What could cause a Parquet column type mismatch in a BigQuery table?\",\\n  \"How do I upload a folder to Google Cloud Storage using the GUI?\"\\n]', '0ca9323fe927c9bffb032fea7ed68f6f': '[\\n  \"What is the reason for an error when reading a table in relation to \\'trips_data_all.external_fhv_tripdata\\'?\",\\n  \"How should the format of dates be considered when predictions from a partitioned or clustered table are unexpected?\",\\n  \"What type mismatch in the \\'trips_data_all.external_fhv_tripdata\\' table could cause an error?\",\\n  \"What column type disparity might cause reading issues in the \\'trips_data_all.external_fhv_tripdata\\' table?\",\\n  \"Which data type issue in Parquet files might affect table reading and require attention?\"\\n]', '562f8b274f9bcaa51c8b21e4168e2850': '[\\n  \"What should I do if there\\'s an error reading the Parquet column \\'DOlocationID\\' due to a type mismatch?\",\\n  \"How should I handle not getting an exact match for one of the options in Module 3 homework Question 6?\",\\n  \"What type mismatch is mentioned in the error for the \\'DOlocationID\\' column in \\'trips_data_all.external_fhv_tripdata\\'?\",\\n  \"Is it normal for the homework answers to not match exactly with the options provided?\",\\n  \"What did Alexey suggest for selecting an answer in the Module 3 homework Question 6?\"\\n]', '7720f64f62d3a05334c70e58c32c8397': '[\\n  \"What error occurs when the \\'utf-8\\' codec can\\'t decode a byte due to an invalid start byte?\",\\n  \"What steps should I follow to fix a UnicodeDecodeError when reading data into a pandas dataframe?\",\\n  \"How should I specify encoding when writing a dataframe to GCS as a CSV?\",\\n  \"What alternative to pd.read_csv can be used if encountering decoding issues with CSV files?\",\\n  \"What does the error message about INT64 and DOUBLE in Parquet files indicate?\"\\n]', '5305ee09e1f8844a37eb47f92e7a8ed2': '[\\n  \"What is a generator in Python?\",\\n  \"How does a generator function work in Python?\",\\n  \"Why are generators memory-efficient compared to lists or tuples?\",\\n  \"What keyword is used to create a generator in Python?\",\\n  \"How do generators benefit when working with large datasets?\"\\n]', '8e46b4e5c2af0bf2358d3277a56c4bb4': '[\"What is the issue with \\'DOlocationID\\' in the error message related to \\'trips_data_all.external_fhv_tripdata\\'?\", \"What data type mismatch error is occurring in the Parquet column \\'DOlocationID\\'?\", \"How can I read multiple files simultaneously using Python?\", \"What does \\'read_parquet\\' function support regarding file input?\", \"How does \\'read_parquet\\' combine multiple files into one table?\"]', '98f0115a5f41cdb68b2bb4e6a98eaa0c': '[\\n    \"What is the correct way to ensure DOlocationID is in Int64 format?\",\\n    \"How do I convert DOlocationID to Int64 format in Python?\",\\n    \"What is the mistake in converting DOlocationID using pd.to_numeric with downcast?\",\\n    \"Why doesn\\'t downcast=integer work for changing DOlocationID type?\",\\n    \"What is the right method to correct the INT64 error when reading DOlocationID?\"\\n]', 'f53995481a9a9e025455a8a1f3847819': '[\\n  \"What error occurs due to the improper type of the Parquet column \\'DOlocationID\\' when reading table \\'trips_data_all.external_fhv_tripdata\\'?\",\\n  \"Which error might occur when there is a path issue related to Prefect\\'s storage in a flow to load data to GCS?\",\\n  \"How can the removal of \\'cache_key_fn=task_input_hash\\' influence the Prefect flow execution?\",\\n  \"In what scenario could a cache key cause an error when running a Prefect flow?\",\\n  \"What is a potential problem when using a cache key on the initial run of the flow in Prefect?\"\\n]', '4e4a3188963f28721840645b19c38c9b': '[\\n    \"How do I download a csv.gz file from a URL in a Prefect environment?\",\\n    \"What is the function signature for a Prefect task that downloads a file?\",\\n    \"Can you provide a sample snippet for downloading files in Prefect?\",\\n    \"Which Prefect function is used to execute a flow that downloads a csv.gz?\",\\n    \"What are the roles of the task and flow decorators in downloading files using Prefect?\"\\n]', 'bc902576bcb529437abb2fd323aa1b15': '[\\n  \"What action should be taken if you encounter a \\'not found in location us\\' error while using dbt and BigQuery?\",\\n  \"How can I resolve a DBT Cloud production error where the prod dataset is not in the EU location?\",\\n  \"What is a simple solution for a production deployment issue in DBT Cloud with BigQuery in the EU region?\",\\n  \"Why might there be a production error when deploying DBT models with a prod dataset in the incorrect location?\",\\n  \"How can I ensure my dataset is available in the correct EU location when scheduling jobs in DBT Cloud?\"\\n]', 'b837581146fa27bb174cd5fe7b6cf752': '[\\n  \"What should I do if my project lacks a development environment?\",\\n  \"Where can I find guidance on setting up a development environment for dbt?\",\\n  \"What does the error about missing a development environment suggest I do?\",\\n  \"Is there a video that explains how to configure development credentials for dbt?\",\\n  \"What resources are available if I encounter a development environment setup error in dbt?\"\\n]', 'e438b8a5df11bdae2b11e27c549a2f11': '[\\n  \"What error might I encounter when dbt can\\'t connect to BigQuery during the setup?\",\\n  \"What causes the Runtime Error when connecting dbt Cloud with BigQuery?\",\\n  \"How can I resolve an access denied issue when setting up dbt Cloud with BigQuery?\",\\n  \"Which roles should I assign to my dbt service account to avoid permission issues?\",\\n  \"Where can I find more information if dbt can\\'t connect to my database?\"\\n]', '664afcccf5f2d4a3132bb6e1333e4eb2': '[\\n  \"What could cause a dbt Cloud run to be cancelled?\",\\n  \"What should I check if there\\'s a dbt build error about a missing project?\",\\n  \"Why do I need a dbt_project.yml file for my dbt Cloud run?\",\\n  \"How can I ensure dbt Cloud runs correctly if my project is in a subdirectory?\",\\n  \"Where in dbt Cloud do I specify the project\\'s location if it\\'s not in the root directory?\"\\n]', '57f9e473194ac44ff5381544ac0febad': '[\\n  \"What should I do if I encounter a permission denied error when attempting to clone the DataTalksClub/data-engineering-zoomcamp repository?\",\\n  \"How can I fix the issue of failing to read from the remote repository due to permission issues when cloning?\",\\n  \"If I cannot write to the DataTalksClub/data-engineering-zoomcamp.git, what are the steps to correctly clone the repository?\",\\n  \"What are the alternative solutions if my cloning attempt fails due to lacking write permissions to the original repository?\",\\n  \"How can I avoid messing up my main repository when working with dbt lessons while still using branching and PRs?\"\\n]', 'a417cd2fea0f4a266393cf118752c7c0': '[\\n    \"Why is the option to trigger a job by pull requests disabled for me when creating a new Continuous Integration job in dbt cloud?\",\\n    \"What should I do if I\\'m on the Developer Plan and want to set up a CI Job in dbt Cloud?\",\\n    \"I\\'m in the Team Plan (trial period), but the trigger by pull requests is still disabled. How can I resolve this?\",\\n    \"Are there any plan requirements for setting up a Continuous Integration job in dbt Cloud?\",\\n    \"How can a user on the Developer Plan proceed if they want to utilize Continuous Integration Jobs?\"\\n]', '9bca93be39d55769af52c5d99f89ec7c': '[\\n    \"How can I solve the issue of my IDE session not starting in Module 4?\",\\n    \"What should I do if the DBT Cloud IDE loads indefinitely?\",\\n    \"Where can I find guidance on making an SSH key for fixing an IDE issue?\",\\n    \"How do I use a deploy key to address IDE session errors?\",\\n    \"What steps are necessary to import a repository into a dbt project to fix IDE issues?\"\\n]', '1189db657523527c5ef3f9d32a14ffed': '[\\n  \"What should I do if Python automatically chooses an incorrect column format when converting from CSV to Parquet in my DBT project?\",\\n  \"How can I resolve the error indicating a mismatched Parquet column type for \\'ehail_fee\\' when running dbt in BigQuery?\",\\n  \"What are the steps to modify \\'web_to_gcp.py\\' pipeline to handle gz files properly?\",\\n  \"Why does the \\'ehail_fee\\' column cause an error in BigQuery, and what are some potential solutions?\",\\n  \"How can I specify data types when importing a CSV file into a pandas dataframe to avoid type mismatch issues?\"\\n]', '33f4dbd02b63cbc4859ae58692aa3814': '[\"What should I do if I get an Access Denied error from the S3 bucket when loading trip data into GCS?\", \"How can I download the trip data if the provided S3 URL isn\\'t working?\", \"Where can I find the GitHub CLI necessary for downloading the trip data?\", \"What commands do I need to use with the GitHub CLI to download the yellow trip data?\", \"How can I upload the trip data files to a GCS bucket after downloading them?\"]', 'c381de406ce99a7983abe013b35a8f7f': '[\\n  \"What causes the error when Airflow tries to convert fhv_tripdata_2020-01.csv using format_to_parquet_task?\",\\n  \"How can I fix the CSV parse error for fhv_tripdata_2020-01.csv during ingestion in Airflow?\",\\n  \"What command should I run to remove unwanted line breaks in fhv_tripdata_2020-01.csv?\",\\n  \"Why is it necessary to convert fhv_tripdata_2020-01.csv for homework question 3?\",\\n  \"How should I re-execute the task in Airflow after fixing the CSV file error?\"\\n]', 'f6230b62b5e6576973f62814abbf0c46': '[\\n  \"How can I efficiently load yellow and green trip data for 2019 and 2020?\",\\n  \"What method was initially used to load yellow trip data and what problem was encountered?\",\\n  \"What was the issue faced when uploading parquet files directly to GCS for the yellow trip data?\",\\n  \"Who suggested the alternative hack shared on Slack for loading data to BigQuery?\",\\n  \"What additional step must be taken when following the YouTube hack for loading data into BigQuery?\"\\n]', '409a0d08dcbd219fdd2a340d081555e0': '[\\n  \"How can I move multiple files from Google Cloud Storage to BigQuery in Module 4?\",\\n  \"What should be added to the destination folder to transfer files from Google Cloud Storage?\",\\n  \"To move many files from a Google storage bucket, what needs to be added in the destination for BigQuery?\",\\n  \"What needs to be specified in the destination folder to move several files from Google storage to BigQuery?\",\\n  \"For analytics engineering with dbt, how do I transfer multiple files to BigQuery from a storage bucket?\"\\n]', '6979d254c9a326d21dafd5d662d44a3b': '[\\n  \"Why did SSH access to my GCP VM fail after a reboot?\",\\n  \"How can I resolve SSH connectivity issues on my GCP VM?\",\\n  \"What might cause SSH to stop working after restarting my VM?\",\\n  \"How do I prevent SSH issues related to running prefect on my VM?\",\\n  \"What should I do with \\'.prefect/storage\\' to avoid SSH problems on my VM?\"\\n]', 'e203ba14d396ab76cca94f2eafcf801f': '[\"What should I do if I lose SSH access to my GCP VM due to lack of space?\", \"How can I resolve a \\'Permission denied (publickey)\\' error on my GCP VM?\", \"What steps can I follow to regain SSH access to my GCP VM?\", \"Why might I be unable to SSH into my GCP VM?\", \"What causes a \\'Permission denied\\' error on a GCP VM and how can I fix it?\"]', '1d7707cfd71be984b4016abf81be12eb': '[\\n  \"What should I do if the dataset eighth-zenith-372015:trip_data_all is not found in BigQuery location us-west1?\",\\n  \"How can I check the location of my source dataset in BigQuery?\",\\n  \"What\\'s the procedure to correct mismatches in dataset regions when using dbt?\",\\n  \"How do I specify a single-region location instead of a multi-regional one in dbt cloud?\",\\n  \"What steps are needed to update the BigQuery location within dbt cloud?\"\\n]', 'eea31aad3d0762ceeeebf2149bceba48': '[\\n    \"What should I do if a warning appears saying dbt_utils.surrogate_key has been replaced by a newer function during dbt run?\",\\n    \"How do I resolve an error related to replacement of dbt_utils.surrogate_key when executing dbt run after installing dbt-utils version 1.0.0?\",\\n    \"What error might occur after creating fact_trips.sql when running dbt, and how can it be fixed?\",\\n    \"How can I fix the Access Denied error caused by missing permissions during globbing file pattern in BigQuery?\",\\n    \"What roles need to be added to the service account to fix BigQuery permission issues when using dbt run?\"\\n]', 'a538abbb25dab6c8353ebf0fee4a73f3': '[\"What should I do if I encounter a dbt_utils not found error?\", \"How do I add dbt_utils to my project?\", \"Where do I create the packages.yml file?\", \"What version of dbt_utils should I specify in packages.yml?\", \"What is the next step after creating the packages.yml file?\"]', 'd17c30f5bbb71d60b08cf066dea9c999': '[\\n    \"How do I handle missing lineage in dbt projects?\",\\n    \"What should I verify if my dbt project has compilation errors?\",\\n    \"If lineage is not working, where do I check for build issues?\",\\n    \"What might cause lineage to be unavailable in dbt?\",\\n    \"How can I view detailed error messages during a dbt run?\"\\n]', '7e5a325f9fc3fd371b05838cda828eaf': '[\"How can I resolve Fact_trips only containing data for a few days?\", \"What command should I use to ensure my dbt build has all data?\", \"What parameter ensures that my dbt run is not a test run?\", \"What should I check if my dbt build is missing data?\", \"How can I correctly quote variables in dbt commands?\"]', 'd5ae6d3d038eb0bed69732d073071ae9': '[\\n  \"Why does my fact_trips table show data for only one month instead of all months?\",\\n  \"How can I ensure my automated flow appends all monthly data to BigQuery?\",\\n  \"What is the impact of setting if_exists to replace when writing data from GCS to BigQuery?\",\\n  \"In what scenario should I set if_exists to append in my data flow process?\",\\n  \"What error might cause only the last inserted month\\'s data to appear in BigQuery?\"\\n]', '9210f659302de46887a6765f4aac8df2': '[\\n  \"What should I revise when BigQuery errors occur on dm_monthly_zone_revenue model runs?\",\\n  \"How do I correct the second SELECT line in dm_monthly_zone_revenue.sql?\",\\n  \"What is the correct syntax for date_trunc in the dm_monthly_zone_revenue model?\",\\n  \"Why does the dm_monthly_zone_revenue.sql model fail?\",\\n  \"What should the date_trunc function look like in dm_monthly_zone_revenue?\"\\n]', '9f5ad916ce1a1c9d892d050fb8c7c6e5': '[\\n  \"What should replace dbt_utils.surrogate_key in our code?\",\\n  \"How can I update dbt_utils.surrogate_key to the latest method?\",\\n  \"What function is recommended instead of dbt_utils.surrogate_key?\",\\n  \"Is there a new way to write dbt_utils.surrogate_key?\",\\n  \"Can you suggest an alternative to dbt_utils.surrogate_key?\"\\n]', '410d421e59dd5fc6c077374237b9e691': '[\\n    \"How do I troubleshoot an error after changing the location in dbt?\",\\n    \"What should I do if dbt run fails after changing the dataset location?\",\\n    \"Why does dbt run still show an error after I updated the dataset\\'s location?\",\\n    \"What steps can resolve a location error in dbt\\'s dataset?\",\\n    \"How can I recreate a dataset in the correct location using dbt?\"\\n]', '2801500c05ebd8c6edc453c7f0fe4bcd': '[\"Why does specifying a variable not change the number of rows in my BigQuery table after running dbt?\", \"How do I stop dbt from creating a new dataset after running a CI/CD job?\", \"What should I do if my BigQuery table does not update with all rows after running dbt?\", \"Why is a new \\'dbt_cloud_pr\\' environment created in BigQuery after my CI/CD job?\", \"How can I merge my dev models into production models in dbt without creating a new environment?\"]', '225313d7c5664c939ea5870af4835a77': '[\\n  \"What purpose does the staging dataset serve in analytics engineering with dbt?\",\\n  \"Why is the staging dataset an intermediate step before the fact and dim tables?\",\\n  \"Why are datasets in staging materialised as views and not tables?\",\\n  \"Did Vic use the staging dataset for the project in Module 4?\",\\n  \"What datasets do we need to create for the project if the staging dataset isn\\'t used?\"\\n]', '2d8e2ef15c0b90f1cbadc3042f57d84e': '[\"How can I make DBT Docs accessible if served but not showing in the browser?\", \"What should I do if I can\\'t view DBT Docs in my browser despite them being served?\", \"If DBT Docs are served but not visible in the browser, what troubleshooting step is recommended?\", \"Which line in docker-compose might need to be removed to access served DBT Docs in a browser?\", \"What solution is suggested for DBT Docs not opening in a browser after being served?\"]', 'f6a6512b657392e9385c51f6ed803f2b': '[\"How can I resolve a 404 Not found error for a BigQuery dataset location in Europe West 6?\", \"What steps should I take if my dbt build is not locating the dataset in europe-west6?\", \"How do I update the GCP location settings in my analytics project for BigQuery?\", \"What should I do if my BigQuery dataset isn\\'t found in the specified region?\", \"How can I ensure my newly built dataset is correctly located in GCP?\"]', '76668af37d135d410c0e639cf2e64d63': '[\\n  \"How do I make changes if the main branch is read-only according to Module 4?\",\\n  \"What should I do in dbt if I can\\'t edit the main branch?\",\\n  \"If the main branch in git is unavailable for changes, what is my course of action?\",\\n  \"What is the recommended approach when the main branch is read-only in dbt?\",\\n  \"How can I edit when the main branch is locked due to being read-only?\"\\n]', '9b1199c201587cc4349e6c5dcda015f4': '[\\n    \"How do I resolve being in read-only mode in dbt+git when I can\\'t edit files?\",\\n    \"What steps should I take to make changes if I\\'m in read-only mode with dbt+git?\",\\n    \"How can I enable editing capabilities in dbt+git when currently in read-only?\",\\n    \"What is the process to follow when unable to edit files due to read-only status in dbt+git?\",\\n    \"When stuck in read-only mode in dbt+git, how can I proceed to edit the files?\"\\n]', 'e8bb94f8325b7fc04997732c7563325e': '[\\n  \"What is the error message received when trying to create CI checks job for deployment to Production using dbt deploy and Git CI?\",\\n  \"Which repositories support triggering pull requests for creating CI checks in dbt?\",\\n  \"What is the recommended integration to use instead of Git Clone according to the guide on the DTC repo?\",\\n  \"Where can I find a step-by-step guide to switch from Git Clone to Github for dbt deployment?\",\\n  \"Is it possible to create CI checks job for deployment to Production with Git Clone in dbt?\"\\n]', 'a35fac43ba748e61d3e794f5f746b880': '[\\n   \"What should I do if I can\\'t see \\'Run on Pull Requests\\' when setting up CI with Github in dbt?\",\\n   \"How do I establish a native connection between dbt and Github for CI purposes?\",\\n   \"Where can I find the option to disconnect my current Github configuration in dbt?\",\\n   \"What permissions need to be allowed when linking my Github account to a dbt project?\",\\n   \"What steps are necessary to reconfigure Github after disconnecting it for dbt CI?\"\\n]', 'b290dbc9a4a8dd9b07cc4907fc6af297': '[\"What should be done if a compilation error occurs due to a missing source while working on model \\'stg_green_tripdata\\'?\", \"How can the lineage graph issue in video DE Zoomcamp 4.3.1 be resolved?\", \"What is a quick solution to fix the missing \\'staging.green_trip_external\\' source error?\", \"At which point in DE Zoomcamp 4.3.1 does the lineage graph problem occur?\", \"What file needs to be saved to resolve the lineage graph and compilation error issues?\"]', '1239fa510881a1f5a631c0431acacd33': '[\\n    \"What does the error \\'NoneType object is not iterable\\' mean in the context of macro test_accepted_values?\",\\n    \"How can I resolve the \\'NoneType object is not iterable\\' error in test_accepted_values?\",\\n    \"What configuration needs to be added to dbt_project.yml to resolve the accepted_values test issue?\",\\n    \"Where do I need to specify the payment_type_values to fix the error in the test for accepted values?\",\\n    \"Which variables should I define in dbt_project.yml to ensure successful testing of payment type values?\"\\n]', '4860112b4167caa1e502ac33e05746a0': '[\\n    \"What is the reason for dbt macro errors with get_payment_type_description(payment_type)?\",\\n    \"How can I resolve the dbt macro error related to get_payment_type_description(payment_type) when working with BigQuery?\",\\n    \"Why does the get_payment_type_description(payment_type) macro have issues in the dbt project?\",\\n    \"What modifications are needed in the get_payment_type_description(payment_type) macro to work with BigQuery?\",\\n    \"What data type change is required for the payment_type in the dbt macro to handle BigQuery errors?\" \\n]', '933a90b803fe78752fe43f338f2e0bf7': '[\\n  \"What should I do if I encounter an error in dbt?\",\\n  \"How can I find the line causing an issue in my dbt query?\",\\n  \"Where does the dbt error log direct me for more details?\",\\n  \"How can I identify the problematic line in a dbt query error?\",\\n  \"What information does the dbt error log provide for troubleshooting?\"\\n]', '2588b789b16c5a507a85c3d82bd1bb9a': '[\"Why does setting the target schema to \\'marts\\' result in a schema named \\'dbt_marts\\'?\", \"Can I change the default dbt behavior that appends custom schema to the initial schema?\", \"How can I prevent dbt from adding a prefix to my schema name?\", \"What is the purpose of the \\'generate_schema_name.sql\\' macro in dbt?\", \"How can I update the schema naming in \\'dbt_project.yml\\'?\"]', 'a224104dbd0cb17795551f0a8da8437b': '[\\n  \"What setting allows you to define a subdirectory as the dbt project root?\",\\n  \"In which platform can you set the Project subdirectory for dbt?\",\\n  \"How can you configure a subdirectory to be the dbt project root in dbt cloud?\",\\n  \"Is there a way to specify a project subdirectory in the dbt settings?\",\\n  \"Where do you set the subdirectory for your dbt project in dbt cloud?\"\\n]', '7f65b4ec80e16bc0b24a5fb8787757e8': '[\\n    \"What should I do if I encounter a compilation error related to a model dependency on a source not found?\",\\n    \"How can I resolve a compilation error when dbt cannot find a specified table name in my model file?\",\\n    \"If dbt throws a source not found error for a model, what modifications should be made in the SQL models?\",\\n    \"What is an example of modifying a SQL model to address a missing source error in dbt?\",\\n    \"How should I correctly reference existing table names in my SQL models to avoid compilation errors?\"\\n]', 'c1618ee0fcadc3c77318502b9390e60e': '[\\n  \"What should I do if I encounter a compilation error stating a model depends on a node which was not found in the production environment?\",\\n  \"How can I resolve a missing node issue in dbt where my model cannot find a specified seed file during compilation?\",\\n  \"What steps should I take to fix a compilation error in the production environment related to a node dependency not being found?\",\\n  \"What actions should I perform to ensure my seed file is recognized in the production environment when facing a compilation error?\",\\n  \"How can I check for issues in .gitignore or branch discrepancies when facing a compilation error about a missing node?\"\\n]', '21e809f69a098e5193f2cb81fcc18336': '[\"What should I do if I encounter \\'Access Denied\\' during dbt run with an external table?\", \"How can I resolve a permission issue with BigQuery when using dbt run?\", \"What steps are needed to fix \\'Permission denied\\' in BigQuery with dbt?\", \"How do I address access issues in BigQuery after running dbt with external tables?\", \"What role additions can resolve permission problems in BigQuery when using dbt?\"]', '63576af395086fb8d1a3855d78b625a4': '[\\n  \"What problem might you encounter when injecting data into BigQuery in terms of column data types?\",\\n  \"What is a solution for dealing with integer columns with missing values in pandas to avoid type errors?\",\\n  \"How can you make pandas automatically infer the correct data types during the data transformation stage?\",\\n  \"What function can you use to automatically infer data types and avoid having to manually specify all integer columns?\",\\n  \"What placeholder value can be used to fill missing values before converting data types with convert_dtypes?\"\\n]', '7a80a0966fcbaf52d51c57a0218e63f1': '[\\n  \"Why am I encountering an error regarding \\'taxi_zone_lookup\\' not found when loading my GitHub repository?\",\\n  \"What should I do if the system cannot find \\'taxi_zone_lookup\\' while loading?\",\\n  \"How do I resolve the exception raised when \\'taxi_zone_lookup\\' is missing during the loading process from GitHub?\",\\n  \"What might cause a \\'taxi_zone_lookup\\' not found exception when loading a GitHub repo?\",\\n  \"How can I fix the issue of \\'taxi_zone_lookup\\' not found when using dbt with my GitHub repository?\"\\n]', '545d5b92fb14b2dd9a129234423caa43': '[\\n  \"What should I check if the ‘taxi_zone_lookup’ file isn\\'t found?\",\\n  \"How can I resolve a dbt error 404 related to a table not found in location?\",\\n  \"Why might a table in my dbt project say it’s not found in Europe-west6?\",\\n  \"What should I verify in dbt settings if my datasets are not in the right region?\",\\n  \"How do I ensure my datasets are in a specific region rather than a generalized region in dbt?\"\\n]', 'da8518e178713c457fea9ae3d8c4959a': '[\"How can I prevent data type errors when using parquet files in Module 4?\", \"What file type should be used to avoid ingestion errors caused by parquet files?\", \"What is the recommended command to create an external table without data type issues?\", \"Which storage location should be used for the CSV files to avoid data type errors?\", \"What file format should be used in the OPTIONS clause to resolve type errors in week 4?\"]', 'b9063449e12981e3117b1870d2aa12c1': '[\\n    \"What could cause an inconsistent number of rows when re-running the fact_trips model?\",\\n    \"How can we achieve consistent row counts in fact_trips upon re-runs?\",\\n    \"Why is the first row chosen from partitions in fact_trips model not consistent?\",\\n    \"What role does the order by clause play in deduplication of staging files?\",\\n    \"How does the unknown borough affect the number of rows when re-running fact_trips?\"\\n]', 'b2e2c5135204ae8635d73daa7781b06f': '[\\n  \"What should I do if I encounter a data type error on the trip_type column?\",\\n  \"Why might I get a data type error in the fact table when using BigQuery?\",\\n  \"How can I resolve data type errors related to trip_type in my analytics engineering?\",\\n  \"What could cause a data type error for trip_type in Module 4\\'s dbt course?\",\\n  \"What is the recommended data type casting solution for trip_type data type errors?\"\\n]', 'c297422bcae4c71b2373ff8e975b017b': '[\\n  \"What might cause an error involving duplicate column names like locationid in a CREATE TABLE query?\",\\n  \"How can one resolve an error caused by using select * without specifying a table name in a query?\",\\n  \"What is the consequence of not specifying the table name in a select * query in terms of column names?\",\\n  \"How should the query be written to avoid duplicate column names when joining tables like fhv and dim_zones?\",\\n  \"What does the given example in module 4 suggest replacing in the query to fix the duplicate column name issue?\"\\n]', '909e6035e92539225be4e762bd56bf5d': '[\\n  \"What causes the Bad int64 value: 0.0 error when working with int64 values?\",\\n  \"How can the safe_cast function help resolve the Bad int64 value: 0.0 error?\",\\n  \"Can dbt_utils function be used to handle null values in integer casting?\",\\n  \"What is an alternative method to using dbt_utils for safe casting in dbt?\",\\n  \"How do you implement safe_cast for the ehail_fee column in dbt?\"\\n]', '9cdcd2e604172fccf755305d85740b28': '[\\n    \"What should I do if I encounter a \\'Bad int64 value: 2.0/1.0\\' error when building the fact_trips.sql model?\",\\n    \"How can I fix the payment_type_description field error that results in the entire field becoming null?\",\\n    \"What approach can I use to resolve a \\'Bad int64 value: 1.0\\' error involving the ratecodeid column?\",\\n    \"How do I address the \\'Bad int64 value\\' error for the trip_type column in the Green_tripdata table?\",\\n    \"Is there a method to cast problematic decimal places to integers when working with dbt models?\"\\n]', '5279d6df98a59765af30892bd7bdf6be': '[\\n  \"What should I do if I encounter a Parquet column type conflict for \\'ehail_fee\\' in fact_trips.sql?\",\\n  \"How can I resolve the issue with Parquet column \\'ehail_fee\\' having type DOUBLE instead of INT64 in dbt?\",\\n  \"What is the recommended fix for the data type mismatch error in green_taxi_2019-01.parquet file?\",\\n  \"How can I alter the ehail_fee column to match the expected type in stg_green_trips.sql?\",\\n  \"Is there a specific dbt function to safely cast the \\'ehail_fee\\' column to the correct type?\"\\n]', '06dd4008f15205b5e36b6ca8f23cae95': '[\\n    \"What should I do if the - vars argument is not recognized as a YAML dictionary?\",\\n    \"How can I ensure the - vars argument is correctly interpreted as a dictionary in dbt?\",\\n    \"Why might the - vars argument be identified as a string instead of a dictionary?\",\\n    \"What common mistake might lead to the - vars argument being the wrong type?\",\\n    \"How can you correctly format the - vars argument in dbt?\" \\n]', 'c49872e18a267f489352bd22d26ad439': '[\"Why is the Environment Type option greyed out and not changeable?\", \"How can I change the environment type when it is inaccessible?\", \"What should I do if I can\\'t change the Environment Type?\", \"What is the reason for the Environment Type being non-selectable?\", \"Why is the Environment Type option locked?\"]', '3fac0fd50cd1cb4805ae561765209bf0': '[\\n  \"What should I do if I receive a permission error when querying the yellow_tripdata table in dbt?\",\\n  \"How can I resolve a database error in model stg_yellow_tripdata related to permission issues?\",\\n  \"Where should I modify the schema.yml file to ensure proper database connection in dbt?\",\\n  \"What steps are necessary to change the branch for my dbt job using Custom Branch settings?\",\\n  \"What might cause a parsing error in a dbt project related to the branch being used?\"\\n]', 'e282b01e152500e83026969d5eee51ab': '[\\n  \"How can I ensure my dbt job runs with the latest modeling file changes?\",\\n  \"What should I do after committing changes to my development branch for dbt?\",\\n  \"Why is my dbt job running on an old file after making changes and committing?\",\\n  \"What steps are necessary for merging changes from a dbt development branch?\",\\n  \"How do I approve merging changes on GitHub for a dbt project?\"\\n]', '54c1e0cc66a54e4dad1fa57279dee056': '[\\n    \"What must I do if nothing appears in my Develop tab after setting up GitHub and BigQuery?\",\\n    \"How do I make data models appear in the Develop tab of dbt?\",\\n    \"Why might my Develop tab be empty after successful setup?\",\\n    \"What steps are necessary after setting up for the Develop tab to show content?\",\\n    \"Why is creating a development environment crucial in dbt?\"\\n]', 'cb85c129af7193e9bf3758e6b8834f0a': '[\\n  \"What should I do if the Prefect Agent encounters a LocalProtocolError while retrieving runs from the queue?\",\\n  \"How can I resolve an error in dbt when I see \\'Invalid input ConnectionInputs.SEND_HEADERS in state ConnectionState.CLOSED\\'?\",\\n  \"What steps do I take if the Prefect Agent fails with a ProtocolError?\",\\n  \"What is a suggested solution when facing LocalProtocolError in Prefect Agent during analytics engineering tasks?\",\\n  \"Why might the Prefect Agent fail with ProtocolError during run retrieval and how can it be solved?\"\\n]', 'cb6a876346f80f5626a79ac306a93f9d': '[\\n    \"What might cause an error when running \\'dbt run\\' with BigQuery after loading taxi data into GCS?\",\\n    \"Why does BigQuery show a type mismatch error for the \\'passenger_count\\' column during a dbt run?\",\\n    \"How can I resolve a data type mismatch error in BigQuery when using dbt with Parquet files?\",\\n    \"Which columns should I apply data type transformations to in order to fix the dbt run error?\",\\n    \"Where can I find more information about solving Parquet column type mismatch issues when using dbt?\"\\n]', '258ca6922e3cb23db89885dd68cd2034': '[\\n  \"What should I do if nothing happens after running the dbt command for stg_green_tripdata?\",\\n  \"Can you suggest an alternative to the specified dbt run command in the tutorial?\",\\n  \"What is the correct syntax for executing dbt run with the stg_green_tripdata model?\",\\n  \"How do I properly use the dbt run command with the is_test_run variable?\",\\n  \"Why doesn\\'t the dbt command using --models and --var work as expected?\"\\n]', 'c837ec46a05dcca407039cca40248b92': '[\\n    \"How can I resolve the \\'No module named pytz\\' error when setting up dbt with Docker?\",\\n    \"What additional line should I add to the Dockerfile to fix the pytz module error?\",\\n    \"During dbt setup with Docker, what command can install the \\'pytz\\' module?\",\\n    \"What change needs to be made in the Dockerfile to address the ModuleNotFoundError for pytz?\",\\n    \"What do I do if I encounter a module not found error for pytz in dbt with BigQuery on Docker?\"\\n]', '3a107db446cbe2be15a991e7bd544025': '[\\n  \"What should I do if I encounter the NoPermissions error in VS Code related to EACCES on Linux while using Docker for dbt configuration?\",\\n  \"How do I resolve a permission denied issue when editing dbt_project.yml in a Docker setup?\",\\n  \"What is the solution to the Internal Error indicating that the DBT profile should not be None after loading?\",\\n  \"When facing editing issues with dbt_project.yml on Linux, which command can help adjust file ownership?\",\\n  \"What step is needed if a dbt debug command requires a directory change due to a newly created subdirectory?\"\\n]', '27b9cdc0054421c76c7afe7650e87c4b': '[\\n  \"What should I do if I encounter a \\'this table is not on the specified location\\' error in BigQuery?\",\\n  \"How can I ensure that all my datasets and tables are in the same location on BigQuery?\",\\n  \"Is there a way to change the query settings to match my location in BigQuery?\",\\n  \"How can I verify that all the paths used in my BigQuery queries are correct?\",\\n  \"What details can I check if I face location issues with a table in BigQuery?\"  \\n]', '5e501c52e258fa527cc2dbdaa31e5de1': '[\\n  \"How can I resolve a cancelled dbt Cloud run due to a missing valid dbt project?\",\\n  \"What should I do if my dbt project directory was moved and the run was cancelled?\",\\n  \"Why might my dbt Cloud project not be found, causing a run cancellation?\",\\n  \"What steps should I take if my dbt project\\'s path doesn\\'t match in the file explorer and settings?\",\\n  \"How do I properly set up the PROD environment for a custom branch in dbt Cloud?\"\\n]', 'ed2f4845500df713b2fcb7240f2fed4e': '[\\n    \"What causes an error when creating a pull request and running CI on BigQuery with dbt in terms of location settings?\",\\n    \"How can the default schema location for CI be changed to \\'EU\\' in a dbt project connected to BigQuery?\",\\n    \"What is the default location setting when dbt creates a new schema on BigQuery during a pull request?\",\\n    \"Where do I modify the location setting to \\'EU\\' in dbt for a connection to BigQuery to prevent errors?\",\\n    \"What should be done in the dbt connection settings to avoid location mismatches with existing BigQuery datasets?\"\\n]', '8f42ae073c1622984691f9e1d0fc4295': '[\\n  \"What is the first step to take when you encounter an error while running a dbt project on production?\",\\n  \"Why is it important to ensure you have the latest version before running a dbt project on production?\",\\n  \"What should you verify about the dbt_project.yml file if an error occurs during production deployment?\",\\n  \"How do you resolve an issue if a valid dbt project is not found on dbt Cloud during deployment?\",\\n  \"Why must the dataset name on BigQuery match the name in the production environment on dbt Cloud?\"\\n]', '20f5e60a9e9dbc583ceb803ffc3f65f8': '[\\n    \"What error might I encounter if I try to build a dbt model without specifying the location?\",\\n    \"Why do I get a \\'Dataset not found in location EU\\' error after building a dbt model?\",\\n    \"How should I resolve a \\'404 Not found\\' error when working with dbt and Bigquery?\",\\n    \"What default location does dbt use for Bigquery when generating new schemas?\",\\n    \"Which steps should I follow to specify the location as \\'EU\\' in Bigquery connection settings?\"\\n]', '3110bd8bd12ecb22a6cfee6cf094d31f': '[\\n  \"What should I append to the URL_TEMPLATE when encountering issues with loading FHV_20?? data into BQ?\",\\n  \"How should I update the URL_PREFIX to resolve problems when loading the FHV_20?? data?\",\\n  \"What keyword must the URL_PREFIX include to ensure FHV_20?? data is loaded correctly?\",\\n  \"What should be avoided in the URL_PREFIX to correctly load FHV_20?? data from GitHub?\",\\n  \"Is it necessary to change anything in the curl command when adjusting the URL_PREFIX for FHV_20?? data?\"\\n]', 'a74fb416e1b26797eda99742659cf25e': '[\\n\"How can I efficiently upload datasets from GitHub for the NYC TLC Data homework in Module 4?\",\\n\"What is the recommended script for ingesting datasets into GCS for the analytics engineering homework?\",\\n\"Who contributed the script for uploading datasets to GCS for the NYC TLC Data homework?\",\\n\"Which module provides a similar script to git_csv_to_gcs.py for data ingestion?\",\\n\"Where can I find a script similar to git_csv_to_gcs.py provided in Module 4?\"\\n]', 'b965ed9defe63caaf66013e3b5deeed4': '[\"What is the best way to store credentials securely for my project?\", \"How do I set environment variables for GOOGLE_APPLICATION_CREDENTIALS and GCP_GCS_BUCKET?\", \"What is the easiest method to inject environment variables into my project?\", \"Which package should I install to manage my environment variables in Python?\", \"How can I access environment variables like GCP_GCS_BUCKET and GOOGLE_APPLICATION_CREDENTIALS in my code?\"]', '081e0f956a3c95d2cd13c70cf7df36a7': '[\\n\"What should I do if I encounter errors with date types after uploading the FHV 2019 CSV files manually?\",\\n\"How can I define the \\'pickup_datetime\\' and \\'dropoff_datetime\\' fields when creating an external table in BigQuery for FHV data?\",\\n\"Which SQL statement can be used to ensure that date fields are first parsed as strings before converting to timestamps in dbt?\",\\n\"What should the \\'format\\' option be set to when defining an external table with CSV files in BigQuery?\",\\n\"What is the purpose of using the TIMESTAMP(CAST(() function in the dbt model for FHV data?\"\\n]', '96ab602ab730326f4e5ade751c685fdc': '[\"What should I do if I encounter parsing errors with FHV data types after manual uploads of parquet files?\", \"How can I correct schema definition errors when loading FHV data into a landing table?\", \"What is a strategy to avoid issues with NULL values in location IDs when ingesting FHV data?\", \"How can I load all monthly FHV parquet files at once without a loop?\", \"What schema should I use to avoid datatype errors when ingesting FHV data?\"]', 'fcc4b084c2720a56e63fc7e44accfea3': '[\"How can I avoid subscribing to the Pro version of Looker Studio after the trial?\", \"Where can I access Looker Studio without being prompted to upgrade?\", \"What URL should I use to get to the free version of Looker Studio?\", \"What should I do if I encounter subscription prompts in Google Cloud Project for Looker Studio?\", \"How do I access the free Looker Studio after the 30-day trial ends?\"]', '5ce328e89269d0ded5ca7fbcd5d5e5f0': '[\\n    \"How can I manage model dependencies effectively in dbt?\",\\n    \"What should I do if loading data with Mage is problematic?\",\\n    \"How can I load and transform FHV data effectively for GCP?\",\\n    \"What steps are necessary to resolve a region mismatch between dbt and BigQuery?\",\\n    \"How do I use the \\'ref\\' keyword in SQL to manage model dependencies in dbt?\"\\n]', 'f9ecb6dac7f8a60e9d54d3ce54e98698': '[\\n  \"What feature should I use for efficient csv data upload in dbt?\",\\n  \"How can I quickly import taxi data to postgres using dbt?\",\\n  \"What is an effective method to upload csv data to dbt-postgres?\",\\n  \"Which command is recommended for importing csv files into postgres in dbt?\",\\n  \"What is the best way to load csv files into a dbt-postgres table?\"\\n]', '763e9f356cdaf083a0e9276b43ae29bf': '[\\n  \"What should I do if I encounter an error message regarding invalid integer type when configuring profiles.yml for dbt-postgres?\",\\n  \"How can I resolve the \\'invalid: 5432\\' error in the dbt-postgres configuration using jinja templates?\",\\n  \"Why does dbt-postgres throw a \\'Credentials in profile, invalid: 5432\\' message during configuration?\",\\n  \"What causes the \\'Credentials in profile target dev, invalid: 5432\\' issue in dbt-postgres?\",\\n  \"How do environment variables affect dbt-postgres configuration in profiles.yml leading to type errors?\"\\n]', 'a59dfa025c6c7266cc363f6bf5d7f268': '[\\n    \"How can I install SDKMAN on a Linux system?\",\\n    \"What command do I use to set up Java 11 using SDKMAN?\",\\n    \"Which version of Spark should I install with SDKMAN on Linux?\",\\n    \"How do I verify the installed version of Java using SDKMAN?\",\\n    \"What is the method to check the Spark version after installation?\"\\n]', '99524554b94b97bcc83feda49bb64162': '[\\n  \"How can I use Spark if I am facing issues setting it up locally on my laptop or VM?\",\\n  \"Is there a guide available to help me set up Spark in Google Colab?\",\\n  \"What is recommended when encountering difficulty with local Spark setup?\",\\n  \"Where can I find a starter notebook for using Spark in Google Colab?\",\\n  \"Why is it advisable to set up Spark locally instead of directly using Google Colab?\"\\n]', '8f0544f83301e18cd1ccaaab11756c5f': '[\\n  \"What should I do if I encounter an IllegalAccessError due to unsupported Java versions when running spark-shell on Windows?\",\\n  \"Why might I see an error related to StorageUtils and DirectBuffer when using spark-shell on Windows?\",\\n  \"Which Java versions are compatible with Spark 3.x if I\\'m seeing errors during setup on Windows?\",\\n  \"How can I resolve the error that mentions java.base module not exporting sun.nio.ch to unnamed module when using spark-shell?\",\\n  \"Where should I install Java from if Java 17 or 19 is causing issues with Spark on Windows?\"\\n]', '3cb00ba3380932fdb5674799e483345f': '[\"What should I do if I encounter a \\'Python was not found\\' error while using PySpark?\", \"How can I resolve the issue of the PYSPARK_PYTHON environment variable not being set correctly on Windows?\", \"What steps should be taken if the crazy_stuff_udf function throws an error during execution?\", \"Is there a way to set up PySpark correctly when using conda on Windows?\", \"What package can help in setting up the Python path correctly for PySpark?\"]', '8e5e165f434bd27c129793409c4883ff': '[\\n    \"What should I do if I encounter a TypeError related to code() argument while running import pyspark in Windows?\",\\n    \"Why am I getting a TypeError involving code() when using PySpark 3.0.3 with Python 3.11?\",\\n    \"How can I resolve the TypeError: code() argument must be str, not int when using older Spark versions?\",\\n    \"Is there a specific Python version I should use to avoid TypeError issues with PySpark 3.0.3?\",\\n    \"Can upgrading PySpark resolve the TypeError with code() argument in Windows with Python 3.11?\"\\n]', 'b137acf26aeadaf2a4bcc9a8153f439a': '[\\n  \"What are the steps to set up a PySpark environment with OpenJDK 11 on macOS?\",\\n  \"How should I handle a Py4JJavaError related to connection issues?\",\\n  \"Which Python versions are compatible with PySpark 3.5.0?\",\\n  \"How do I configure environment variables for Java, Spark, and Hadoop on Windows?\",\\n  \"What should I do if I encounter errors while writing a DataFrame to a local file on Windows?\"\\n]', 'a19590a5a1c7fd33a7fc784c61b09875': '[\"What should I do if I encounter a RuntimeError indicating the \\'Java gateway process exited before sending its port number\\' when using PySpark?\",\\n\"How can I fix the error related to the Java gateway process exit after setting up SparkSession in Jupyter Notebook?\",\\n\"What is the purpose of installing \\'findspark\\' to resolve the Java gateway issue in PySpark?\",\\n\"What steps should I take if PySpark is not pointing to the correct location after installation?\",\\n\"How can I set environment variables permanently to resolve path-related errors with PySpark on a Windows PC?\"]', '1afc8e18f9f068c525de42599f0d814a': '[\\n    \"How can I resolve a Module Not Found Error for pyspark in Jupyter Notebook?\",\\n    \"What command should I use if !pip install doesn\\'t solve the pyspark issue?\",\\n    \"What additional packages might I need to install to get pyspark working in Jupyter?\",\\n    \"How can I filter data based on multiple columns using pyspark?\",\\n    \"What is the function used to filter rows in a DataFrame based on specific conditions?\"\\n]', 'e49f125d1310abe1a8b1d8455b6891a7': '[\\n  \"How can I resolve the Py4JJavaError related to ModuleNotFoundError for py4j while executing import pyspark?\",\\n  \"What steps should I take to identify and update the py4j version to fix the ModuleNotFoundError in pyspark?\",\\n  \"If encountering a ModuleNotFoundError for py4j, what command can confirm and set the correct py4j file in the export path?\",\\n  \"Besides updating the export command with the correct py4j version, what additional step might resolve the ModuleNotFoundError?\",\\n  \"What might be the necessary action if adding the correct version of py4j doesn’t fix the ModuleNotFoundError in pyspark?\"\\n]', 'ac127fff86fe89f5bc25f8285899126a': '[\\n  \"How can I resolve the Py4J Error that says \\'ModuleNotFoundError: No module named py4j\\'?\",\\n  \"What steps should I follow to install the latest version of py4j using conda?\",\\n  \"Which environment variable should I modify to include the Python path for Spark?\",\\n  \"Where do I need to add the Py4J configuration in order to fix the module error?\",\\n  \"How do I ensure I am using the most up-to-date version of py4j to avoid errors?\"\\n]', '7cc3ea352b4c376c1b1339c235ae2b5d': '[\\n    \"What should I do if Jupyter Notebook command is not found even though it\\'s installed?\",\\n    \"Why might Jupyter Notebook not work even after exporting the paths correctly?\",\\n    \"What are the steps to create a Python virtual environment for Jupyter Notebook?\",\\n    \"How can I update and upgrade packages before installing Jupyter Notebook?\",\\n    \"What commands are required to install Python, pip, and virtualenv for setting up Jupyter Notebook?\"\\n]', '6aa300af5d0e6cfac152847861edd4b8': '[\\n    \"What causes the java.io.FileNotFoundException error when using mode=\\'overwrite\\' in PySpark?\",\\n    \"Why does df.write.parquet in PySpark lead to a FileNotFoundException error?\",\\n    \"How can I resolve the FileNotFoundException when it occurs with the overwrite mode in Spark?\",\\n    \"Why does Spark report a nonexistent file error at df.write when using overwrite mode?\",\\n    \"What solution should I implement to avoid a FileNotFoundException with parquet files in Spark?\"\\n]', 'c4a2e1885760193839c02d8a5202733a': '[\\n    \"Why do I encounter a FileNotFoundException error regarding Hadoop\\'s bin directory when using Windows?\",\\n    \"What is the solution for the missing Hadoop bin directory error on Windows?\",\\n    \"How can I fix the issue with the Hadoop bin directory not existing during installation on Windows?\",\\n    \"What step is necessary for resolving the Hadoop FileNotFoundException on Windows?\",\\n    \"Why do I need to manually create a /bin directory for Hadoop on Windows?\"\\n]', 'd014331085eae2d80c1140b93c7e91ce': '[\\n  \"What is the independent type of SQL used by Spark in Module 5 of the course?\",\\n  \"Can you explain how Spark SQL compares to other SQL providers like Postgres and MySQL?\",\\n  \"Where can I find the list of built-in functions for Spark SQL?\",\\n  \"What are some common SQL clauses supported by Spark SQL as mentioned in Module 5?\",\\n  \"Where can I find additional information about Spark SQL, including its integration with other deployments?\"\\n]', 'a5f8891568cba573c020ec37fd15dbba': '[\\n  \"Why is the Spark UI not appearing on localhost:4040 for my current run in Module 5?\",\\n  \"What could be the reason for Spark using a different port than localhost:4040 in the pyspark module?\",\\n  \"How can I identify which port my Spark notebook is currently using?\",\\n  \"What steps should I follow if the Spark web UI does not load on the expected port?\",\\n  \"How do I ensure I\\'m viewing the correct Spark run on its respective port?\"\\n]', 'bb45bbfc5afb32200bb09242c1ce21f6': '[\"What should I do to fix the NoSuchMethodError during repartition call in PySpark with Conda installation?\", \"Which Java Developer Kit version is recommended to resolve the NoSuchMethodError in PySpark?\", \"How can I resolve the Java gateway process exiting error before sending port number in PySpark?\", \"What might be indicated by the notebook log when experiencing a Java gateway process exit error in PySpark?\", \"Where can I find resources to fix the Java gateway process exited error in PySpark?\"]', '603340328200d8f6beacde6c42ee5ab2': '[\\n    \"What could be causing Spark to fail when reading from BigQuery and using show on SELECT queries?\",\\n    \"Which version of the gcs-connector Hadoop jar is needed to fix issues with Spark reading from BigQuery?\",\\n    \"What are the necessary files for authenticating with Google Cloud Storage while using Spark?\",\\n    \"How should the SparkSession be configured to read from BigQuery in the context of your course?\",\\n    \"What configurations are needed to enable offHeap memory in Spark when working with BigQuery data?\"\\n]', '3746748ccd5b32d49bb2d279f4a7cda6': '[\\n  \"How can I automatically configure the Spark BigQuery connector when creating a SparkSession?\",\\n  \"What configuration should I use in the SparkSession to ensure necessary jars are downloaded for BigQuery?\",\\n  \"Is there a way to avoid managing BigQuery connector dependencies manually in PySpark?\",\\n  \"What is the correct package name for the Spark BigQuery connector for dependency management?\",\\n  \"Can you explain how to set up the Spark BigQuery connector in a PySpark session?\"\\n]', 'ef69af8d2b3bebcd836a7f5ba146b2c3': '[\\n  \"What are the necessary steps to connect PySpark with GCS using Cloud Storage connector?\",\\n  \"Which directory should the Cloud Storage connector .jar file be placed in when using Spark on MacOS?\",\\n  \"What are the required imports for setting up a Spark session to read from GCS?\",\\n  \"How do you configure Spark to use the GCS connector before building the SparkSession?\",\\n  \"Once the SparkSession is correctly set up, how can files be read directly from GCS?\"\\n]', '3cfa199e26be92db5fe0e82278678ae0': '[\\n  \"How can I load a limited number of rows using PyArrow from a parquet file in Module 5?\",\\n  \"What are the steps to convert a small Parquet table to a pandas dataframe in the PySpark module?\",\\n  \"Is there an alternative method without using PyArrow to read a few rows from a parquet file in this course?\",\\n  \"What PySpark code allows me to read and limit rows from a parquet file and convert it to a pandas dataframe?\",\\n  \"How can I sort and limit the rows retrieved from a parquet file using PySpark?\"\\n]', '0e232e0a013a67c9a634bb8a91abf459': '[\\n  \"What can cause a DataType error when creating a Spark DataFrame with a specified schema?\",\\n  \"Why do I get an error about Parquet column not being convertible in file?\",\\n  \"How should I modify the schema when using the Parquet file from the TLC website?\",\\n  \"What type should PULocation and DOLocationID be defined as in the schema when using the Parquet file?\",\\n  \"In which module video is the use of Parquet file with schema demonstrated?\"\\n]', 'ee03fe6273414518d98ba680a60c541f': '[\\n  \"How can I eliminate white spaces from column headers in a Pyspark DataFrame?\",\\n  \"What method helps to replace spaces in DataFrame column names within Pyspark?\",\\n  \"Which Pyspark operation allows for column name cleanup by removing spaces?\",\\n  \"What is a way to modify Pyspark column names to remove spaces?\",\\n  \"How do I transform Pyspark DataFrame column titles to exclude white spaces?\"\\n]', 'aab10b7d86d0bddbdeddfa92e1063349': '[\\n  \"What causes an AttributeError for \\'DataFrame\\' object in Spark video 5.3.1?\",\\n  \"Why does running spark.createDataFrame(df1_pandas).show() result in an error?\",\\n  \"How can I resolve compatibility issues between pandas 2.0.0 and Spark 3.3.2?\",\\n  \"What is a workaround if I don\\'t want to downgrade pandas from version 2.0.0?\",\\n  \"Is there a Spark version that fixes the incompatibility with pandas 2.0.0?\"\\n]', '256b5291e16db15e8bae86d25f5df694': '[\\n  \"What alternative solution is there if I encounter an AttributeError related to the \\'DataFrame\\' object in Pyspark?\",\\n  \"How can I resolve the issue where the \\'DataFrame\\' object has no attribute \\'iteritems\\' in pyspark?\",\\n  \"Which pandas version works well with Pyspark 3.5.1?\",\\n  \"What should I do with my environment variable to fix an issue with Pyspark DataFrame?\",\\n  \"How can I edit my environment variables to be compatible with Pyspark 3.5.1?\"\\n]', 'a3a8e3e93ee8783eaa5c01e2ee8038ba': '[\\n  \"How do I start a Spark master node on Windows in standalone mode?\",\\n  \"What command should I use to initiate a Spark worker node?\",\\n  \"Where can I find the correct homework file for Module 5?\",\\n  \"How can I access the Spark UI once the nodes are running?\",\\n  \"What do I need to do if I want to run a Spark worker on a different machine?\"\\n]', 'cf9d9351dab5911ebc9151af387d3238': '[\\n  \"How can I make the PYTHONPATH export command in Linux permanent?\",\\n  \"What should I do to avoid typing the export command every session?\",\\n  \"Where can I add the export command to make it persistent?\",\\n  \"Is there a way to initialize PYTHONPATH automatically when starting a new session?\",\\n  \"How do I use findspark to set up PYTHONPATH at the start of a session?\"\\n]', '1be833a1c124f03d12bd3ed9461bac6a': '[\\n    \"What should I do if a compressed file ends before the end-of-stream marker is reached in PySpark?\",\\n    \"How can I resolve the issue of a compressed file ending prematurely in Module 5?\",\\n    \"What is the solution for when a compressed file in PySpark doesn\\'t reach the end-of-stream marker?\",\\n    \"How do I fix a compressed file error in the PySpark module that ends unexpectedly?\",\\n    \"What steps should I take to address the problem of a compressed file ending before reaching the end of stream?\"\\n]', 'f4e0f5879ea0de15e6bd1bdc10074ec1': '[\\n  \"What should I do if zcat outputs gibberish when handling CSV files from the course?\",\\n  \"How can I prevent compression errors when downloading CSV files for PySpark module exercises?\",\\n  \"What modifications are necessary for the bash script to avoid double compression of files?\",\\n  \"Why do CSV files downloaded from the course repo produce gibberish output when using zcat?\",\\n  \"How can I ensure compatibility with the showSchema command when working with downloaded CSV files?\"\\n]', '035917ef6e885d7ee8eaf6da5d8c676b': '[\"What steps should I follow if I encounter a PicklingError when using spark.createDataFrame?\", \"What Python version should be used with Spark to avoid a PicklingError as of March 2023?\", \"How can I create a new conda environment with Python 3.10 to address serialization issues in Spark?\", \"Why does a PicklingError occur when using spark.createDataFrame with Python 3.11?\", \"How do I switch between Python environments using conda to resolve Spark compatibility issues?\"]', 'c3fa79ba562f0fd5b0fc29020aa45848': '[\\n  \"What should I check if Spark on my local machine cannot find my Google credentials?\",\\n  \"Where should my Google Cloud credentials be located for Spark to find them?\",\\n  \"If Spark fails to access GCS, what might be the issue with my Google credentials?\",\\n  \"How can I ensure that my VM recognizes my GCP credentials when using Spark?\",\\n  \"Why does Spark not recognize my Google credentials as instructed in the video?\"\\n]', '80f7ce009b2369b4d56932d6fae4b176': '[  \\n   \"How do I set up Spark using docker-compose with Bitnami\\'s repository?\",  \\n   \"What changes are necessary in the Dockerfile to update Spark and Java versions?\",  \\n   \"How do I build the Spark Docker image from the Bitnami repository?\",   \\n   \"What services and configurations are included in the provided docker-compose.yml file?\",  \\n   \"How can I access the Jupyter notebook once docker-compose is running?\"  \\n]', 'b999c095b22d7d507de677ff90744029': '[\\n  \"What library is required to read data stored in GCS using pandas?\",\\n  \"How do you access files in GCS for pandas data processing on a local machine?\",\\n  \"What is the initial step to work with GCS data in pandas?\",\\n  \"Which Python package must be installed for handling GCS reads with pandas?\",\\n  \"How can a pandas DataFrame be created from a GCS file?\"\\n]', '06c6ca3b4be4246efe928c469ab43bad': '[\\n    \"What causes the TypeError when using spark.createDataFrame on a pandas DataFrame?\",\\n    \"Why cannot the column Affiliated_base_number be set to DoubleType when converting to PySpark DataFrame?\",\\n    \"How can I avoid the TypeError when converting a pandas DataFrame to a PySpark DataFrame with mixed data types?\",\\n    \"What should I do if the Affiliated_base_number column contains null values when converting to a PySpark DataFrame?\",\\n    \"How can I ensure proper data type inference in PySpark when reading a CSV file?\"\\n]', '72ad729b22d445dbb927ec884b50b898': '[\\n    \"What causes the MemoryManager error related to heap memory in Module 5?\",\\n    \"How much is the default memory allocated to each executor in the PySpark module?\",\\n    \"When does the heap memory error typically occur during the course?\",\\n    \"What must be done for the updated executor memory configuration to take effect?\",\\n    \"How can I resolve the heap memory allocation error when working with Spark?\"\\n]', 'd81a49dfa4ba62959242f4240f77d7f9': '[\"How can I run a spark standalone cluster on Windows OS?\", \"What is the command to start a Spark Master in a standalone cluster on Windows?\", \"How do I change the directory to the Spark directory if I have set up the SPARK_HOME variable?\", \"What command is used to start up a cluster with Spark on Windows?\", \"Where should I navigate if I haven\\'t set up my SPARK_HOME variable to run a spark standalone cluster?\"]', 'c20435830845e10f30e49d717687cd5c': '[\\n    \"What should I do if environment variables set in ~/.bashrc are not recognized by Jupyter in VS Code?\",\\n    \"Why might import pyspark work in iPython in the terminal but not in a Jupyter notebook in VS Code?\",\\n    \"How can I activate the new lines added to ~/.bashrc for environment variables?\",\\n    \"Is there an alternative to configuring paths in ~/.bashrc for my workspace?\",\\n    \"What are some of the environment variables I might need to set for pyspark?\"\\n]', '519addda0a15aba3f0c6c788360ed46e': '[\\n  \"What command can I use for port forwarding outside of VS Code?\",\\n  \"How do I set up port forwarding via SSH on a GCP VM?\",\\n  \"Can you explain the method to forward ports without Visual Studio?\",\\n  \"What steps are involved in SSH port forwarding for a GCP virtual machine?\",\\n  \"How do I use SSH for port forwarding on a cloud VM without VS Code?\"\\n]', '3d8c043971825a76ef02c7226a38f90a': '[\\n    \"What should I do if wc -l shows an unexpected result with a gzip file?\",\\n    \"How can I accurately count lines in a compressed file with wc -l?\",\\n    \"Why does wc -l return different results for a gzip file compared to the video?\",\\n    \"How can I match the wc -l result from the video using fhvhv_tripdata_2021-01.csv?\",\\n    \"What steps should I follow if wc -l output is incorrect on a gzipped file?\"\\n]', '13a2c593633508013c6aa4bd5c2acede': '[\\n    \"What should I do if I encounter a loopback address error when using spark-submit?\",\\n    \"How can I resolve the issue of an unrecognized --master option in Spark version 3.4.2?\",\\n    \"What is causing the error when my Spark application warns about resolving the hostname to a loopback address?\",\\n    \"How should I correct the master URL in the spark-submit command to avoid errors?\",\\n    \"What changes are necessary in the spark-submit command for Spark version 3.4.2 to handle an unrecognized option error?\"\\n]', '695c6359e5f9f242896d00a9abd27e16': '[\\n  \"How can I resolve the UnsatisfiedLinkError issue when writing to Parquet in Windows?\",\\n  \"What should be the value of the HADOOP_HOME variable to fix the path issue?\",\\n  \"Where can I find more tips on resolving Hadoop UnsatisfiedLinkError problems?\",\\n  \"What configuration changes are necessary for Windows when encountering a native IO error with Hadoop?\",\\n  \"How does setting the PATH variable help in fixing Hadoop errors on Windows?\"\\n]', 'f0d13fb4e53f55eb357b8955e071a0b7': '[\"What should I do if I encounter the error \\'Java.io.IOException. Cannot run program C:\\\\\\\\hadoop\\\\\\\\bin\\\\\\\\winutils.exe\\' while using pyspark?\", \"How can I resolve the CreateProcess error=216 issue in my pyspark setup?\", \"What steps should I take to fix the incompatibility error with winutils.exe on Windows?\", \"Is there a specific hadoop version I need to use to fix the winutils.exe error in my pyspark environment?\", \"Where can I find additional solutions if changing the hadoop version doesn\\'t fix the winutils.exe compatibility error?\"]', '06ab85e4257d34390ded3fd2bc80642d': '{\\n  \"questions\": [\\n    \"What error message might occur if the project property is not set when submitting a pyspark job in Dataproc?\",\\n    \"How can I resolve the issue when gcloud shows an error about the project property not being set?\",\\n    \"What command can be used to submit a pyspark job with the correct project flag in gcloud?\",\\n    \"Where can I find the project ID necessary for setting the project property in gcloud commands?\",\\n    \"Which flag should be used to specify the project ID when using gcloud to submit a Dataproc job?\"\\n  ]\\n}', '676f6d5ab7b75bfa7864f72489efaf3b': '[\"How do I start a local Spark cluster on Windows 10 using CMD?\", \"What command do I use to run the Spark master in CMD?\", \"How do I connect a worker to a Spark master using CMD?\", \"How do I set up a SparkSession in a Jupyter notebook for my cluster?\", \"Where can I check the status of my Spark master, worker, and app?\"]', 'a58ff4a93ac26c573c7b78e7ca974746': '[\"How can I resolve a 401 error indicating that an anonymous caller does not have access to list objects in a Google Cloud Storage bucket?\", \"What steps should I take if I encounter a permission denied message while trying to list objects in a GCS bucket?\", \"Which command should I use to authenticate and avoid issues with access permissions on Google Cloud Storage?\", \"How do I properly set the project ID in Google Cloud to manage access to GCS?\", \"What is the process for uploading a directory to a GCS bucket after authentication and project configuration?\"]', 'de3e16ac06292bc6aa63d3150ff2e32e': '[\\n  \"What changes can I make if I encounter a Py4JJavaError in the log panel within Dataproc?\",\\n  \"Which operating system version should I select for the Versioning Control feature to address Java errors when submitting a job?\",\\n  \"How can switching to Ubuntu 20.02-Hadoop3.3-Spark3.3 help resolve issues in Google Cloud Platform Dataproc?\",\\n  \"Why did you choose Ubuntu 20.02 over Debian-Hadoop-Spark for your cluster, and what problem did it solve?\",\\n  \"Is there any documentation available for troubleshooting Java-related errors when using Dataproc with specific operating system versions?\"\\n]', '2bf0b694bd7ffff3d5872171929e62d1': '[\\n    \"What should I use if df.repartition(6) results in 8 partitions?\",\\n    \"After repartitioning a DataFrame to 6 partitions, what should I do to ensure exactly 6 partitions?\",\\n    \"How can I ensure a DataFrame has exactly 6 partitions when using repartition?\",\\n    \"What technique should be used along with repartition to achieve a specific number of partitions?\",\\n    \"How do I resolve the issue of getting more partitions than expected with df.repartition?\"\\n]', 'fc79af0540172bab1d5c6f3551084578': '[\\n  \"What should I try if Jupyter Notebook or SparkUI isn\\'t loading properly at localhost after port forwarding from VS Code?\",\\n  \"How can I forward the port using ssh cli instead of VS Code for accessing Jupyter Notebook?\",\\n  \"What happens if I logout from the session while using ssh port forwarding?\",\\n  \"What should I notice when creating the Spark session related to the block\\'s log?\",\\n  \"How can I handle trouble accessing localhost ports from a GCP VM more effectively?\"\\n]', '832f7a1e7dba2516aa4f96ba00b8e25e': '[\\n    \"What command can be used to list available Java SDK versions in codespaces?\",\\n    \"What is the command to install Java 11.0.22-amzn on a codespace?\",\\n    \"Should I confirm if prompted to change the default Java version during installation?\",\\n    \"How do I verify the installed Java version on my codespace?\",\\n    \"What should I do if Java is not working correctly after installation?\"    \\n]', '6774f22277396a3e83c94161fc8c148a': '[\\n    \"What might cause an error about insufficient \\'SSD_TOTAL_GB\\' quota when creating a dataproc cluster on GCP?\",\\n    \"How can changing the boot-disk type help solve a quota error when using terraform?\",\\n    \"What is one possible solution if there are not enough resources in the GCP region to fulfill a request for creating a cluster?\",\\n    \"Is there a specific type of error that can occur when there are not enough regional resources for a GCP cluster, and how can it be resolved?\",\\n    \"What adjustment in terraform is suggested to address the \\'SSD_TOTAL_GB\\' quota error?\"\\n]', 'cc9fa4185dec182eb7d9812fea9286d6': '[\\n  \"How can I express the time difference between two timestamps in hours using PySpark?\",\\n  \"What are the parameters needed for the datediff SQL function in PySpark?\",\\n  \"How does PySpark represent the duration difference between two TimestampType values?\",\\n  \"What units of time does Python\\'s datetime.timedelta object use to store durations?\",\\n  \"How can I get the total duration in hours using datediff in PySpark?\"\\n]', '9116035e365bcc72d9cb2ed4bda7f46c': '[\\n    \"What version of PySpark should I use to avoid serialization errors?\",\\n    \"Which Pandas version is recommended to fix the PicklingError?\",\\n    \"How can I solve a serialization issue in PySpark related to tuple indexing?\",\\n    \"What is the suggested PySpark and Pandas version combination for module 5?\",\\n    \"If updating versions doesn\\'t solve the PicklingError, what should I do next?\"\\n]', '18e0e2369c993ca3cce46a9fa1ffe064': '[\\n  \"What should I do if I encounter a Py4JJavaError regarding a Python worker failing to connect back when using Spark?\",\\n  \"How can I resolve a SparkException caused by a Python worker failing to connect back when running PySpark?\",\\n  \"What is the solution for a stage failure in Spark due to a Python worker connection issue?\",\\n  \"How do I fix a Spark job aborted because of a failed task due to Python worker connection problems?\",\\n  \"What steps can be taken to address a task failure in Spark caused by a Python worker failing to connect?\"\\n]', '5ae366189ebef7af23ee05f65bd4ebee': '[\\n    \"How can I resolve the RuntimeError caused by different Python versions in worker and driver when using PySpark?\",\\n    \"What steps should I take to ensure my PySpark environment variables are set correctly to handle version mismatches?\",\\n    \"Where can I find information on Dataproc pricing for on GKE?\",\\n    \"What environment variables need to be set for consistent Python versions in PySpark?\",\\n    \"Which Python executable should be used to avoid version conflicts in PySpark?\"\\n]', 'abd9de5b5e98af387767b0628afce923': '[\\n  \"Is it mandatory to use a VM for submitting Dataproc jobs on Google Cloud Platform?\",\\n  \"Can I submit a Dataproc job from a local machine instead of using a GCP VM?\",\\n  \"How can I execute a Dataproc job from my personal computer?\",\\n  \"What tools are required to submit a Dataproc job without a VM?\",\\n  \"What is the command to submit a pyspark job to Dataproc from a local computer?\"\\n]', 'befb5094bf6f3a77b3431045a07d1dd3': '```json\\n[\\n    \"What error occurs when running spark.createDataFrame(df_pandas).show() in module 5.3.1 and why?\",\\n    \"How can I fix the AttributeError related to \\'DataFrame\\' object having no attribute \\'iteritems\\' in pyspark?\",\\n    \"In DE Zoomcamp 5.6.3, what configuration should I use to fix the \\'Insufficient SSD_TOTAL_GB quota\\' error?\",\\n    \"How is the total memory allocation divided between master and worker nodes in the recommended Dataproc Cluster configuration?\",\\n    \"What is the maximum available quota of SSD_TOTAL_GB when setting up a Dataproc Cluster with the provided error message?\"\\n]\\n```', '3d4e700944839df22eef5139a35e303d': '[\\n  \"What should I do differently when setting JAVA_HOME on an Apple Silicon Mac compared to an Intel-based Mac?\",\\n  \"Where should I set the JAVA_HOME environment variable on an Apple Silicon Mac using Homebrew?\",\\n  \"How can I confirm that JAVA_HOME has been set correctly on my Apple Silicon Mac?\",\\n  \"What is the expected output when I check the java path on an Apple Silicon Mac?\",\\n  \"Where can I find the MacOS setup instructions for setting JAVA_HOME using Homebrew?\"\\n]', 'b555f0b62071b2e95b13c8533cf0e92d': '[\\n    \"What should I verify in my docker-compose.yaml file when the \\'control-center\\' image won\\'t start in Kafka streaming?\",\\n    \"How can I resolve issues with starting the Kafka control center on Mac OSX 12.2.1?\",\\n    \"What configurations should I ensure are correct in docker-compose.yaml for a successful container startup?\",\\n    \"Why might \\'docker ps\\' not show running images that are visible in Docker Desktop?\",\\n    \"What troubleshooting steps helped solve the problem of starting the Kafka environment on Mac OSX 12.2.1?\"\\n]', 'e1f9867f4f46565dce33b85c589478b1': '[\\n    \"How can I resolve the \\'Module “kafka” not found\\' error when running producer.py?\",\\n    \"What are the steps to create and use a virtual environment for running Kafka in this course?\",\\n    \"Is the process for activating the virtual environment different on Windows compared to MacOS and Linux?\",\\n    \"Do I need to have Docker images running before using the virtual environment to run the python file?\",\\n    \"What is the command to deactivate the virtual environment once I am done using it?\"\\n]', '49f2a5695e4021660a2ca19d21fc09ec': '[\"How can I solve the ImportError related to cimpl DLL while running Avro examples?\", \"What should I check if I encounter an ImportError with cimpl regarding module compatibility?\", \"Is there a specific DLL load step necessary for avro in Kafka streaming, and how is it done?\", \"What is the alternative solution in powershell to fix DLL load issues with cimpl in Conda?\", \"Where can I find more information about issues with confluent-kafka-python related to this error?\"]', '41d1a1e9024e2f9b969cc0931e91c0e1': '[\\n  \"How can I resolve the ModuleNotFoundError for \\'avro\\' in Module 6 of the course?\",\\n  \"What should I do if Conda doesn’t include \\'avro\\' when installing confluent-kafka?\",\\n  \"Is there a pip command to install the missing \\'avro\\' module for confluent-kafka?\",\\n  \"Where can I find more information on issues with Anaconda and confluent-kafka?\",\\n  \"What online resources can help troubleshoot \\'avro\\' module errors with confluent-kafka?\"\\n]', '6ae94a0148b4a03e49eb5aa8fb7ff3b3': '[\"What steps should I take if I encounter an error running python3 stream.py worker?\",\\n\"How can I resolve issues with kafka-python during module 6?\",\\n\"What is Redpanda and what algorithm is it built on?\",\\n\"How does Redpanda compare to Kafka in terms of performance and latency?\",\\n\"Why would I choose Redpanda over Kafka for a streaming data platform?\"]', '32b3f865b191a53c8dc1a8003470c007': '[\\n    \"What might cause a Negsignal:SIGKILL error when handling large dta files?\",\\n    \"How do I resolve memory exhaustion issues in Docker when working with large files?\",\\n    \"Why does my Docker container crash when converting large datasets to parquet?\",\\n    \"What strategy can be used to manage large dataset conversions in a resource-limited environment?\",\\n    \"How can I successfully convert an 800MB dta file to multiple parquet files without memory issues?\"\\n]', 'c411665e99458d26806a509c901493de': '[\\n  \"Where can I find the missing rides.csv file for the Python examples in Module 6?\",\\n  \"What should I do if the rides.csv file is not present in the Python resources for streaming with Kafka?\",\\n  \"Is there an alternate location for the rides.csv in the Kafka module if it\\'s missing for Python?\",\\n  \"How can I obtain the rides.csv file for the streaming exercises in the Python section of Module 6?\",\\n  \"If the rides.csv file is unavailable in the Python resources directory, where can I copy it from?\"\\n]', '9832e366499256e280005967d71b9586': '[\"How can I improve the audio quality of the Kafka Python videos since the audio is low?\", \"What should I do if I find the audio in the Kafka Python videos hard to follow?\", \"Where can I find more information about the rides.csv data used in the producer.py programs?\", \"Is there a way to enhance my understanding of the rides.csv file used in the course?\", \"Can you recommend tools or methods to make the audio in the Kafka Python videos more comprehensible?\"]', '8ca42bdb0a4ec0b17a9e73af4536dbc6': '[\"How do I resolve the kafka.errors.NoBrokersAvailable error?\", \"What should I do if my Kafka broker container is not working?\", \"How can I check if my Kafka broker docker container is running?\", \"What is the command to start all the instances for Kafka?\", \"Where should I run the docker compose up -d command?\"]', 'c8b2f0bbadf1355c878887d16ebbf0b1': '[\\n  \"What should we focus on for Kafka Homework Q3 regarding options that enhance scaling?\",\\n  \"How should we think about scaling concerning Kafka in Module 6?\",\\n  \"Which scaling option did Ankush say to emphasize for consumer scaling in Kafka?\",\\n  \"From which end should we consider scaling when working with Kafka streaming?\",\\n  \"What type of scaling approach was recommended for consuming messages in Kafka?\"\\n]', '8239bd33320791fbff1b2223156b2eb3': '[\\n  \"What should I do if I encounter a \\'pull access denied\\' error when using Docker Compose?\",\\n  \"Why might I receive an error stating that the repository for spark-3.3.1 does not exist?\",\\n  \"How can I resolve a denied access error when trying to pull spark images for Kafka streaming?\",\\n  \"What is the solution if I cannot find the spark-3.3.1 images in dockerHub while setting up Kafka streaming?\",\\n  \"Which command should be executed in the spark folder to build necessary images for Docker Compose?\"\\n]', 'd6b9b507f661e0ec320f8d11e338c0ce': '[\\n  \"What should I do if I encounter a \\'Permission denied\\' error with build.sh in Python Kafka?\",\\n  \"How can I resolve a \\'Permission denied\\' issue when running build.sh for Kafka streaming?\",\\n  \"What command do I need to use to fix the permission error with build.sh?\",\\n  \"Which directory should I be in to change permissions for build.sh?\",\\n  \"How can I make sure build.sh can be executed without permission errors in a Kafka setup?\"\\n]', '563333ceaf1725c8d9aaf3c68e8703d0': '[\"Why does KafkaTimeoutError occur with my stream-example/producer.py?\", \"What should I do to fix Kafka metadata update failure?\", \"How can I resolve KafkaTimeoutError in my Python Kafka setup?\", \"What is a possible solution for fixing Kafka metadata issues?\", \"How do I troubleshoot a KafkaTimeoutError in my streaming application?\"]', 'cc0d5667f5f6101d3f199fe7fb5864a3': '[\"What should I do if I encounter an error stating \\'StandaloneSchedulerBackend: Application has been killed\\' when running ./spark-submit.sh streaming.py?\", \"How can I resolve a \\'py4j.protocol.Py4JJavaError\\' related to calling SparkSession methods on a stopped SparkContext?\", \"What could cause a failed connection to the master when running spark, and how should I address it?\", \"How do I check my local Spark version to ensure compatibility?\", \"Why would downgrading PySpark to version 3.3.1 resolve connectivity issues when masters are unresponsive?\"]', 'e2003657cff057b9b11fa88708601532': '[\\n  \"How can I troubleshoot a Spark master connection failure in Module 6?\",\\n  \"What is the process to investigate a connection issue with the Spark master?\",\\n  \"Where do I find error logs for a failed Spark master connection in this module?\",\\n  \"How do I identify the cause of a Spark master connection failure using logs?\",\\n  \"What steps should I follow to diagnose a connection problem with the Spark master?\"\\n]', '3545aa543db98bab5ed0dd317248d690': '[\\n  \"When running streaming.py with spark-submit, how can I resolve the Py4JJavaError?\",\\n  \"What should I verify if I encounter a Py4JJavaError with JavaSparkContext?\",\\n  \"Is there a specific version of Java needed to avoid errors with spark-submit for streaming?\",\\n  \"How can I check if I\\'m running the correct Java version when using spark-submit for Kafka streaming?\",\\n  \"What steps can I follow to set Java 11 as the default for Kafka streaming in Spark?\"\\n]', 'b00d735fd96ccfeae9bf11b81ade28cc': '[\\n  \"How can I resolve package xxx does not exist error in Java Kafka after gradle build?\",\\n  \"What should I do if dependencies aren\\'t installed in my Java Kafka project jar?\",\\n  \"How do I modify build.gradle for a Java Kafka project if gradle build fails?\",\\n  \"What steps are required to create a shadow jar for Java Kafka?\",\\n  \"Why should I use the shadowjar plugin in a Java Kafka project?\"\\n]', '2f3d8121460e42bfb02905897a28263d': '[\\n  \"What are the steps to install dependencies needed for running the avro_example producer script in Python 3?\",\\n  \"Is it possible to use the Faust library in Module 6 with Python, considering there might be dependency issues?\",\\n  \"What should I do if I don\\'t know Java but need to understand the streaming concepts in this module?\",\\n  \"Where can I find Python-focused videos for Module 6 that do not involve Java-based coding?\",\\n  \"What is the recommended way to explore the Python version of streaming examples, given that Faust is no longer maintained?\"\\n]', '3be81666e9ee97d3888e971491790f22': '[\\n  \"How do I execute a Kafka producer in the terminal using Java?\",\\n  \"What is the terminal command to run a Kafka consumer in Java?\",\\n  \"How can I launch Kafka streams from the terminal in Java?\",\\n  \"What steps are needed to execute Java-based Kafka streams in the terminal?\",\\n  \"How do you run a Kafka application in the terminal using Java?\"\\n]', '0775e143c04555f51e577b6e38a0dcd6': '[\\n  \"What should I check if my Java Kafka producer or consumer scripts are not retrieving any results or sending messages?\",\\n  \"How can I resolve a SaslAuthenticationException error when running my Kafka Java scripts?\",\\n  \"What changes need to be made in StreamsConfig for my Java Kafka scripts to connect correctly?\",\\n  \"Where should I update the cluster key and secrets for Kafka Java scripts?\",\\n  \"What example server URL should I verify for my Kafka application in the StreamsConfig?\"\\n]', '113e318ef9a1564f5e5e109147db7b1d': '[\\n    \"What might be the issue if I can\\'t see the test icons in VS Code when working with Java Kafka?\",\\n    \"How can I fix the problem of not seeing the triangle icon next to Java tests in VS Code?\",\\n    \"Where in VS Code can I find the option to clean the workspace for my Java Kafka project?\",\\n    \"What steps should I follow to ensure the triangle icons appear next to my Java tests in VS Code?\",\\n    \"In the VS Code Explorer, what should I do if the triangle icons for Java tests are missing?\"\\n]', 'f165bbc5ea0c6fd9b2c1edd81f1cfb47': '[\\n  \"How can I locate the schema registry URL in Confluent Kafka?\",\\n  \"Where do I find the schema registry URL in Confluent Cloud?\",\\n  \"Which steps lead to finding the schema registry URL for Kafka?\",\\n  \"Where is the schema registry URL located in the Confluent Cloud interface?\",\\n  \"How do I retrieve the schema registry URL in my Confluent environment?\"\\n]', '110ee05c84a8da330c0073e457ad036f': '[\\n    \"What command can I use to find my local Spark version?\",\\n    \"How can I ensure that my local Spark version and the one in the container align?\",\\n    \"What file should I check to verify the Spark version in the container?\",\\n    \"Which Python tool\\'s version should match the container\\'s Spark version?\",\\n    \"Where do I find the Spark version in the Python folder?\"\\n]', '4fd4cee0d329ac68c3c8aa1e11f6d66e': '[\\n  \"How can the error \\'No module named kafka.vendor.six.moves\\' be resolved?\",\\n  \"What is the recommended solution for fixing \\'ModuleNotFoundError\\' related to kafka?\",\\n  \"Where should I report issues related to releases for kafka-python?\",\\n  \"What is the suggested installation command to resolve kafka module errors?\",\\n  \"Which alternative source is recommended for kafka-python errors due to releases?\"\\n]', 'c4e16ffcdfa8407ad9b936b04c248830': '[\\n  \"How will my final project grade be determined?\",\\n  \"What happens if I don\\'t grade my peers\\' projects?\",\\n  \"How many students will review my submitted capstone project?\",\\n  \"What should I follow when evaluating others\\' projects?\",\\n  \"Who is responsible for grading my capstone project?\"\\n]', 'aa0f6dc880990aaefc2c9aa58b829278': '[\\n    \"What is the total number of projects required for this Zoomcamp?\",\\n    \"Is it mandatory to submit two separate projects during the course?\",\\n    \"Can I resubmit my project if I fail the initial attempt?\",\\n    \"Is a second chance offered if I miss the first project submission?\",\\n    \"What should I do if I\\'m unable to submit my project due to a holiday?\"\\n]', 'e7269e67dfbe1ec4a64436fc02f02373': '[\\n  \"Where can I find a collection of large datasets for our project?\",\\n  \"Is there a link to access various datasets for reference?\",\\n  \"Where is the recommended list of datasets mentioned?\",\\n  \"Can you guide me to a resource for datasets that I can use?\",\\n  \"Do you have a URL for a compilation of suitable datasets?\"\\n]', 'e285789a6f8e83a09bbf3d971b9e8d61': '[\\n    \"What must I do to execute a Python startup script?\",\\n    \"How can I set up a Python script to launch at startup?\",\\n    \"What steps should I take to redefine the Python environment variable?\",\\n    \"How do I configure an account-specific startup script in Python?\",\\n    \"What is needed to initiate Python scripts during startup?\"\\n]', '291322bd208f0a494b4a5e10d8e4ce6e': '[\\n  \"How can I set up a Spark Streaming job to read data from multiple topics without blocking the Spark session?\",\\n  \"What is the method to initiate a Spark Session suitable for streaming data from multiple sources?\",\\n  \"In Spark Streaming, how do I ensure that my stream processing continues even if one topic receives a termination signal?\",\\n  \"What is the key difference between using awaitAnyTermination and awaitTermination in Spark Streaming when handling multiple topics?\",\\n  \"How does initiating multiple queries in one Spark session help when reading from different topics in Spark Streaming?\"\\n]', '84f46a2b0875c3b521703113d51b4090': '[\\n  \"How should data be transferred from Databricks to Azure SQL DB?\",\\n  \"What is an alternative to moving data directly from Databricks to Azure SQL DB?\",\\n  \"Can data from Databricks be loaded directly into Azure SQL DB?\",\\n  \"Is there an intermediary step recommended for moving data from Databricks to Azure SQL DB?\",\\n  \"What role does Azure blob storage play in transferring data from Databricks to Azure SQL DB?\"\\n]', 'f8ab07612ed2f947bea125d808d43533': '[\\n  \"How can I integrate dbt with Airflow for orchestration?\",\\n  \"What is required to access the dbt API with a trial account?\",\\n  \"Can I automate a dbt job with Airflow without adding it manually?\",\\n  \"What best practices should I follow to secure sensitive information when using Airflow with dbt?\",\\n  \"Where can I find a source code example for orchestrating dbt with Airflow?\"\\n]', '073de3f77432f2f8e0f7fb0c9a7940e0': '[\\n  \"What roles should be assigned to the service account for orchestrating DataProc with Airflow?\",\\n  \"Which operators are essential when setting up DataProc with Airflow?\",\\n  \"Where can I find the documentation for orchestrating DataProc with Airflow?\",\\n  \"Why must dataproc_jars be specified when using DataprocSubmitPySparkJobOperator?\",\\n  \"What is necessary to include when using DataprocSubmitPySparkJobOperator with BigQuery?\"\\n]', '726d0b05be7abd9ed3bda797dc6d32fb': '[\\n    \"How can I trigger a dbt job within a Mage pipeline?\",\\n    \"What is necessary to safely store when triggering a dbt job using Mage?\",\\n    \"Where can I find the API key necessary for triggering a dbt job?\",\\n    \"What type of block should be created in Mage to trigger a dbt job via HTTP request?\",\\n    \"Which Python library is used for loading environment variables in this process?\"\\n]', 'c82295cf44f53a841db3718d7250d274': '[\\n  \"How is the peer review for project reproducibility conducted if re-running everything might not be feasible?\",\\n  \"What should reviewers do if they don\\'t have time to re-run the entire project during evaluation?\",\\n  \"How important is it to document every single step for project reproducibility?\",\\n  \"Can you provide more clarity on how project reproducibility is judged if the code isn\\'t completely re-run?\",\\n  \"What approach is acceptable for evaluating project reproducibility if not all steps can be verified through execution?\"\\n]', '91218468ec989e5b8e50d4ef82cfde73': '[\\n  \"How can credentials be securely stored in the Azure cloud stack?\",\\n  \"What is a use case for Azure Key Vault with SQL databases?\",\\n  \"How does Azure Key Vault help in managing secrets?\",\\n  \"Why would one use Azure Key Vault instead of exposing credentials?\",\\n  \"Where can passwords be stored to prevent exposure in Azure services?\"\\n]', '43d25ae151481e5321b5634562400801': '[\"What command can I use to find the py4j version in Docker when facing a module not found error?\", \"How do I check the py4j library version in a Docker container?\", \"Which command helps in verifying py4j installation in a Dockerized Spark environment?\", \"If I encounter a py4j module error in Spark, which Docker command should I execute?\", \"What is the Docker command to list Python libraries in Spark for checking py4j?\"]', '3b57283d8324a140421a765c1df5824c': '[\\n  \"How can I resolve the psycopg2 environment incompatibility issue due to architecture differences?\",\\n  \"What is the recommended tool for managing virtual environments to avoid psycopg2 incompatibility?\",\\n  \"How should I install psycopg2 using conda to handle architecture compatibility?\",\\n  \"What is the correct command to install psycopg2 with conda-forge to avoid incompatibility problems?\",\\n  \"Is it possible to use both conda and pip together for managing psycopg2 installations?\"\\n]', '2fc7f3a21cf2a85642b4fd30cdfb5b7b': '[\"How can I set up dbt locally using Docker and Postgres?\", \"What steps should I follow to create a `profiles.yml` for dbt in Docker?\", \"How do I ensure the profile name matches in `profiles.yml` and `dbt_project.yml`?\", \"Where should I add `config-version: 2` in the `dbt_project.yml` file?\", \"What can I do if I encounter issues running dbt with Docker and Postgres?\"]', 'a7b8a576a477f57cfd696e8e7824bcd7': '[\\n  \"What configuration line is necessary for Pyspark to interface with BigQuery?\",\\n  \"Which packages should be included for BigQuery with Pyspark?\",\\n  \"How do I set up a Spark session to work with BigQuery?\",\\n  \"What specific configuration is needed for Pyspark to connect to Google BigQuery?\",\\n  \"How to modify SparkSession to include BigQuery support?\"\\n]', '8227bb1c36d53ac76c6838d997e21f45': '[\\n  \"What package needs to be installed to run a dbt-core project as an Airflow Task Group on Google Cloud Composer?\",\\n  \"Where should the dbt-core project be placed in the Google Cloud Composer environment?\",\\n  \"How should the profiles.yml file be configured for authentication using a service account key?\",\\n  \"Which class should be used to create a new DAG for running a dbt-core project in Cloud Composer?\",\\n  \"What file path should be specified in the ProfileConfig when using a JSON key file for authentication?\"\\n]', '1cb02764cfdce5825411683a4c861447': '[\"How can I change the name that appears on the course leaderboard?\", \"Is the name on my certificate the same as my leaderboard name?\", \"Can I choose not to display my name on the course leaderboard?\", \"Are URLs from the NY Taxi data website supported for creating external tables in BigQuery?\", \"What data stores are supported for creating external tables in BigQuery?\"]', 'd9b792f6da4b7d1c51784ba2fb9b8f4a': '[\\n  \"What command should I use to set up dlt with duckdb for our code?\",\\n  \"Is there a specific package required to execute the code in Workshop 1?\",\\n  \"What should I verify is installed locally before loading the duckdb package?\",\\n  \"Can you guide me on installing dependencies to run Workshop 1 code effectively?\",\\n  \"How do I ensure that I have all the necessary installations for dlthub\\'s Workshop 1?\"\\n]', '458839e84ebff673b6f17ed9a9f03300': '[\\n  \"What should I do if Jupyter Notebook requires additional packages?\",\\n  \"Is there a command to install necessary packages for the starter Jupyter Notebook?\",\\n  \"How can I set up Jupyter Notebook on a new Codespace?\",\\n  \"What procedure should I follow when running Jupyter on a new virtual environment?\",\\n  \"Which package must I install to ensure the starter Jupyter Notebook functions correctly?\"\\n]', '09f9484093eb6cbdf414b0247bd07044': '{\\n    \"questions\": [\\n        \"What alternative is there for using DuckDB with dlt in-memory?\",\\n        \"Are there other storage options for DuckDB besides in-memory?\",\\n        \"What should I do if I don\\'t want to use DuckDB in-memory with dlt?\",\\n        \"Is it possible to change the storage mode when using DuckDB with dlt?\",\\n        \"How do I switch from in-memory to another storage type in DuckDB with dlt?\"\\n    ]\\n}', '88febe0cf787aa6e960fe7dce1a74ff1': '[\\n    \"How many records should be loaded after performing the operation as instructed in Workshop 2 - RisingWave Exercise 3?\",\\n    \"What modifications should be made to calculate the sum of ages from the loaded data to match given choices?\",\\n    \"What should the age of the person with ID 3 be after successfully loading records in Exercise 3?\",\\n    \"Why are my dlt files being saved to the C drive despite changing the file path in the \\'Load to Parquet file\\' section?\",\\n    \"What environment variable should be set to save dlt files in a desired local folder path?\"\\n]', 'ea3d3a960ba159bf98491eded27989f0': '[\\n    \"What should I do if I encounter an error indicating \\'no such file or directory: command.sh\\'?\",\\n    \"How can I confirm if command.sh is in the correct location in the repository?\",\\n    \"Which repository should I clone to ensure command.sh is available for the workshop?\",\\n    \"How do I verify that I have cloned the correct repository when encountering a missing command.sh?\",\\n    \"What steps are recommended if the command.sh file is not found in the root folder?\"\\n]', '2cf36f471360b3a13014a0597fbbdc0d': '[\"What should I use if psql command is not found in my setup?\", \"Why is psql command unavailable when working with PostgreSQL in a container?\", \"What is the recommended tool to run SQL scripts in this course?\", \"How do I install usql on Windows?\", \"What command line tool should I use to run taxi_trips.sql script?\"]', 'fb51819cf3bcf5c13d3a406c96e00432': '[\\n    \"What should I do if I see an error saying \\'docker-compose\\' not found during setup?\",\\n    \"How can I fix the error regarding \\'docker-compose\\' when I\\'m certain it\\'s installed?\",\\n    \"What modification can I make in command.sh if docker compose is used without a hyphen?\",\\n    \"How do I start the cluster if \\'docker-compose\\' is not recognized due to the hyphen?\",\\n    \"Why might I need to edit command.sh to correct a \\'docker-compose\\' error?\"\\n]', '6a5f8d387e22fc62af84cc3333074df7': '[\\n  \"What is causing the invalid top-level property error during the setup of the RisingWave workshop\\'s docker-compose file?\",\\n  \"Why might I see an \\'Invalid top-level property x-image\\' error when starting a cluster in Workshop 2: RisingWave?\",\\n  \"How can I resolve the \\'Invalid top-level property x-image\\' error when using docker-compose?\",\\n  \"What should I check if I get an error about an unsupported docker-compose version on Ubuntu during Workshop 2?\",\\n  \"What are the valid top-level sections I should expect in a Compose file used in Workshop 2: RisingWave?\"\\n]', '00174d4104af17a4165a9a1ed514ceba': '[\"Is it expected for the records to be ingested ten at a time during the stream-kafka process?\", \"Why does the script change the date timestamp to the current time during real-time query work?\", \"How can I ensure that my queries with now() filter work correctly?\", \"What should I do while the stream-kafka script runs in the background?\", \"How can I change the ingestion to 100 records at a time and where can I find the latest updates?\"]', '0845bdd1a52cab95f456cc06f803bc9a': '[\\n  \"Is installing Kafka necessary for the workshop on RisingWave?\",\\n  \"Do I need to set up Kafka for the RisingWave course session?\",\\n  \"Is there a requirement to have Kafka installed to participate in the RisingWave workshop?\",\\n  \"Must Kafka be installed before attending the RisingWave workshop?\",\\n  \"Is a Kafka installation needed for the RisingWave training?\"\\n]', '6b6dbecd61063dfb0cb08890d69d1040': '[\\n  \"How much disk space is required to provision all the containers?\",\\n  \"What is the approximate total disk space needed including the ingestion of taxi data?\",\\n  \"How much free space should be available for the psql to function properly?\",\\n  \"What is the recommended disk space allocation for setting up the containers and running the necessary processes?\",\\n  \"What is the suggested amount of disk space to handle all components including data ingestion?\"\\n]', 'a5d38913b987d5f42c35fbd193e57b5d': '[\\n  \"How should I resolve psycopg2 issues while executing the stream-kafka script?\",\\n  \"Which change do I need to make in the requirements.txt to fix psycopg2 difficulties?\",\\n  \"What replacement should I use for psycopg2==2.9.9 in the requirements?\",\\n  \"What step must I not forget when opening a new terminal for running psql?\",\\n  \"What is the necessary terminal command for each session to ensure psql works properly?\"\\n]', '70d1f8b1d282f556e48fefe11165fdad': '[\"What steps should I follow to resolve the psycopg2 wheel build issue in Anaconda?\", \"Why is installing GCC necessary for the installation of pyproject.toml-based projects in Conda?\", \"How is the GNU Compiler Collection (GCC) used during the software development process?\", \"Why does the psycopg2 installation fail without GCC on a Conda base?\", \"What programming languages are supported by the GNU Compiler Collection (GCC)?\"]', 'ea8c784ab439dbdb3087011c1eee4761': '[\"What steps should I take to resolve the Psycopg2 InternalError when running the seed-kafka command in Windows?\", \"How can I activate the Python virtual environment in Windows using git bash?\", \"What modification is needed in the seed_kafka.py file to run the seed-kafka command successfully?\", \"How can I connect to the RisingWave cluster from Powershell without being asked for a password?\", \"What is the Powershell equivalent of the source commands.sh command from the workshop directory?\"]', '10181da938d84f637e137f0226f4eb68': '[\\n  \"What should I do if the stream-kafka script gets stuck with a Connection Refused error?\",\\n  \"How can I resolve the \\'insufficient physical memory\\' error when running the stream-kafka script?\",\\n  \"What changes should be made in the docker-compose file to fix the memory allocation issue for message_queue?\",\\n  \"How can I fix the syntax error when running psql on trip_data.sql file after starting services?\",\\n  \"What preliminary step is necessary to ensure psql commands run correctly in different terminal windows?\"\\n]', 'f6543d6ee50cbeee6c7d710a9a0e238c': '[\\n  \"How can I obtain a static set of results for the homework questions?\",\\n  \"Which Kafka should I use to process a specific number of records?\",\\n  \"Is there a particular Kafka required for processing records in the homework?\",\\n  \"What Kafka variant ensures a static result set according to the homework instructions?\",\\n  \"For the final answer in the homework, which Kafka should I avoid using?\"\\n]', '33305d774c403f3582e5876726447e06': '[\\n    \"What is the best way to ensure consistent results when using a materialized view?\",\\n    \"What steps should I follow if the homework answers don\\'t match the provided options?\",\\n    \"In what order should the clean-cluster and docker volume prune commands be used for the homework?\",\\n    \"What tool should be used instead of stream-kafka to ensure the homework answers match?\",\\n    \"How many records should be ensured for consistency in the homework?\"\\n]', '3b60fc98d5bfbb0f84337f26a177275f': '[\"What are the commands to install PostgreSQL on a Linux system for the workshop?\", \"How can I update the package list when installing PostgreSQL on Linux?\", \"What should I do if the PostgreSQL service is not running after installation?\", \"How do I add the PostgreSQL repository key on a Linux system?\", \"What command can I use to check the status of the PostgreSQL service after installation?\"]', '826b6f92b5fdcb2c55178cd9c690e04e': '[\\n  \"What should I do if xdg-open fails to launch a browser for the RisingWave workshop?\",\\n  \"How can I resolve the issue of xdg-open not opening any browser during the workshop?\",\\n  \"Is there an alternative to w3m for accessing the RisingWave dashboard if xdg-open doesn\\'t work?\",\\n  \"What\\'s a workaround for accessing the RisingWave dashboard if xdg-open isn\\'t functioning on my system?\",\\n  \"If xdg-open isn\\'t opening a browser for RisingWave, how can I open the index.html file manually?\"\\n]', 'dd77d96cf763ebd55127e444ee040829': '[\"What might cause a Python script with a specific shebang line to fail in a Unix-like environment?\", \"How can I find the correct Python 3 interpreter path to update the shebang line?\", \"What command can convert Windows-style line endings to Unix-style in a script?\", \"Why might a Python script edited in Windows cause errors in a Unix environment?\", \"What does the presence of the \\'\\\\\\\\r\\' character in the shebang line suggest about the script\\'s origin?\"]', '7dd07e55a5a7a82dd25a24dcc51e54f1': '[\\n  \"What is the purpose of using windowing in streaming SQL?\",\\n  \"Can you explain how windowing handles data in a streaming SQL environment?\",\\n  \"How are boundaries defined in a streaming SQL windowing context?\",\\n  \"What benefits does windowing provide for managing streaming data?\",\\n  \"In what ways can windowing be applied to SQL streaming data analysis?\"\\n]', 'c01c4a327793171e1e6b8acbab0afd26': '[\\n  \"Why am I encountering the \\'ModuleNotFoundError: No module named kafka.vendor.six.moves\\' when running \\'from kafka import KafkaProducer\\' in Jupyter Notebook for Module 6 Homework?\",\\n  \"What should I do if the path to profiles.yml is not correctly specified when executing the Mage pipeline?\",\\n  \"How can I resolve Mage blocks failing to generate OAuth and authenticate with GCP even after configuring the io_config.yaml file?\",\\n  \"What steps do I need to follow to add triggers in Mage pipelines via CLI?\",\\n  \"Is it possible to perform data partitioning and clustering using a dbt pipeline without manually doing it in BigQuery afterwards?\"\\n]', '3cf2153faa38890af19cc530f3ebe09b': '[\"How can I create a Docker image from a base image using Docker commands?\", \"What is the command to list all running Docker containers?\", \"How do I attach to a stopped container with Docker?\", \"Which command allows for copying files from a host to a Docker container?\", \"What is the procedure to install a package on Debian?\"]', '7244bc052ce8176085f4b05d6029162f': '[\\n    \"Where can I find guidance on frequent issues?\",\\n    \"What is the purpose of this document?\",\\n    \"Where is the sign-up link located?\",\\n    \"How should I structure my questions and answers?\",\\n    \"What course benefited from using an FAQ approach?\"\\n]', '4e09802e6092fd04a5f7cf0d2f6635a4': '[\\n  \"Are the course videos live or pre-recorded?\",\\n  \"When can I watch the videos for this course?\",\\n  \"What are office hours in this course?\",\\n  \"Will the office hours sessions be recorded?\",\\n  \"Where can I find the course playlist?\"\\n]', 'c168d158841ef3e7dee19ebf2291b467': '[\\n    \"Are sessions recorded if I\\'m absent?\",\\n    \"How can I catch up if I don\\'t attend a class?\",\\n    \"Is there a way to ask questions if I can\\'t join the session?\",\\n    \"Where can I raise queries if I\\'m unable to attend live?\",\\n    \"What happens if I can\\'t make a live session?\"\\n]', '065698c11cf3a55a7ad1a51a55678e36': '[\\n    \"What is the primary focus of the course content?\",\\n    \"Will the course go in-depth into theoretical concepts?\",\\n    \"How will logistic regression be addressed in this course?\",\\n    \"Is there a comprehensive coverage of theory like gradient derivations?\",\\n    \"Are there alternative courses for more detailed theory?\"\\n]', 'e504ab92e0a32ffa91ed79299a100ae2': '[\\n  \"Is there a lot of math involved in this course?\",\\n  \"What resources can I use to learn linear algebra for this course?\",\\n  \"If I have questions about the material, what should I do?\",\\n  \"Will the course focus more on algebra or coding?\",\\n  \"Can someone with no math background succeed in this course?\"\\n]', '5854fd9855f800f031d0c0b14105bb81': '[\"How long should I wait for the confirmation email after filling the form?\", \"What should I do if I can\\'t find the confirmation email in my inbox?\", \"Will I receive any course updates if I\\'ve unsubscribed from the newsletter?\", \"Why might I not receive a confirmation email for the course?\", \"Which channels should I join to stay updated if I don\\'t get confirmation emails?\"]', '3a19c31d89c34a3745e118e47ddec9c3': '[\\n    \"How does the duration of the course compare if I choose to do additional projects?\",\\n    \"What is the approximate time frame for completing the course?\",\\n    \"Can the course take longer than 4 months with extra activities?\",\\n    \"How much time should I allocate to complete the course if I engage in extra assignments?\",\\n    \"Is there flexibility in course duration for students wanting additional experience?\"\\n]', '55e044b2713c1bb49818df4613b45545': '[  \\n    \"What is the recommended weekly time commitment for this course?\",  \\n    \"How many hours per week are typically required for students in this course?\",  \\n    \"What did Timur Kamaliev\\'s analysis reveal about the weekly time investment necessary?\",  \\n    \"Can you provide an estimate of weekly hours needed for course modules and projects?\",  \\n    \"How much weekly time should I allocate to complete the course successfully?\"  \\n]', '85e4e1d751919a66bc80400d34e01a77': '[\\n  \"What are the requirements for earning a certificate in this course?\",\\n  \"How many projects must I finish to receive the course certificate?\",\\n  \"Are peer reviews part of the criteria for obtaining a certificate?\",\\n  \"Is there a deadline for completing the projects to qualify for the certificate?\",\\n  \"Are there different versions of the course certificate available?\"\\n]', 'b93f158736ea7a144e931919d9996508': '[\\n  \"Is it possible to receive a certificate without completing the midterm project?\",\\n  \"Can I still earn a certificate if I didn\\'t do the midterm project?\",\\n  \"Will missing the midterm project affect my eligibility for a certificate?\",\\n  \"Do I need to complete all projects to receive a certificate?\",\\n  \"If I skip the midterm project, can I still get a certificate?\"\\n]', 'ab25b906c3ad4e99bfd2e97c166cd101': '[\\n  \"Where can I find a resource to learn the necessary Python skills for this course?\",\\n  \"What are the Python basics I should be familiar with before starting?\",\\n  \"Is there a specific platform recommended for learning Python basics?\",\\n  \"What operations are important to understand for data analysis in Python?\",\\n  \"What should I do if I don\\'t know enough Python for the course?\"\\n]', '53af797a68ee2940b86fa469363c9e85': '[\\n  \"What hardware is necessary for the machine learning section of the course?\",\\n  \"Is a personal computer sufficient for all parts of the course?\",\\n  \"Do I need special equipment for deep learning, or is cloud usage an option?\",\\n  \"Which cloud service is recommended for the deep learning portion?\",\\n  \"Can the entire course be completed with just an internet-capable laptop?\"\\n]', '6947c5f1004b1113235bb43c680e4a64': '[\"What is the process to configure TensorFlow with GPU on Ubuntu?\", \"Could you share a resource for setting up TensorFlow on Ubuntu with GPU support?\", \"Where can I find guidance on enabling GPU support for TensorFlow on Ubuntu?\", \"How do I install TensorFlow on Ubuntu with GPU compatibility?\", \"Is there a recommended article for setting up TensorFlow on Ubuntu with GPU?\"]', 'ba72d5fa09d37e69971e4c04f13789af': '[\\n    \"How do I join the course channel on Slack?\",\\n    \"Is it mandatory to submit the GitHub repository link for homework?\",\\n    \"Where do I find the list of public channels in Slack for this course?\",\\n    \"What steps should I take if I can\\'t see \\'All channels\\' in Slack?\",\\n    \"Can you guide me on how to provide the correct GitHub link for my homework?\"\\n]', '76b2cfa6443b8bf034146208c79d1bd6': '[\\n  \"If I enroll after the course start date, am I able to join and continue?\",\\n  \"What are the requirements to receive a certificate in this course?\",\\n  \"Is it possible to submit all assignments if I join late?\",\\n  \"What is the latest I can join the course and still earn a certificate?\",\\n  \"How many course projects do I need to complete for certification?\"\\n]', '83a6c7d555f0b7200d4ff58a112a94ec': '[\\n    \"Is there a flexible option to start the course at any time?\",\\n    \"Can I participate in the course alongside other students in a specific month?\",\\n    \"What are the options if I want to study with a group?\",\\n    \"When is the next cohort beginning?\",\\n    \"Do the course iterations occur annually in September?\"\\n]', '1b6d80a4ee79f28d8077abd62e6d4f7e': '[\\n    \"Is it possible to turn in homework late?\",\\n    \"Are late homework submissions accepted?\",\\n    \"Can homework be submitted past the deadline?\",\\n    \"Is there any way to submit assignments after the due date?\",\\n    \"Can I upload homework if the deadline has passed?\"\\n]', '7909413e274d6f41bce99fb496c42db6': '[\\n  \"What should I do right after joining the course and where can I find the materials?\",\\n  \"Which link should I use to access the course syllabus and videos?\",\\n  \"Where can I find information specific to my cohort year\\'s curriculum?\",\\n  \"Is there any available content from prior cohorts\\' office hours that I can watch?\",\\n  \"When was ML Zoomcamp initially launched, and how can I find playlists for my course year?\"  \\n]', '22dc3e5404a2dd02e0ed9359a6a69221': '[\\n  \"Where can I find the important dates for the class?\",\\n  \"Are the submission cutoff times listed on the course site?\",\\n  \"Is there a place online to check assignment due dates?\",\\n  \"Which page shows the 2023 course schedule deadlines?\",\\n  \"How do I find out when assignments are due for the 2023 cohort?\"\\n]', '52e422ebd01108b373a0570b5ffc638e': '[\\n  \"How many modules from 2022 are still included in the 2023 course?\",\\n  \"Is there any new special module introduced in the 2023 course?\",\\n  \"What notable changes have been made to the homework in 2023?\",\\n  \"Are the core topics of the course unchanged from 2022 to 2023?\",\\n  \"Was BentoML still a part of the 2023 course offering?\"\\n]', '2ec616a8ebb9ab48551c2929b20c8ddd': '[\"Are there any plans to re-record the course videos for this iteration?\", \"Are the videos from 2021 still applicable for the current course?\", \"Is it beneficial for new students to start with the old course videos?\", \"Has the focus or main skills of the course changed since the last iteration?\", \"Should we use a different Python version than previously recommended?\"]', '26691c7bec9cf4b93271649d4c0a25e6': '[\\n  \"How do I gain extra scores for posting my course learnings on social media?\",\\n  \"Is there a limit to the number of links I can submit in a single homework for extra points?\",\\n  \"Can I receive additional points for similar content if posted on different platforms?\",\\n  \"What is the maximum number of points I can earn for midterms or capstone projects through public posts?\",\\n  \"Where do I include my learning in public links when submitting my homework?\"\\n]', 'bade674918646e25a056585b7977c25d': '[\\n    \"How can I include my own notes in the course repository?\",\\n    \"What is the process to connect my notes to the main course repo?\",\\n    \"What steps are required to add a link to my notes under \\'Community Notes\\'?\",\\n    \"What should I do after forking the original course repository?\",\\n    \"How do I sync my own course repository with the original one?\"\\n]', 'fbc42b7dab9cd1a9c28d3b724afa500a': '[\\n  \"What links should students use to access the 2023 and 2022 leaderboards?\",\\n  \"How can a student compute the hash for their email address using Python?\",\\n  \"Why are quotes necessary when entering an email into the compute_hash function?\",\\n  \"Where can students directly enter their email to obtain a hashed version without using Python code?\",\\n  \"How do students use their hashed email to view their scores on the leaderboard?\"\\n]', '9f9dc2283b11f5ca51c5499c744a4426': '[\\n  \"How can I resolve the error wget is not recognized as an internal or external command on Ubuntu?\",\\n  \"What is the easiest way to install wget on a Mac?\",\\n  \"Can I use Python to download a file instead of wget and how?\",\\n  \"How do I bypass HTTPS checks when reading a CSV file from a URL in Python?\",\\n  \"What Python library function can I use to download files from URLs across all platforms?\"\\n]', '7befb81549f486a729a302e8cfaf06bb': '[\\n  \"How can I download a CSV file inside a notebook?\",\\n  \"What is the purpose of the exclamation mark in a notebook?\",\\n  \"How can I create a new directory from within a notebook?\",\\n  \"What command would I use to move a file to a different directory in a notebook?\",\\n  \"Is it possible to run shell commands like ls or cp inside a notebook?\"\\n]', 'f646e1c25f4182ea0eb8126ea35d4490': '[\\n  \"How can I set up a WSL development environment on my Windows 11 device?\",\\n  \"What is the purpose of the Microsoft verified VS Code extension \\'WSL\\'?\",\\n  \"How can I make VS Code connect to my WSL Ubuntu instance?\",\\n  \"Where can I find instructions to use WSL on a Windows 11 device for Linux access?\",\\n  \"Who provided the answer concerning using WSL with VS Code?\"\\n]', '9b5955bbeaabe6174ad037a3d922d013': '[\"What should I do if I encounter an error when I try to push my code to Github for the first time?\", \"How can I fix the error \\'src refspec master does not match any\\' when pushing to my Github repository?\", \"What are the steps to push my initial commit to Github if I used the wrong branch name?\", \"Where can I find a beginner-friendly tutorial on using Github for uploading code?\", \"What alternatives can I use to submit my code to Github if I\\'m having trouble with the command line?\"]', 'c17ec2ceecce49e934923be85d8216ab': '[\\n    \"What causes a singular matrix error in our matrix inversion task?\",\\n    \"Why should we pay attention to the order of matrix multiplication in homework?\",\\n    \"In what scenario does X.dot(Y) differ from Y.dot(X) in multiplication?\",\\n    \"Why is matrix multiplication not commutative in the context of this course?\",\\n    \"How can incorrect order in matrix methods lead to errors in homework?\"\\n]', '2bfd5a9384fe183960be4a03007ded18': '[\\n    \"What can I do if the \\'conda create -n ml-zoomcamp python=3.9\\' command isn\\'t working in my terminal?\",\\n    \"How can I solve the issue of Conda not being recognized as an internal command on Windows?\",\\n    \"What terminal should I use on Windows if I just installed Anaconda and face issues with Conda?\",\\n    \"What should I install if I don\\'t have Anaconda or Miniconda but need to use Conda commands?\",\\n    \"Which Python versions are compatible with the \\'ml-zoomcamp\\' environment creation command?\"\\n]', '0bd43356dd61d4ae2a3321d2deba034c': '[\"How can I correctly read a CSV file using Pandas in Windows?\", \"What causes conflicts when using backslashes in file paths in Python?\", \"Why is my code for reading a CSV file in Python not working on Windows?\", \"How should I modify the file path to avoid escape sequence issues in Python?\", \"What is the correct way to write the file path in a Python script on Windows?\"]', '9604ee6ba9ca95ca675bffb758b2aba5': '[\"How can I resolve a \\'403 Forbidden\\' error when pushing to a GitHub repo?\", \"What steps should I follow to check the configuration of my GitHub repository URL?\", \"How do I update the GitHub repository URL to fix a \\'403 Forbidden\\' error?\", \"Who added the instructions for fixing the \\'403 Forbidden\\' error in the course FAQ?\", \"What command should I use to verify if the remote URL change is applied?\"]', '3364e783c6f4bed7f4f19429cb92c34f': '[\\n  \"What should I do if I receive a fatal authentication failed message when pushing code to GitHub?\",\\n  \"How can I authenticate with GitHub now that password authentication is no longer supported?\",\\n  \"What is the solution for pushing changes to GitHub if password authentication is disabled?\",\\n  \"Where can I find information on recommended authentication modes for GitHub?\",\\n  \"How do I create a personal access token for pushing changes to GitHub?\"\\n]', 'bb35b7b7c896a1b6199a124628c0047a': '[\\n  \"What error might you encounter when using wget in Kaggle to import a dataset from GitHub?\",\\n  \"What is a potential cause of receiving a \\'wget: unable to resolve host address\\' error in Kaggle?\",\\n  \"How can you resolve the issue of Kaggle not connecting to raw.githubusercontent.com when trying to wget a file?\",\\n  \"What step should you take in Kaggle to enable downloading datasets from GitHub?\",\\n  \"Why might Kaggle ask to verify your phone number when resolving a wget error?\"\\n]', 'f9ebc1e7fafb8f3ca928d2abdf3f332e': '[\\n  \"How can I create a virtual environment for Python using Visual Studio Code?\",\\n  \"Is it possible to use Jupyter Notebooks directly in Visual Studio Code without a web browser?\",\\n  \"What do I need to execute remote Jupyter Notebooks on a server from my local machine using Visual Studio Code?\",\\n  \"Can I use Visual Studio Code to work with GitHub for staging and commits?\",\\n  \"Where can I find resources on setting up a programming environment in Visual Studio Code?\"\\n]', 'ff1be685e151dfb2edbcd9dc26921f83': '[\"Is it necessary to run \\'conda create -n ...\\' every time I start working on the project?\", \"Do I need to activate the conda environment every time I open VS Code?\", \"How can I reproduce my conda environment for others?\", \"What command exports the current conda environment to a YAML file?\", \"What is the purpose of \\'conda env create -f environment.yml\\'?\"]', 'b7ec7282f866bdc5e0fd65b9c73239d9': '[\\n  \"Why didn\\'t my matrix multiplication return an exact identity matrix in the homework for Week 1, Question 7?\",\\n  \"What causes small discrepancies when multiplying an inverse matrix with its original?\",\\n  \"Can you explain why floating point math can lead to unexpected results in matrix operations?\",\\n  \"How does floating point precision affect matrix multiplication results in programming tasks?\",\\n  \"Where can I find more information about issues with floating point math on computers?\"\\n]', 'ec86334b1ea53a6a0979afc871818227': '[\"What function provides indexing, entry count, and column info for a DataFrame?\", \"How can I get details on data types, not-null counts, and memory usage of a dataset?\", \"Which method do I use to print the dataset\\'s index datatype and memory usage?\", \"What is used to see column information including the datatype in pandas?\", \"How do I get an overview of column info and memory usage for a DataFrame?\"]', 'c1047363f7b00426ff5ee9c3ffcefc5b': '[\\n  \"What should you do if \\'np\\' is not defined in your code?\",\\n  \"How can you resolve a \\'pd\\' NameError in your program?\",\\n  \"What might be missing if you encounter a \\'name not defined\\' error for \\'np\\'?\",\\n  \"Why am I getting a NameError for pandas not being defined?\",\\n  \"Which import statements are necessary before using pandas and numpy?\"\\n]', '500489499b2be17ee9afd089a127e8d6': '[\\n    \"What is a concise way to obtain columns with numeric data types in a DataFrame?\",\\n    \"How can you select only object type columns from a DataFrame more efficiently?\",\\n    \"What method helps in retrieving numeric columns when dealing with many columns?\",\\n    \"How do you efficiently obtain columns with object data types when faced with numerous columns?\",\\n    \"What technique is used to list columns based on their data type, like numeric or object, in a concise manner?\"\\n]', 'cfad7ba25e59e35011d7dc11ed891b98': '[\\n  \"What method can you use in Pandas to find the shape of a dataset?\",\\n  \"Which attribute in Pandas can help you determine the number of rows in a dataset?\",\\n  \"How can you identify the number of columns in a dataset using Pandas?\",\\n  \"What tool is mentioned by Radikal Lukafiardi to see the dataset\\'s shape in Pandas?\",\\n  \"What code would you use to see the dimensions of a dataframe using Pandas?\"\\n]', 'ee5f5b300844d59accea03846067497c': '[\\n    \"What method should I use to avoid value errors with matrix multiplication in homework?\",\\n    \"Why is the order of multiplication important in matrix operations?\",\\n    \"What should match for successful matrix multiplication in terms of dimensions?\",\\n    \"How can I rearrange matrices to avoid dimension mismatch in matrix multiplication?\",\\n    \"Who contributed the advice on avoiding value errors with array shapes?\"\\n]', 'eb2c416dbe00003af019404345cba699': '[\"What is the process of replacing NaN values with the column average in machine learning?\", \"Why do we impute NaN values instead of removing rows in datasets?\", \"How can we handle null values in a dataset without losing important data?\", \"Can you explain the benefit of using average imputation for NaN values?\", \"What method is used to replace NaN values with an average in data columns?\"]', 'cc9536ffdda9c796b1006b1a0ee999ab': '[\\n  \"What mathematical approach can be used to solve the initial problem in linear regression from Matrix X to target y?\",\\n  \"Which materials are suggested for further study after learning the linear regression formula?\",\\n  \"Who contributed to the section about the mathematical formula for linear regression?\",\\n  \"How can one calculate the target variable in the linear regression problem described in question 7?\",\\n  \"What are the recommended additional readings for understanding the OLS in linear regression?\"\\n]', 'a345f42c06c620f19902c0fc71ff4963': '[\"Why is the final multiplication missing a column?\", \"What happens if I interchange the first step in multiplication?\", \"What might cause an error in the multiplication step?\", \"How can swapping steps affect the final multiplication?\", \"What should I check if the multiplication result is incorrect?\"]', 'b9648c6b49f00c7a769f79d7aaa538e8': '[\\n  \"What operator is used for matrix multiplication in numpy?\",\\n  \"Which operator is preferred for element-wise multiplication in numpy?\",\\n  \"For matrix-matrix multiplication, should I use @ or * in numpy?\",\\n  \"What function is recommended for scalar multiplication in numpy?\",\\n  \"Is numpy.dot() suitable for matrix-matrix multiplication?\"\\n]', '2446b53cc24d3d7e92ccafa05271fbf6': '[\\n    \"What should I do if I encounter an ImportError related to \\'contextfilter\\' in Jupyter Notebook?\",\\n    \"How can I fix a launch error in Jupyter notebook for a new environment due to jinja2?\",\\n    \"What is the solution if I see an ImportError when opening a new notebook related to jinja2?\",\\n    \"How do I address an ImportError involving \\'contextfilter\\' when launching Jupyter in a new environment?\",\\n    \"What steps should I follow if Jupyter notebook won\\'t open due to an ImportError from jinja2?\"\\n]', '692b0a2bdf52dc30d67a7eff9d208cf4': '[\\n    \"What should I do if wget hangs on MacOS Ventura M1?\",\\n    \"How can I resolve wget connectivity issues on my Mac?\",\\n    \"What steps are needed if wget stalls while downloading a file on MacOS?\",\\n    \"How to configure network settings to fix wget problems on a Mac?\",\\n    \"What\\'s the solution for wget showing IPv6 addresses and hanging on MacOS Ventura?\"\\n]', '929c10279f11813bf53bb3f6c85679f0': '[\"How can I download files on macOS without using Wget?\", \"What is the alternative to Wget for macOS, according to the course materials?\", \"What command should I use to save a downloaded file using curl?\", \"How do I specify the file name when downloading with curl?\", \"Where can I find more information about using curl?\"]', 'e9fcd0bc674ddc8246bb38addb7cd3d4': '[\\n    \"What are two methods to limit the number of decimal places when outputting numbers?\",\\n    \"How can you round a number to four decimal places?\",\\n    \"What Python string formatting method can be used to display numbers with three decimal places?\",\\n    \"Is there a way to round all values in a pandas Series?\",\\n    \"Where can I find additional information on rounding values in a pandas Series?\"\\n]', '265b8599867fb8f9dd036cdc4434d98b': '[\\n  \"What are the important links I need to start with Week 2 starting September 18, 2023?\",\\n  \"Where can I ask questions for live sessions during Week 2?\",\\n  \"How can I find the calendar for the weekly meetings?\",\\n  \"Where do I submit the Week 2 homework?\",\\n  \"Is there a GitHub repository for learning the theory part of the course?\"\\n]', '56a61daa23e66bd43ea433db7f20a267': '[\\n  \"What tools can I use to visualize the long tail of my data distribution?\",\\n  \"How can I check the distribution of the \\'median_house_value\\' variable in my dataset?\",\\n  \"What method can I use to assess the skewness of my dataset in the course materials?\",\\n  \"How could I verify the skewness in the \\'median_house_value\\' field using Python?\",\\n  \"Is there a Python library that can help me plot histograms during the exploratory data analysis phase?\"  \\n]', 'efed81d393751fe56ecc21d576687061': '[\"When following the videos, what is a potential reason for encountering a Singular Matrix error?\", \"Where in the course is the explanation for why a Singular Matrix error occurs?\", \"Is encountering a Singular Matrix error a normal part of the learning process in this course?\", \"What might cause an error when inverting a matrix in my code?\", \"Who contributed the information about encountering a Singular Matrix error in the course material?\"]', '4c1c901393fce63334991fad68ef1137': '[\\n    \"Where can I find information about the California housing data used in this course?\",\\n    \"Is there a resource for learning more about the California housing dataset?\",\\n    \"Where is the detailed description of the California housing dataset available?\",\\n    \"Can you guide me to the description of the California housing dataset mentioned in this section?\",\\n    \"Are there any links provided for understanding the California housing dataset?\"\\n]', 'a4979b0f48774fd78cd0d8221c05a5d2': '[\\n    \"Why might I be getting NaNs when calculating the mean during RMSE evaluation?\",\\n    \"What should I do if my RMSE calculation results in NaN values?\",\\n    \"What could cause NaNs to appear after applying the .mean() function in my dataset?\",\\n    \"How can I resolve NaNs that arise from my RMSE computation using for loops?\",\\n    \"What is a solution to NaNs appearing in my RMSE results when using split datasets?\"\\n]', '5f8b7ba5c6b3584bcdffb7a5a31049b6': '[\"When should we consider transforming the target variable to a logarithm distribution in a project?\", \"Is transforming the target variable to logarithm recommended for every machine learning project?\", \"What is a simple method to determine if the target variable is skewed?\", \"Why is it important to evaluate skewness in the target variable distribution?\", \"Can you provide a resource for understanding skewness better?\"]', '53164980b53ff5dd5dea98999ce8eaed': '[\\n  \"How can I read a dataset from GitHub into a pandas dataframe?\",\\n  \"What is the method to import a CSV file from a URL into pandas?\",\\n  \"Could you explain how to load data from a GitHub link into pandas?\",\\n  \"What approach should I take to fetch a dataset from a GitHub repository using pandas?\",\\n  \"How do I extract data from a GitHub link into a Python dataframe?\"\\n]', '95a2c0cc815c7d87f01d7833d7e72939': '[\\n    \"How can users of Kaggle Notebooks load the dataset directly?\",\\n    \"What is essential to remember when using wget in Kaggle Notebooks?\",\\n    \"Once the dataset is loaded on Kaggle, how can it be read with pandas?\",\\n    \"Is there a specific command to load datasets through widgets in Kaggle Notebooks?\",\\n    \"What command can be used to read the \\'housing.csv\\' file using pandas?\"\\n]', 'feeebcbcdbfec16fe15cc4b3e0264f8d': '[\\n  \"How can I filter a dataset using specific values from a column?\",\\n  \"What operators can be used to filter datasets by multiple conditions?\",\\n  \"Is there an alternative method to filter datasets by values in a column?\",\\n  \"How do we filter a dataset for rows where the \\'ocean_proximity\\' is either \\'<1H OCEAN\\' or \\'INLAND\\'?\",\\n  \"Can you provide an example of filtering a dataset using the \\'isin\\' method?\"\\n]', 'c8e84ce581b5f06a2f58c810960f92bb': '[\\n\"How can I load a dataset from GitHub using the requests library?\",\\n\"What Python library can I use to download data from a URL in a script?\",\\n\"Is there an alternative method to load datasets for homework using requests?\",\\n\"What should I do if a data download attempt using requests fails?\",\\n\"How do I save the contents of a response in a file when using requests?\"\\n]', '79b4c7ec45962be67144b3ca01f04b91': '[\\n    \"Why does a null column appear after using .fillna() in my dataframe copy?\",\\n    \"How can I ensure that my dataframe copy doesn\\'t reference the original variable?\",\\n    \"What is the difference between a shallow copy and a deep copy in dataframes?\",\\n    \"How can I create a deep copy of my dataframe to avoid reference issues?\",\\n    \"What steps should I take to prevent null columns from appearing in my dataframe copy?\"\\n]', '33ca0c080d9a2ac5db09e1c8f84de6fc': '[\\n  \"Is it permissible to use Scikit-Learn’s train_test_split function this week?\",\\n  \"Am I allowed to apply Scikit-Learn’s train_test_split earlier in the course?\",\\n  \"Can I start utilizing Scikit-Learn’s built-in train_test_split function ahead of schedule?\",\\n  \"Are we implementing train_test_split ourselves instead of using Scikit-Learn immediately?\",\\n  \"May I opt to use Scikit-Learn’s train_test_split before it is taught in detail later on?\"\\n]', '380d799d4c6d38443e1da1d87c4ac79b': '[\\n  \"Can I apply LinearRegression from Scikit-Learn this week for my project?\",\\n  \"Is LinearRegression from Scikit-Learn applicable in this week\\'s lessons?\",\\n  \"Will we utilize LinearRegression from Scikit-Learn in our current lesson?\",\\n  \"Can we implement LinearRegression from Scikit-Learn during this week\\'s module?\",\\n  \"Should I be prepared to use LinearRegression from Scikit-Learn in my assignments this week?\"\\n]', '453ba8cec7ef7438223ac37e390b3eb1': '[\\n  \"Which Scikit-Learn function is used for linear regression without regularization in week 2?\",\\n  \"What is the corresponding Scikit-Learn function for linear regression with regularization mentioned in the course?\",\\n  \"Where can I find explanations of linear models in Scikit-Learn?\",\\n  \"In week 2, what is the Scikit-Learn equivalent for a linear regression model without regularization?\",\\n  \"Who added the information about the Scikit-Learn functions for linear regression models?\"\\n]', '2040f7ffb048200bcb1ba6a60474f44b': '[\\n    \"In what way is the parameter `r` utilized differently than `alpha` in the matrix calculation?\",\\n    \"What role does the regularization parameter `r` play in addressing multicollinearity?\",\\n    \"How does `r` contribute to the prevention of computing inverse matrices in our regression formula?\",\\n    \"What impact does strengthening the regularization by increasing `r` or `alpha` have on the regression model?\",\\n    \"Is the purpose of `r` similar to `alpha` in reducing model complexity via regularization?\"\\n]', '67d417889e9a9b84847635441332ab5c': '[\\n  \"Why doesn\\'t linear regression fit perfectly to the data?\",\\n  \"In lesson 2.8, why is the predicted y different from the actual y?\",\\n  \"Why can\\'t linear regression achieve a 100% fit?\",\\n  \"What happens if we try to fit all the data points in linear regression?\",\\n  \"Why is achieving a perfect fit a sign of overfitting in linear regression?\"\\n]', '948a552dc33d7582f13928d487389b01': '[\\n    \"Why does using a random seed of 42 result in all missing values ending up in the training dataframe?\",\\n    \"What is the purpose of setting a seed value like 42 when generating the proportion split in the homework?\",\\n    \"How does changing the seed value from 42 to another number like 9 affect the distribution of missing values in different dataframes?\",\\n    \"Why do we use the specific seed value of 42 in our machine learning homework?\",\\n    \"What happens to the missing values if I use a random seed different from 42 in the homework assignment?\" \\n]', '86dae10a3e601b27f767a0b6a511e3f3': '[\"How can I shuffle the dataset using pandas in a regression course?\", \"What is the pandas command to shuffle an entire dataset?\", \"How do I maintain the same randomization for my dataset as shown in the course?\", \"What does setting frac=1 do when shuffling a dataset in pandas?\", \"How can I reset the index after shuffling my dataset using pandas?\"]', '3e589eef6e09d2dec9783ec476d0ed6c': '[\\n  \"What should I do if my homework answer doesn\\'t match any options?\",\\n  \"If my answer isn\\'t listed among the options, do I need to worry?\",\\n  \"How should I proceed if my homework result is not an exact match with the given choices?\",\\n  \"Is it common to have difficulty finding an answer option that matches exactly?\",\\n  \"How do differences in computing environments affect homework answers?\"\\n]', 'f512202c894b1fb92a574f6cf4c15256': '[\\n    \"What dataset should be used to calculate the mean in homework 2, question 3?\",\\n    \"Is it correct to use the validation set for mean calculation in question 3 of homework 2?\",\\n    \"Can I use df_train[\\'column_name\\'].mean() for mean calculation in homework 2?\",\\n    \"What is the purpose of using df_train[\\'column_name\\'].describe() in homework 2?\",\\n    \"In question 3 of homework 2, which datasets should I exclude when computing the mean?\"\\n]', '81127530e91aed3c306066aed52ad6fc': '[\\n  \"When is it advisable to apply a logarithm transformation to the target variable?\",\\n  \"In what situations should np.log1p() be used on the target variable?\",\\n  \"Why might transforming the target variable with a logarithm be necessary?\",\\n  \"What should be considered if the target variable has a long tail distribution?\",\\n  \"What limitation does np.log1p() have regarding target variable values?\"\\n]', '74c27f64ac62387edea312c198cb0add': '[\\n    \"What might cause a ValueError related to shape alignment in machine learning for regression?\",\\n    \"How can we resolve an error where operands could not be broadcast together due to shape issues?\",\\n    \"In what situation might using the * operator instead of dot() help solve shape alignment problems?\",\\n    \"What are some scenarios where broadcasting between arrays fails due to shape differences?\",\\n    \"Why might we encounter an error with shapes not aligned during arithmetic operations in regression tasks?\"\\n]', '161a73673dcf8fa7d602fdf881460749': '[\\n  \"How can a student create a duplicate of a dataframe without altering the original?\",\\n  \"What is the method to create a deep copy of a dataframe?\",\\n  \"What happens if X_copy is set equal to X instead of using the copy method?\",\\n  \"In dataframe copying, why might changes to one object affect the other?\",\\n  \"What is created if X_copy is directly assigned the value of X?\"\\n]', '42bf88049d0ad5b9e7687f21adc4fe77': '[\"What is a characteristic of the normal distribution related to mean, median, and mode?\", \"How does the long tail affect the area under the curve in a distribution?\", \"In a long tail distribution, how are mean, median, and mode affected?\", \"What is a consequence of a distribution having a long tail?\", \"How does a long tail affect the representativeness of the mean?\"]', 'c9608149a7f792b457fbe11c1191ade2': '[\"What does standard deviation represent in statistics?\", \"How is standard deviation related to the mean of a dataset?\", \"What does a low standard deviation indicate about a set of values?\", \"What does a high standard deviation suggest about data dispersion?\", \"What is the formula to calculate standard deviation?\"]', '7df2789f666327ea20c50e0467ad7eaf': '[\\n    \"Is regularization necessary with small datasets or complex models?\",\\n    \"Should regularization be applied in every machine learning scenario?\",\\n    \"How do data quality and size affect the need for regularization?\",\\n    \"When is it advisable to evaluate the necessity of regularization?\",\\n    \"Does the requirement for regularization change with different problems?\"\\n]', '73f53619ef43dd934b9cdda867198d34': '[\\n    \"What is the benefit of defining functions for faster execution in machine learning for regression?\",\\n    \"How can I prepare all three dataframes and y_vectors quickly?\",\\n    \"Is it effective to use fillna() before splitting the initial dataframe?\",\\n    \"Which existing functions can I reuse from the class notebook to enhance my workflow?\",\\n    \"Who provided the guidance on preparing the dataframes and y_vectors?\"\\n]', '5a2eac9aa1ea41af88b0f5ab91caf495': '[\\n  \"How can I calculate the standard deviation of a data list using pandas?\",\\n  \"What is the process to obtain standard deviation from a pandas series?\",\\n  \"Which pandas function is used to find the standard deviation of a series?\",\\n  \"Can you demonstrate using pandas to compute standard deviation on a list of numbers?\",\\n  \"What steps are involved in finding standard deviation of a dataset with pandas?\"\\n]', 'abddb96e0e1c6d92684c9e7939e27651': '[\\n  \"What is the main difference in standard deviation computation between Numpy and Pandas?\",\\n  \"Why does Pandas use a different equation than Numpy for standard deviation by default?\",\\n  \"How can I change Numpy\\'s standard deviation to match Pandas using sample standard deviation?\",\\n  \"Which degree of freedom does Pandas use for default standard deviation calculation?\",\\n  \"How can I modify the degree of freedom in Numpy to get an unbiased estimator?\"\\n]', '50ca61b1500ebcda644cbacf51c56f43': '[\\n    \"How do you find the standard deviation for a specific column using Pandas in machine learning?\",\\n    \"What is the Pandas function to compute the standard deviation of multiple columns?\",\\n    \"Can you use Pandas to get the standard deviation of a single column, and if so, how?\",\\n    \"What is the method to calculate the standard deviation for several columns in Pandas?\",\\n    \"What built-in function does Pandas provide to find the column standard deviation?\"\\n]', 'cc4e75564b7e5cc61cef326c061ba61c': '[\\n    \"What function can I use to combine two dataframes in Python?\",\\n    \"How can one concatenate two numpy arrays?\",\\n    \"Could you show me code to join train and validation dataframes?\",\\n    \"Is there a specific method for merging validation and train datasets with pandas?\",\\n    \"What is the approach for appending two arrays using numpy?\"\\n]', '037867dbb09b27c75e757671aef80011': '[\\n  \"What is Root Mean Squared Error and how is it used to evaluate a regression model?\",\\n  \"Could you explain the method to calculate the RMSE score for a regression model?\",\\n  \"Which libraries are required to compute the Root Mean Squared Error?\",\\n  \"What are actual values and predicted values in the calculation of RMSE?\",\\n  \"How does RMSE help in understanding a model\\'s forecasting ability?\"\\n]', 'c92e11ece3b6faea3ffc6db7531dd115': '[\\n  \"How can I correctly use multiple conditions in Pandas with logical operators?\",\\n  \"What symbols represent OR and AND operators in Pandas?\",\\n  \"What syntax error occurs when using multiple conditions incorrectly in Pandas?\",\\n  \"Which logical operator in Pandas is represented by |?\",\\n  \"Which symbol should I use in Pandas for the AND condition?\"\\n]', '115967ddfdf1aad7cf7779509422cea2': '[\\n    \"What resources can help me understand the normal equation in regression?\",\\n    \"Where can I find a video to explain the derivation of the normal equation for linear regression?\",\\n    \"Can you suggest a useful video to learn about the normal form in regression?\",\\n    \"Is there a recommended video for grasping the normal equation in linear regression?\",\\n    \"How can I better understand the derivation of the normal equation used in regression?\"\\n]', '87912ccaf1b8446a3d14e78022f16df4': '[\\n  \"What is a useful guide for handling missing data in machine learning?\",\\n  \"Where can I find a resource on missing data treatment in Python?\",\\n  \"Who provides insights on managing missing values in the course materials?\",\\n  \"Can you recommend any notebooks on handling missing data in machine learning?\",\\n  \"What resource can I use to learn about missing value treatment in regression analysis?\"\\n]', '0696a2ee44dd4f20d6372ab53797fbb2': '[\\n    \"In Week-2 homework for the 2023 cohort, where is the instruction for applying log transformation to the \\'median_house_value\\' variable found?\",\\n    \"While doing the homework for Week-2, I forgot a step that caused a huge RMSE in Q5. What did I forget to apply to the target variable?\",\\n    \"Why did my RMSE turn out abnormally large in Q5 of the Week-2 homework even though I was following the steps?\",\\n    \"What oversight may cause issues when solving Q5 in the Week-2 homework?\",\\n    \"Who added the caution about applying log transformation to the target variable in the Week-2 homework FAQ?\"\\n]', '191c10a9df384a42ba4f9460c7875a15': '[\\n    \"Which versions of sklearn and Python are shown in the videos?\",\\n    \"In the lectures, what sklearn version is demonstrated?\",\\n    \"What is the Python version covered in the course videos?\",\\n    \"Can you tell me the version of sklearn used by Alexey in the materials?\",\\n    \"What specific versions of software are used in the YouTube content?\"\\n]', '6e7eb498c10ac417dc21baa2a04dcbdc': '[\"What is the link to access the homework assignment for Week 3?\", \"Where can I submit my homework for Week 3?\", \"Where can I find all the homework assignments for this course?\", \"Is there an evaluation matrix available for the course assignments?\", \"Where can I find the theory materials related to the course on GitHub?\"]', 'bc3f720b1f424ace2d3348e198ee909f': '[\\n  \"What typically causes the error \\'could not convert string to float: Nissan\\' in machine learning models?\",\\n  \"Why can\\'t a model convert the car brand \\'Nissan\\' into a numerical value suitable for processing?\",\\n  \"How can categorical variables like car brands be encoded into numerical values for machine learning?\",\\n  \"Could you give an example of using pandas to encode a column with car brands using one-hot encoding?\",\\n  \"What does the function pd.get_dummies() accomplish when applied to a DataFrame containing categorical data?\"\\n]', '94b63fcda02773814ae66aa2b5d033aa': '[\\n  \"What is the importance of converting continuous targets into binary format for mutual information calculation?\",\\n  \"Why aren\\'t continuous variables used directly for mutual information score calculation?\",\\n  \"What could happen if continuous variables were used in mutual information without modification?\",\\n  \"What changes are necessary for a continuous target to be used in mutual information calculations?\",\\n  \"Why was the target variable median_house_value altered in the homework exercise?\"\\n]', 'fc186d32c9accb9ee86a339acecf7858': '[\\n  \"Which dataset should we use when calculating the correlation matrix?\",\\n  \"Is the median_house_value conversion applied to df_train as opposed to df_train_full?\",\\n  \"Why should we avoid using df_train_full when building the correlation matrix?\",\\n  \"Does the question about correlation matrix explicitly refer to the training set?\",\\n  \"Should we exclude df_train_full to ensure the validation data remains untouched?\"\\n]', '4db7b9b7f392545dd2f1a9d9571ad54f': '[\\n    \"How can the background of a pandas DataFrame be colored based on its numerical values for enhanced visualization?\",\\n    \"What method is used to color the correlation matrix of a DataFrame using a colormap?\",\\n    \"Is it possible to pass a specific colormap when coloring the background of a DataFrame and could you give an example?\",\\n    \"What is a necessary condition for data types in a DataFrame before applying the correlation method?\",\\n    \"What example is shown using \\'background_gradient\\' on a DataFrame containing random values?\"\\n]', '8f452775534dadd43f8c87a83aaff908': '[\\n    \"How can I identify highly correlated feature pairs in my dataset?\",\\n    \"What Pandas function can I use to create a sorted DataFrame of correlation values?\",\\n    \"How can I use Seaborn to visualize correlations among numerical features?\",\\n    \"What is a concise way to generate a correlation heatmap in Seaborn?\",\\n    \"How can I plot a correlation heatmap with only the triangle section and a color gradient?\"\\n]', '607da6002f3b38137c0e3e8655b2719b': '[\\n  \"What dataset is recommended for EDA in the context of classification coursework?\",\\n  \"Is it acceptable to include the validation dataset when performing EDA?\",\\n  \"Why should we avoid using the test dataset when conducting EDA?\",\\n  \"According to the course, what is the best practice for selecting a dataset for EDA?\",\\n  \"What is the reasoning behind treating the test dataset as future unseen data during EDA?\"\\n]', 'c88e1b73fe62e97d3d0c2c39ba95d7da': '[\\n  \"Why should we avoid fitting the DictVectorizer on the validation dataset when using machine learning for classification?\",\\n  \"How does the fit method of the DictVectorizer work with df_train dictionaries in terms of mapping values?\",\\n  \"Is it necessary to initialize another instance of DictVectorizer after fitting it on the train dataset?\",\\n  \"What is the correct procedure for using fit_transform and transform with the DictVectorizer on train, validation, and test sets?\",\\n  \"How does not fitting the validation dataset help in understanding model performance on unseen data?\"\\n]', '56ac050051357b58949378ce66d75e7f': '[\\n    \"For Q5 in the homework, do we consider smallest differences in accuracy using absolute values?\",\\n    \"Should we understand smallest difference as absolute or real values when doing feature elimination?\",\\n    \"What does a negative difference in accuracy mean when a feature is removed?\",\\n    \"In feature elimination, why is it better to use absolute values for smallest difference calculations?\",\\n    \"Does selecting the smallest difference relate to model improvement in feature elimination?\"\\n]', '6a9df331310f6af680bc8107db1695d3': '[\\n  \"What should I use instead of the deprecated get_feature_names function?\",\\n  \"In which version will get_feature_names be removed?\",\\n  \"How can I resolve the deprecation warning for get_feature_names?\",\\n  \"Is there any need to worry about the FutureWarning for get_feature_names?\",\\n  \"What method from DictVectorizer replaces the deprecated get_feature_names?\"\\n]', '6cbe8c488fcb873eedc584c23663e13f': '[\\n  \"Why does my Jupyter kernel crash when using logistic regression?\",\\n  \"What should I check if logistic regression fitting takes too long?\",\\n  \"How can I prevent a crash when calling predict() with logistic regression?\",\\n  \"What is a common issue with logistic regression in Jupyter?\",\\n  \"Why is it important for the target variable to be binary in logistic regression?\"\\n]', '0e3605e80cb743093adf01f06f0ff376': '[\\n  \"What is the primary purpose of Ridge regression in predictive modeling?\",\\n  \"How does the sag solver optimize Ridge regression for large datasets?\",\\n  \"What does the alpha parameter control in Ridge regression?\",\\n  \"Why is it beneficial to apply a higher alpha value in Ridge regression?\",\\n  \"Which Python module is used for implementing Ridge regression with the sag solver?\"\\n]', 'b50cd609a1ec135bbab2a1b1e072fd76': '[\"Does DictVectorizer(sparse=True) improve memory efficiency compared to pandas.get_dummies()?\", \"Why is DictVectorizer(sparse=True) preferred during the fit process?\", \"What are the drawbacks of using sparse=False with pandas.get_dummies() or DictVectorizer?\", \"What performance issues might occur with high class numbers using sparse=False?\", \"How does using sparse=False affect regression model convergence?\"]', '193819cbd2779cb7a4abbc89d99e8737': '[\"What might cause convergence issues in Week 3, Question 6 with ridge regression?\", \"How can I resolve the warning about max_iter being reached in Week 3, Question 6?\", \"Which scalers are suggested to address the convergence warning in the context of our course?\", \"Why is it important to scale features when using the sag solver for ridge regression?\", \"What preprocessing steps should be taken for numeric and categorical features to prevent convergence warnings?\"]', 'bf11b5026fd5cfe9cb5cc0a446de77a3': '[\\n  \"What are some steps to take when facing convergence errors in Ridge regression models?\",\\n  \"How can I normalize numerical features to prevent convergence issues in machine learning?\",\\n  \"What method should be used to encode categorical features for Ridge regression?\",\\n  \"How do I combine numerical and categorical features for training a Ridge regression model?\",\\n  \"Why is it important to use OneHotEncoder for categorical feature encoding in the context of Ridge regression?\"\\n]', 'f04e49953daf8c152f8da7426c2e3888': '[\\n    \"What is the advantage of using a sparse matrix over a dense matrix in terms of memory efficiency?\",\\n    \"Why is a sparse matrix particularly useful for datasets with many zero or missing values?\",\\n    \"What is the default configuration of a DictVectorizer and why might it be preferred in certain cases?\",\\n    \"In what week and question of the course is using a sparse matrix suggested as an option?\",\\n    \"What issues did training the model avoid when using sparse mode compared to dense mode?\"\\n]', '9285d6ee0481e63bba89a2c057d4a918': '[\\n  \"How can I prevent warnings from showing up while using Jupyter Notebooks?\",\\n  \"What steps can I take to ignore warnings in Jupyter?\",\\n  \"Is there a way to disable warnings in Jupyter Notebooks?\",\\n  \"Which commands should be used to avoid warnings in Jupyter?\",\\n  \"How do I suppress warnings in a Jupyter Notebook session?\"\\n]', 'ff9825d481a61b29aeb2807d816d71dd': '[\"How do we determine the optimal alpha parameter in question six of section three?\", \"What is the process for deciding which RMSE score to trust when selecting an alpha?\", \"Why did everyone in the study group choose the wrong RMSE score in the week two homework?\", \"How do we proceed if RMSE scores are identical when choosing an alpha?\", \"Why does the person with the lowest score still have the wrong selection in the RMSE discussion?\"]', 'b0f581d2368ce6df550addf64f3d61e5': '[\\n    \"What variable should be used with binarized price to calculate mutual information?\",\\n    \"Could you guide me on which variable pairs with above_average for mutual info score?\",\\n    \"Which original categorical variable pairs with binarized price for HW3 Q3?\",\\n    \"For calculating mutual information with binarized price, what is the other variable?\",\\n    \"Can you tell me the second variable needed alongside above_average for mutual info calculation?\"\\n]', '2005f14d3667d528a34e7c5244e59a43': '[\\n  \"What features do we need to consider for model evaluation for homework question 5?\",\\n  \"Should we train the model with all features and then remove one at a time to compare accuracies?\",\\n  \"How do we evaluate the impact of each feature on the model\\'s accuracy?\",\\n  \"When comparing accuracies, should we consider the smallest absolute difference between scores?\",\\n  \"Why do we use absolute values when calculating the difference between accuracy scores?\"\\n]', '2ae0bc90c250ed885d419501d47db31e': '[\\n    \"How does the input differ between OneHotEncoder and DictVectorizer?\",\\n    \"Which type of input does OneHotEncoder require compared to DictVectorizer?\",\\n    \"What is the main distinction in input data when using DictVectorizer as opposed to OneHotEncoder?\",\\n    \"Are features sorted differently when using OneHotEncoder versus DictVectorizer?\",\\n    \"How do the inputs for OneHotEncoder and DictVectorizer differ in terms of data structure?\"\\n]', 'ae6b731ddb6e79e7cee85eff22fe82ee': '[\\n    \"How do pandas get_dummies and sklearn OneHotEncoder differ in terms of their application in dataframes and machine learning pipelines?\",\\n    \"In what contexts would you choose pandas get_dummies over sklearn OneHotEncoder for encoding categorical variables?\",\\n    \"Can you explain when it is better to use OneHotEncoder in a scikit-learn pipeline instead of pandas get_dummies?\",\\n    \"What are the key differences in the output types between pandas get_dummies and sklearn OneHotEncoder?\",\\n    \"Why might handling missing values be different when using pandas get_dummies compared to sklearn OneHotEncoder?\"\\n]', '8cbf71d4c5dadeda1075f7be24d32b5c': '[\\n  \"In HW3\\'s test_train_split task for week 3, should we use random_state 42 for both splits?\",\\n  \"Is it necessary to set the random seed to 42 in both instances of test_train_split in homework 3?\",\\n  \"When using test_train_split in the Week 3 assignment, do we apply random_state 42 twice?\",\\n  \"For the third week\\'s homework, should random_state be consistently set to 42 in test_train_split?\",\\n  \"Does the instruction for random_state 42 apply to each test_train_split in week 3 homework?\"\\n]', 'ce0b1f229531da0792373107e088f5ca': '[\\n    \"Should correlation be calculated on the data before or after splitting it into train and test sets?\",\\n    \"Is the correlation matrix created before or after the data is split?\",\\n    \"At what point should I calculate correlation, before splitting the data or after?\",\\n    \"What dataset should I use for finding the correlation matrix, train or full dataset?\",\\n    \"How do I identify the two most correlated features using the correlation matrix?\"\\n]', 'ef6cd7f33d82c3534be809f0c54bf1cf': '[\\n  \"Why is it important to use only numerical features in a ridge regression model?\",\\n  \"What should be done with categorical features before training a ridge regression model?\",\\n  \"How can categorical features be transformed for use in a ridge regression model?\",\\n  \"What parameter should be set to true to avoid errors when using one-hot encoding in ridge regression?\",\\n  \"Why is setting sparse=True important in the context of ridge regression with one-hot encoded features?\"\\n]', '4886d226be093ed37b95435b451bddd6': '[\\n  \"What features do I need to use for Homework 3 Question 6 in the Machine Learning for Classification section?\",\\n  \"Is it necessary to include the average variable created previously when working on Homework 3 Question 6?\",\\n  \"How can I prevent convergence errors when using DictVectorizer for Homework 3 Question 6?\",\\n  \"Should I use StandardScalar for numerical variables in Homework 3 Question 6, or is it okay to omit it?\",\\n  \"What is the target variable for Homework 3 Question 6 in the Machine Learning for Classification section?\"\\n]', '185d8a6316cca23bbbd979ffe67e0b72': '[\\n  \"How can I convert categorical data into numerical format for machine learning?\",\\n  \"Which tools are recommended for encoding non-numerical data in sklearn?\",\\n  \"What methods can I use to transform non-numeric columns for classification?\",\\n  \"Can you suggest sklearn utilities for converting non-numeric features to numeric?\",\\n  \"What is the process to encode categorical variables using sklearn encoders?\"\\n]', '264a4e8ccf8964aea1605f13d376896f': '[\\n  \"What are the input types for FeatureHasher and DictVectorizer?\",\\n  \"Which method is preferable for high cardinality categorical features, FeatureHasher or DictVectorizer?\",\\n  \"How does memory usage compare between DictVectorizer and FeatureHasher?\",\\n  \"What is an advantage of using DictVectorizer if feature names are important?\",\\n  \"Where can I find additional reading material on FeatureHasher versus DictVectorizer?\"\\n]', '99371da31d2fe95526ed39b6f6fff0e2': '[\\n  \"Why is it not recommended to use DictVectorizer or get dummies before splitting the data into train/val/test?\",\\n  \"What is data leakage, and how does it relate to using DictVectorizer or get dummies?\",\\n  \"Does using DictVectorizer before data splitting influence the training stage?\",\\n  \"When is it good practice to apply encoders like DictVectorizer in a machine learning workflow?\",\\n  \"Where can I find additional information about common pitfalls in machine learning practices according to scikit-learn?\"\\n]', '5defbfd0459fcbdd00b737193ac4c11e': '[\"Why do I get an accuracy of 1.0 in HW3Q4?\", \"Have I overfitted my model if accuracy is 1.0?\", \"What might cause 1.0 accuracy in my classification problem?\", \"Should I drop msrp/price if my accuracy is too high?\", \"What action can help if my model\\'s accuracy is perfect?\"]', 'efa750046cb16c49c9e31d81fcbb21fe': '[\\n  \"What packages are recommended for calculating the Root Mean Squared Error in classification tasks?\",\\n  \"Where can I find a code example for computing RMSE using Python libraries?\",\\n  \"Can you point me to a resource that includes code for calculating RMSE?\",\\n  \"Who contributed the information about using sklearn and numpy for RMSE calculation?\",\\n  \"Is there a custom function example for computing RMSE shared in the course materials?\"\\n]', '84dda2be2026ba7f0d0305d100572c2c': '[\\n  \"What should I use instead of \\'get_feature_names\\' with DictVectorizer?\",\\n  \"Which method replaces \\'get_feature_names\\' for DictVectorizer?\",\\n  \"How do I resolve the AttributeError for DictVectorizer regarding \\'get_feature_names\\'?\",\\n  \"What is the correct attribute for obtaining feature names from DictVectorizer?\",\\n  \"Where can I find more details about using \\'get_feature_names_out\\' with DictVectorizer?\"\\n]', '565ea1d0fe1d49918f7ce7b27540426f': '[\\n  \"How can I calculate RMSE using sklearn without involving mathematical formulas or numpy?\",\\n  \"Is there an sklearn function that calculates Root Mean Squared Error directly?\",\\n  \"What is the purpose of the squared kwarg in the mean_squared_error function?\",\\n  \"How do I set up sklearn\\'s mean_squared_error function to obtain RMSE?\",\\n  \"Where can I find more information about RMSE calculation in Python libraries?\"\\n]', '8843a79b2c1a41adc0ee6e7d24c4cecb': '[\\n  \"What are some techniques used for encoding categorical variables in machine learning classification?\",\\n  \"Where can I find a resource to learn more about encoding methods for classification?\",\\n  \"Who authored the article that explains categorical variable encoding techniques?\",\\n  \"What is the title of the article mentioned in the course about encoding methods?\",\\n  \"Can you provide a link to the article on encoding techniques discussed in this section?\"\\n]', 'd108a7fdd1cd21bdff85da49d1efc5e6': '[\\n  \"What might cause a TypeError when using accuracy_score with sklearn in Jupyter?\",\\n  \"How can I fix a TypeError involving numpy.float64 with accuracy_score?\",\\n  \"Which library should I import to address errors with accuracy_score in sklearn?\",\\n  \"What is a common issue with code \\'accuracy_score(y_val, y_pred >= 0.5)\\'?\",\\n  \"Can you suggest a corrected approach to using accuracy_score with thresholding?\"\\n]', '38f82e48a25a51073b147593b36161cc': '[\\n  \"Where can I find the homework for Week 4 in the course?\",\\n  \"What resource provides a complete list of all homework assignments for the course?\",\\n  \"Is there a document available for evaluation metrics in the course?\",\\n  \"Where can I access the theory related to evaluation metrics on GitHub?\",\\n  \"Can you provide a YouTube link for video lectures on Week 4\\'s evaluation metrics?\"\\n]', '8bb1eefd0b17fc14b53c8c3751e321ed': '[\\n  \"Where can I find more information about using a variable to score in evaluation metrics for classification?\",\\n  \"Are metrics applicable to a series or dataframe in classification evaluation?\",\\n  \"Is there a link to a Slack discussion about scoring with a variable?\",\\n  \"Who provided information on metrics usage for series and dataframes?\",\\n  \"Does the evaluation metric section cover using a variable for scoring?\"\\n]', '6c474e0fac88190e328bdc1fbd9ee57e': '[\\n   \"Why should we set random_state when shuffling datasets for consistent results?\",\\n   \"In what situations is random_state necessary for dataset shuffling?\",\\n   \"Why is random_state important for ensuring reproducibility in data processing?\",\\n   \"How does setting random_state affect the reproducibility of model evaluation?\",\\n   \"Why do we need both random_state and shuffle parameters set for dataset consistency?\"\\n]', 'deac6ed567c687ca8461c779db9f0a5e': '[\\n    \"How can I obtain precision, recall, and f1 score at once?\",\\n    \"What tool provides comprehensive classification metrics including accuracy?\",\\n    \"How to efficiently access multiple classification metrics together?\",\\n    \"Which sklearn function generates a full classification metric report?\",\\n    \"Where can I find detailed information on calculating classification metrics?\"\\n]', '9ecfe1ad05495eeff16ec98e5ec6842e': '[\\n    \"How should I handle multiple thresholds resulting in the same F1 score?\",\\n    \"Is picking the lowest threshold a valid method when I have multiple ones with the same F1 score?\",\\n    \"What library can I use to verify the F1 score results from my code?\",\\n    \"Which tool can provide precision, recall, and F1-score reports for evaluation?\",\\n    \"Should I verify my results with standard libraries or rely entirely on my own calculations?\"\\n]', 'cc6b86ba6b9ecc380c2dffef8f95b673': '[\\n    \"Why does the model require at least two classes to work properly?\",\\n    \"What does the error mean when it mentions samples of at least 2 classes?\",\\n    \"How can I fix the issue if my data only contains one class, such as 0?\",\\n    \"What transformation might cause a column to contain only 0\\'s in a dataset?\",\\n    \"What should I do if I encounter a ValueError related to data containing only one class?\"\\n]', 'fd7b6f4378621548db034a2d5e1ff20b': '[\\n    \"How can I generate attractive reports for classification models?\",\\n    \"What tool integrates scikit-learn and matplotlib for visualizations?\",\\n    \"Which library provides vivid classification model visualizations?\",\\n    \"How does one create colorful classification reports?\",\\n    \"What solution uses Yellowbrick to enhance classification reports?\"\\n]', '1aac108535145469ec3aad9b89f12c12': '[\"Why doesn\\'t my homework match the exact result?\", \"Is it acceptable if my homework\\'s result is not precise?\", \"Should my homework answer match exactly?\", \"What if my homework result isn\\'t an exact match?\", \"Will my homework be correct if the result isn\\'t exact?\"]', 'a6e27afffa1708122b15a86c9fc6a94a': '[\"How can I evaluate feature importance for numerical variables?\", \"Which metric from the 2021 course helps determine feature importance?\", \"Where can I find solutions related to feature evaluation in the 2021 course?\", \"What score should be used to assess feature importance in numerical data?\", \"Can roc_auc_score be used for evaluating numerical feature importance?\"]', '11656496eed7b6a0230a95ce86b8864e': '[\"How do you use numerical values as scores when computing AUC?\", \"What parameters does sklearn\\'s roc_auc_score function require?\", \"Which parameter is passed the numerical value when using roc_auc_score?\", \"Which parameter represents the target variable in roc_auc_score?\", \"What does roc_auc_score compute using y_true and numerical values as y_score?\"]', '5bbf3a9ac33f92e9acbf64fbabab5ca2': '[\\n    \"Which dataset is required for calculating metrics in later sections?\",\\n    \"For accuracy calculations in Question 3, which dataset is relevant?\",\\n    \"Is there a specific dataset for evaluating classification tasks?\",\\n    \"What is the dataset designation for metric computation in Question 3?\",\\n    \"When performing evaluations, which dataset should be utilized?\"\\n]', 'ded1ec3f6b87f390edaa70ae30a59744': '[\\n    \"Why does placing KFold inside or outside the loop not affect the results?\",\\n    \"What role does the random_state parameter play in KFold?\",\\n    \"Does changing random_state always affect the results with KFold?\",\\n    \"Why is it better to generate the KFold object before the loop?\",\\n    \"How should we loop through different values of C using KFold?\"\\n]', 'a98f89e6d0b56eb328c20b3b54570f5f': '[\\n  \"What error might occur if I incorrectly pass parameters to roc_auc_score?\",\\n  \"What is the correct way to pass parameters to roc_auc_score to avoid a multi_class error?\",\\n  \"Which function throws a ValueError related to the parameter multi_class?\",\\n  \"What mistake can lead to a ValueError when using roc_auc_score for classification evaluation?\",\\n  \"When evaluating feature importance, how should I pass parameters to roc_auc_score?\"\\n]', 'efe6dabef6b46fde18b1651049a05e71': '[\\n    \"Which module can be used to track progress of code execution visually?\",\\n    \"How can you monitor the waiting time and progress of code execution in a terminal?\",\\n    \"What tool provides a terminal progress bar feature for tracking code progress?\",\\n    \"Which Python library offers a way to display progress bars for code execution?\",\\n    \"In which section can you find information related to metrics for evaluating classification models?\"\\n]', '06c365650094a70d3945a1cb23e5a64b': '[\\n  \"Why should I invert variables with low ROC AUC scores?\",\\n  \"How does negating variables improve feature importance?\",\\n  \"When is it beneficial to invert variables in a model?\",\\n  \"What effect does inverting variables have on correlation?\",\\n  \"Why align variable direction with algorithm expectations?\"\\n]', '48878bd932f685d909627ffbae7a7244': '[\\n  \"What does predict(X) provide in terms of classification results?\",\\n  \"How can using predict(X) impact evaluation metrics?\",\\n  \"What advantage does predict_proba(X)[:, 1] offer over predict(X)?\",\\n  \"Why might predict_proba(X)[:, 1] lead to better evaluation values?\",\\n  \"Who mentioned that predict_proba shows probabilities per class?\"\\n]', '4b2aa348c70eb0d3846a2fad82f54446': '[\\n    \"What happens to the FPR and TPR if the threshold is set to 1.0 in a binary classification model?\",\\n    \"Why does the condition for belonging to the positive class not get satisfied when the threshold is 1.0?\",\\n    \"How does the value range of the sigmoid function affect TPR and FPR at a threshold of 1.0?\",\\n    \"Why are there no positive predicted values when the threshold equals 1.0 in this scenario?\",\\n    \"Why can\\'t the sigmoid function reach the value of 1, affecting the classification at this threshold?\"\\n]', '24bd0704cf5d7de5416aced916cfa18d': '[\"What method can I use to add labels and arrows to a graph in Matplotlib?\", \"How can I point to a specific X, Y location on a graph with an arrow and text?\", \"Which library offers a feature to annotate graphs with an arrow and text?\", \"How do I display an optimal threshold and F1 score on a graph?\", \"What is an example of annotating a graph at an optimal threshold point in Matplotlib?\"]', '18876cd801be054ee5f20b437aa3ac11': '[\\n  \"What should I do if I\\'m struggling to understand the ROC curve?\",\\n  \"Is it essential to fully grasp the ROC curve before moving on?\",\\n  \"Should I watch additional resources if I don\\'t get the ROC curve?\",\\n  \"Can I progress in the course without understanding the ROC concept?\",\\n  \"Why is the ROC AUC important in binary classification models?\"\\n]', 'cdf6ad94a5cc7f9210f1d67a98034582': '[\\n  \"How does the method of splitting data affect the accuracy values in my homework?\",\\n  \"Why does the same data split ratio still lead to different training sets?\",\\n  \"What train/validation/test split is recommended for consistency with the course material?\",\\n  \"How do the two data splitting options differ if they achieve the same ratio?\",\\n  \"What role does random state play in data splitting and accuracy results?\"\\n]', 'a6a516391147d3f73984d78d3274c086': '[\"What method can be used with numpy to find where precision and recall curves intersect?\", \"Which numpy functions are essential for determining the intersection of precision and recall?\", \"How can I identify the indices where precision and recall have the same value using numpy?\", \"What is the meaning of np.diff and np.sign when comparing precision and recall?\", \"How can I print the threshold at which precision and recall curves meet in my dataset?\"]', '25ad560e0b1afeeab14274692a6a9c7c': '[\\n    \"How can I calculate precision, recall, and F1 Score in Python?\",\\n    \"Is there a way to compute evaluation metrics without manually defining true positives or false negatives?\",\\n    \"Which library provides functions for computing precision, recall, and F1 Score?\",\\n    \"What parameters should be passed to precision_score in scikit-learn for binary classification?\",\\n    \"Where can I find a demonstration of calculating precision and recall in your course?\"\\n]', '6db5fd7b10504fcce8b510ff7f203306': '[\"What is the purpose of using cross-validation in model evaluation?\", \"Can you explain how cross-validation splits the dataset for model training and validation?\", \"How does a smaller \\'C\\' value affect the regularization and decision boundary in a model?\", \"What impact do larger \\'C\\' values have on the overfitting of a model?\", \"In which models is the hyperparameter \\'C\\' commonly associated with regularization?\"]', '37ab06961acdb2efa1e7e62434e75959': '[\\n  \"What library can be used for easily computing model evaluation metrics?\",\\n  \"How does using scikit learn metrics compare to using numpy and pandas for model evaluation?\",\\n  \"Could you name some metrics available in scikit learn for model evaluation?\",\\n  \"How would you calculate the accuracy score using scikit learn?\",\\n  \"What is the benefit of using scikit learn for model evaluation metrics?\"\\n]', 'dbf3a73e6b6e0f698ca58f5e3b3adffe': '[\"Is there a Scikit-learn method for Precision, Recall, and F1?\", \"Can precision_recall_fscore_support compute Precision and Recall?\", \"What library provides precision_recall_fscore_support?\", \"How can I use Scikit-learn for F1 score calculation?\", \"What is a method in Scikit-learn to support precision and recall calculations?\"]', '155490b1ed5b75e96684788363cf3573': '[\\n    \"What kind of datasets make precision-recall curves more suitable than ROC curves?\",\\n    \"Why are ROC curves considered less reliable for datasets with class imbalance?\",\\n    \"How are ROC curves affected by changes in the positive to negative instance ratio?\",\\n    \"Why do ROC curves offer an optimistic picture of the model on imbalanced datasets?\",\\n    \"What metrics change when class distribution in a dataset changes, even if classifier performance remains the same?\"\\n]', '803e6fb0a12a99c4fedd2bfed62d2af3': '[\\n    \"How can we assess the significance of numerical features using AUC in classification?\",\\n    \"What function allows us to determine AUC for numerical features?\",\\n    \"Which module offers the roc_auc_score to measure feature significance?\",\\n    \"To calculate AUC for numerical data, what arguments does roc_auc_score require?\",\\n    \"How does roc_auc_score help in evaluating numerical feature importance?\"\\n]', 'f5d50dd68ab7579102300c848c3e4547': '[\\n  \"How does the F-score relate to class imbalance?\",\\n  \"Why is it challenging to compare F-scores across different problems?\",\\n  \"Which metric depends on the ratio of positive to negative test cases?\",\\n  \"What should be used to make F-score comparisons more reliable?\",\\n  \"How should differing class ratios be addressed in comparisons?\"\\n]', 'fa33b290538a559cbdfb0567656a8db1': '[\\n  \"How can I plot a Precision-Recall curve using scikit-learn?\",\\n  \"What is the method for visualizing Precision and Recall for a classifier?\",\\n  \"How do I generate a Precision-Recall graph with scikit-learn?\",\\n  \"What steps can I take to plot a Precision-Recall curve in Python?\",\\n  \"How can I use scikit-learn to visualize classifier performance with a Precision-Recall curve?\"\\n]', 'ca52cb39a8c7fe4e61e5ddacc38c6ce9': '[\"What is the importance of Stratified k-fold in multiclass classification?\", \"How does Stratified k-fold help in maintaining class balance?\", \"Where can I find the implementation of Stratified k-fold?\", \"Which library provides Stratified k-fold functionality for data splitting?\", \"What ensures that folds have similar class distribution in multiclass settings?\"]', '6d246a3c374185ecba2faa2df8aa3f35': '[\\n    \"Where can I find the homework for Week 5 of the machine learning course?\",\\n    \"Is there a resource that consolidates all the homework assignments from this course?\",\\n    \"Where can I access the solution for Homework 3 in this machine learning program?\",\\n    \"Which document should I refer to for the evaluation metrics of this course?\",\\n    \"Is there a YouTube video related to section 5 of the \\'Deploying Machine Learning Models\\' topic?\"\\n]', '6a145bb6b034831ea166fe7f5b8b1695': '[\\n    \"What should I do if I encounter errors with the default environment like WSL or Ubuntu in week 5?\",\\n    \"How can I set up my environment for the homework in week 5?\",\\n    \"Is there a guide for creating an AWS EC2 instance for our course homework?\",\\n    \"Are AWS instances free for use while working on course assignments?\",\\n    \"Where can I find alternatives to setting up my environment for the course?\"   \\n]', '9f2cc906abb3b908f97d0870450a64d3': '[\\n  \"What initial step should I take to download data with the Kaggle API in Jupyter Notebook?\",\\n  \"Where do I place the kaggle.json file in relation to my Jupyter Notebook?\",\\n  \"What command should I use to set permissions on the kaggle.json file?\",\\n  \"How do I configure the environment variable for the Kaggle configuration directory?\",\\n  \"What command allows me to download and extract a dataset from Kaggle in Jupyter Notebook?\"\\n]', '207171c5bdef404a3dae2db4d6881db5': '[\\n    \"What command is used to navigate one directory up in Ubuntu?\",\\n    \"How can I view the list of directories and files in my current location?\",\\n    \"Which command allows me to switch to a different directory using its path?\",\\n    \"How do I identify my current working directory?\",\\n    \"What command opens a text file for editing in Ubuntu?\"\\n]', 'a828b5624b2758d4633d3948a0cbc9a2': '[\"How can I verify which version of Python is installed on my laptop?\", \"What is the recommended way to download Python 3.10 or higher on Windows?\", \"During Python installation, which option should I check to ensure smooth operation on Windows?\", \"How do I update my existing Python installation using the command line?\", \"Where can I find the official downloads for the latest Python versions?\"]', 'b8a614d2fe2742140eaf1886b120ef84': '[\\n  \"What are the steps to set up WSL on a Windows system?\",\\n  \"Where can I find instructions to install WSL on Windows 10 and 11?\",\\n  \"How can I confirm that the \\'Virtual Machine Platform\\' feature is activated for WSL setup?\",\\n  \"What should I do when my password does not appear while setting up WSL?\",\\n  \"How can I change the default starting folder in the Ubuntu terminal within WSL?\"\\n]', '154d4fb88393be4eab281ecae1e4dfc6': '[\\n  \"What specific error message might I encounter when building a Docker image on a Mac with an M1 chip?\",\\n  \"How can I resolve the error that states there is no such file directory on Mac M1 when building a Docker image?\",\\n  \"What modification needs to be made to the Dockerfile to successfully build a Docker image on M1 silicon?\",\\n  \"Which file should I edit to fix the Docker image build error on my Mac M1 silicon?\",\\n  \"How much time should I expect to spend when successfully building the Docker image after fixing the configuration on Mac M1?\"\\n]', 'd808bb99fbf16a866ead50d5630aff0a': '[\\n    \"How can I determine the version of Python libraries installed in a Jupyter Notebook?\",\\n    \"Is there a way to check the installed version of a library using Jupyter Notebook?\",\\n    \"What steps should I follow to find the version of an installed Python library in Jupyter?\",\\n    \"Could you guide me on identifying the version of Python packages in a Jupyter environment?\",\\n    \"How do I verify the version of a Python library within Jupyter Notebook?\"\\n]', '5e19a4b2ddf84f43a32336e694d84326': '[\"What should I do if I cannot connect to the Docker daemon while deploying a model?\", \"How can I resolve the Docker daemon connection error on WSL when deploying machine learning models?\", \"What steps should I take if I encounter a Docker error during installation on Linux?\", \"How do I fix the issue of not being able to run Docker\\'s hello-world on my machine?\", \"What is the solution for the Docker daemon error related to unix:///var/run/docker.sock?\"]', '844707d4d50276428a66087068291a13': '[\\n    \"What should I do if I encounter a non-zero code error after using \\'docker build -t churn-prediction .\\'?\",\\n    \"How can I find the Python version installed in my system to fix the Docker image creation error?\",\\n    \"Why does changing the Python version in the Dockerfile help resolve the non-zero code error?\",\\n    \"What is the correct format for specifying the Python version in the Dockerfile to avoid build errors?\",\\n    \"Who provided the solution for fixing the Docker image error related to the Python version?\"\\n]', '792e926546302ecf2272fe3b3aecbfca': '[\\n    \"What should I do if I encounter errors while trying to install sklearn with pipenv?\",\\n    \"How can I successfully install Scikit-Learn version 1.0.2 for my homework?\",\\n    \"Which version of sklearn was used in lectures, and did it install correctly?\",\\n    \"What is the correct command to install Scikit-Learn version 1.3.1?\",\\n    \"Why is it necessary to use the full name of sklearn for version 1.0.2?\"\\n]', 'c5fe16723eb15047f9a26ac547718f1a': '[\\n    \"Why is it important to use the --rm flag when running docker containers during development?\",\\n    \"What happens if we don\\'t remove docker containers after they have stopped running?\",\\n    \"Why should we distinguish between docker images and containers in our ML deployment process?\",\\n    \"When and why should we rebuild a docker image while deploying models?\",\\n    \"What are the implications of not specifying a version tag while building a docker image?\"\\n]', 'be097c735f03c1a1b208e72b8af212f1': '[\\n    \"What is a common mistake that might lead to an error when building a Docker image?\",\\n    \"Why is my Docker build failing when trying to read the Dockerfile?\",\\n    \"What should I name my Dockerfile to avoid build errors?\",\\n    \"How can I ensure the Dockerfile is recognized when building an image?\",\\n    \"What file extension should I avoid when naming my Dockerfile?\"\\n]', 'bb81a7bcb52a844074a8d6415a35f392': '[\"How can I install Docker on a Mac?\", \"Where can I find instructions to install Docker on MacOS?\", \"Is there a difference between installing Docker on an Apple chip versus an Intel chip?\", \"What steps do I need to follow to set up Docker on my Mac?\", \"Where should I go to find the Docker installation guide for MacOS?\"]', '9a1c26f6bae6bec492ee2ec374ef05d1': '[\"What should I do if I encounter an error with the docker pull svizor/zoomcamp-model command?\", \"Why am I receiving a \\'manifest not found\\' error when using docker pull?\", \"How can I resolve the \\'manifest unknown\\' error in Docker?\", \"What is the solution if Docker defaults to the latest tag for an image?\", \"Who contributed the solution for the docker pull error in the record?\"]', '2cea86e22c08413e2a7c93ecc4da009d': '[\\n    \"How can I view the size of a specific Docker image using a command?\",\\n    \"What command should I use to list all information for local Docker images?\",\\n    \"Is there an option to format the output to show only the size of a specified image?\",\\n    \"What are the alternative commands for dumping specific information about a Docker image?\",\\n    \"Can I retrieve information for just one Docker image instead of all local images?\"\\n]', 'c4059020b8c86385c133680dc95f071c': '[\\n  \"How does pipenv determine the name for a virtual environment it creates?\",\\n  \"What is the location where pipenv stores virtual environments on Windows?\",\\n  \"Where are pipenv environments created on OSX/Linux systems?\",\\n  \"What must be done to activate a pipenv environment after it is created?\",\\n  \"What is the naming convention for the folder of a pipenv environment?\"  \\n]', '61d50f53fe4c63a2cd0ec015a612e7af': '[\\n    \"How can I troubleshoot a Docker container?\",\\n    \"What command allows interactive access to a running Docker container?\",\\n    \"How do I run a Docker image with a bash command by default?\",\\n    \"What steps should I follow to execute a bash session inside a running Docker container?\",\\n    \"What is the process for access if a Docker container is already active?\"\\n]', '6f45fff70e012e3d61fb337dca42f479': '[\"What solution is suggested if the input device is not a TTY when running docker?\", \"Which tool should you use to fix the TTY issue on Windows when using GitBash?\", \"What is one function of a TTY?\", \"What is the purpose of Winpty on Windows?\", \"Where can I find more information about terminal and shell differences?\"]', 'ee65666b0720f3844f1fd364a1e02aec': '[\\n  \"What error message indicates that the \\'model2.bin\\' file is missing when deploying a machine learning model?\",\\n  \"What files did the original setup include when attempting to deploy the model?\",\\n  \"Where did the error occur when trying to load \\'model2.bin\\' and \\'dv.bin\\'?\",\\n  \"What temporary fix was suggested for the \\'failed to compute cache key\\' error during model deployment?\",\\n  \"Who provided the solution for the error encountered with \\'model2.bin\\' in the deployment process?\"\\n]', 'fb15e5735f6e8bf5d41909ab87b74e98': '[\\n    \"How can I resolve the issue of not writing dependencies to pipfile and piplock file?\",\\n    \"What steps should I take if I fail to add dependencies to pipfile and piplock?\",\\n    \"Can you suggest a solution for when the dependencies won\\'t write to pipfile and piplock?\",\\n    \"What is the recommended command to handle pipfile and piplock dependency failures?\",\\n    \"How do I manage dependencies if writing to pipfile and piplock fails?\"\\n]', '602ef70a7d39c55317ff4578f5301cc7': '[\\n  \"Why am I seeing an error with the f-string after importing pickle?\",\\n  \"What is the issue with using parentheses instead of curly braces in f-strings?\",\\n  \"How should the correct f-string format look for \\'model_C\\'?\",\\n  \"What did Sriniketh notice about my use of parentheses with pickle.dump?\",\\n  \"What is the correct syntax for using pickle.dump with two variables?\"\\n]', '59223c7494a81f02c374cfe9b07299a9': '[\"Why might \\'pipenv\\' be unrecognized as a command in Windows?\", \"How can I solve the \\'pipenv\\' recognition error in Windows?\", \"What steps should I follow to edit the PATH for \\'pipenv\\' in Windows?\", \"Why is using Anaconda suggested over pipenv on Windows?\", \"Which two directory locations should be added to PATH for \\'pipenv\\' access on Windows?\"]', '3dd44f9019eb41c373135c8b5e57b708': '[\\n    \"What should I do if I encounter an AttributeError related to \\'MutableMapping\\' when using pipenv?\",\\n    \"How can I resolve the \\'collections\\' module error shown during the library installation?\",\\n    \"Which Python version should I use to avoid MutableMapping errors during deployments?\",\\n    \"Is there a solution in week-5.6 of the course for the library installation error with pipenv?\",\\n    \"Why is using Python 3.10 causing an attribute error in my deployment?\"\\n]', '50cfac15bacd8a12f0f5df083ee9ced5': '[\"How can I resolve the ValueError related to path not found or generated on Windows?\", \"What should I do if I face errors when installing packages after using pipenv shell?\", \"How can I fix PATH issues when using pipenv on Windows?\", \"What are the terminal commands for fixing VIRTUAL_ENV on Windows and Unix?\", \"How can manually re-creating a removed folder help with my ValueError issue?\"]', '6570f58890d0000f3ad14dd0eb689c83': '[\\n    \"How can I resolve a ConnectionError due to a RemoteDisconnected issue when deploying a model?\",\\n    \"What should I modify in the flask app to fix a Remote end closed connection error?\",\\n    \"How can setting the host to ‘0.0.0.0’ help with connection issues in deployment?\",\\n    \"What change is needed in the dockerfile to fix a connection abort error during deployment?\",\\n    \"What URL should I use locally to run the app after setting up the host properly in Flask?\"\\n]', 'd792e61efa896b0ba26aa604ae93e99c': '[\\n  \"What is the recommended type of quotes to use around filenames in Dockerfiles to avoid errors?\",\\n  \"How can I resolve a Docker build error related to file copying?\",\\n  \"What might cause an error during the file copying step in a Docker build?\",\\n  \"Which quotes should be avoided in Docker filenames to prevent build issues?\",\\n  \"What adjustment is necessary for correcting the Docker COPY error due to quoting?\"\\n]', '82129c3c40523f54e82e9c42d2f6eec1': '[\\n    \"What should I do if I encounter an error with Pipfile during Docker container installation?\",\\n    \"How can I fix Pipfile.lock issues when deploying in Docker?\",\\n    \"What is a recommended command to resolve Pipfile installation errors in Docker?\",\\n    \"Is there a command to update Pipfile.lock when deploying machine learning models?\",\\n    \"What solution can I try if \\'pipenv lock\\' does not resolve the error during installation?\"\\n]', '87b18641a7e97953d5db0ed42939755d': '[\"What could cause an error after executing the Docker run command in my project?\", \"Why might it not be possible to remove an orphan container during deployment?\", \"Which command lists all running docker containers in a terminal?\", \"What steps should be taken to stop and remove a problematic container in Docker?\", \"How can I successfully serve a test script to an endpoint using Docker after an error?\"]', '366421de718a96cf8bb9f34a5e32312f': '[\\n  \"What might cause the error message stating that the port for 0.0.0.0:9696 is already allocated when deploying a machine learning model?\",\\n  \"How can I resolve the error involving a failure to bind for 0.0.0.0:9696 due to port allocation?\",\\n  \"What error might occur if 0.0.0.0:9696 is considered already in use when rebuilding a Docker image?\",\\n  \"Which command should be executed to address port allocation issues on Docker during model deployment?\",\\n  \"Where can I find more information regarding the port allocation issue in Docker when deploying machine learning models?\"\\n]', '0711e4ba64a1d333ec786c78f3330be3': '[\\n  \"What error appears on the client side when attempting to bind to 127.0.0.1:5000?\",\\n  \"What is the server-side error associated with gunicorn?\",\\n  \"What command runs smoothly from the server side?\",\\n  \"What is the recommended IP address to use instead of 127.0.0.1:5000 to avoid connection issues?\",\\n  \"Who provided the solution for the connection error?\"\\n]', '561cc6ba0ceb212861a37f4ed576b0e8': '[\\n  \"How can I install md5sum on a Mac for machine learning purposes?\",\\n  \"What command should I use to install md5sha1sum using brew on MacOS?\",\\n  \"How do I verify if two files have the same hash with md5sum?\",\\n  \"Can you explain how to use md5sum to check file integrity on Mac?\",\\n  \"What is the command to run md5sum on MacOS for file comparison?\"\\n]', '1020bfc6e8d7f5d046ac334172d018a9': '[\\n  \"How do I execute a Python script without stopping a web-server that\\'s already running?\",\\n  \"What method allows me to run a new script while my server is up in the terminal?\",\\n  \"Can I launch another script if my web-server is active, and how?\",\\n  \"What steps should I take to make a request to a running server using another script?\",\\n  \"Is it possible to run a Python script that communicates with my server without shutting down the current terminal session?\"\\n]', 'f80bf69df3d7fa738354986cd6992b31': '[\\n    \"What warning might appear when using different Scikit-Learn versions in pipenv?\",\\n    \"How do I fix a version conflict warning involving DictVectorizer in pipenv?\",\\n    \"What should I check if my model breaks when deploying with pipenv?\",\\n    \"Why is matching Scikit-Learn versions important during deployment in pipenv?\",\\n    \"What might cause invalid results when running gunicorn in pipenv shell?\"\\n]', 'c6a2405883a6cc7012a9e4491c0bfeb4': '[\\n  \"Why do I receive a Python_version and Python_full_version error when running pipenv install?\",\\n  \"What should I do if I see a ValidationError related to python_version and python_full_version in Pipfile?\",\\n  \"How can I fix the conflict between python_version and python_full_version in my Pipfile?\",\\n  \"What are the steps to remove a python version error in Pipfile using nano editor?\",\\n  \"How do I create a Pipfile.lock after fixing a python version conflict in Pipfile?\"\\n]', 'c0031a068ad0c1338e76cebcb4110cd5': '[\\n  \"What should I do if I encounter an error about my Pipfile.lock being out of date during a Docker build?\",\\n  \"How can I rebuild the Pipfile.lock if it\\'s causing issues during deployment?\",\\n  \"What steps can I take if removing and rebuilding the Pipfile.lock doesn\\'t resolve the error?\",\\n  \"How do I remove the pipenv environment and Pipfile to address deployment errors?\",\\n  \"Which commands are used to delete the Pipfile.lock and recreate it before a Docker build?\"\\n]', 'e32e576dd75d0ae462c219c7bbfc892a': '[\\n  \"What is an alternative to gunicorn in a Conda environment when using Windows?\",\\n  \"What should you do if the mlflow server fails to run after several uses?\",\\n  \"How can you fix an issue where the mlflow server stops working with waitress?\",\\n  \"What steps are suggested when mlflow fails and you are using waitress?\",\\n  \"Is it necessary to reinstall waitress after resolving an mlflow server issue?\"\\n]', '13ca66af0f968a442fb044297afeade5': '[\\n    \"What should I do if I can\\'t find my environment on AWS after creating it locally?\",\\n    \"Why am I unable to locate my environment on AWS, and could the region be a factor?\",\\n    \"If I created an environment but it\\'s not visible on AWS, what troubleshooting steps can I take?\",\\n    \"Could being in the wrong AWS region affect finding my created environment?\",\\n    \"How do I ensure I\\'m in the correct AWS region to see my created environment?\"\\n]', 'efde23609818da55f3d038d6a9353c3f': '[\\n  \"What should I do if the \\'waitress-serve\\' command is not found when installing waitress using GitBash on Windows?\",\\n  \"How can I ensure that the \\'waitress-serve.exe\\' is recognized after installing it via pip in Jupyter Notebook?\",\\n  \"What steps should I follow to add \\'waitress-serve.exe\\' to GitBash\\'s PATH?\",\\n  \"Why do I get a warning when installing waitress with pip in a Jupyter notebook?\",\\n  \"What command should I use to modify the PATH variable in GitBash so it recognizes \\'waitress-serve.exe\\'?\"\\n]', '432181a5aa3326a5b05d2cf943503821': '[\"What does the warning \\'the environment variable LANG is not set\\' mean while deploying machine learning models in our course?\", \"Is the warning about the environment variable LANG being unset a critical issue when using Scikit-Learn in the ml-zoomcamp conda environment?\", \"What are the steps to address the warning about the unset LANG environment variable?\", \"Can I continue with the deployment of machine learning models without setting the LANG environment variable?\", \"Where can I find more information about fixing the LANG environment variable warning in a bash profile?\"]', '585556680fc532218c32484d6465d3f3': '[\"What image should be used for deploying models in Module5 HW Question 6?\", \"Which files are necessary for question 6 in Module 5?\", \"Who added the information about the image for question 6?\", \"What is the purpose of the svizor/zoomcamp-model:3.10.12-slim image?\", \"Where can I find the dictvectorizer for Module5 HW Question 6?\"]', '53366c049a2d14ba8da33fa99da3b22b': '[\\n  \"What terminal is demonstrated in week 5 videos?\",\\n  \"Where can I find the terminal used in section 5?\",\\n  \"Which terminal application is shown in the week 5 lectures?\",\\n  \"How can I access the terminal from the week 5 videos?\",\\n  \"Can you provide a link to the terminal discussed in section 5?\"\\n]', '497d3204cf6d98a0e0caf0e7c5de9034': '[\"What should I do if waitress-serve reports a Malformed application error with dashes in the file name?\", \"How can I resolve a ValueError caused by waitress when using a dash in the Python file name?\", \"Why does waitress-serve throw an error for Malformed application when my filename contains a dash?\", \"What causes Malformed application errors with waitress-serve and how can I fix it?\", \"How do I fix a Malformed application error in waitress-serve due to an issue with the filename?\"]', '8464d54ee0edae1da0805894bb1d6e5d': '[\\n    \"How can I quickly verify HTTP POST requests using the command line?\",\\n    \"What is the command to test HTTP POST requests with curl?\",\\n    \"Can I use curl to send a JSON payload in a POST request?\",\\n    \"Is it possible to test HTTP POST requests with curl on Windows using WSL2?\",\\n    \"Could you provide an example of sending JSON data with a curl command using echo and piping?\"\\n]', 'a0e9cc981fdb8f5e9fece448728310ea': '[\\n    \"What error occurs when using \\'eb local run --port 9696\\' with certain Docker platforms?\",\\n    \"How can I resolve the NotSupportedError when executing \\'eb local run\\'?\",\\n    \"What configurations should be done to fix the NotSupportedError for Docker platforms?\",\\n    \"Why could editing ‘.elasticbeanstalk/config.yml’ be a temporary solution for the error?\",\\n    \"Who contributed the solution for the NotSupportedError related to Docker platforms?\"\\n]', 'f91359a78eeaa2a789b3510ea9db6220': '[\"What should I include in the URL to avoid the \\\\\"No connection adapters were found\\\\\" error?\", \"Why does the absence of http:// in the URL cause a connection issue?\", \"How can using HTTP:// instead of http:// in a URL affect the connection?\", \"What does the requests library need to identify how to connect to a server?\", \"Why is it important for the protocol scheme in URLs to be in lowercase?\"]', '0acab16f3be10af4e2feeb1cfe3e23f9': '[\\n  \"Why might I get identical results running the Docker image?\",\\n  \"How can I ensure different outcomes when executing my prediction test?\",\\n  \"What should be checked if the Docker image yields the same prediction repeatedly?\",\\n  \"What could cause the model to produce identical results every time?\",\\n  \"How can changing the model affect prediction test outcomes?\"\\n]', 'd4e87080dc55c2be9024bc3b1251cfca': '[\\n  \"How can I resolve the issue of my Docker container not starting?\",\\n  \"What tool should I use to install the modules required for a Docker image?\",\\n  \"What should be included in the virtual environment when building a Docker image?\",\\n  \"Which module is essential for running a Docker container process successfully?\",\\n  \"Who provided the solution for the Docker container process issue?\"\\n]', '2520a5a75ebaca5fa3569209d9913312': '[\\n  \"What command is used to transfer files from a local machine into a Docker container?\",\\n  \"How can files be moved from my computer to a Docker container?\",\\n  \"Which command allows copying of local files to a running Docker environment?\",\\n  \"What is the basic syntax for moving a file to a Docker container?\",\\n  \"How do I specify the destination path when copying files to a Docker container?\"\\n]', 'e80b890fe2a2603a350689e3d5a42452': '[\"How can I transfer files from my local system into a Docker container\\'s working directory?\", \"What command is used to copy files into a Docker container from a local folder?\", \"What is the syntax for copying files into Docker using a Dockerfile?\", \"Which command aids in file transfer to a Docker container?\", \"How do you specify the files to copy over in a Dockerfile?\"]', '41c9d54dca60a67eb6e9ac64c3ec45ce': '[\\n    \"What should I do if I encounter a NotSupportedError when using \\'eb local\\' with AWS Elastic Beanstalk?\",\\n    \"How can I modify the command to successfully create an environment for the tumor-diagnosis-serving application?\",\\n    \"Is there a solution for the error that occurs when trying to run \\'eb local run --port 9696\\'?\",\\n    \"What alternative command can be used if the initial \\'eb init\\' command doesn\\'t work?\",\\n    \"Who provided the solution to the error encountered during the AWS Elastic Beanstalk deployment?\"\\n]', '15d5dbfd3db890303d72c10a9d20b31c': '[\\n  \"Why was there an error deploying the AWS ElasticBean environment?\",\\n  \"What files need to be included in the source bundle for AWS ElasticBean deployment?\",\\n  \"What steps can resolve the issue of missing Dockerfile in the AWS ElasticBean deployment?\",\\n  \"What specific files were forgotten that caused the AWS ElasticBean deployment to fail?\",\\n  \"How did Mélanie Fouesnard fix the AWS ElasticBean deployment error?\"\\n]', '0ec2b0ab972965f701879e8707b1ed45': '[\\n    \"Where can I find the homework for week 6 on Decision Trees and Ensemble Learning?\",\\n    \"Which resource contains solutions for the homework from week 4?\",\\n    \"How can I access all the homework assignments for this course?\",\\n    \"Is there a video resource available for week 6\\'s content?\",\\n    \"Where can I find a document with frequently asked questions about the course?\"\\n]', 'd9b24c009fa641e5bd61391366e51101': '[\"How can I extract training and validation metrics from XGBoost more straightforwardly?\", \"What parameter in XGBoost can help me track training progress and metrics?\", \"Is there a way to visualize training and validation AUC from XGBoost data?\", \"What lesson covers how to parse training and validation metrics from XGBoost output?\", \"Who contributed the information about using evals_result for XGBoost metrics?\"]', '9db04c3146607746b9f45d89057214fd': '[\\n  \"What object should I create to solve regression problems using random forest in scikit-learn?\",\\n  \"Is the sklearn.ensemble.RandomForestRegressor similar to any other scikit-learn object?\",\\n  \"Where can I find more information about sklearn\\'s random forest regressor?\",\\n  \"For regression problems in sklearn, should I use RandomForestRegressor or RandomForestClassificator?\",\\n  \"Who provided the information about solving regression problems with random forest in scikit-learn?\"\\n]', '27dec32eebbac5e1f15f9f11d2a72aaf': '[\\n  \"What is the solution to the ValueError regarding feature names containing special characters like [ or < when creating DMatrix?\",\\n  \"How can I fix the ValueError related to feature names with special characters during DMatrix creation?\",\\n  \"What steps should be taken to resolve a ValueError due to special characters in feature names in DMatrix?\",\\n  \"Can you provide an alternative method to handle the ValueError caused by special characters in feature names for DMatrix?\",\\n  \"What is Peter Ernicke\\'s suggested solution to handle special characters in feature names for DMatrix without affecting the equal sign?\"\\n]', 'c4df4ca30ff70e2119e8abccb6d80506': '[\\n  \"What should I do if I encounter a TypeError related to sequence of strings for feature names while training an xgboost model?\",\\n  \"How can I resolve the \\'Expecting a sequence of strings for feature names\\' error in xgboost?\",\\n  \"Why am I getting a \\'TypeError: Expecting a sequence of strings for feature names\\' when using xgboost?\",\\n  \"What is causing the TypeError about feature names in xgboost and how can I fix it?\",\\n  \"If feature names are in a numpy array instead of a list, what should I do to fix the error in xgboost?\"\\n]', '32b2a3b60c0055145b85cefc29c7a293': '{\\n    \"questions\": [\\n        \"What should I do if I encounter a TypeError regarding feature names while using xgb.DMatrix?\",\\n        \"How do I resolve a ValueError related to feature names containing invalid characters for XGBoost?\",\\n        \"Why does using dv.get_feature_names_out() for feature names in xgb.DMatrix lead to errors?\",\\n        \"What is the pattern to avoid when naming features for xgb.DMatrix to prevent ValueError?\",\\n        \"Who provided the solution for handling inappropriate symbols in feature names for xgb.DMatrix?\"\\n    ]\\n}', '8d39b7a4fa665044813dbc72826e2485': '[\\n    \"What is the command to install Xgboost in a Jupyter notebook?\",\\n    \"How can I update my pip version to 21.3 or higher?\",\\n    \"Where can I find more information about Xgboost?\",\\n    \"Who is the author of the instruction on installing Xgboost?\",\\n    \"Is there any specific pip version required for installing Xgboost?\"\\n]', 'a8426da17557492761c080b5470d1eec': '[\"What does eta refer to in XGBoost\\'s hyperparameters?\", \"How does the learning rate affect XGBoost\\'s model training?\", \"In the context of XGBoost, what is the purpose of tuning the learning rate?\", \"Why is adjusting eta important in gradient descent within XGBoost?\", \"How does eta influence the process of finding minimum weights in XGBoost?\"]', 'cb30043b27daf6cd6d9ac3b81ea0dca4': '[\"How does Random Forest demonstrate the concept of bagging in ensemble learning?\", \"What methodology does XGBoost employ to implement boosting?\", \"Why is boosting more prone to overfitting compared to bagging?\", \"How are predictions combined in bagging when dealing with regression tasks?\", \"What role do weights play in the boosting algorithm like XGBoost?\"]', '6af8910a834fa11636fc06208de23d91': '[\\n    \"How can I capture stdout for each iteration of a loop separately without running the cell multiple times?\",\\n    \"Is there a way to directly capture xgboost training output for different eta values into a dictionary?\",\\n    \"Which IPython module can be used to capture output from a loop in a Jupyter Notebook?\",\\n    \"What is the purpose of using the magic cell command \\'%%capture output\\' in a Jupyter Notebook?\",\\n    \"How can I ensure separate outputs are stored in a dictionary when running a loop in Jupyter Notebook?\"\\n]', '4e31bc15b1e447cea1d3313959451276': '[\"What causes a ValueError due to unsupported continuous format in decision trees?\", \"How can I fix the ValueError caused by roc_auc_score() when calculating AUC?\", \"What should be the order of arguments for roc_auc_score() to avoid errors?\", \"What argument order should I use for roc_auc_score() to get AUC correctly?\", \"How do I resolve the error when roc_auc_score() is called with arguments in the wrong order?\"]', '1d07dceb20e3a806e75233ea5bbed2ba': '[\\n  \"What should I do if my RMSE starts to increase before decreasing to its lowest value?\",\\n  \"If RMSE initially increases, should I note the n_estimators when it first rises or when it is lowest overall?\",\\n  \"How can I identify when RMSE stops improving during ensemble learning?\",\\n  \"In Decision Trees, when RMSE varies, should I focus on the lowest RMSE or an initial increase?\",\\n  \"Should I record the n_estimators count at the lowest RMSE point or when it first increases?\"\\n]', 'd1c6c2fbe088be06c9cc9381b9a25db9': '[\\n  \"How can I visualize decision trees in Python using Graphviz?\",\\n  \"What is the method for exporting decision tree structures in Sklearn?\",\\n  \"Can you explain how to plot decision trees with feature names?\",\\n  \"What tool can I use to display a decision tree with filled nodes?\",\\n  \"How do I include feature names when plotting a tree in Sklearn?\"\\n]', '39a837d5e05f091a6e43ccc158df3f2b': '[\\n  \"What error might I encounter if I mistakenly use DecisionTreeClassifier for a regression task?\",\\n  \"How can I resolve the ValueError related to an unknown label type in decision trees?\",\\n  \"Why would using DecisionTreeClassifier cause a ValueError for continuous labels?\",\\n  \"When should I use DecisionTreeRegressor instead of DecisionTreeClassifier?\",\\n  \"How can checking the type of decision tree I need help avoid common errors?\"\\n]', '96db1737e1def888edc68cbce43ed19c': '[\\n  \"Why do I get different AUC values each time I re-run the DecisionTreeClassifier code?\",\\n  \"How can I achieve consistent AUC results when re-running my code for decision trees?\",\\n  \"What could cause varying AUC scores in the DecisionTreeClassifier between reruns?\",\\n  \"Is there a way to avoid fluctuations in AUC when using DecisionTreeClassifier?\",\\n  \"How can I stabilize AUC values over multiple runs of DecisionTreeClassifier?\"\\n]', '09c6f646dc6a6488242a712ca9305cbc': '[\\n  \"Why is there less typing if we let the script create the server instead of running gunicorn directly?\",\\n  \"What are the benefits of using a Python script to start a server compared to running gunicorn directly?\",\\n  \"Is there any functional difference between using a Python file and running gunicorn directly to create a server?\",\\n  \"Does using a script to create a server involve more or less typing compared to running gunicorn manually?\",\\n  \"How do the processes of letting a Python file create a server and running gunicorn directly compare in practice?\"\\n]', 'ea506af813490109f8566532ad670067': '[\\n  \"What should I do if I can\\'t import the \\'ping\\' module when running an example from the video?\",\\n  \"How can I successfully import the \\'ping\\' function in my code?\",\\n  \"What import statement should I use if \\'import ping\\' is unsuccessful?\",\\n  \"What is a solution if I encounter a \\'No module named ping\\' error?\",\\n  \"Who provided the solution for importing \\'ping\\' when having issues?\"\\n]', '4c7e4365fa4b8869cfd452b85b7be923': '[\\n  \"How can I obtain feature names when using DictVectorizer for one hot encoding?\",\\n  \"What function does DictVectorizer offer to get the feature names after fitting predictor and response arrays?\",\\n  \"What format does the get_feature_names_out() function return the feature names in, and is there a way to convert it?\",\\n  \"What is required before accessing feature names with DictVectorizer?\",\\n  \"Why might you need to convert the output of get_feature_names_out() to a list?\"\\n]', 'b4d77e3c5cc5b68cca43d17bab4bf30b': '[\\n    \"What difference does it make if we let the Python script initiate the server?\",\\n    \"Is it okay to execute gunicorn directly instead of letting the script handle it?\",\\n    \"Does executing the server through a script change the functionality?\",\\n    \"Is there any advantage to typing less when starting the server script?\",\\n    \"Are there any implications of letting the script manage server initiation?\"\\n]', '8c68ab5778d085d80d22fdd9281cbed1': '[\\n\"What causes a ValueError related to feature names in decision trees?\",\\n\"How can I fix feature names with unsupported characters such as \\'<\\'?\",\\n\"What is the solution for replacing problematic characters in feature names?\",\\n\"Is there a way to ensure feature names only contain supported characters?\",\\n\"What code can I use to handle feature names with special characters?\"\\n]', '1638ccc4263903c3017e64ab0395744b': '[\"How can we determine which features are important using a chart?\", \"What steps should we follow to extract and sort feature importances?\", \"How do we create a bar chart to display feature importance?\", \"What is a recommended layout size for the chart based on feature importance?\", \"Which Python library is suggested for plotting the horizontal bar chart?\"]', '355cecc82ea80f633a225c12a811a67a': '[\\n  \"How can I compute RMSE without using np.sqrt()?\",\\n  \"Is there a method to directly extract RMSE using mean_squared_error?\",\\n  \"What is the alternative approach to calculate RMSE in Decision Trees?\",\\n  \"Can I obtain RMSE from mean_squared_error without additional steps?\",\\n  \"How does mean_squared_error simplify the calculation of RMSE?\"\\n]', 'c14cca9134819afae33a076fe6afd363': '[\\n    \"What is a good visual implementation for features importance in decision trees?\",\\n    \"Where can I find an example of features importance graph in scikit-learn?\",\\n    \"How does scikit-learn\\'s features importance graph help with model explainability?\",\\n    \"What additional information does the scikit-learn features importance graph include?\",\\n    \"Who provided the information on the features importance visual implementation?\"\\n]', '7e27729ed3c52d4dd48475038d323607': '[\\n    \"What should I do if I encounter an xgboost.core.XGBoostError related to sklearn?\",\\n    \"How can I resolve an XGBoostError regarding missing sklearn installation?\",\\n    \"What error might occur when using XGBoost without sklearn?\",\\n    \"Why would an XGBoost application error due to sklearn?\",\\n    \"What is a common solution for xgboost.core.XGBoostError involving sklearn?\"\\n]', '676f19982cad5de986a7b38ce2fdeecb': '[\\n  \"What concept is used to measure the knowledge acquired about Y from X in decision trees?\",\\n  \"How can one describe the mutual information of Y and X in the context of decision trees?\",\\n  \"When X provides no knowledge about Y, what is the impact on information gain?\",\\n  \"What happens when X fully predicts Y with regards to information gain in decision trees?\",\\n  \"Who has been associated with the description of information gain in this course material?\"\\n]', '01225a621c34e0e72cd8de074cdae79b': '[\\n  \"What causes data leakage when filling in missing values?\",\\n  \"How should missing values be handled to avoid data leakage?\",\\n  \"Why is it important to split data before addressing missing values?\",\\n  \"What is a common mistake that leads to data leakage in preprocessing?\",\\n  \"How does data leakage impact model performance when handling missing data?\"\\n]', '61e2278f604f3966d4f3301ae8bfd5bb': '[\\n    \"How can I save an Xgboost model to avoid serialization errors?\",\\n    \"What is the recommended method to save an Xgboost model?\",\\n    \"Where can I find information about loading a serialized Xgboost model?\",\\n    \"Who is associated with the information regarding model serialization in the course?\",\\n    \"Where is the section on Serialized Model Xgboost error now located?\"\\n]', 'c10ea3b6a9f954bd836fb69a8b9b5f52': '[\\n    \"What materials should I review before starting Week 8?\",\\n    \"Are there any prerequisites for understanding the concepts in Week 8?\",\\n    \"How can I effectively prepare for the content covered in Week 8?\",\\n    \"What resources would you recommend to supplement learning in Week 8?\",\\n    \"What key topics will be explored in Week 8 on neural networks and deep learning?\"\\n]', 'd2e6a25f0321fd20215538d1b047d788': '[\\n    \"How can I set up my notebook for deep learning on Kaggle?\",\\n    \"What are the steps to select a GPU on Kaggle?\",\\n    \"How do I start using a T4 GPU in a Kaggle notebook?\",\\n    \"Where can I find the option to add a GPU in Kaggle?\",\\n    \"What should I do first to utilize Kaggle for deep learning tasks?\"\\n]', 'ddc2079bb6b77d174ca719ba8fea1832': '{\\n    \"questions\": [\\n        \"How can I set up a notebook in Google Colab for deep learning tasks?\",\\n        \"What steps should I follow to use a GPU in Google Colab?\",\\n        \"Which settings do I need to adjust to optimize Colab for neural networks?\",\\n        \"What is the method to activate GPU support in Google Colab?\",\\n        \"How do I change the runtime type for deep learning work in Google Colab?\"\\n    ]\\n}', '6e91ecf6b9aa8074396e735491493dcd': '[\"What is the process for connecting my GPU on Saturn Cloud to a Github repository?\", \"Can I download a notebook from Saturn Cloud and manually copy it to my Github folder?\", \"How can I create an SSH private and public key for Github?\", \"Where can I find instructions to add SSH keys to secrets and authenticate through a terminal on Saturn Cloud?\", \"How do I use the default public keys provided by Saturn Cloud to connect to Github?\"]', '8375cc3fae4fb7e4a7afe3ee627e81fd': '[\\n    \"Where can I find the Python TensorFlow template?\",\\n    \"Is the video location of the template currently accurate?\",\\n    \"What is the updated location for the TensorFlow template on Saturn Cloud?\",\\n    \"Who mentioned the change of the TensorFlow template\\'s location?\",\\n    \"Which section\\'s video discusses setting up the environment on Saturn Cloud?\"\\n]', 'db8cc281ccfc3f80e7389ea992bc5ed5': '[\\n  \"What should be done if the scipy module is not found during model training in Saturn Cloud\\'s TensorFlow image?\",\\n  \"How can I ensure that the scipy package is automatically installed when my Saturn Cloud resource spins up?\",\\n  \"What is the process to install additional Python packages in a Saturn Cloud Jupyter server?\",\\n  \"Why is the scipy module missing in the Saturn Cloud TensorFlow image?\",\\n  \"Where can I specify extra packages to be installed in the Saturn Cloud Jupyter resource?\"\\n]', '6bcb68cc13c1063d8999ba7134b9ccf6': '[\"What is a more efficient way to upload Kaggle data to Saturn Cloud?\", \"How can I create a Kaggle API token from my account?\", \"Where should I upload the kaggle.json file in my notebook?\", \"What command should I run to download a dataset from Kaggle?\", \"How can I set the correct permissions for the kaggle.json file in Saturn Cloud?\"]', 'b24757fdba7f919208a5ce15c1c775e6': '[\\n    \"What is needed to run TensorFlow with GPU on a local machine?\",\\n    \"Who provided a simplified guide for setting up CUDA and cuDNN?\",\\n    \"Why might the installation process of CUDA and cuDNN be challenging?\",\\n    \"What operating system is mentioned for CUDA and cuDNN installation?\",\\n    \"What needs to be installed to enable TensorFlow GPU support?\"    \\n]', '9bba8021c88751162684b437181da32d': '[\\n    \"What error might occur when loading HDF5 weights into a subclassed Model?\",\\n    \"How can I resolve a ValueError related to loading model weights in HDF5 format?\",\\n    \"What should be done before loading weights in a subclassed Model?\",\\n    \"Why does an error occur when loading weights into a Model that hasn\\'t created its variables yet?\",\\n    \"What is a necessary step before loading HDF5 weights in a saved model?\"\\n]', '0d0f38725ec4ae51ac195e5cbdf46524': '[\"What should I do if I encounter a \\'permission denied\\' error when connecting to git on Saturn Cloud as shown in the module 8.1b video?\", \"How can I set up git in Saturn Cloud to avoid the permission denied error mentioned when running ssh commands?\", \"What steps should I follow to generate an SSH key in Saturn Cloud and use it with a git account to prevent permission issues?\", \"Where can I find a tutorial to set up git with an SSH key in my Saturn Cloud environment?\", \"Who added the solution for the git permission denied issue in the FAQ record?\"]', 'a9a4d23c32b7be586deb92f8883c4d82': '[\\n  \"Why do I encounter a host key verification failure when cloning a repository using SSH?\",\\n  \"What is the error message I receive when the host key verification fails during a Git clone operation?\",\\n  \"How can I successfully clone the clothing-dataset repository without encountering an SSH error?\",\\n  \"What alternative URL format can I use to clone a repository if I don\\'t have my SSH key configured?\",\\n  \"Who added the solution to the host key verification failed problem in the course FAQ?\"\\n]', '3dc59418fe426b2af51e8c583685294e': '[\\n    \"What should be set to \\'binary\\' in the homework if accuracy and loss are the same?\",\\n    \"What could be wrong if accuracy and loss remain unchanged during training?\",\\n    \"What parameter might be incorrectly set to cause stagnant accuracy and loss?\",\\n    \"Who added the solution description to set class_mode for stagnant training metrics?\",\\n    \"Which factors should be checked if there is no change in accuracy and loss?\"\\n]', '273da83bda937557297207bfafe7e9a5': '[\"What should I do if my model shows high loss and bad accuracy after resuming training with augmentation?\", \"Why does my model\\'s accuracy remain around 0.5 after augmentation?\", \"How can I prevent my model\\'s loss from skyrocketing after using augmented images?\", \"What is a possible cause for my model performing no better than a random guess post-augmentation?\", \"Which configuration option should I ensure is included in the augmented ImageDataGenerator to avoid high loss?\"]', '11136fea8f7e66ae3aa7075ff6d161cc': '[\"How can I fix the missing channel value error when reloading a saved model?\", \"What causes the ValueError regarding the channel dimension when loading a model in TensorFlow?\", \"Why is the channel dimension sometimes not stored when saving a model?\", \"How should the number of channels be specified in the Input layer for a TensorFlow model?\", \"What components are saved when using model.save or checkpoint in a TensorFlow model?\"]', '55c1c7bce14a225cb153e59eacbd899d': '[\\n  \"How can I suppress the output when unzipping a dataset inside a Jupyter Notebook?\",\\n  \"What is the method to unzip a folder in Jupyter Notebook without displaying the file extraction process?\",\\n  \"How do I extract files from a zip archive while avoiding detailed output in a notebook?\",\\n  \"What command should I use to extract a large zipped dataset quietly within a Jupyter Notebook?\",\\n  \"Can you show a way to unzip a dataset in a notebook without outputting each file being processed?\"\\n]', '2f30087c83594875acab221ed6beb306': '[\\n    \"How does keras flow_from_directory determine class names for images?\",\\n    \"Does flow_from_directory understand class names from folder names directly?\",\\n    \"Can a folder with any name, like \\'xyz\\', be considered a class in keras flow_from_directory?\",\\n    \"Is there a deeper mechanism behind flow_from_directory recognizing class names from folders?\",\\n    \"Does the flow_from_directory function require specific folder names to classify images?\"\\n]', '1eacb1a95c47393d6f8831307f3dca95': '[\\n    \"What should I do if I encounter an error about a missing scipy module while fitting a model in SaturnCloud?\",\\n    \"How do I resolve an issue with missing scipy module in a newly created environment on SaturnCloud?\",\\n    \"If I receive an error related to scipy module when using Tensorflow in SaturnCloud, what is the solution?\",\\n    \"What steps should be taken to fix a scipy missing module error in SaturnCloud?\",\\n    \"How can I install the scipy module if it\\'s missing in SaturnCloud while working with Tensorflow?\"\\n]', 'd70f647f03bf7b998fa3687cca39b8ef': '[\\n    \"How are the numeric class labels assigned when using flow_from_directory with binary class mode in Keras?\",\\n    \"In what order does flow_from_directory read the folders within the dataset?\",\\n    \"What does a Keras model predict when it uses binary labels and the sigmoid activation function?\",\\n    \"How can we interpret the prediction of a probability value for class 1 by a binary Keras model?\",\\n    \"What is the significance of using from_logits when interpreting outcomes from a Keras model?\"\\n]', 'f7617cd0a8cf43612e4f91cca2bdfbae': '[\\n  \"Do the actual values need to be perfect after predicting with a neural network?\",\\n  \"Should predicted values from a neural network be seen as absolute?\",\\n  \"Are small deviations in predicted values normal when using a neural network?\",\\n  \"Can predicted values be treated as probabilities of class membership in neural networks?\",\\n  \"Is it okay for predicted outcomes to have some variation in neural network predictions?\"\\n]', '1bab02ab87235b5f9f6fd62e4b5f3c9d': '[\"What should I do if my model\\'s accuracy and standard deviation training loss differ from the homework answers?\", \"Why might my mac laptop show different results for accuracy and standard deviation when running the wasp/bee model?\", \"How can I solve discrepancies between my results and the homework answers when running the model on my personal device?\", \"What did Quinn Avila suggest to do if I encounter different accuracy results on my mac laptop?\", \"What are the benefits of using Google Collab for running my model compared to a laptop\\'s CPU?\"]', 'cb62dfc4d46f311271fef8b6ca808750': '[\"What parameter can be specified in \\'model.fit()\\' to speed up data generation?\", \"What is the default value for the \\'workers\\' parameter in \\'model.fit()\\'?\", \"How can I determine the best \\'workers\\' value for data loading in \\'model.fit()\\'?\", \"Where can I find more information about \\'model.fit()\\' parameters?\", \"Who contributed the information on multi-threading for data generation in \\'model.fit()\\'?\"]', '5c6ce80521bb8d7136e7b647da5d42b5': '[\"How can I achieve reproducibility in TensorFlow for training runs?\", \"What should be done to ensure deterministic behavior in TensorFlow scripts?\", \"What is the method to set a seed for reproducibility using TensorFlow?\", \"Is there a specific seed value recommended for TensorFlow reproducibility?\", \"Who provided the instructions for achieving reproducibility in TensorFlow?\"]', '7f2a8ff92ccbdd41f846e4bc9c975455': '[\\n  \"Is it possible to use a different framework than Keras for the lesson or homework tasks?\",\\n  \"Could we utilize Pytorch to perform the same tasks outlined in the lesson using Keras?\",\\n  \"Where can I find a guide to building a CNN using Pytorch from the ground up?\",\\n  \"Are the functions in Pytorch and Keras intended to accomplish similar objectives?\",\\n  \"If I prefer using Pytorch, am I allowed to submit my work as a pull request?\"\\n]', 'b3cf5754f3c323603c840cda731281b2': '[\"What does the error \\'Failed to find data adapter\\' indicate when training a Keras model?\", \"Why might a \\'Failed to find data adapter\\' error occur with Keras and ImageDataGenerator?\", \"How should the ImageDataGenerator output be used when fitting a Keras model?\", \"What is the correct data parameter to pass to model.fit when using ImageDataGenerator?\", \"What is a simple fix for the \\'Failed to find data adapter\\' error involving ImageDataGenerator?\"]', '591293975f887b506fbb3f9517711c78': '[\"How can I run \\'nvidia-smi\\' to update automatically without using \\'watch\\'?\", \"Is there a way to execute \\'nvidia-smi\\' at regular intervals, and how can I do it?\", \"What command should I use to refresh \\'nvidia-smi\\' output every few seconds on its own?\", \"Can \\'nvidia-smi\\' be set to refresh its details automatically, and if so, how?\", \"How do I run \\'nvidia-smi\\' so that it updates every 2 seconds?\"]', 'df008438cfaced4c9ef5d1eeb9356c89': '[\\n    \"What Python package functions like an interactive GPU process viewer?\",\\n    \"Which tool is similar to \\'htop\\' but for monitoring GPUs?\",\\n    \"Where can I find the \\'nvitop\\' package documentation?\",\\n    \"Who added the information about GPU process viewing in the course material?\",\\n    \"Is \\'nvitop\\' used for CPU or GPU process monitoring?\"\\n]', '8e5f71565fad6b1219f22fb1640a2755': '[\\n  \"How is the parameter count of 896 for the Conv2D layer calculated?\",\\n  \"How do you get the number 6272 for the Flatten layer features?\",\\n  \"What does adding 1 in the Conv2D parameter formula account for?\",\\n  \"Why does the Conv2D layer have an output shape of (None, 148, 148, 32)?\",\\n  \"What is the significance of 128 filters in calculating Flatten layer features?\"\\n]', 'c83dd270bf1b433dd80e50c4dc909e92': '[\\n  \"What is the easier model mode to use in Keras for beginners, the Sequential or Functional Model API?\",\\n  \"Why would the Functional Model API be more suitable when doing Transfer Learning compared to the Sequential Model API?\",\\n  \"What could be the consequence of not restarting the kernel after correcting an error in the neural network architecture?\",\\n  \"Where can I find an example of a Sequential model, as mentioned in the course, for further learning?\",\\n  \"What is a key implementation difference between Sequential and Functional Model APIs in the context of Alexey’s videos?\"\\n]', '4df7998bfd58c2ffc7c0b71a9f16e746': '[\\n  \"What is a recommended way to address out of memory errors with TensorFlow on Nvidia GPUs?\",\\n  \"Is the solution for OOM errors in TensorFlow applicable to CPUs as well?\",\\n  \"Where can I find documentation for setting memory growth in TensorFlow?\",\\n  \"What Python exception handling is used for modifying virtual devices in TensorFlow?\",\\n  \"Which TensorFlow API function allows configuring memory growth on Nvidia GPUs?\"\\n]', 'ced6e47975e87a13d6f1f29d50073851': '[\\n  \"How can I speed up model training in Google Colab with a T4 GPU?\",\\n  \"What is the default number of workers used in Google Colab\\'s T4 GPU for model training?\",\\n  \"What modification helped improve the slow model training on Google Colab\\'s T4 GPU?\",\\n  \"What did Ibai Irastorza suggest changing in the fit function to improve performance in Colab?\",\\n  \"Where can I find more information about setting number of cores for Google Colab\\'s GPU?\"\\n]', '8f728a4f4d6501724df41ffb55630ba7': '[\\n    \"What does the keras documentation advise against for loading images?\",\\n    \"How should new code load and transform image datasets according to keras?\",\\n    \"Why is ImageDataGenerator not recommended by keras?\",\\n    \"What alternative does keras suggest in place of ImageDataGenerator?\",\\n    \"Where can I find more information about loading and augmenting images in keras?\"\\n]', '60f7f42de59b3faa600193c5dd566ba9': '[\\n    \"What is the initial step for beginning Week 9 activities?\",\\n    \"Where can I find guidance on Week 9\\'s introductory tasks?\",\\n    \"How should I proceed to start Week 9\\'s coursework?\",\\n    \"What are the instructions for starting Week 9?\",\\n    \"What is the first action to take for Week 9?\"\\n]', '621faa30f52767ad72c1f8fb4b9e0b07': '[\\n  \"Where can I find the deep learning models for week 9?\",\\n  \"What resource does week 9 refer to for model access?\",\\n  \"Where should I go for the week 9 models after the original link was changed?\",\\n  \"How do I access the models for week 9 via GitHub?\",\\n  \"Where is the relocated GitHub link for week 9 models?\"\\n]', '7e321de8c3fd80a24b6e8174cb72bf22': '[\\n  \"Why does executing echo ${REMOTE_URI} return nothing?\",\\n  \"How can I assign a value to a local variable REMOTE_URI?\",\\n  \"What should I do if I encounter an issue on Ubuntu terminal with REMOTE_URI?\",\\n  \"Why should I not use curly brackets when echoing REMOTE_URI?\",\\n  \"What happens to the REMOTE_URI variable once the session ends?\"\\n]', '2977f27e7bde5689fbddbfb8ac53a640': '[\\n  \"What should I do if I receive an invalid choice error when getting the password from aws-cli?\",\\n  \"How can I simplify the login process using aws ecr get-login-password?\",\\n  \"What is an alternative command to use instead of aws ecr get-login --no-include-email?\",\\n  \"Could you provide the steps to log in to AWS ECR using a password from the CLI?\",\\n  \"Who added the solution to the syntax error in the FAQ record?\"\\n]', '9603a34f367ea284b18184925065dab4': '[\\n  \"How can I effectively pass multiple parameters in a CNN model?\",\\n  \"What function is recommended for handling multiple parameters in a deep learning model?\",\\n  \"Can you explain how to pass parameters in keras for deep learning models?\",\\n  \"Which keras function is suitable for passing numerous CNN parameters?\",\\n  \"What method does Krishna Anand suggest for passing many CNN parameters?\"\\n]', 'af2b9a536bb7f627cb54ef7934e63ad8': '[\"What causes ERROR [internal] load metadata for public.ecr.aws/lambda/python:3.8?\", \"How can I resolve the error when building a docker image from the Amazon python base?\", \"What steps should I take if updating Docker Desktop does not resolve the issue?\", \"What command can be run if restarting Docker Desktop and terminal doesn\\'t work?\", \"Who provided the optional solution steps for the docker build error?\"]', '8e2857ede622bffe9684e97aa9f0133e': '[\\n\"Why does running \\'!ls -lh\\' in a Windows Jupyter Notebook result in an error message?\",\\n\"What command should replace \\'!ls -lh\\' in a Windows environment to achieve similar results?\",\\n\"How can we identify files in Windows Jupyter Notebook without using \\'!ls -lh\\'?\",\\n\"What is a suitable alternative to \\'!ls -lh\\' for Windows users in Jupyter Notebook?\",\\n\"What is a common solution for \\'!ls -lh\\' command issues in Windows Jupyter Notebook?\"\\n]', '141590484a728217929538da71278186': '[\\n  \"What error might occur if I import both tensorflow and tflite_runtime.interpreter in the same notebook?\",\\n  \"How do I resolve the \\'type \\'InterpreterWrapper\\' is already registered\\' error?\",\\n  \"What import statement causes the InterpreterWrapper registration error?\",\\n  \"How can I fix an error related to importing InterpreterWrapper in a Jupyter notebook?\",\\n  \"What is the recommended way to import tflite_runtime.interpreter to avoid errors?\"\\n]', '9303ef7093060d61a61f1541a19f6be8': '[\"What should I do if I encounter an error about the Windows version not being up-to-date while building a Docker image?\", \"Why might I see a message indicating that the system cannot find a specified file when using Docker?\", \"How do I resolve an error stating that the Docker daemon is not running?\", \"What does the error indicating a Windows version issue in Docker suggest?\", \"What steps should be taken if a third-party program stops Docker from running?\"]', '7758843297650c72ac994e45cacd7bb9': '[\\n  \"What should I do if I encounter a pip version warning while running docker build?\",\\n  \"How can I resolve the error caused by using a version of the wheel with Python 8?\",\\n  \"Why does copying the link instead of downloading the raw format cause an error?\",\\n  \"Where can I find the correct wheel version needed for Python 9?\",\\n  \"What is a common mistake that results in a pip version error during this week\\'s task?\"\\n]', '00f558adaab1229863167ed29b1ec776': '[\\n    \"What should we enter for Default output format when configuring AWS CLI after installation?\",\\n    \"Is it acceptable to leave the Default output format as None during AWS configuration?\",\\n    \"Do I need to change anything other than the Access Key ID and Secret Access Key when configuring AWS CLI?\",\\n    \"What steps are involved in configuring AWS CLI after successfully installing it?\",\\n    \"Are there any fields besides the Access Key and Secret Access Key that require non-default values in AWS configuration?\"\\n]', '43454dc810a6f315779efc9726479902': '[\\n\"What causes the error indicating an object of type float32 is not JSON serializable in serverless deep learning?\",\\n\"How can I resolve errors related to float32 not being JSON serializable when testing with a docker instance?\",\\n\"What modification is necessary for model outputs to prevent JSON serialization errors in a lambda function?\",\\n\"Why does my lambda function work locally but fail during docker instance testing due to serialization issues?\",\\n\"How should I modify the predict function in lambda_function.py to avoid marshaling errors?\"\\n]', '594a994630887c94cfd61e9f4f052142': '[\\n  \"What causes the ValueError when running interpreter.set_tensor with X in video 9.3?\",\\n  \"How can I fix the issue of setting a tensor with incorrect type when using interpreter.set_tensor?\",\\n  \"Why do I get a ValueError for expected FLOAT32 type in TensorFlow?\",\\n  \"What should I do to resolve the type mismatch in interpreter.set_tensor for input 0?\",\\n  \"Is the issue of mismatched tensor types related to using TensorFlow 2.15.0?\"\\n]', '0f4100ec6e99ee180b2d45c98ebab2d5': '[\"What is the PowerShell command to determine file size?\", \"How can I view the file size of an item using PowerShell?\", \"What steps are involved to check file size in MB with PowerShell?\", \"Where can one find a guide for getting file size using PowerShell?\", \"Who contributed the information on checking file sizes in PowerShell?\"]', 'd03538833252857f5032dbebd61179c4': '[\\n  \"What resources explain Lambda container images and their initialization?\",\\n  \"Where can I find documentation on creating Lambda images?\",\\n  \"Where is detailed information on Lambda runtimes API?\",\\n  \"Who contributed the information about Lambda container images?\",\\n  \"Why did Alejandro Aponte research Lambda container images?\"\\n]', '9d28ad6f3b4bcfcbc983665fc020a993': '[\"How can I deploy a docker image on AWS Lambda as a REST API using AWS Serverless Framework?\", \"What steps are involved in exposing a REST API through APIGatewayService using AWS Serverless Framework?\", \"What is the process to create and push a docker image for AWS Lambda?\", \"Where can I find a detailed guide on using AWS Serverless Framework with AWS Lambda?\", \"Who contributed the information on deploying a containerized serverless flask to AWS Lambda?\"]', '06fcbe1939e788f8c5e431f5c6374f6c': '[\\n  \"What error might I encounter when building a Docker image in Section 9.5 on an M1 Mac?\",\\n  \"How can I resolve the pip install error for the tflite runtime when building a Docker image?\",\\n  \"What is the cause of the Docker image build error related to architecture on M1 Mac?\",\\n  \"What solution should I use to build a Docker image for compatibility on M1 Mac?\",\\n  \"Which command allows running the Docker image on M1 Mac without errors?\"\\n]', '5319cdb77465358c03ac9e3860943ca5': '[\\n    \"What should I do if I encounter a \\'Missing Authentication Token\\' error while testing API Gateway locally as described in section 9?\",\\n    \"Why am I getting a \\'Missing Authentication Token\\' error during local testing of API Gateway in section 9?\",\\n    \"How can I resolve the error message related to \\'Missing Authentication Token\\' when following the instructions in section 9.7?\",\\n    \"What is the solution to the issue of \\'Missing Authentication Token\\' that occurs in section 9.7 when testing the API Gateway?\",\\n    \"What steps should be taken if the error message indicates a \\'Missing Authentication Token\\' while implementing section 9.7?\"\\n]', '0a61e05189bf7dd0380871e4c209d3e6': '[\\n  \"What should I do if I encounter an error saying it could not find a version that satisfies the requirement for tflite_runtime?\",\\n  \"Where can I find the available os-python version combinations for tflite_runtime?\",\\n  \"Is there an alternative way to install tflite_runtime if my os-python combination is missing?\",\\n  \"Can I use a virtual machine to run serverless deep learning with tflite_runtime?\",\\n  \"Are there cloud-based solutions where I can run the code if tflite_runtime installation fails locally?\"\\n]', '5049e2902559dd3669c5940427cbc9a7': '[\"What should I do if I encounter an error when running Docker in the serverless deep learning section?\", \"How can I resolve a read-only file system error in Docker?\", \"What action should be taken to fix a Docker run error as listed in the section about serverless deep learning?\", \"How do I handle a file system error indicated by the Docker daemon error message?\", \"What is the solution to the Docker daemon error response about a read-only file system?\"]', 'a1a1bb6b2249cb798ad0b72e657c8302': '[\"How can I save a Docker image to my local machine?\", \"What command allows exporting a Docker image to tar format?\", \"How do I export a Docker image locally?\", \"What is the process to view the contents of a Docker image?\", \"How can I extract the individual layers of a Docker image?\"]', 'fd7a9ca97288539a604b80e4034da149': '[\"How do I resolve my Jupyter notebook not recognizing an installed package?\", \"What steps can I take if my Jupyter notebook doesn\\'t see a newly installed module?\", \"Why might my Jupyter notebook fail to import a package I just installed?\", \"What can I do if after installing a package, the Jupyter notebook still doesn\\'t recognize it?\", \"How can restarting Jupyter notebook solve import errors after installing a package?\"]', 'a2476a1a4e01cbc9d99eba05d5373743': '[\\n    \"What can I do if I run out of storage on a 30-GB AWS instance?\",\\n    \"Why doesn\\'t deleting Docker images free up space on AWS?\",\\n    \"What additional step is required after removing Docker images to free storage?\",\\n    \"How can I manage storage space when using Docker on AWS?\",\\n    \"What is a common issue when experimenting on a small AWS instance?\"\\n]', 'c948222d538f461124a1277a453c163e': '[\"Do we need TensorFlow 2.15 for AWS, or is an older version sufficient?\", \"What Python version is recommended when deploying with TensorFlow on AWS?\", \"If TensorFlow 2.15 is incompatible, what alternative version should I try?\", \"Does TensorFlow 2.4.4 require a specific Python version?\", \"Who added the information about TensorFlow deployment?\"]', '61d91abc64e54524a56fa4d1e8cf27f8': '[\\n  \"What should I do if I encounter an \\'Invalid choice\\' error when using the \\'aws ecr get-login --no-include-email\\' command?\",\\n  \"How can I resolve the issue with the \\'aws ecr get-login --no-include-email\\' command returning an error?\",\\n  \"What is the solution to the \\'aws: error: argument operation: Invalid choice\\' message when executing a command?\",\\n  \"Where can I find guidance on fixing the \\'Invalid choice\\' error related to \\'get-login\\' in AWS CLI?\",\\n  \"How do I handle an AWS CLI error with \\'get-login --no-include-email\\' showing \\'Invalid choice\\'?\"\\n]', '338f372c8aa3fd7c3f7867bb464a4f53': '[\\n  \"How do I log in to the AWS Console to begin the Serverless Deep Learning course in Week 9?\",\\n  \"What steps are required to create a new IAM policy within the AWS Console for the serverless project?\",\\n  \"Which specific ECR actions need to be included in the JSON policy for the serverless deep learning project?\",\\n  \"What error solution can help fix a failed solve related to public.ecr.aws/lambda/python:3.10?\",\\n  \"What should be done if \\'docker-credential-desktop.exe\\' is not found in $PATH on a WSL2 system?\"\\n]', '096d6a2f35d8211c7d4f19013d2203b0': '[\\n    \"How can I resolve Docker\\'s temporary failure in name resolution?\",\\n    \"What should I add to vim /etc/docker/daemon.json to fix DNS issues?\",\\n    \"Which DNS servers should I use for Docker troubleshooting?\",\\n    \"What command do I run after editing Docker\\'s daemon.json to fix DNS?\",\\n    \"Who provided the solution for Docker\\'s name resolution error?\"\\n]', '578115d16ba2842512511913118092da': '[\\n  \"What should I do if my Keras model .h5 file fails to load due to a weight_decay error?\",\\n  \"How can I fix the issue when loading a .h5 model file that reports weight_decay is not a valid argument?\",\\n  \"Is there a solution for the Keras model loading error related to optimizer_experimental.Optimizer and weight_decay?\",\\n  \"Which parameter needs to be changed in the load_model function to solve the weight_decay argument error?\",\\n  \"What should be added to the load_model function call when encountering a weight_decay issue with a loaded Keras .h5 model?\"\\n]', 'ded28c389c4b0c24d220468caacc29e3': '[\\n  \"How can I locally test an AWS Lambda deployment with Docker?\",\\n  \"What command should I use to run the Docker image locally for testing?\",\\n  \"Which localhost link should I use to test the AWS Lambda deployment locally?\",\\n  \"What curl command can I use to test the AWS Lambda function endpoint on a Unix system?\",\\n  \"If I receive a JSON serialization error during testing, how can I resolve it?\"\\n]', '59628b07aea809626e8f2d1027ae8b3f': '[\"What should I do if I encounter an error stating \\'No module named tensorflow\\' when running my test script?\", \"How can I resolve the issue of importing lambda_function with tensorflow errors?\", \"What changes are necessary to fix the import error related to tensorflow in test.py?\", \"How do I modify my code to avoid dependencies on the tensorflow library in serverless setups?\", \"What is a common reason for the error \\'Unable to import module\\' in serverless deep learning projects?\"]', '37d72ff6b561af17a4f078cfe12f72e4': '{\\n  \"questions\": [\\n    \"How can I run Docker containers in Google Colab?\",\\n    \"What is the solution for \\'Authorization header requires Credential parameter\\' error?\",\\n    \"How do you test a Lambda API Gateway method using Boto3?\",\\n    \"What steps should I follow to install tflite_runtime when GitHub wheel links fail?\",\\n    \"Can you provide a guide on resolving \\'Missing Authentication Token\\' in AWS?\"\\n  ]\\n}', '0a2852d73cf0ef0ed751377e6d1e56a9': '[\\n  \"What is the first step to begin with the Kubernetes and TensorFlow Serving section?\",\\n  \"Can you guide me on how to initiate Week 10?\",\\n  \"What should I do to start with the Kubernetes and TensorFlow Serving topic?\",\\n  \"How can I kick off the activities for Week 10?\",\\n  \"What actions are required to commence the Kubernetes and TensorFlow Serving section?\"\\n]', 'e98ad19f6a1e5e8195d5adfb94768b65': '[\"What are the steps to install TensorFlow with CUDA support on an Ubuntu WSL2 system?\", \"Where can I find resources to install CUDA for TensorFlow on my local machine?\", \"Can you recommend a platform version for setting up PyTorch with CUDA support?\", \"Why would I need to use my local machine to run TensorFlow with CUDA support?\", \"What additional resource should I check out when installing TensorFlow with CUDA?\"]', 'ceb7f4feaeca95bcd1c48af44dd815ae': '[\\n  \"What should I do if I see the Allocator ran out of memory error while running TensorFlow on my machine?\",\\n  \"How can I potentially reduce memory allocation errors in my TensorFlow projects?\",\\n  \"Is there any TensorFlow configuration that could help if I encounter memory issues with my GPU?\",\\n  \"What steps can I take in my notebook to address memory allocation errors with TensorFlow?\",\\n  \"Why is my TensorFlow script failing even though it suggests it\\'s not a failure, and how can I fix it?\"\\n]', '0440ffba02182dd352129ddf89cbdd79': '[\\n    \"What steps should be taken if encountering a TypeError related to descriptors when running gateway.py in session 10.3?\",\\n    \"How can I address the issue of outdated generated code from a _pb2.py file during session 10.3?\",\\n    \"Is there a specific way to fix the protobuf version to avoid descriptor creation errors in session 10.3?\",\\n    \"What are the possible workarounds for dealing with newer versions of protobuf in session 10.3?\",\\n    \"Can you recommend a way to create a virtual environment to avoid issues with recent protobuf versions in session 10.3?\"\\n]', '915257c65c5e7ed9e61d164570dc3881': '[\\n  \"What should I do if WSL cannot connect to the Docker daemon with an error message about docker.sock?\",\\n  \"How can I resolve the issue of Docker Desktop not connecting to WSL Linux distro?\",\\n  \"What steps should I follow in Docker Desktop to fix a connection issue with WSL?\",\\n  \"When Docker command shows \\'Cannot connect to the Docker daemon,\\' what might be the problem?\",\\n  \"How do I enable additional WSL distros in Docker Desktop to fix connection problems?\"\\n]', 'ebe79c3e41db5d90262e26ddeb58a6d0': '[\\n  \"What should I do if the HPA instance does not run properly even after installing the latest version of Metrics Server from the components.yaml manifest?\",\\n  \"Why do the targets still appear as unknown after installing the Metrics Server?\",\\n  \"How can I edit the metrics-server deployment in the kube-system namespace to fix HPA issues?\",\\n  \"What line needs to be added to the metrics-server\\'s args to resolve the HPA problem?\",\\n  \"After making changes to the metrics-server\\'s args, what should I run to verify the HPA status?\"\\n]', 'a4cbffa230ddc2cff653ba8ecd714a3f': '[\"What should I do if the HPA instance encounters issues despite updating the Metrics Server?\", \"How can I resolve targets still appearing as \\'<unknown>\\' after deploying the latest Metrics Server?\", \"Is there a specific command to run when HPA doesn\\'t function correctly with the latest Metrics Server update?\", \"Where can I find a metrics server deployment file that includes the \\'--kubelet-insecure-tls\\' option?\", \"Who contributed the solution for running the HPA instance properly in Kubernetes?\"]', '9d5a8b136403df77c867b27a18c99fe4': '[\"What should I do if I encounter an Access is denied error while installing packages on Windows?\", \"How can I successfully install grpcio and tensorflow-serving-api on a Windows machine?\", \"Which error might occur with pip install on Windows, and how can I resolve it?\", \"What command can I use on Windows to overcome permissions issues while installing Python libraries?\", \"Who provided the solution for the Access is denied error during library installation?\"]', '294c230af467061a021c9913e274508e': '[\"What error message do you encounter when running gateway.py after code modification and virtual environment creation in video 10.3?\", \"What does the error \\'Descriptors cannot not be created directly\\' mean in your Kubernetes and TensorFlow Serving setup?\", \"What steps can I take if regenerating protos with protoc >= 3.19.0 is not possible?\", \"How was the \\'Descriptors cannot not be created directly\\' issue resolved in your setup?\", \"What version should protobuf be downgraded to in order to fix the error in Kubernetes and TensorFlow Serving?\"]', 'abeb2197c569223d7a2d2f243c311cbd': '[\\n  \"How can I install kubectl on a Windows system using the terminal in VSCode?\",\\n  \"What tool can be used alongside kubectl on Windows and how is it installed?\",\\n  \"Where should I place the exe file after downloading kubectl on Windows?\",\\n  \"Why is it necessary to update the PATH environment variable during installation on Windows?\",\\n  \"Can kind be installed on Windows similarly to kubectl, and if so, what should be done?\"\\n]', 'ca51435199986f273f24448c99711933': '[\"What is the first step to install kind using the choco library on Windows?\", \"How do you open a powershell terminal with the necessary privileges?\", \"What command should be executed to install the choco library?\", \"Which security protocol is set during the choco installation process?\", \"What is the PowerShell command to download the Chocolatey installation script?\"]', '0b057a8fdf9abd478c1e29fab7e6c6c4': '[\\n  \"How can I install Kind using the Go package if I encounter issues with Powershell?\",\\n  \"What steps do I follow to confirm the installation of Go?\",\\n  \"What command should I use to install Kind once Go is set up?\",\\n  \"How can I verify that Kind was installed correctly?\",\\n  \"Where can I find instructions to download and install Go?\"\\n]', 'ac3c3ce21e1b4ec7872c1e7108d02bac': '[\"What should I do if I receive a \\'The connection to the server localhost:8080 was refused\\' error?\", \"How can I resolve issues with kubectl not working due to wrong host or port?\", \"What steps should I follow to fix kubectl connection errors?\", \"How can I reset my Kubernetes cluster to fix connection issues?\", \"What does running \\'kind delete cluster\\' followed by \\'kind create cluster\\' solve?\"]', 'f50665e446a9f2ac310d638d91669521': '[\"What should I do if I\\'m running out of storage while working with Docker images in my AWS instance?\", \"How can I identify which Docker images are taking up the most space on my system?\", \"Why does deleting Docker images not free up space on my instance as expected?\", \"What additional step is needed after removing Docker images to actually free up storage?\", \"Where can I find more information on why Docker doesn\\'t free up storage space after removing images and containers?\"]', 'e4e1c54f2f795ab38bcaf3ec23eef563': '[\\n  \"What should be specified in the yaml file for HW10 Q6?\",\\n  \"Are CPU and memory values arbitrary in HW10 Q6?\",\\n  \"What values have a defined correct value in HW10 Q6?\",\\n  \"Does HW10 Q6 require specific values for CPU and memory?\",\\n  \"Which part of HW10 Q6 has a defined correct value?\"\\n]', '27cea99988c1266e03a3ae0f9e7e75a0': '[\"What does the \\'m\\' in Kubernetes CPU specifications represent?\", \"Why are Kubernetes CPU values like \\'100m\\' and \\'500m\\' in milliCPUs?\", \"What is the meaning of \\'100m\\' for a CPU request in Kubernetes?\", \"How does \\'500m\\' as a CPU limit relate to the CPU core in Kubernetes?\", \"Why use milliCPUs for Kubernetes resource specifications?\"]', '0112e5b9e588d63f17f279ed5a3fb9e6': '[\\n  \"What should I do if I get an error saying \\'no nodes found for cluster\\' when loading a docker image to kind?\",\\n  \"How can I specify the cluster name when loading a docker-image using kind?\",\\n  \"What is the solution if kind cannot load a docker image to a named cluster?\",\\n  \"What does the error \\'no nodes found for cluster kind\\' indicate when working with docker images in kind?\",\\n  \"What command should I use to load a docker-image to a specific kind cluster called clothing-model?\"\\n]', '075bc676aadf8c0ee4c69d72b04ef783': '[\\n  \"Why does \\'kind\\' command return an error on my Windows system?\",\\n  \"How do I fix the error message indicating \\'kind\\' is not recognized as a command?\",\\n  \"What is the solution for \\'kind\\' not being operable in Windows?\",\\n  \"Is there a naming issue with the kind executable on Windows?\",\\n  \"How can I set up the kind executable on my Windows system?\"\\n]', '75d2e5daff67a186a23360f80ace3039': '[\\n  \"What should I know about using kind with Rootless Docker on Linux?\",\\n  \"How does Rootless Podman require system changes for kind?\",\\n  \"What are the steps to run kind with Rootless tools on Linux?\",\\n  \"Where can I find information about kind\\'s Rootless configuration?\",\\n  \"Does running kind on Linux with Rootless Docker involve system adjustments?\"\\n]', 'f581e9ab3fae36f172c3a5c83210c38a': '[\\n    \"How can I deploy the Kubernetes Dashboard?\",\\n    \"What steps are needed to access the Kubernetes Dashboard?\",\\n    \"Could you explain how to set up a Kubernetes Dashboard?\",\\n    \"What is the process to deploy the Kubernetes-dashboard?\",\\n    \"How do I access the Kubernetes Dashboard once deployed?\"\\n]', '0ba10287f39b3d3891b4f20d9a930fe8': '[\"How do I check the required AWS CLI version for eksctl?\", \"Which AWS CLI version should I use for eksctl?\", \"Where can I find instructions for AWS CLI v2 migration?\", \"How can I verify my current AWS CLI version?\", \"What is the URL for AWS CLI v2 migration instructions?\"]', 'ab74a5e4a79eaaa12fe484085939d60c': '[\\n  \"What causes the TypeError when testing a Flask service in video 10.3?\",\\n  \"How can I resolve the \\'unbound_message\\' error when running python gateway.py?\",\\n  \"Why does importing Flask cause an unexpected keyword argument in error?\",\\n  \"What versions of Flask and Werkzeug trigger the TypeError issue?\",\\n  \"How did you fix the flask versioning problem mentioned in video 10.3?\"\\n]', '4e1366f3af8cc429a3400e12d7a9d13c': '[\"What should I do if the command \\'aws ecr get-login --no-include-email\\' results in an invalid choice error?\", \"How do I log in to AWS ECR using the updated command?\", \"What changes do I need to make to the AWS ECR login command for it to work?\", \"Is there a way to use the AWS ECR login command without modifying any fields?\", \"Where can I find more information about pushing Docker images to AWS ECR?\"]', 'cc50497d6aee99da4ff4262bc737fc05': '[\\n    \"What is the solution to the error encountered when downloading tensorflow/serving:2.7.0 on an Apple M1 Mac?\",\\n    \"How can I fix the uncaught target signal 6 error when trying to run tensorflow/serving:2.7.0 on an Apple M1 Mac?\",\\n    \"Where can I find more information about running TensorFlow Serving on Apple M1?\",\\n    \"Who provided the solution for downloading tensorflow/serving:2.7.0 on Apple M1 Mac?\",\\n    \"What alternative docker image can be used on Apple M1 for running TensorFlow Serving?\"\\n]', '2ce105dde7ed320837722951c2211f00': '[\"What causes the illegal instruction error when running tensorflow/serving image on Mac M2 or M1?\", \"Why is the emacski repository not suitable for tensorflow/serving on Mac M2 or M1?\", \"How should I configure Docker to run TensorFlow Serving on Mac M2?\", \"What are the alternative solutions for running tensorflow/serving on Mac Apple Silicon?\", \"How can I use docker-compose to run TensorFlow Serving on Mac Apple Silicon?\"]', 'b06639a62405b002972be022c08411a3': '[\"What might cause CPU metrics to show as unknown for HPA?\", \"How can we resolve the issue of unknown CPU metrics in HPA?\", \"What are the possible failure messages when HPA CPU metrics do not show?\", \"What is a probable step after deleting a problematic HPA?\", \"Who added the solution for fixing HPA CPU metrics issues?\"]', 'e61c6e5eca9d79791178b763984ad543': '[\"What is causing errors with Istio when trying to install KServe using the quick_install.sh script?\", \"How can I verify the version of kubectl on my system?\", \"What specific changes need to be made to the quick_install.sh file to resolve the Istio installation errors?\", \"Where can I find the version matrix for Istio and Knative required for editing the installation script?\", \"Who provided the solution for the Istio and KServe installation error in the FAQ record?\"]', '1467e5ede07206cf5952dd4e3f97719e': '[\\n  \"What is the nature of the problem described in the Projects section?\",\\n  \"Can you outline the problem for our midterm project?\",\\n  \"Could you provide the solution overview mentioned in the FAQ?\",\\n  \"Who is credited with adding the optional details in the FAQ section?\",\\n  \"What is the problem title according to the project description?\"\\n]', 'ace5e52c7be8c00aa5e25dbd88c14861': '[\\n    \"Where can I find the deadlines for the projects?\",\\n    \"How can I check the specific project phases due dates?\",\\n    \"Where should we look for the project completion dates?\",\\n    \"How do I access deadlines related to my cohort?\",\\n    \"Where is the precise timing for midterm and capstone submissions located?\"\\n]', '2353e349b07f608580ce6edc9f8c0080': '[\\n  \"Are the midterm projects designed to be done individually or with others?\",\\n  \"Do the capstone assignments require collaboration or are they solo efforts?\",\\n  \"Are students expected to complete their midterms alone or in a group setting?\",\\n  \"Should the capstone be accomplished by a single person or a team?\",\\n  \"Is group work allowed for the midterm projects?\"\\n]', '5f3ab56d8fa9eb9235c76866f0e44203': '[\"What are the modules and topics a midterm project should cover?\", \"Can I include topics not covered in class in my capstone project?\", \"Where can I find previous cohorts\\' office hours recordings?\", \"When was the ML Zoomcamp first launched?\", \"What resources are available for further discussions on projects?\"]', '6871008d8570757c7fd465d42e0594e3': '[\\n    \"What links should I refer to for the midterm project sample and deliverables?\",\\n    \"Where can I submit my midterm project once it\\'s completed?\",\\n    \"What are the suggested resources to find datasets for my project?\",\\n    \"What steps should I follow to prepare and describe my problem and dataset for the project?\",\\n    \"How can I deploy my model as a web service and gain bonus points?\"\\n]', 'fc8bcc0c2c53cd932edab7239d3103a4': '[\\n  \"What resource provides instructions for conducting peer reviews for projects?\",\\n  \"Where can I find the page containing previous cohorts\\' project instructions?\",\\n  \"Who will compile the list of submitted projects with hashed emails for peer review?\",\\n  \"How can I check which projects are assigned to me for peer review?\",\\n  \"What is the process to review projects within the evaluation deadline?\"\\n]', 'ab88940ae0bc1f047182302d0a3047a2': '[\\n  \"How can I find the solution for computing the hash for project reviews?\",\\n  \"Where should I look to compute the hash for project review?\",\\n  \"What should I refer to for computing the hash for a project review?\",\\n  \"Where can I see the solution to compute the hash for a project review?\",\\n  \"How do I verify the method for computing the hash in project reviews?\"\\n]', 'c189205da73c79259cf1099a8c7ff7b2': '[\\n    \"How many posts are required for the midterm project\\'s learning in public component?\",\\n    \"Is the total value of the learning in public requirement 14 for the midterm project?\",\\n    \"Do we need to create one post with a total value of 14 for the project?\",\\n    \"Are we expected to make 14 separate posts for each day of the project?\",\\n    \"Does the learning in public requirement involve creating seven posts with a value of 2 each for the midterm project?\"\\n]', '94c75e7781e43c7c9d2b049669f38bd1': '[\\n    \"How can I manage a large dataset that won\\'t load into my GitHub repository?\",\\n    \"What tool can I use to handle large files on GitHub?\",\\n    \"Is there a recommended solution for uploading oversized files to GitHub?\",\\n    \"What is the suggested method for managing large datasets on GitHub?\",\\n    \"Which service can assist with large data uploads to GitHub?\"\\n]', 'ca63f072ddca8fb2ed87bf2fca34f69e': '[\\n    \"Will I receive a certificate if I complete only two projects and review three projects each?\",\\n    \"Is the course certificate awarded upon submitting two projects and completing peer reviews?\",\\n    \"Do I meet the requirements for a certificate if I miss submitting the third project?\",\\n    \"What is the policy on certificate eligibility with two project submissions?\",\\n    \"Does Alexey Grigorev confirm two project submissions as sufficient for certification?\"\\n]', '5056561ffc1494759b96754946154e7b': '[\\n  \"If I skip a project, do I still have to peer review others in the second capstone?\",\\n  \"Am I exempt from peer reviews during the second capstone by not submitting the last project?\",\\n  \"Does participation in peer reviews depend on submitting my own project for the capstone?\",\\n  \"Can I avoid reviewing peers by not doing the last project before the capstone?\",\\n  \"Is submission of a project required for participation in peer reviews during the second capstone?\"\\n]', '2d6c03861f5e54f688123ea688ae572b': '[\"How many models are required for the midterm project?\", \"What does \\'multiple models\\' mean in the midterm deliverables?\", \"Is there a minimum number of models I should train for the midterm?\", \"Can I train only one model for the midterm assessment?\", \"What is the guideline for the number of models to tune for the project?\"]', 'ac3315adba73af2c744392977b855d93': '[\\n  \"How do I find the projects I am supposed to review for the capstone project?\",\\n  \"What is the process to evaluate peer projects and where is it explained in detail?\",\\n  \"How can I calculate my email\\'s hash value for the peer review assignment?\",\\n  \"What tool or method should I use to find my assigned peer projects from the spreadsheet?\",\\n  \"How does the peer review link relate to the hash value of my email address?\"\\n]', '5fe708a5fba5b58eded57b557f10c188': '[\\n    \"How is the decision made to pass or fail a project?\",\\n    \"Are project passes determined by collective score averages or individual performance?\",\\n    \"What criteria are used to decide passing a project?\",\\n    \"Does a project pass depend on overall or specific scores?\",\\n    \"Is it necessary to achieve a total score to ensure project approval?\"\\n]', 'cbe3eab9746a2abed3f8bcc80368a41d': '[\\n    \"What is the purpose of providing a train.py file in addition to notebook.ipynb?\",\\n    \"How does train.py help with peer review during the midterm project?\",\\n    \"Why must train.py be included in the environment setup with conda or pipenv?\",\\n    \"Do peers need train.py to verify the training process on different systems?\",\\n    \"Is the train.py file necessary for replicating the project\\'s functionality?\"\\n]', '151ad0962b2cb9bb352255df00611290': '[\\n    \"How do I install the pillow library necessary for image processing?\",\\n    \"What is the initial step to load an image using the PILLOW library?\",\\n    \"Which function opens an image file with pillow in Python?\",\\n    \"How do I convert an image to a numpy array using numpy?\",\\n    \"What import statement is needed to use the asarray function from numpy?\"\\n]', '6d53c71ee5b7ddb2286f5cc5963cfd19': '[\\n    \"Why is a train.py required if a train.ipynb already exists in our project?\",\\n    \"Can you explain the importance of having a train.py for model training?\",\\n    \"How is using a train.py different from using a train.ipynb in practical applications?\",\\n    \"Is executing a train.py file simpler than a train.ipynb for training jobs?\",\\n    \"What are the reasons for preferring train.py over train.ipynb in real-world scenarios?\"\\n]', '4df9c654ac3b0b58d392e91684a1051e': '[\\n    \"How can users enter data for the model to process?\", \\n    \"Is backend validation necessary when serving forms to users?\", \\n    \"What is a good tool to create a form interface for model data input?\", \\n    \"Are there recommendations for creating user-facing data entry apps?\", \\n    \"Where can I find information on using Streamlit for form creation?\"\\n]', '4f66960804eb377e09f8731539827458': '[\\n  \"How can I retrieve feature importance if I get an AttributeError using model.feature_importances_?\",\\n  \"What should I do when model.feature_importances_ results in an AttributeError?\",\\n  \"When using xgb.train, which method should be used to obtain feature importance?\",\\n  \"How is feature importance obtained for an XGboost model trained with xgb.train?\",\\n  \"What method is recommended for getting feature scores when model.feature_importances_ is unavailable?\"\\n]', 'c20b18871c86fca4d29b26ea37369f66': '[\\n    \"What should I do if I receive an \\'[Errno 12] Cannot allocate memory\\' error in AWS Elastic Container Service?\",\\n    \"How can I resolve memory allocation issues in my Elastic Container Service task?\",\\n    \"What is the solution for encountering \\'[Errno 12] Cannot allocate memory\\' in ECS logs?\",\\n    \"How do I fix the failure to allocate memory error in AWS Elastic Container Service?\",\\n    \"What steps are recommended when AWS ECS tasks cannot allocate memory as indicated by error \\'[Errno 12]\\'?\"\\n]', 'abc1a129f3f84f8670ca97b7c8226f8a': '[\\n  \"What is a common cause of the pickle error mentioning can\\'t get attribute on module __main__ when using docker with waitress?\",\\n  \"Why does the pickle error not occur when using Flask directly instead of through waitress?\",\\n  \"What is the key issue causing pickle to throw an error related to a custom column transformer class?\",\\n  \"What is the recommended solution to avoid the pickle error when the model uses a custom class for column transformation?\",\\n  \"Why does running the prediction script with Flask work even if the class definition is in the same namespace as when the model was saved?\"\\n]', '85dd4a4e49b90feb5320b368eae7b3c0': '[\"What are some common techniques to handle outliers in a dataset?\", \"Can log transformation be used to manage outliers in data?\", \"Is clipping high values a method to deal with outliers?\", \"Should dropping observations be considered for handling outliers?\", \"What dataset transformation methods help with outliers?\"]', '3b652cf0bc53a2cd353195841f32f8cf': '[\\n  \"What error might occur if an incorrect module name is specified when creating a Docker image using Bentoml?\",\\n  \"How can I resolve a \\'No module named sklearn\\' error in Bentoml?\",\\n  \"What modification is needed in bentofile.yaml to fix a module import error related to \\'sklearn\\'?\",\\n  \"Which correct module name should be used in bentofile.yaml instead of \\'sklearn\\'?\",\\n  \"What additional pip packages are mentioned as required by the service in the solution description?\"\\n]', '28f6ff86960aed563603acd5424957c0': '[\\n  \"What error might occur with BentoML when using the production flag and encountering sparse matrices?\",\\n  \"Why might BentoML give a code 500 error with an empty string in swagger UI?\",\\n  \"How does setting DictVectorizer or OHE to sparse affect a BentoML pipeline during saving?\",\\n  \"What causes inputs to convert to different sized sparse matrices in a BentoML model?\",\\n  \"How should BentoML model signatures be configured for production to handle inconsistent input lengths?\"\\n]', 'd2b2794b865902bbc7a43170740bad8f': '[\"Do we need to execute all the provided files for the course?\", \"If running all files isn\\'t possible, what should we focus on?\", \"Is it necessary to execute neural networks provided in the course materials?\", \"What should we verify is available for successful reproduction if not running the files?\", \"Where can I find a related discussion or help on this topic?\"]', 'd9b44fa2c323742f5aa77a872f561df1': '[\\n  \"What can be done if a model is too large for GitHub?\",\\n  \"How can I compress a large machine learning model?\",\\n  \"Which library is recommended for compressing models?\",\\n  \"What compression method is suggested for model size reduction?\",\\n  \"What should I keep in mind when compressing a model using joblib?\"\\n]', 'e19b6fad0d25ecd80ba0f19a06c75d1f': '[\\n    \"What should I do if I get an unauthorized error when pushing a docker image to Google Container Registry?\",\\n    \"How can I configure my console to push docker images to Google Container Registry?\",\\n    \"What command should I use if I encounter a permissions error with Google Container Registry?\",\\n    \"Is there a specific tool I need to install to resolve Google Container Registry permission issues?\",\\n    \"How do I authenticate my docker console to push images to Google Container Registry?\"\\n]', '4cdb7e28200277e80e4fb376801dd66a': '[\\n    \"What should I do if I\\'m facing issues installing tflite_runtime in a pipenv environment?\",\\n    \"Why might tflite_runtime not install correctly on Python 3.10?\",\\n    \"Where can I check all available versions of tflite_runtime?\",\\n    \"If I cannot find a suitable version of tflite_runtime, what alternative options can I try?\",\\n    \"What should I use for local development if tflite_runtime installation fails?\"\\n]', 'f68ad91be6c09ac117e7f5f7ce2137c1': '[\\n  \"What should I do if I get an error about \\'scipy\\' not being defined when using ImageDataGenerator?\",\\n  \"How can I resolve an issue with ImageDataGenerator\\'s flow_from_dataframe method related to \\'scipy\\'?\",\\n  \"Why might I receive an error from ImageDataGenerator about \\'scipy\\' being undefined?\",\\n  \"What\\'s the solution when \\'scipy\\' is not recognized by ImageDataGenerator?\",\\n  \"What steps can be taken if ImageDataGenerator encounters a \\'scipy\\' error?\"\\n]', '4b0823f5c7fc68d962111c3cbe35b33b': '[\\n  \"Where can I find a tutorial on using BentoML with AWS Lambda?\",\\n  \"Who created content on deploying BentoML in a Docker container to Lambda?\",\\n  \"What is the video link for passing BentoML content to Amazon Lambda?\",\\n  \"How do I pass a Docker container to AWS Lambda using BentoML?\",\\n  \"Is there a specific source to learn about deploying BentoML to Amazon services?\"\\n]', '7231e71b11e0556966ad0963f15af2af': '[\\n    \"What is the solution if I encounter an UnidentifiedImageError when testing my model locally with an image from a URL?\",\\n    \"How can I fix the error caused by an image file not being identified when using a URL to test my model?\",\\n    \"If I see UnidentifiedImageError while testing my model with a GitHub image, what should I do?\",\\n    \"What steps can be taken to resolve an UnidentifiedImageError with a test image URL?\",\\n    \"How do I modify the GitHub URL to prevent an UnidentifiedImageError when using test images in my model?\"\\n]', 'e9a81ff9b453184cebfa12b4df363785': '[\"What should I do if my pipenv dependencies aren\\'t resolving?\", \"How can I resolve a sub-dependency mismatch in pipenv?\", \"What does the ResolutionFailure warning mean when using pipenv?\", \"How do I fix dependency resolution issues in pipenv?\", \"What steps can I take if I encounter a pipenv install error due to dependency issues?\"]', 'fcf2af4e2a6f37ad1cc51776a42e8c90': '[\"Why does the function get_feature_names() not work on my computer?\", \"How should I update the code for the get_feature_names() function in recent library versions?\", \"What might be causing issues with get_feature_names() in my current setup?\", \"Where can I find more information about changes to the get_feature_names() function?\", \"What is the recommended replacement for get_feature_names() according to the course?\"]', '3bf31b54530e664bf31290e56aff2e02': '[\\n    \"What could cause an error decoding JSON response with the message \\'Expecting value: line 1 column 1 (char 0)\\'?\",\\n    \"Why might a server have trouble processing JSON data during a prediction test?\",\\n    \"What should I do if my model input format is incorrect when contacting the server?\",\\n    \"How should data be formatted to prevent JSON decoding errors when sent to the server?\",\\n    \"What conversion is needed if the server does not accept data in JSON format for the model?\"\\n]', 'da4de71db0570bbbcc2c5bab9059e476': '[\\n  \"What are some free cloud platforms that provide microinstances?\",\\n  \"Which cloud service offers free GPU instances?\",\\n  \"Where can I find the promotional link for Saturn\\'s free service?\",\\n  \"What should I mention while signing up for Saturn to receive extra GPU hours?\",\\n  \"Who contributed the information about free cloud alternatives?\"\\n]', '19590d745940b3ac629c9d3a5fc0ffff': '[\\n    \"How can I convert a \\'day_of_the_month\\' integer column and a string \\'month_of_the_year\\' column into a single integer \\'day_of_the_year\\' column using pandas?\",\\n    \"What is the process to transform \\'day_of_the_month\\' from integers to strings in a pandas DataFrame?\",\\n    \"How can I map string month names like jan and feb to integers like 1 and 2 using pandas?\",\\n    \"What method should I use in pandas to combine separate day and month columns into a single datetime object?\",\\n    \"How do I extract the day of the year from a \\'date_formatted\\' pandas column after converting day and month to a datetime object?\"\\n]', 'f442f5f66810a0a9962be0a6ac4d87e8': '[\"How can I visualize predictions for each class after training a neural network?\", \"What is the method for displaying class predictions in a chart?\", \"Could you explain how to create a bar chart for neural net predictions?\", \"What steps are needed to show predictions per class after training?\", \"Can you describe the process to plot predictions for neural net classes?\"]', '38a09ec7bb690b7e82b2b986533662b2': '[\\n    \"How do I convert dictionary values into a DataFrame?\",\\n    \"What method allows conversion of prediction output to a DataFrame?\",\\n    \"Which function helps in creating a DataFrame from a dictionary?\",\\n    \"What are the steps to create a DataFrame from a dictionary\\'s index?\",\\n    \"How can I format dictionary prediction values into a DataFrame table?\"\\n]', 'e159bc922cbe7b2b8836ce4e5447bf14': '[\\n    \"What is the purpose of the Kitchenware Classification Competition Dataset Generator?\",\\n    \"How does the dataset layout for the Kitchenware Classification Competition differ from the dino vs dragon lesson?\",\\n    \"Where can I find the script for generating the Kitchenware dataset?\",\\n    \"Who wrote the script for generating the Kitchenware dataset?\",\\n    \"Why might participants prefer the layout used in the dino vs dragon lesson?\"\\n]', '6df8f72f8265cd9b94b8515f5b054473': '[\"How do I install the NVIDIA drivers needed for Tensorflow?\", \"What are the two methods for installing Tensorflow on Windows?\", \"How should I proceed with CUDA installation on WSL/Linux?\", \"Is it necessary to post about my course progress on social media?\", \"Where can I find help for uploading a dataset to Kaggle?\"]', 'a3ec382a0376921c029a4718936b4c5a': '[\\n  \"Why is the order of multiplication important when multiplying matrices?\",\\n  \"What happens to the resulting matrix sizes if the order of multiplication is changed?\",\\n  \"Can changing the order of matrix multiplication impact the resulting values?\",\\n  \"How do the dimensions relate when multiplying matrix A (m x n) by matrix B (n x p)?\",\\n  \"What results when matrix B (n x p) is multiplied first in the order before matrix A (m x n)?\"\\n]', 'b54b0caeec9e361c6bc8dd1e5d0740b8': '[\\n  \"Where can I find instructions for installing the environment on a Mac with an M1 chip?\",\\n  \"Is there a specific resource for setting up the course environment on macOS?\",\\n  \"Does someone have a guide for environment installation on Macs with M1 processors?\",\\n  \"Are there any alternative instructions available for installing the environment on a Mac?\",\\n  \"Where should I look for Mac-specific setup instructions for the course?\"\\n]', '3c6fa58fac642deaf805cbf7360a47c2': '[\"Can I submit my assignment after the deadline?\", \"What happens if the submission form is closed when I try to submit?\", \"Is it possible for my late homework to be assessed?\", \"What determines if my late submission is accepted?\", \"Will my assignment be evaluated if the submission period ended?\"]', '85615f5486c458fa1fd40b5e6b3ca204': '[\\n    \"Is a public GitHub repository required for homework submission?\",\\n    \"What is the procedure to set up the Conda environment locally?\",\\n    \"Can you recommend an IDE suitable for machine learning?\",\\n    \"Why must the repository be public for homework evaluation?\",\\n    \"Where can I find the process to install Conda on my computer?\"\\n]', 'df45abafe1046415f746199c0ec2adc3': '[\"How can I install wget in Google Colab?\", \"What command should I use to download a file with wget in Google Colab?\", \"Where will files be downloaded when using wget in Google Colab?\", \"Who contributed the information about using wget in Google Colab?\", \"Is there a specific path to download data with wget in Google Colab?\"]', '16f99fe1e7e90ef8756210ba20130a94': '[\\n    \"What is the required format for features to be used in scikit-learn?\",\\n    \"How can I convert a 1-dimensional array into a 2-dimensional array for scikit-learn?\",\\n    \"When preparing data for scikit-learn, how should I format my features, or X?\",\\n    \"If I have a 1D array, what should I do to make it compatible with scikit-learn?\",\\n    \"Why do features in scikit-learn need to be in a 2-D array format?\"\\n]', '03001f0b00ed4abe5cc1d53a7d5bf5a2': '[\\n    \"What to do if I encounter a FutureWarning with is_categorical_dtype when plotting in Matplotlib?\",\\n    \"How to adjust my code to handle the deprecated is_categorical_dtype warning?\",\\n    \"What error suggests I use isinstance with CategoricalDtype in Matplotlib?\",\\n    \"How can I resolve the deprecated is_categorical_dtype warning message?\",\\n    \"What code modification is suggested for the FutureWarning in Matplotlib plots?\"\\n]', 'b5e135ae63fe68074963969844d36afa': '[\\n  \"What should I do if the docker file rerun on Windows results in a \\'Python 3.11 not found\\' error?\",\\n  \"How can I resolve the issue of \\'pipenv\\' or \\'asdf\\' not found when running a docker file on Windows?\",\\n  \"What is a potential fix for Python-related installation issues when using docker on Windows compared to Linux?\",\\n  \"Who provided the solution for addressing the Python installation error in the FAQ?\",\\n  \"Is there a way to specify a Python version for pipenv if it\\'s not installed automatically?\"\\n]', '926b920089f94c8f3bb09217cc2b70e5': '[\\n    \"How can I deploy my project to DigitalOcean App Cloud?\",\\n    \"What is the monthly cost for deploying to DigitalOcean?\",\\n    \"What should I do if my project is not in the root directory of my repo when deploying?\",\\n    \"Why is it important to edit the Dockerfile path when deploying to DigitalOcean?\",\\n    \"Who provided the deployment steps for DigitalOcean?\"\\n]', '8691f27730fb77d7f618b1ea6b907b88': '[\\n    \"Is it advisable to exclusively use the most important features for model training?\",\\n    \"Why is it beneficial to evaluate predictive value when selecting features?\",\\n    \"How can we decide whether to include or exclude a feature after conducting feature importance analysis?\",\\n    \"What role does correlation play in deciding which features to drop during model training?\",\\n    \"Which feature selection techniques should I consider reading about?\"\\n]', '6d3f457298134de9c80764a2ed50458d': '[\\n  \"What are some strategies to handle large datasets like the New York Yellow Taxi dataset with over a million rows?\",\\n  \"Can you explain the chunking method for processing large datasets that don’t fit in memory?\",\\n  \"How can I optimize data types in Pandas to reduce memory usage when working with big datasets?\",\\n  \"What is Dask and how can it help in working with large datasets in Python?\",\\n  \"Why is sampling useful during the exploratory phase of working with large datasets?\"\\n]', '09b79fee79cb13c29b69bb607492fac0': '[\\n  \"Is it advisable to take the course using R or Scala instead of Python?\",\\n  \"Why might using a language other than Python 3.10 be problematic for homeworks?\",\\n  \"How could using a language like R or Scala affect my multiple-choice question answers?\",\\n  \"What issues could arise during peer review if I use a language other than Python?\",\\n  \"Is it possible to write the course lessons in other languages for personal learning purposes?\"\\n]', '260001ea4e3531efaad000d9a02a3aae': '[\\n  \"Are resources like fast.ai or huggingface permissible during the competition phase?\",\\n  \"Can participants use fast.ai libraries during their capstone projects?\",\\n  \"Is the usage of huggingface tools considered acceptable in the capstone?\",\\n  \"Does the course permit the integration of fast.ai in assignments?\",\\n  \"Is employing libraries such as huggingface seen as getting excessive assistance?\"\\n]', 'c4c8b27c0fe69d03955c85cbe6f01a90': '[\\n  \"Is the version compatibility between TensorFlow and TensorFlow Serving affecting my image testing?\",\\n  \"Why am I having difficulties testing my TensorFlow Serving image despite a successful build?\",\\n  \"What could cause a discrepancy in testing the TensorFlow Serving image after building it?\",\\n  \"Could a mismatch in TensorFlow and TensorFlow Serving versions be the problem with my testing?\",\\n  \"What issue might arise from mismatched versions of TensorFlow and TensorFlow Serving when building images?\"\\n]', '488fac1e084f401cff20a96e222c668f': '[\\n  \"What should be the title for listing my participation in the Machine Learning Zoomcamp on LinkedIn?\",\\n  \"Can I list the Machine Learning Zoomcamp as a job experience on LinkedIn?\",\\n  \"Which sections on LinkedIn can I use to include my experience from the Machine Learning Zoomcamp?\",\\n  \"Is it appropriate to describe the Machine Learning Zoomcamp experience as an internship?\",\\n  \"How can I showcase my project work from the Machine Learning Zoomcamp on my LinkedIn profile?\"\\n]', 'de5571f2b29aba5627b6ddd455741919': '[\\n  \"What is the intended purpose of this FAQ document?\",\\n  \"How should questions in the course be formatted?\",\\n  \"Can you suggest a resource for structuring questions effectively?\",\\n  \"Is there an example of a previous course that used a similar FAQ format?\",\\n  \"Who can contribute to adding solutions in the FAQ?\"\\n]', 'd020b236701aa37c5dc2d3d6f31bae7e': '[\\n    \"What is the full duration of both the course and each individual module?\",\\n    \"How long does each module take to complete, generally speaking?\",\\n    \"Can you clarify the time needed for the capstone project?\",\\n    \"What is the estimated time for peer reviews within the course structure?\",\\n    \"Are there extensions available for module deadlines, and what is the impact on the course timeline?\"\\n]', 'f37ba2636bea20f7a957a4587cfad0d9': '[\\n    \"What content is updated in the 2023 course compared to 2022?\",\\n    \"Which modules have been re-recorded for the 2023 version?\",\\n    \"Are the homework assignments different in the 2023 course?\",\\n    \"Is the updated course content primarily focused on Orchestration and Monitoring?\",\\n    \"Will any parts of the 2023 course remain the same as in 2022?\"\\n]', 'ac9e1fb42e479db5281670651ebc96c8': '{\\n  \"questions\": [\\n    \"Is there going to be a new cohort in 2024?\",\\n    \"When can I expect the 2024 cohort to begin?\",\\n    \"Will there be a cohort for the year 2024?\",\\n    \"What is the starting month for the 2024 cohort?\",\\n    \"Are you planning a cohort in 2024 and when will it start?\"\\n  ]\\n}', 'a5efe489cdfcbad1ec90da6b8637d6a9': '[\\n    \"What should I do if my response doesn\\'t match the given options exactly?\",\\n    \"Is it allowed to share my answer in the course slack channel?\",\\n    \"How do I select an option if my answer is slightly different from those provided?\",\\n    \"Should I post my alternative answer in the course slack channel if it differs from the choices?\",\\n    \"What\\'s the protocol for handling discrepancies between my answers and the options available?\"  \\n]', 'f83dfca33a0ff6cab84de25836b1b414': '[\\n  \"Can I select any problem for my final project?\",\\n  \"Where can I find potential datasets for my final project?\",\\n  \"Am I allowed to choose my own topic for the course\\'s final assignment?\",\\n  \"What are some sources for datasets for our project?\",\\n  \"Is it up to us to decide the topic for the final project?\"\\n]', 'ea4a7f64506743f953f09eefbf42278a': '[\\n  \"Is completing the capstone project required for certification?\",\\n  \"Are weekly homework tasks necessary for course completion?\",\\n  \"Will unfinished weekly assignments affect my graduation eligibility?\",\\n  \"Can I rank on the leaderboard without completing all assignments?\",\\n  \"Is the final project the only requirement for the certificate?\"\\n]', '5104db77514719ad3938b9c1013f213d': '[\\n  \"Is deploying the final project on the cloud mandatory?\",\\n  \"Can I earn cloud points by using kubernetes locally?\",\\n  \"Is it possible to use a local stack to simulate AWS?\",\\n  \"Who can earn additional points through cloud deployment in the final project?\",\\n  \"Who answered Ben Pacheco\\'s question about the final project deployment?\"\\n]', '16104e9253a9a6e6775c35b9eb9df0b7': '[\\n  \"How can I automate port-forwarding for Jupyter Notebook without using Visual Studio?\",\\n  \"What line of code should I add to my ~/.ssh/config file for port-forwarding?\",\\n  \"How do I launch Jupyter Notebook with a specific port and no browser?\",\\n  \"What additional steps are required to access Jupyter Notebook if I\\'m not using VSCode?\",\\n  \"Who contributed the information on port-forwarding in the FAQ?\"\\n]', 'e33dab806bfef18c4638888031782458': '[\\n    \"How can I open notebooks in VSCode?\",\\n    \"What extension do I need to open Jupyter in VSCode?\",\\n    \"Who added the information about opening Jupyter in VSCode?\",\\n    \"Where can I find instructions for using Jupyter notebooks in VSCode?\",\\n    \"What is needed to run Jupyter in a VSCode environment?\"\\n]', 'a2cfc7ff1df75af95e3a531f82fd7f32': '[\"How can I set up a GitHub repository to work from a remote virtual machine?\", \"What tutorials can help me set up GitHub on an AWS instance?\", \"How do I configure keys on an AWS instance for GitHub access?\", \"What should I do after setting up GitHub on a remote VM to push to my repository?\", \"Who can I contact for more information about configuring GitHub on a remote VM?\"]', '094e4e81d7f4f9c4160ffdadc19fd8c4': '[\"How can I access Jupyter Notebook on AWS from my desktop?\", \"What should I do if I\\'m unable to open Jupyter Notebook on AWS?\", \"Which configuration file needs to be edited to access Jupyter on AWS?\", \"What line should be added to the Jupyter configuration file?\", \"Who contributed the solution to the Jupyter Notebook setup issue on AWS?\"]', '4869e57c522676ac7043af1a05abc14e': '[\\n  \"How can I set up WSL on my Windows machine to use Jupyter and Anaconda?\",\\n  \"What is the command to install wget on WSL?\",\\n  \"How can I clone a GitHub repository using WSL?\",\\n  \"What command should I use in Powershell to install WSL on Windows?\",\\n  \"Is there a single platform where I can find integrated services like PyCharm and Jupyter?\"\\n]', '54a0bc5e96c8a8f0c4f6dcb22fe13342': '[\\n  \"How can I ensure that datasets or raw files in my local repository are not pushed to the remote repository?\",\\n  \"What steps can I follow to create a .gitignore file?\",\\n  \"What rule should I add to my .gitignore file to ensure parquet files are ignored?\",\\n  \"How should I name the file to ensure it functions as a .gitignore file?\",\\n  \"Where can I find more information about creating patterns for a .gitignore file?\"\\n]', '4a4574e5ef12fcfccfd7075a7c75a4eb': '[\\n    \"How can I ensure an EC2 instance is completely stopped?\",\\n    \"What do the different color indicators mean for EC2 instance status?\",\\n    \"Are there additional charges when an EC2 instance is stopped?\",\\n    \"What should I do if I\\'m unsure about the EC2 instance status page\\'s accuracy?\",\\n    \"Is there advice on setting up billing alerts for AWS charges?\"\\n]', '3a11ec901cb051b7faf0bd2f4eff8344': '[\\n  \"How can I get an invitation code for IBM Cloud?\",\\n  \"Is IBM Cloud an alternative to AWS in terms of characteristics?\",\\n  \"Where can I find a video related to IBM Cloud introduction?\",\\n  \"How do I verify my IBM Cloud account using an invitation code?\",\\n  \"Why do you love IBM Cloud?\"\\n]', '9b211b68d0c9a20e97d056c37533762a': '[\\n    \"How can I manage AWS costs effectively during the course?\",\\n    \"What is the monthly cost of running the specified AWS instance?\",\\n    \"What happens to the IP address of my AWS instance if I restart it?\",\\n    \"Is there a way to set up alerts if my AWS spending exceeds a certain budget?\",\\n    \"Where can I find a tool to estimate AWS service costs on my own?\"\\n]', '97f3149c35490eb1be805c4e6ce66e44': '{\\n    \"questions\": [\\n        \"Can I complete most of the course using only AWS free tier?\",\\n        \"Are there any AWS services used in the course that are not covered by the free tier?\",\\n        \"Is Kinesis available in the AWS free tier for this course?\",\\n        \"If a service like Kinesis is not in the free tier, how can I use it?\",\\n        \"Does localstack work with AWS services missing from the free tier?\"\\n    ]\\n}', 'a64dffd54ced277b03e5bceb88262bdc': '[\"When attempting to open an EC2 instance IP address in a browser, why might I receive an error stating \\'This site can’t be reached\\'?\", \"What is the purpose of the IP address for an AWS EC2 instance if it produces a \\'site can’t be reached\\' error in the browser?\", \"How can I correctly connect to an AWS EC2 instance if opening the IP address in a browser doesn\\'t work?\", \"What command should I use to connect to an EC2 instance from my local terminal rather than using a browser?\", \"Why is the downloaded key important when connecting to an EC2 instance, and where should it be stored?\"]', '0dcc74c08245ec55d3102debc32de14a': '[\\n  \"What is meant by an \\'unprotected private key file\\' and how can I resolve it?\",\\n  \"How can I fix the \\'unprotected private key file\\' error in SSH?\",\\n  \"What should I do if I encounter an \\'unprotected private key file\\' error after using an SSH command?\",\\n  \"How do I change file permissions to fix an \\'unprotected private key file\\' issue?\",\\n  \"Where can I find a guide to resolve the \\'unprotected private key file\\' error when connecting to an EC2 instance?\"\\n]', 'cb0315f608ed4c3c6539d08ac6cf7524': '[\\n  \"What should I do if my AWS EC2 SSH connection frequently drops and I need to reconnect?\",\\n  \"Why might my SSH connection to an AWS EC2 instance not last more than a few minutes?\",\\n  \"What could be causing disconnections when running code like \\'import mlflow\\' on AWS EC2?\",\\n  \"How can I troubleshoot and fix AWS EC2 SSH connection errors related to memory?\",\\n  \"What steps can be taken if I\\'m unable to maintain a stable SSH connection to my AWS EC2 due to RAM issues?\"\\n]', 'ef418500228056c9f1931686175edb85': '[\"How can I automate updating the IP address of my EC2 instance after restart?\", \"Is there a way to stop manually updating the config file each time my EC2 instance restarts?\", \"Where can I find a script to update my EC2 instance\\'s IP address automatically?\", \"What solution exists for addressing the changing IP issue when restarting an EC2 instance?\", \"Can I use a script to handle IP address changes for my EC2 instance?\"]', 'cb37aa1c58b620f36a4b682b1bf0b86b': '[\"What should I do if VS Code crashes while using Jupyter?\", \"How can I ensure smooth performance with VS Code and Jupyter?\", \"What instance type should I use if VS Code crashes?\", \"How do I monitor my instance power for VS Code and Jupyter?\", \"What dashboard can I use to check my instance\\'s performance?\"]', '0f608fdaff538d8adccaa417145713ed': '[\"What should I do if my Linear Regression model gives a ValueError about incorrect feature count?\", \"How can I resolve a feature mismatch error in Linear Regression using DictVectorizer?\", \"What is a solution if DictVectorizer causes a feature count error in validation data?\", \"Why does my Linear Regression model expect fewer features than provided, and how do I fix it?\", \"How should I use DictVectorizer with validation data to avoid feature count issues?\"]', 'fdbdaed1bb6cfa7ce6bed60ee183e9ee': '[\\n    \"What should I do if I encounter missing dependencies while working on the course materials?\",\\n    \"Which packages need to be installed if I\\'m missing dependencies?\",\\n    \"What should I install to resolve an error when using pandas.read_parquet()?\",\\n    \"How do I install packages if I\\'m using Conda instead of pip?\",\\n    \"Is there a difference between fastparquet and pyarrow in terms of functionality for this course?\"\\n]', '0e14f0199be6021f2f4d52314305f13a': '[\"What should I do if the RMSE value I obtain does not match any of the given options?\", \"How can I address the deprecation warning related to the distplot function in seaborn?\", \"What technique should be used to filter outliers when evaluating the model on February data?\", \"How can I handle null values to ensure the RMSE is close to the provided options?\", \"What is a recommended way to suppress deprecation warnings in my notebook?\"]', '8dcf139e3f9238821cdf6f4e412eb0de': '[\\n  \"What is the alternative to using sns.distplot for visualizing data distributions?\",\\n  \"How can I modify my code to replace sns.distplot with sns.histplot?\",\\n  \"What parameters should I use in sns.histplot to achieve similar results as sns.distplot?\",\\n  \"In replacing sns.distplot with sns.histplot, how do I ensure the density is considered?\",\\n  \"How do I adjust sns.histplot to include kernel density estimation?\"\\n]', '5cd639ad001fe27b79fd0cb3939f62f7': '[\\n  \"What should I do if I encounter a KeyError related to \\'PULocationID\\'?\",\\n  \"How can a KeyError involving \\'DOLocationID\\' be resolved?\",\\n  \"What is the fix for a KeyError with \\'PULocationID\\' in the code?\",\\n  \"How do I address the KeyError: \\'DOLocationID\\' issue?\",\\n  \"What correction is needed if the code throws a KeyError for \\'PULocationID\\'?\"\\n]', '206ddc41c00d74c24164480ca7ee7a10': '[\\n    \"What problem did you encounter when reading large parquet files in Jupyter?\",\\n    \"What was the error message received when trying to read the parquet file?\",\\n    \"How did you ultimately solve the issue with reading the large parquet file?\",\\n    \"Who suggested using the Pyspark library to solve the problem?\",\\n    \"Whose solution involved directly performing the homework as a python script?\"\\n]', '90b885d78f8c3c3e929594dc0d9ebe16': '[\\n  \"How can I reduce the plotting time for a distplot in Module 1?\",\\n  \"What should I do if my distplot is taking too long to display?\",\\n  \"Is there a way to speed up plotting when using distplot as taught in the introduction?\",\\n  \"Why is my distplot taking too long to render in the introduction module?\",\\n  \"What is the recommended step before plotting to improve speed within the introduction section?\"\\n]', '786af40ca7da8563e4fb939e029ffe95': '[\\n  \"Why is RMSE on the test set high when using hot encoding on the validation set?\",\\n  \"How do OneHotEncoder and DictVectorizer treat categorical features?\",\\n  \"What causes unknown categories to be misrepresented in hot encoding?\",\\n  \"How should features be encoded in binary columns for this week\\'s homework?\",\\n  \"Why does DictVectorizer yield the correct RMSE while OneHotEncoder does not?\"\\n]', 'a2f77d04d827c58779fa5f8ba276180e': '[\\n    \"What can cause train and validation sets to have different columns when using OneHotEncoder instead of DictVectorizer?\",\\n    \"How does DictVectorizer handle missing data differently from OneHotEncoder?\",\\n    \"Why might OneHotEncoder produce results in a different order compared to using DictVectorizer?\",\\n    \"Where can I find more information about choosing between OneHotEncoder, LabelEncoder, and DictVectorizer?\",\\n    \"What resources are suggested for learning more about smarter encoding methods in data science?\"\\n]', '02138bceab351188ea1d056b1ad99378': '[\\n    \"Why did we use DictVectorizer instead of OneHotEncoder for one-hot encoding?\",\\n    \"What are the benefits of using DictVectorizer for encoding over other methods?\",\\n    \"Why didn\\'t we choose to use get_dummies from the pandas library for one-hot encoding?\",\\n    \"What advantages does DictVectorizer provide when encoding data?\",\\n    \"Why might someone select DictVectorizer instead of using numpy.eye or numpy.identity for one-hot encoding?\"\\n]', 'bfe8a2fd46f6ffcfe4dc4700330a3ca1': '[\"How can I verify that outliers have been successfully removed from the dataset?\", \"What pandas function helps in checking data distribution after clipping outliers?\", \"Which function provides a distribution report to confirm outlier removal?\", \"How can statistics be used to ensure outliers were clipped?\", \"Which method is suggested for checking min and max values after outlier clipping?\"]', 'c26077350764aff451ace6f4f937a562': '[\\n  \"Why should I convert the numeric values in PUlocationID and DOlocationID to strings?\",\\n  \"What happens to NaN values when I convert PUlocationID and DOlocationID to strings?\",\\n  \"Is it necessary to replace NaN values with \\'-1\\' for string conversion in One-Hot Encoding?\",\\n  \"Does the choice of string representation for NaN values affect RMSE when using DictVectorizer?\",\\n  \"Can I use a string other than \\'-1\\' to represent NaN values in string conversion?\"\\n]', '7d35018068805f3b826caf902377acd7': '[\\n  \"What could cause my LinearRegression RSME to differ slightly from the expected answer?\",\\n  \"Why should LinearRegression produce the same results with the same inputs?\",\\n  \"How can I ensure I\\'ve correctly treated outliers in my datasets?\",\\n  \"What should the shape of my one hot encoded feature matrix be to confirm it\\'s done correctly?\",\\n  \"What might be wrong if my one hot encoded feature matrix has only two features?\"\\n]', 'eaafb995cb99444531a5ef33b71f780b': '[\\n  \"What should I do if I experience an unusually low RMSE score like 4.3451e-6?\",\\n  \"How can I ensure my model is not learning the target variable prematurely?\",\\n  \"What might cause my model to have an extremely low RMSE?\",\\n  \"Is there a potential issue if my model consistently generates a very low RMSE score?\",\\n  \"Why should I verify that X_train does not overlap with y_train?\"\\n]', 'e2730accf21f2599e48b43ec688d3d42': '[\\n  \"How do I enable auto-completion in Jupyter Notebook if Tab isn\\'t working?\",\\n  \"What is the command to fix auto-completion issues in Jupyter Notebook?\",\\n  \"Which version of jedi should be installed to enable Jupyter Notebook auto-completion?\",\\n  \"Who can I contact for issues with auto-completion in Jupyter Notebook?\",\\n  \"What package needs upgrading to resolve the Tab auto-completion problem in Jupyter Notebook?\"\\n]', '8f0f92a59fa0c721f284b19e68c27195': '[\\n    \"How can I fix a 403 Forbidden error when trying to download data from the NY Taxis datasets?\",\\n    \"What should I do if I encounter a forbidden access error while following the video instructions for downloading datasets?\",\\n    \"Where should I navigate to download the NY Taxis data without getting a forbidden access error?\",\\n    \"What alternative link can I use to successfully download NY Taxis data if the cloudfront.net link does not work?\",\\n    \"How should I modify the download URL to access the NY Taxis trip records if the original link fails?\"\\n]', 'ec75504b4eb9aa6fa03201a6a4b2c357': '[\\n  \"What should I do if PyCharm doesn’t recognize the conda execution path on a remote server?\",\\n  \"How can I activate a conda environment on a remote server to use it with PyCharm?\",\\n  \"What command helps to find the python execution path after activating a conda environment remotely?\",\\n  \"How do I add a new interpreter in PyCharm using a remote python path?\",\\n  \"Who can I contact for more help with remote development issues in this course?\"\\n]', 'a0fa3d1b33c448fef8124e3af7565e7e': '[\\n    \"What is a solution for dealing with memory issues when using DictVectorizer?\",\\n    \"Why might setting the \\'sparse\\' parameter in DictVectorizer to False cause problems?\",\\n    \"How can I prevent running out of memory when fitting a linear regression model on a 16 GB machine?\",\\n    \"What is the default setting for the \\'sparse\\' parameter in DictVectorizer according to the FAQ?\",\\n    \"Who can I contact for more details about memory issues in DictVectorizer?\"\\n]', '4bf86ad66627592964535a008f098fbf': '[\\n    \"How can I activate the Anaconda environment if installing it doesn\\'t modify the .bashrc profile?\",\\n    \"What should I do if my Anaconda environment is not activated after relaunching the Unix shell?\",\\n    \"How do I initiate conda for bash to modify the .bashrc file?\",\\n    \"What command should I use to reload my .bashrc after making changes?\",\\n    \"Who provided the solution for activating Anaconda environment in the .bashrc file?\"\\n]', '013b0f2cc2675be9065927ea161fb5d7': '[\\n    \"Why are the feature sizes different for the training set and the validation set in HW1?\",\\n    \"What might cause the feature size discrepancy between training and validation datasets?\",\\n    \"How should I use the dictionary vectorizer if I notice different feature sizes in the datasets?\",\\n    \"What common mistake could lead to different feature sizes in training and validation during HW1?\",\\n    \"Why should I avoid executing the fit pipeline on the model if I already have a dictionary vectorizer?\"\\n]', '0e0321eeab46d2da8f108961d1192211': '[\"How can I regain access to my AWS machine after removing my public key?\", \"Where can I find guidance on fixing the \\'Permission denied (publickey)\\' error?\", \"What command can I use to retrieve my old public key on AWS?\", \"What resource provides detailed steps on resolving EC2 Linux permission errors?\", \"Who can I contact for additional help regarding AWS public key issues?\"]', 'e17359f221b43593ad1470632dcca645': '[\\n    \"What should you do if you encounter an absurdly high RMSE on the validation dataset after processing it similarly to the training dataset?\",\\n    \"Why should the sparsematrix result from DictVectorizer not be turned into an ndarray when dealing with high RMSE issues?\",\\n    \"How should the February dataset be prepared when used as a validation or test dataset?\",\\n    \"What action resolved the issue of high RMSE associated with the validation dataset?\",\\n    \"Who provided the solution regarding the RMSE problem in the February dataset?\"\\n]', '5971bf10e1fbd24c9068eafc1e3ceb8e': '[\\n    \"What should I do if I encounter an import error with sklearn?\",\\n    \"How do I resolve an issue with the sklearn DictVectorizer?\",\\n    \"Who can I contact for help with a sklearn import problem?\",\\n    \"What is a solution for fixing sklearn import errors in my code?\",\\n    \"What command can resolve issues with importing scikit-learn?\"\\n]', '249a160aa0bcbcbd116fef138f714142': '[\\n  \"What should I do if I can\\'t access Localhost:5000 due to authorization issues?\",\\n  \"How can I solve access denied problems on Localhost:5000 using Chrome?\",\\n  \"What steps are needed if my access to Localhost:5000 is unavailable?\",\\n  \"How do I handle Localhost:5000 authorization problems in Chrome?\",\\n  \"Where in Chrome can I flush socket pools to fix Localhost:5000 access issues?\"\\n]', '307c3f7cfa8ac3b19acbd554afe88bb0': '{\\n    \"questions\": [\\n        \"How can I stop a process running on port 5000?\",\\n        \"What command do I use on a Mac terminal to identify the process ID for port 5000?\",\\n        \"How can I terminate all processes using port 5000?\",\\n        \"What is the alternative command to kill a running port?\",\\n        \"How do I change the mlflow UI to run on a different port?\"\\n    ]\\n}', '1e0239c8d7cba02d0ec32e4c413edfff': '[\\n  \"What error is encountered when running python register_model.py?\",\\n  \"What is the suggested solution for the ValueError related to string conversion in register_model.py?\",\\n  \"Where in the code should the \\'with\\' statement and \\'log_params\\' function be placed to avoid the parameter logging error?\",\\n  \"What potential risk is mentioned if parameters are logged in a group?\",\\n  \"Who added the solution to the error encountered in the python script?\"\\n]', 'ae11ab52124371633aea0b764dfcf1d3': '[\\n  \"How should I ensure my experiment is visible in the MLflow UI?\",\\n  \"What is the correct tracking URI if mlflow.db is in a subdirectory called database?\",\\n  \"What should be the tracking URI if the mlflow.db is one directory above the current one?\",\\n  \"Is it possible to use an absolute path to mlflow.db in the experiment tracking?\",\\n  \"How can I launch the MLflow UI directly from my notebook?\"\\n]', '1ad3eb7a8560790ddce7d2fb813b7ce6': '[\\n    \"What should I do if I encounter a hash mismatch error while installing MLFlow?\",\\n    \"How do I resolve a package hash mismatch issue during the MLFlow installation process?\",\\n    \"What is a possible solution if MLFlow installation results in a hash mismatch error for Numpy?\",\\n    \"What happens if I get a hash mismatch error message during the installation of Numpy via MLFlow?\",\\n    \"Is there a recommended approach if MLFlow installation does not match the expected package hashes?\"\\n]', 'ee548078f9a9a1a6b4b667d149a04ca4': '[\"What steps should I take to permanently remove an experiment in MLFlow?\", \"How can I remove an experiment from MLFlow’s database entirely?\", \"What is the process to delete an experiment permanently using a SQLite database?\", \"How do I ensure a deleted MLFlow experiment doesn\\'t persist in the database?\", \"What is needed to delete an experiment from the database using ipython-sql?\"]', '9f78c8a4e83c42ce56befe7315f756fb': '[\\n    \"What command should I use to update my Git repo with recent commits from the public repo without overwriting my own changes?\",\\n    \"What is the recommended method to update changes from a public repo when I\\'ve already pushed edits to my own repository?\",\\n    \"How do I integrate recent updates from the original Git repo into my forked repository without losing my own changes?\",\\n    \"What is the preferred approach for syncing my fork with updates from the upstream repository?\",\\n    \"How can I ensure my personal changes are preserved while incorporating new commits from the public Git repository?\"\\n]', '6daf29d95200a36b6f717ff94a6d1e80': '[\\n  \"What can be done if the image size of 460x93139 pixels is too large during experiment tracking?\",\\n  \"How can I resolve the large image size error caused by mlflow.xgboost.autolog in version 1.6.1 of xgboost?\",\\n  \"Which version of xgboost should I use to avoid the image size error when tracking experiments?\",\\n  \"What command should I run to install the correct xgboost version to fix the image size issue?\",\\n  \"Who provided the solution for the image size issue in experiment tracking?\"\\n]', '479916e2ff58426c63126cb993a3a1a0': '[\\n    \"What method should I use instead of list_experiments in MlflowClient after version 1.29?\",\\n    \"Why can’t I find the list_experiments method in the latest version of MlflowClient?\",\\n    \"Which method replaced list_experiments in MlflowClient after it was deprecated?\",\\n    \"Whose contribution noted the change from list_experiments to search_experiments in the FAQ?\",\\n    \"When was the method list_experiments removed from MlflowClient according to the FAQ?\"\\n]', '431ae75021ca64b81e1f710f8cfe29f0': '[\\n    \"What is the correct order to use mlflow.autolog() with mlflow.start_run()?\",\\n    \"What can I do if mlflow autologging is not working?\",\\n    \"What should I check if there\\'s a warning about uninstalled dependencies when using mlflow autolog?\",\\n    \"What must be ensured regarding dependencies for mlflow autologger?\",\\n    \"What additional installation might be necessary for mlflow autologger to function properly?\"\\n]', '283369fc1142b58f9fd844ae7281c54b': '[\\n  \"What should I do if the MLflow URL doesn\\'t open in my browser?\",\\n  \"How can I access MLflow running on a remote virtual machine?\",\\n  \"What steps should I follow if I\\'m experiencing issues accessing MLflow locally?\",\\n  \"What is the solution for opening MLflow on a remote server using VS Code?\",\\n  \"What alternative URL can I try if 127.0.0.1:5000 shows a blank page?\"\\n]', 'e6e47eb50689a92d5a1fd92fd3b5e697': '[\\n    \"What should I do if I encounter a warning message with mlflow.xgboost.autolog?\",\\n    \"How do I confirm that my model is tracked if I see a warning in MLflow?\",\\n    \"Whose experience is shared regarding the mlflow.xgboost.autolog warning message?\",\\n    \"Who provided answers to the question about the MLflow autolog warning?\",\\n    \"What was the solution to the model signature failure warning in MLflow?\"\\n]', '762755fb10edbe607e3291303186c4a3': '[\\n  \"What error occurs when trying to set a deleted experiment as active in MLflow?\",\\n  \"How can I address the issue of not being able to set a deleted experiment in MLflow?\",\\n  \"What does the MlflowException \\'Cannot set a deleted experiment\\' mean?\",\\n  \"Where can I find solutions for managing deleted experiments in MLflow?\",\\n  \"What should I do if MLflow prevents me from setting a deleted experiment as active?\"\\n]', '29c2f41d45ec35d6d52de9a1e1c55e9e': '[\"What should I do if I encounter an OSError[Errno 28] due to no space left on the device during installation of the requirements?\", \"How can I increase the base EBS volume to resolve disk space issues?\", \"What steps should I follow to add an external disk and configure installations on it to mitigate insufficient disk space?\", \"On GCP, how can I mount an additional disk to resolve space issues for my virtual machine?\", \"Is there an alternative to installing Anaconda to manage space more efficiently, and if so, how should I proceed?\"]', '9b2dfec8068d863f5d6d4842c19d3c75': '[\"Why did I receive an incorrect number of parameters in my homework for Question 3?\", \"What caused the parameters mismatch in my experiment for Module 2?\", \"How did the deprecated min_impurity_split affect my experiment tracking?\", \"What should I do to fix the parameter mismatch issue in Q3?\", \"Why do I need to upgrade sklearn for the correct number of params?\"]', '4e8a6d84f1c3d511e5c5b431811dbbe0': '[\\n  \"What should I do if I encounter a Protobuf error when trying to run MLflow from the terminal?\",\\n  \"How can I resolve an error related to the \\'protobuf\\' module when executing MLflow?\",\\n  \"What version of the \\'protobuf\\' module should I install to avoid errors with MLflow?\",\\n  \"Who provided the solution for the Protobuf error in the MLflow installation?\",\\n  \"How did Aashnna Soni fix the issue with MLflow and the \\'protobuf\\' module?\"\\n]', '8d5bfaabbfd4fc5dfb8da9a88f706cb3': '[\\n    \"What should I verify before executing the mlflow ui command?\",\\n    \"Where should I run the mlflow ui or mlflow server command?\",\\n    \"How can I ensure that the mlflow ui is set up correctly?\",\\n    \"What is essential to check when starting mlflow ui?\",\\n    \"In which directory must the mlflow server be initiated?\"\\n]', '6f698aec31e0538b6b36115de1adfc50': '[\\n  \"What resources can help with setting up MLflow experiment tracking on GCP?\",\\n  \"Where can I find guidance for MLflow configuration on GCP?\",\\n  \"Are there any tutorials available for installing MLflow on Google Cloud?\",\\n  \"How can I resolve issues encountered while setting up MLflow experiments on GCP?\",\\n  \"Which articles should I read to understand MLflow setup on Google Cloud?\"\\n]', '51c0dbdb9b731bdcd66a567141043e19': '[\\n  \"What should I do if I encounter a warning about setuptools replacing distutils with MLflow autolog?\",\\n  \"How can I resolve an MLflow autolog warning regarding setuptools?\",\\n  \"What is the recommended action for fixing warnings about distutils replacement in MLflow?\",\\n  \"If MLflow autolog raises a setuptools warning, what solution is suggested?\",\\n  \"How do I address warnings from MLflow about distutils being replaced by setuptools?\"\\n]', '7ff6720ecd6f27ad253c8aa800fcd1f9': '[\\n  \"How can I sort runs in the MLflow UI?\",\\n  \"Why can\\'t I sort my MLflow runs?\",\\n  \"What should I do if I am unable to sort runs in MLflow?\",\\n  \"Is there a specific view required to sort runs in MLflow?\",\\n  \"Who provided the solution for sorting runs in MLflow UI?\"\\n]', 'd08a40b84a7e04fbad3530edcb466391': '[\"What should I do if I encounter a TypeError related to \\'max_age\\' during MLflow UI launch?\", \"How can I resolve an exception with mlflow ui not loading in my browser?\", \"Why might I need to uninstall and reinstall Flask on a remote server for MLflow?\", \"What is causing the page with mlflow ui to not load when using a remote server?\", \"Who added the solution about fixing Flask version issues during MLflow UI launch?\"]', '3e36a02a6d4a26e8dd4dadb22a308e3d': '[\"What steps should I take if I encounter a FileNotFoundError when running mlflow ui on Windows?\", \"How can I resolve the system cannot find the file specified error for the mlflow ui command?\", \"What should I add to the PATH if mlflow ui gives me a FileNotFoundError on Windows?\", \"After installing mlflow on Windows, which directory needs to be added to the PATH to avoid errors?\", \"When running mlflow ui on Windows, how can I fix the FileNotFoundError WinError 2?\"]', '0eae1161956abb2f99e39188ada82bf9': '[\\n    \"What error occurs when running the script hpo.py with the given parameters?\",\\n    \"Where does the unsupported operand type error appear in the hpo.py execution?\",\\n    \"What modification is needed to fix the unsupported operand type issue in hpo.py?\",\\n    \"What causes the TypeError related to operand types in the hpo.py script?\",\\n    \"How can the datatype issue be resolved in the --max_evals argument of hpo.py?\"\\n]', '9789072446754f52e953c4674ce946ab': '[\\n  \"What warning might occur when using an unsupported sklearn version with mlflow.sklearn?\",\\n  \"Why do I see a warning about my sklearn version when using mlflow.sklearn?\",\\n  \"What versions of scikit-learn are recommended for use with mlflow.sklearn?\",\\n  \"Where can I find information about the sklearn versions compatible with mlflow.sklearn?\",\\n  \"What should I do if I encounter errors during autologging with sklearn in mlflow?\"\\n]', 'ce2b5d584316c9da8198195a6981f90f': '[\\n    \"How can I resolve the issue of CLI commands not returning experiments?\",\\n    \"What should I set to fix Mlflow CLI not displaying experiments?\",\\n    \"What is the solution for Mlflow experiments not appearing in CLI?\",\\n    \"Which environment variable is required for the Tracking URI in Mlflow?\",\\n    \"What might be causing the Mlflow experiments list command to return no results?\"\\n]', 'd0b457a1d02e8424132e6df3943f2174': '[\\n  \"What environment variable needs to be set to view MLflow experiments with the CLI?\",\\n  \"How can MLflow CLI commands access the experiments run with the tracking server?\",\\n  \"What issue arises when using mlflow cli commands after starting the tracking server?\",\\n  \"How should we handle the issue of mlflow gc not recognizing the tracking URI?\",\\n  \"Who can be contacted for further assistance regarding MLflow CLI issues?\"\\n]', '9afff3ca35f1c7fe02d3a9143139de0f': '[\"How can I view the data stored in the SQLite database used by mlflow?\", \"What tool can I use to inspect the tables in the mlflow\\'s SQLite database?\", \"Is it possible to manually query the mlflow database using SQL?\", \"Can I connect to the SQLite database for mlflow using Pycharm?\", \"Why is it useful to inspect the structure of the data in mlflow\\'s database?\"]', 'f91d6e75bd0462c5fd44009500c2c9be': '[\\n  \"What is the purpose of running the tracking server?\",\\n  \"Why would you host mlflow remotely?\",\\n  \"In what scenario would colleagues use a single mlflow server?\",\\n  \"Why not run mlflow on an individual laptop?\",\\n  \"What does remote hosting of an mlflow server facilitate?\"\\n]', 'd87db980a9bc4ec3a18498ed37de2edc': '[\\n    \"How can we address the issue if a parameter, such as max_depth, is not recognized during model registry?\",\\n    \"What is the solution for adding a parameter before it is recognized during the model registry?\",\\n    \"What steps should be taken if parameters need to be added before model registration?\",\\n    \"How should the parameters be added to ensure they are recognized in the data.run.params?\",\\n    \"Who provided the solution for adding parameters before the model registry issue?\"\\n]', '766cebf96cab78fe44e961424b42efcc': '[\"Why is max_depth not recognized even after using mlflow.log_params?\", \"How can I ensure parameters are logged correctly in mlflow?\", \"What should I do if my model parameters are not being tracked?\", \"Is there a way to fix parameter logging without deleting previous runs?\", \"Where should I add the mlflow.log_params function in my script?\"]', 'b9566b5aa78114e289749aa11b237a9d': '[\\n    \"What should I do if I encounter an AttributeError regarding a \\'tuple\\' lacking \\'tb_frame\\' in week_2 homework?\",\\n    \"How can I resolve an error in register_model.py caused by copying it into a Jupyter notebook?\",\\n    \"What is the solution to fixing the AttributeError related to \\'tb_frame\\' in Module 2 tasks?\",\\n    \"How do I address the issue with the \\'tuple\\' object error in the context of the click decorators?\",\\n    \"What changes are needed in the register_model.py script to prevent the AttributeError in Jupyter notebook?\"\\n]', '4f4269db5664281f83a850ff1680d3fb': '[\"What should I do if I encounter an error with the WandB API while running preprocess_data.py?\", \"How can I configure my WandB API key to resolve an error message?\", \"Where can I find my WandB API key in my profile settings?\", \"What steps do I need to take before running preprocess_data.py to fix the API error?\", \"Who provided the solution to the WandB API error issue?\"]', '896c9be2f0e7ce716731f07db7398a58': '[\\n  \"What should I do if I encounter the warning \\'Failed to infer model signature\\' when using MLflow with XGBoost?\",\\n  \"How can I ensure that autologging is properly enabled before constructing the dataset for XGBoost?\",\\n  \"What is the correct order of operations to follow when using MLflow autologging with XGBoost?\",\\n  \"What types of data formats are compatible with XGBoost for autologging in MLflow?\",\\n  \"Who should I contact if I need further clarification on enabling autologging for XGBoost in MLflow?\"\\n]', '4b6d5425deccab2c2c9cf1629045ed5a': '[\\n  \"How can I resolve wget command issues in a Windows environment?\",\\n  \"What should I do if pip command is not recognized despite using a virtual environment?\",\\n  \"Is there an alternative way to execute the wget command on Windows?\",\\n  \"How can I properly download scripts in Visual Studio when wget fails?\",\\n  \"What command structure should I use for wget and pip on Windows?\"\\n]', '505f06b8351840a50d99b07f4f10d299': '[\\n  \"How can I directly open a GitHub notebook in Google Colab?\",\\n  \"What should I change in the URL to open a GitHub notebook in Google Colab?\",\\n  \"Does the solution for opening a GitHub notebook in Google Colab work with private repositories?\",\\n  \"Where can I find a solution if navigating the Wandb UI becomes difficult?\",\\n  \"Who added the solution for difficulties in the Wandb UI?\"\\n]', 'f97ea3ccf8ec39f702b9948a7a70bf00': '[\\n    \"Why is it important to use time-based splits like Jan/Feb/March instead of a random split for train, test, and validation datasets?\",\\n    \"What are some potential issues of using a random sample for training and testing predictive models on future data?\",\\n    \"How can we detect seasonality in our data using the January, February, and March split approach?\",\\n    \"What problem might occur when trying to run the MLflow server with an incompatible urllib3 version, and how can it be resolved?\",\\n    \"Why is the validation phase in a model important for leadership and regulatory reporting?\"\\n]', 'a5e6f1bae9c35eb1985b52fd2b089014': '[\\n  \"What is discussed in Module 3: Orchestration?\",\\n  \"Could you provide a description of the problem in Module 3?\",\\n  \"What solution is given in Module 3: Orchestration record?\",\\n  \"Who added additional information in the record for Module 3?\",\\n  \"What is the title of the problem in Module 3: Orchestration?\"\\n]', '51bb6b2926d6e5350c2b70d15d9e7c2c': '{\\n  \"questions\": [\\n    \"How can I find the FAQ for Prefect questions in the course materials?\",\\n    \"Can you guide me to the location of the Prefect questions FAQ?\",\\n    \"Where should I look for the FAQ related to Prefect questions?\",\\n    \"Is there a specific section for Prefect questions in the FAQ?\",\\n    \"What is the right place to locate the Prefect questions FAQ?\"\\n  ]\\n}', '33eaf37bce297bd2feab85ed3931507b': '{\\n  \"questions\": [\\n    \"What is the solution to the \\'aws.exe: error: argument operation: Invalid choice\\' error when Docker cannot login to ECR?\",\\n    \"How can I fix the issue with AWS CLI where an error occurs while logging into ECR on Windows?\",\\n    \"Which command should be used instead of \\'aws ecr get-login --no-include-email\\' to avoid the Invalid choice error?\",\\n    \"Where can I find more information about the correct AWS CLI command to use for logging into ECR?\",\\n    \"Who added the solution to the AWS CLI error regarding the ECR login process?\"\\n  ]\\n}', '895c6f82871a13a8d45a48a139ec3a45': '[\\n    \"How do you continue a command onto a new line in Windows PowerShell?\",\\n    \"What is the escape character for quotation marks in PowerShell commands?\",\\n    \"How can you create temporary environment variables in Windows PowerShell?\",\\n    \"What is an example of using an environment variable within an AWS CLI command in Windows PowerShell?\",\\n    \"Who contributed the multiline command guidance in Module 4?\"\\n]', '42de97a12a5254118539615c24dcacbd': '[\"What should I do if I encounter an error saying \\'module collections has no attribute MutableMapping\\' when installing pipenv?\", \"How can I resolve pipenv failures when using the system Python version 3.10?\", \"What steps are involved in fixing the pipenv installation issue caused by using the system Python?\", \"Why is it recommended to install a non-system Python for resolving pipenv installation errors?\", \"What installation method should be used for pipenv if I previously installed it with apt-get?\"]', 'de18f769b10ca2825125b598041ead71': '[\\n    \"How do I resolve a module that is not connecting to an HTTPS URL?\",\\n    \"What should I verify if a module is unavailable due to SSL issues?\",\\n    \"What command checks the SSL module configuration for HTTPS connections?\",\\n    \"If the SSL configuration seems fine, what\\'s the next step in solving the HTTPS URL issue?\",\\n    \"Who contributed the solution for checking SSL configuration and upgrading pipenv for module availability?\"\\n]', '25e72e5dd439891eb2398bc1e4f4135f': '[\\n    \"What should be done when encountering the error \\'No module named pip._vendor.six\\' during scikit-learn installation?\",\\n    \"How can I fix the ModuleNotFoundError related to \\'pip._vendor.six\\'?\",\\n    \"What command should I run to resolve an installation error with scikit-learn involving \\'pip._vendor.six\\'?\",\\n    \"What error might be encountered during scikit-learn installation and how can it be resolved?\",\\n    \"If I see \\'No module named pip._vendor.six\\' when installing scikit-learn, what steps should I take?\"\\n]', 'c694fe6ca1a6d260738173c045765f7b': '[\\n  \"How do we use Jupyter notebooks within a Pipenv environment?\",\\n  \"What are the steps to install Jupyter and ipykernel using Pipenv?\",\\n  \"How can I register a kernel inside a Pipenv shell?\",\\n  \"What command is used to add a virtual environment to the list of kernels in VS Code?\",\\n  \"Where can I find more information on integrating Jupyter with Pipenv?\"\\n]', '2a9f0d9567341b678f1d5ec0f0abe3b2': '[\\n    \"What might be causing no output in Jupyter when using pipenv?\",\\n    \"Which version of scikit-learn and Python were used with pipenv?\",\\n    \"What version of Tornado was problematic when running Jupyter?\",\\n    \"How can the Tornado-related bug in Jupyter be resolved?\",\\n    \"Where can I find more information about the Jupyter no output issue?\"\\n]', '75a7db3c1f7f8d3ec7adde510142686d': '[\\n  \"What argument should I pass to resolve the \\'Invalid base64\\' error when using aws kinesis put-record?\",\\n  \"How can I fix the \\'Invalid base64\\' error that occurs with AWS CLI version 2 during kinesis operations?\",\\n  \"What causes the \\'Invalid base64\\' error when running aws kinesis put-record on my local machine?\",\\n  \"In Module 4, what error might we see at 57:42 in video 4.4 related to AWS CLI?\",\\n  \"What specific CLI version is mentioned in the FAQ as possibly causing the \\'Invalid base64\\' error?\"\\n]', 'aa9c7b9c891da9d8ed6974cb45a3fea2': '[\\n  \"What might cause the error \\'index 311297 is out of bounds for axis 0 with size 131483\\'?\",\\n  \"How can the error in starter.ipynb for homework’s Question 1 be resolved?\",\\n  \"What should be updated to fix the error encountered in Module 4\\'s deployment task?\",\\n  \"Who contributed the solution for the out-of-bounds error in the FAQ record?\",\\n  \"In which module does the error related to loading a parquet file occur?\"\\n]', '195a5f78be1035d1fded1bdcbebaf1d0': '[\\n  \"What command can I use if a Pipfile.lock was not generated?\",\\n  \"How can I create a Pipfile.lock if it\\'s missing?\",\\n  \"What should I do when Pipfile.lock is not created automatically?\",\\n  \"Is there a specific command to force the creation of Pipfile.lock?\",\\n  \"How do I generate a Pipfile.lock file?\"\\n]', 'a5091feacf9f542c208fe3545c323b59': '[\"What could cause a permission denied error when using Pipenv?\", \"How can I fix the permission denied issue related to Pipenv?\", \"What module is often responsible for permission errors in Pipenv?\", \"Where can I find the solution for fixing Pipenv\\'s permission error?\", \"Who provided the information about fixing the pythonfinder module in Pipenv?\"]', '71fd63e1f928ff37f5084016b79c9868': '[\\n    \"What is the reason for the ValueError when using f-string to format a string input as a number?\",\\n    \"How can I convert a command line input to an integer before using it in an f-string?\",\\n    \"How do I modify the code to use the click library for passing a year as an integer?\",\\n    \"What changes are necessary in my code to format a year input correctly from the command line?\",\\n    \"Who provided the solution to handle the parsing error with CLI arguments and f-string formatting?\"\\n]', '5fee2d116c3aa0be8478320edb0623ff': '[\"What should I ensure when choosing the image to derive from in Docker?\", \"How should data be copied from local to a Docker image?\", \"What command helps avoid issues when setting paths in a Docker image?\", \"What is the command to build a container in Docker?\", \"What does the name <mlops-learn> signify when executing a script in Docker run?\"]', '51bdd592d6db17f6c87ffcaf1b60291f': '[\\n  \"How can I run both a Flask gunicorn service and an MLFlow server in the same Docker container?\",\\n  \"What should I do if defining Flask gunicorn and MLFlow server in the Dockerfile CMD only runs MLFlow?\",\\n  \"How should I structure shell scripts to run both Flask gunicorn and MLFlow server from the same Docker container?\",\\n  \"What is the purpose of using a wrapper script when deploying multiple services in one Docker container?\",\\n  \"Why is it necessary to give executable permissions to all the scripts when running multiple services in Docker?\"\\n]', '9e22e0a08dd5b0de17533058b54fc33a': '[\"What should I do if I encounter an InstallationError related to pip9.exceptions during deployment?\", \"How can I resolve the \\'python setup.py egg_info\\' error in Module 4?\", \"What command can I run to upgrade pipenv and wheel when facing deployment issues?\", \"How do I force an upgrade of pip and pipenv to fix a pipfile.lock generation issue?\", \"What steps should I take when \\'Command \\\\\"python setup.py egg_info\\\\\" fails with error code 1 during deployment?\"]', '94171bc778d9dee91a4a35430d2748a4': '[\\n  \"How can we connect an S3 bucket to MLflow and what tools are essential for this process?\",\\n  \"What role do access keys play in connecting boto3 to AWS servers when using MLflow?\",\\n  \"Is it necessary to have access keys for an S3 bucket if it\\'s set to public access?\",\\n  \"Why might someone be denied access to an S3 bucket when connecting through boto3?\",\\n  \"Where can I find more information on managing AWS credentials with boto3?\"\\n]', '2d9fda811c15f543064ac26c70aee65d': '[\\n  \"Why does uploading to S3 in deployment fail due to an InvalidAccessKeyId error?\",\\n  \"What should I do if S3 upload fails despite working with AWS CLI and boto3 in Jupyter?\",\\n  \"How can I resolve an InvalidAccessKeyId error during S3 upload in my deployment process?\",\\n  \"What is the solution if my AWS Access Key Id is not recognized during S3 deployment upload?\",\\n  \"How can setting the AWS_PROFILE environment variable help with S3 upload errors?\"\\n]', 'ef9c425a124bc6a06b0dfcff86f17888': '[\\n  \"What is the problem with Dockerizing LightGBM in Module 4 of the Deployment section?\",\\n  \"What causes the \\'image not found\\' error when working with LightGBM in a Docker container?\",\\n  \"How can you fix the \\'lib_lightgbm.so image not found\\' error when Dockerizing LightGBM?\",\\n  \"Who provided the solution to the Dockerizing LightGBM issue in the Deployment module?\",\\n  \"What specific command should be added to the Dockerfile to resolve the LightGBM issue?\"\\n]', 'd037e0c1ae983488650946df9cdfdd4f': '[\"What should I do if I encounter an AttributeError related to \\'dataclasses\\' when using mlflow in a lambda function?\", \"How can I resolve an unexpected error raised by mlflow when processing a request in a lambda function?\", \"What warning message might mlflow display if there\\'s a model dependency mismatch error in a lambda function?\", \"How can I address a memory issue in a lambda function when deploying with mlflow?\", \"What changes should be made if mlflow raises an error during model loading in a lambda function?\"]', '8d88dd227ae11ab76c8eb89382e15e6b': '[\\n    \"Is the notebook different from the final state of the video?\",\\n    \"Do I need to use both the video and the notebook simultaneously?\",\\n    \"Will everything work if I just watch the video patiently?\",\\n    \"Does the notebook illustrate the use of mlflow pipelines as shown in the video?\",\\n    \"Who added the note about the notebook being the end state of the video?\"\\n]', '9787f610a2d74c3430ff0956a60ac67f': '[\\n    \"How can I pass environment variables to my Docker image for AWS credentials?\",\\n    \"What is the correct order for specifying environment variables and the image name in a Docker run command?\",\\n    \"If my AWS credentials are not found, what is an alternative to setting environment variables for a Docker container?\",\\n    \"How can I map my AWS configuration files into a Docker container?\",\\n    \"Which AWS configuration files should I map into my Docker container to provide credentials?\"\\n]', 'd4a7032feae1759992c78ffe90345e1a': '[\\n    \"How can I see the model located in the docker container within the app directory?\",\\n    \"What command should I use to build a Docker image using the svizor/zoomcamp-model:mlops-3.10.0-slim base image?\",\\n    \"How does the \\'docker run -it zoomcamp_test ls /app\\' command help in verifying the model in the container?\",\\n    \"Can you explain why a CMD is used in the Dockerfile instead of a RUN command in this context?\",\\n    \"What should I consider when modifying a Dockerfile and trying to run a new version of the container?\"\\n]', '26ddd8b9fd8752486cc1069cd834f359': '{\\n    \"questions\": [\\n        \"How do I fix image platform mismatch when deploying?\",\\n        \"What command should I use to build a Docker image for ARM64?\",\\n        \"How can I specify a platform when building a Docker image?\",\\n        \"Which platform tag resolves the deployment warning in Docker?\",\\n        \"What does the command look like to build on linux/arm64?\"\\n    ]\\n}', '44c7dff8226a72063959e1a424c8046b': '[\\n  \"How can I resolve the HTTP Error 403: Forbidden when using apply_model() in score.ipynb?\",\\n  \"What is the correct URL format to use in input_file for accessing trip data?\",\\n  \"Who should I contact if I encounter issues with score.ipynb deployment?\",\\n  \"Is there an alternative URL to the S3 bucket for retrieving trip data?\",\\n  \"What modification should be made to the input_file string to avoid a 403 error?\"\\n]', '97001e4f7fcdc2566fe08c637afcef3d': '[\\n  \"What command can resolve the ModuleNotFoundError related to \\'pipenv.patched.pip._vendor.urllib3.response\\'?\",\\n  \"How can I fix the error with site-packages concerning urllib3.connectionpool.py?\",\\n  \"Is there a specific command to force reinstall pipenv to resolve module issues?\",\\n  \"What should I do if updating pip doesn\\'t fix the urllib3 connection pool error?\",\\n  \"Which package might need an update to resolve pip and urllib3 module errors?\"\\n]', 'e98a42fa0d06c0599547359bc091f70c': '[\\n  \"What credentials should I use when asked for a username and password in Grafana after running docker-compose up?\",\\n  \"How do I access Grafana when prompted with a login window during Module 5?\",\\n  \"What are the default Grafana login details for accessing http://localhost:3000/?\",\\n  \"When presented with a login request in Grafana, what username and password is required?\",\\n  \"After executing docker-compose up, what login information should I use for Grafana?\"\\n]', '0b51fbbb7fcb14d12e9dfbcffa9544b2': '[\\n    \"What might cause an error when starting monitoring services in Linux using docker compose?\",\\n    \"Why does the command \\'docker compose up --build\\' fail in Linux for module 5.2?\",\\n    \"How can I resolve the issue of \\'unknown flag: --build\\' when starting services?\",\\n    \"What modification is needed in the command to start services correctly in Linux?\",\\n    \"Whose advice is included in the solution for starting services in Linux?\"\\n]', '7a57570937bcf253b7052ee11aab74ea': '[\\n  \"What should I do if I encounter a KeyError ‘content-length’ while running prepare.py?\",\\n  \"What is the solution suggested by Emeli Dral for the KeyError encountered in prepare.py?\",\\n  \"Why might the link used in prepare.py to download taxi data not work anymore?\",\\n  \"How can I modify the URL in prepare.py to fix the KeyError issue?\",\\n  \"What URL should be used in prepare.py to download taxi data successfully?\"\\n]', 'aa10a4a8148ac297c92d90b837d5a12b': '[\\n  \"What should I do if my evidently service exits with code 2 when running \\'docker-compose up --build\\'?\",\\n  \"How can I fix the \\'Max retries exceeded with url: /api\\' error in the real-time prediction service?\",\\n  \"What module might be missing if \\'app.py\\' in evidently service can\\'t import and causes an exit code 2?\",\\n  \"What are some solutions if my evidently service can\\'t import pyarrow and exits with code 2?\",\\n  \"Who contributed the solution for the evidently service issue related to pyarrow?\"\\n]', '5a7afd62f24c4ba7c75c01fa96023a90': '{\\n    \"questions\": [\\n        \"What should I do if I encounter a ValueError for an incorrect item in Module 5?\",\\n        \"How can I fix a metric error in the Monitoring module of the course?\",\\n        \"Why am I receiving a ValueError related to metrics in Module 5?\",\\n        \"What is the solution for the error about passing the wrong item to Report?\",\\n        \"How do I resolve an evidently error involving metrics in the course?\"\\n    ]\\n}', '28d09f34ee06b1570e75b8a6a5ac0909': '[\\n  \"What happens if I don\\'t include target=\\'duration_min\\' when using RegressionQualityMetric()?\",\\n  \"Is it necessary to add ‘duration_min’ in current_data to make RegressionQualityMetric() work?\",\\n  \"How do I avoid errors with RegressionQualityMetric() in my reports?\",\\n  \"What\\'s the importance of specifying target=\\'duration_min\\' for RegressionQualityMetric()?\",\\n  \"Can I use RegressionQualityMetric() without adding duration_min to my data?\"\\n]', '8e4dcb5aaee41544dd4fd1e11f368f66': '[\"What causes a ValueError due to an array with 0 sample(s) in LinearRegression?\", \"Why might the training dataset be empty when using generated data?\", \"How can the issue with the empty dataset be resolved in the monitoring module?\", \"What adjustment is needed in the datetime configuration to fix the sample array error?\", \"Who contributed the solution to the monitoring issue of empty dataset?\"]', 'ba95528064d06b6aac5d7c695d7cde9b': '[\\n  \"What is a common error encountered when adding a new metric?\",\\n  \"How can I resolve errors related to missing target or prediction columns?\",\\n  \"What should be reviewed before adding a new metric?\",\\n  \"Which metric evaluates correlations among features without needing parameters?\",\\n  \"Who provided the solution for adding a new metric in the FAQ?\"\\n]', 'bf7ce471bd833f01add470cc1acce4db': '[\\n  \"What should I do if I encounter an error when logging into Grafana with default credentials?\",\\n  \"How can I resolve the issue of standard login failure in Grafana?\",\\n  \"What steps should I take if the Grafana admin password doesn\\'t work?\",\\n  \"How can I fix an error encountered during Grafana login with \\'admin/admin\\'?\",\\n  \"What command fixes the Grafana login error caused by default credentials?\"\\n]', '1cf8a1bdccb9e16ce04d30a681a92f78': '[\\n  \"What should I do if my charts in Grafana are not updating?\",\\n  \"How often should I set the refresh interval in Grafana for updates?\",\\n  \"What timezone should I use for Grafana updates if the default one doesn\\'t work?\",\\n  \"Why might my Grafana charts stop receiving data while my metric script runs?\",\\n  \"What are the steps to resolve a Grafana update issue related to time zones?\"\\n]', 'a325661019183981ead4f7e7f9ab8500': '[\\n  \"Why might the Prefect server not run locally when starting with the command?\",\\n  \"What steps can be taken if the Prefect server stops immediately after starting?\",\\n  \"What alternative did the user find to run the script when the local server failed?\",\\n  \"How did the user address the issue with the Prefect server not running locally?\",\\n  \"Who reported the problem about the Prefect server not running locally?\"\\n]', '80c9291b69fc37d5f5213e222b2a25d2': '[\\n  \"What command can I use if I encounter a no disk space left error in Docker?\",\\n  \"How can I identify what\\'s occupying space in Docker before I prune?\",\\n  \"Who provided the solution for clearing disk space in Docker?\",\\n  \"Which Docker CLI command helps remove unused items like build cache and images?\",\\n  \"If I experience a no disk space left error during docker compose up, what steps should I take?\"\\n]', 'e1ba3b69ee91dbb28273f62ae9d2f1f0': '[\\n  \"What should I do if I encounter a listen error on :::8080 due to getaddrinfo failure with docker-compose?\",\\n  \"How can I fix the address family not supported issue when using docker-compose up?\",\\n  \"What is a specific solution for php_network_getaddresses error in a docker-compose setup?\",\\n  \"How do I modify the docker-compose yml to resolve php -S 0.0.0.0:8080 issues?\",\\n  \"What command should be added to the adminer block for docker-compose to avoid network errors?\"\\n]', 'ce26dbe4e119384a1451ab82a704d829': '[\"How can charts similar to Evidently be created in Grafana?\", \"What Grafana panels are needed to mimic Evidently charts?\", \"Is there a quick way to recreate Evidently dashboards in Grafana?\", \"How can Evidently visualizations be generated outside Grafana?\", \"What purpose does the specific Evidently plot with performance segments serve?\"]', '0ca8062600a96090813095df210757d1': '[\"What should I do if I encounter the error \\'Unable to locate credentials\\' after running localstack with kinesis?\", \"How can I resolve the \\'Unable to locate credentials\\' error from test_docker.py when using localstack with kinesis?\", \"What environment variables must be added in docker-compose.yaml to fix the credentials error when running localstack with kinesis?\", \"Is there an alternative way to configure AWS credentials to avoid the error with localstack and kinesis?\", \"What random values can I use for AWS credentials when setting up localstack without causing an error?\"]', '055fae9ff5aac649bf41ebbde8c3cb75': '[\\n  \"What might cause an error when creating a bucket with localstack using the boto3 client?\",\\n  \"How can I fix the IllegalLocationConstraintException error in localstack when calling the CreateBucket operation?\",\\n  \"What is the correct method to create a bucket to avoid location constraint errors in localstack?\",\\n  \"What changes need to be made to avoid getting the unspecified location constraint error when using s3_client.create_bucket?\",\\n  \"What does the error message \\'unspecified location constraint is incompatible\\' signify when creating a bucket?\"\\n]', 'b16440f055a72ba074270b6824df3b31': '[\\n    \"What steps must I take when I encounter the error <botocore.awsrequest.AWSRequest object at 0x7fbaf2666280> in an AWS CLI command?\",\\n    \"How can I resolve the error related to <botocore.awsrequest.AWSRequest object> when using AWS CLI?\",\\n    \"What solution does Giovanni Pecoraro suggest for fixing the AWS CLI <botocore.awsrequest.AWSRequest object> error?\",\\n    \"Is there a workaround involving AWS CLI environment variables for the error <botocore.awsrequest.AWSRequest object>?\",\\n    \"What should be set in the environment variables to address the <botocore.awsrequest.AWSRequest object> error?\"\\n]', 'ea817a38edd157119f4c0a43ae2da931': '[\\n  \"What should I do if pre-commit yields an \\'error at every commit\\'?\",\\n  \"How can I fix the error shown when \\'mapping values are not allowed\\' in pre-commit?\",\\n  \"What is the recommended indentation for repo statements in .pre-commit-config.yaml?\",\\n  \"What might cause pre-commit hooks not to run due to configuration issues?\",\\n  \"Who added the suggestion regarding pre-commit errors in our FAQ record?\"\\n]', 'bc51c77ec280383e48a0b6a1a0d8378a': '[\\n    \"How can I start fresh with pytest after finishing with a previous project?\",\\n    \"What should I do if I can\\'t reconfigure pytest completely?\",\\n    \"Is there any way to eliminate a previous pytest setup?\",\\n    \"How do I handle a persistent configuration issue with pytest?\",\\n    \"What steps are necessary to clear the pytest configuration in a folder?\"\\n]', '7a0b4e451d811e4a5bb6b91b52032399': '[\\n  \"How can I resolve the issue of empty Records when using Kinesis Get Records with LocalStack as described in video 6.3?\",\\n  \"What additional parameter should be added to the Kinesis Get Records call to avoid empty Records?\",\\n  \"Which command modification is needed in video 6.3 at minute 11:23 for Kinesis get records to work properly?\",\\n  \"What is the recommended solution for dealing with empty records in Kinesis as per Module 6?\",\\n  \"What flag is suggested to bypass the issue of empty records in Kinesis according to Module 6?\"\\n]', 'ac70ec8a940c84538b5785c939a006c3': '[\"How can I fix the utf-8 encoding error in PowerShell when committing with Git after creating a pre-commit yaml file?\", \"What is the solution to the utf-8 codec decode byte error in .pre-commit-config.yaml?\", \"What command should I use to set utf-8 encoding for the pre-commit yaml file in PowerShell?\", \"How should I create a pre-commit yaml file to avoid utf-8 encoding errors in Git?\", \"Who provided the solution to the utf-8 encoding error related to pre-commit in Git?\"]', 'bae24d3ecb9d08d8f0221637d2cba5ca': '[\\n    \"What should I do if I encounter an AttributeError related to \\'PythonInfo\\' when committing with pre-commit hook?\",\\n    \"How can I resolve the error \\'PythonInfo\\' object has no attribute \\'version_nodot\\' during a git commit?\",\\n    \"What is the solution for errors occurring when using pre-commit hooks and \\'PythonInfo\\' attributes?\",\\n    \"What steps are needed when the git commit shows \\'PythonInfo\\' version_nodot attribute error?\",\\n    \"How do I clear app-data of a virtualenv to solve pre-commit hook errors?\"\\n]', '26571e186fa55eb06f58b0b3656296cd': '[\\n    \"What is a common error when using custom packages with pytest in the source code?\",\\n    \"Why does running python test_model_service.py from the sources directory work but pytest does not?\",\\n    \"How can I fix the \\'No module named production\\' error when using pytest?\",\\n    \"What command should I use to run pytest with custom packages?\",\\n    \"Why doesn\\'t pytest add the path to sys.path when running it?\"\\n]', '256ce11fbb773de610aa6d94e2a4859a': '[\\n  \"What causes the error \\'No module named production\\' when using pytest with pre-commit hooks?\",\\n  \"What is the correct pre-commit hook configuration to prevent the \\'module not found\\' error in pytest?\",\\n  \"How should the run.sh script be set up to fix the \\'No module named production\\' error?\",\\n  \"What adjustments need to be made to the PYTHONPATH to resolve import issues in pytest?\",\\n  \"Who added the solution for the pytest pre-commit hook error?\"\\n]', 'a8ed7eaf24e97b506f589801fb0c95e9': '[\"What should I do if I encounter a permission error when executing a script in a CI action on GitHub?\", \"How can I resolve an error code 126 during GitHub actions?\", \"What is the solution if my GitHub Action script raises a \\'Permission denied\\' error?\", \"How do I add execution permission to a script for GitHub CI?\", \"Who provided the solution for fixing the permission error in Module 6?\"]', '0e4752977464f817fda294828b1ae9cf': '[\\n  \"What issue arises when a docker-compose file has many containers?\",\\n  \"How can I select only a specific group of containers for testing?\",\\n  \"What should be included in the service definition to manage profiles?\",\\n  \"Is there a command option to utilize a specific profile when starting a service?\",\\n  \"Who contributed the solution for managing multiple Docker containers with docker-compose profiles?\"\\n]', '2d79c5c6ffb2916afff8734235c3587a': '[\\n    \"What should I verify if integration tests with Kinesis are failing?\",\\n    \"How can I ensure the AWS regions are correctly matched for docker-compose?\",\\n    \"What issue might arise if AWS regions do not match in local config and docker-compose?\",\\n    \"Can you give an example of setting AWS region in configuration files?\",\\n    \"Who contributed the information about AWS region configuration for Module 6?\"\\n]', '1ee71c5700f4cbe479fa13272ccb838e': '[\\n    \"What is the solution if the pre-commit command fails with the isort repo?\",\\n    \"Which isort version should be used to fix the pre-commit command failure?\",\\n    \"Who added the solution for the isort pre-commit issue?\",\\n    \"What problem was occurring with the isort repo in pre-commit?\",\\n    \"In which module can I find the solution for the isort pre-commit issue?\"\\n]', '229c6b390dce6d14c9344e792ca1ec08': '[\\n  \"What steps are required to dismantle AWS infrastructure created using a GitHub CD-Deploy Action?\",\\n  \"Can you describe the process to demolish Terraform infrastructure from a local machine?\",\\n  \"What initial command should be executed to configure the backend for destruction?\",\\n  \"How do you specify variable files while destroying infrastructure?\",\\n  \"Who contributed the solution for destroying infrastructure in this module?\"\\n]'}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d265559b-de13-45a3-ba4d-b5f84a3c070a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bae7a31e6abaddb52b4061dcf238fc61\n",
      "3e5d4959603c68a1e154fa2a6bd9d1e8\n",
      "60a31bbef930b3d6b127405fcd0b618e\n",
      "386dcf67c83b203e5b424f2ba7489370\n",
      "6e3550ba00f652ce2fa74706751c4983\n",
      "f8323339d264dc9d40d9dad5a34c06b5\n",
      "d10eed624489c36f17500750ff21c868\n",
      "cb86516adcdcafa29f0758ae6ca28a0b\n",
      "fa5c1523945f27f6bb5d9a04f2146a7a\n",
      "a5737de4f33219a4fcfe02f2c746d5a3\n",
      "c3b5714cc4d5a6db4fd5912404f30c31\n",
      "c47f302e889f61f7fd3cee025ef939b6\n",
      "6a439fc15426567b38e84acc6142c461\n",
      "093b06705cc2cc1141f667e241e14a06\n",
      "1e1c6528694d3abbfc7488bb3abea5e2\n",
      "3a90237d0692b97f3f9b322a855cc4fd\n",
      "9a0107663b3bab95ee3787a55673cde6\n",
      "b666ae0b62d6a6dfc38396ba510cdc6b\n",
      "d418a703e60b8555c41dc80863f589dc\n",
      "2a0887600c276fd6f9d5a5ead914df0b\n",
      "821ffc7eef9760e690cf6f6f3d01f191\n",
      "c4138ffefdd3d00b85496250ace2c836\n",
      "13cc1cfada6c696dab0c83e781774b93\n",
      "3cc081b0deeb66dac1f6e6f80b859bc3\n",
      "5a76ede2a7f68e863f6da7276d0ce106\n",
      "905a9a5de263c002c6b8b311a20f4f03\n",
      "6cb80ff3a8a0c09a2273b073933b7311\n",
      "e5a35da13d8561565696b031a6a2f7be\n",
      "6777d05ae24b183acd0ce819e589662c\n",
      "cd540c158b2253368bde0deecc09cfea\n",
      "66ed5eeacae24a08854edfbdd2cef8c6\n",
      "43f190716d165eb1f69e48c7354d84d8\n",
      "d9faa54bdb2c801f4fc21fd5d8632468\n",
      "0d049ccb21a1875fe936664a7738f885\n",
      "70bab3635e1e14c864d64ff31a7f2086\n",
      "e38d8a50c8c70bc8ab52ee1454385d5f\n",
      "db45cb7fc7b4b9952fac580fe960b547\n",
      "be3ebb503529b4378a61baf869e1283c\n",
      "7d4799d5ce9bf3ba9fbc9fb9dfb8b6b0\n",
      "27967023a6e120e0295d6ccbeea93ab3\n",
      "a4a893511970b4f7c6042d6dd3245aab\n",
      "b94eb7c49602d342b87e6fa16e0933e6\n",
      "410e2883d6fde933ca239f0561ffcdf3\n",
      "4eacb446aca37f3c80e188822e941b64\n",
      "1c0bf6109b0a22dcc724356f80bdc64b\n",
      "986c62fecab40efdcdb546d45bc9eaaa\n",
      "45fafc4085fb6c0006de2cd36daa932a\n",
      "f8d07d24cff4ccd512186da29f1a0f29\n",
      "7e938b178c613e38d4ad0b60cbbd5ec5\n",
      "ee6ddd0a3149f6fb83a51da174ca36b0\n",
      "af897dc72fc98964eb311728956ac0cf\n",
      "da30e93aa55a403a1b97ea56540179a8\n",
      "0fe88ef23e6b302f64c0306a9a4db629\n",
      "2c141bf23500c4a0b3310bb6041cac2f\n",
      "b1da87c134123eec20fbdba58a0b0aaa\n",
      "30313b1b331a278d08a0a4a12497cdb5\n",
      "d529aa12653e124c6ca7aacf8ff02a3f\n",
      "5bcdc9b262f8cacfc146e5850803809b\n",
      "e91c73638688cb35359988653675589c\n",
      "46d432046a6c65e5a83f8e2c6d975e7c\n",
      "330244beecd57dd10ec034a87be3b486\n",
      "ef728ab7bd73e5885bb969c5ed448735\n",
      "01c76f428c5063a556ab487016fc8d92\n",
      "baf320ea02c708552e1587f4074271cb\n",
      "f8af77b84b0f9ede78ea7ae3f1d88e23\n",
      "09505439c01f8a62ddf2d893d6ffc433\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Invalid \\escape: line 3 column 54 (char 149)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc_id, json_questions \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(doc_id)\n\u001b[0;32m----> 5\u001b[0m     parsed_resulst[doc_id] \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_questions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Invalid \\escape: line 3 column 54 (char 149)"
     ]
    }
   ],
   "source": [
    "parsed_resulst = {}\n",
    "\n",
    "for doc_id, json_questions in results.items():\n",
    "    parsed_resulst[doc_id] = json.loads(json_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33ddf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "'[\\n  \"What alteration is necessary if Docker throws an invalid mode error related to the path?\",\\n  \"How can I resolve a Docker daemon error involving \\\\Program Files\\\\Git\\\\var\\\\lib\\\\postgresql\\\\data?\",\\n  \"What\\'s an appropriate mounting path to use instead of \\\\Program Files\\\\Git\\\\var\\\\lib\\\\postgresql\\\\data in Docker?\",\\n  \"Which mounting path adjustments can fix a Docker invalid mode error regarding PostgreSQL data?\",\\n  \"What changes should I make to the mounting path when encountering an invalid mode error with Docker?\"\\n]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8b2748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  \"What alteration is necessary if Docker throws an invalid mode error related to the path?\",\n",
      "  \"How can I resolve a Docker daemon error involving \\Program Files\\Git\\var\\lib\\postgresql\\data?\",\n",
      "  \"What's an appropriate mounting path to use instead of \\Program Files\\Git\\var\\lib\\postgresql\\data in Docker?\",\n",
      "  \"Which mounting path adjustments can fix a Docker invalid mode error regarding PostgreSQL data?\",\n",
      "  \"What changes should I make to the mounting path when encountering an invalid mode error with Docker?\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "json_questions = [\n",
    "    \"What alteration is necessary if Docker throws an invalid mode error related to the path?\",\n",
    "    r\"How can I resolve a Docker daemon error involving \\Program Files\\Git\\var\\lib\\postgresql\\data?\",\n",
    "    r\"What's an appropriate mounting path to use instead of \\Program Files\\Git\\var\\lib\\postgresql\\data in Docker?\",\n",
    "    \"Which mounting path adjustments can fix a Docker invalid mode error regarding PostgreSQL data?\",\n",
    "    \"What changes should I make to the mounting path when encountering an invalid mode error with Docker?\"\n",
    "]\n",
    "\n",
    "parsed_resulst = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6c928923-d3d8-4b8f-b093-887dffc8e6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_index = {d['id']: d for d in documents}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1ad018b8-33d7-4b80-85df-de3a115aa2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = []\n",
    "\n",
    "for doc_id, questions in parsed_resulst.items():\n",
    "    course = doc_index[doc_id]['course']\n",
    "    for q in questions:\n",
    "        final_results.append((q, course, doc_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f7e44c9c-c383-4b7a-8c7a-404d9c2ec8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c5adba2e-d628-47e5-a107-0ddad14fd667",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(final_results, columns=['question', 'course', 'document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9e4d5ba5-c59c-4e0f-9e79-8f4cd86bb5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('ground-truth-data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f88e2b0d-53ba-4766-9f5c-aa5d85eff47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question,course,document\n",
      "When does the course begin?,data-engineering-zoomcamp,c02e79ef\n",
      "How can I get the course schedule?,data-engineering-zoomcamp,c02e79ef\n",
      "What is the link for course registration?,data-engineering-zoomcamp,c02e79ef\n",
      "How can I receive course announcements?,data-engineering-zoomcamp,c02e79ef\n",
      "Where do I join the Slack channel?,data-engineering-zoomcamp,c02e79ef\n",
      "Where can I find the prerequisites for this course?,data-engineering-zoomcamp,1f6520ca\n",
      "How do I check the prerequisites for this course?,data-engineering-zoomcamp,1f6520ca\n",
      "Where are the course prerequisites listed?,data-engineering-zoomcamp,1f6520ca\n",
      "What are the requirements for joining this course?,data-engineering-zoomcamp,1f6520ca\n"
     ]
    }
   ],
   "source": [
    "!head ground-truth-data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46afefd7-2230-4a08-ae31-5600de189d6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
